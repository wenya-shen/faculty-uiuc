# Mehmet Ahsen
- Assistant Professor of Business Administration and Deloitte Scholar
### education
- Ph.D., Biomedical Engineering, University of Texas at Dallas at Dallas, 2015
- M.S., Electrical and Electronics Engineering, Bilkent University, 2011
- B.S., Electrical and Electronics Engineering, Middle East Technical University, 2009
- B.S., Mathematics, Middle East Technical University, 2009
### research interest (chatgpt says...)
- Machine Learning for Biomedical Decision Making 
- AI-Human Collaboration and Algorithmic Decision Systems
- Drug Response Modeling and Combination Therapies
### teaching
- **Business Analytics II (BADM 211)** Builds on the foundation from the Business Analytics I (BADM 210), synthesizes concepts through hands-on application and project-based learning. 
- **Proseminar in Informat Systems (BADM 591)**

```
title: Economics of AI and human task sharing for decision making in screening mammography
authors: Mehmet Eren Ahsen, Mehmet U. S. Ayvaci, Radha Mookerjee & Gustavo Stolovitzky
journal: Nature Communications
published: 2025
```

# Executive Summary
- This study investigates the economic feasibility of incorporating AI into mammography screening, comparing full automation, task delegation between AI and radiologists, and radiologist-only approaches.
- The research formulates an **optimization model** to minimize screening costs across three strategies: **expert-alone**, **automation**, and **delegation**.
- **Key theoretical** / **conceptual framework** discussions:
    - **Disease prevalence** is a critical factor, influencing the balance between costs of false positives (follow-up) and false negatives (litigation).
    - **Net Financial Benefit (NFB)** considers the trade-off between preventing litigation risk through true positives and the cost of false positives.
    - **Balanced Accuracy (BA)** assesses the expert's (radiologist's) overall performance considering sensitivity and specificity.
    - **Information Content (I)** quantifies the AI algorithm's ability to distinguish between healthy and sick patients.
- Key **findings** / arguments including:
    - **Delegation strategy** (AI-human collaboration) is often optimal, potentially yielding cost savings of 17.5% to 30.1% compared to relying solely on human experts.
    - **Proposition 1** provides a comprehensive framework for determining the optimal strategy (expert-alone, delegation, or automation) based on disease prevalence, expert performance (NFB and BA), and AI performance (I).
        - When prevalence is low, the transition from expert judgment to automated systems can be abrupt as AI performance increases.
    - **Corollary 1** states that automation becomes optimal once AI algorithms surpass a specific performance level, reducing reliance on human radiologists.
    - **Proposition 2** suggests that assigning higher liability costs to AI errors could shift preferences towards human involvement.
    - **Empirical validation** using data from the Digital Mammography DREAM Challenge supports the optimality of the delegation strategy and quantifies potential cost savings.
    - The optimal strategy shifts based on the interplay of algorithm costs, litigation costs, expert performance, and disease prevalence.
    - Lower algorithm and litigation costs tend to favor delegation strategies over expert-alone approaches.

#ai_in_healthcare #mammography_screening #economic_feasibility #task_delegation #optimization_model #disease_prevalence #false_positives #false_negatives #litigation_costs #machine_learning #human_ai_collaboration #expert_alone #automation #delegation #net_financial_benefit #balanced_accuracy #information_content #crowdsourcing_challenge

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Mammography is crucial for early breast cancer detection, with nearly 39 million women undergoing screening in the US in 2021.
- The increasing incidence of breast cancer and a shortage of radiologists create a demand-supply gap that AI could help fill [1].
- AI algorithms currently underperform radiologists in mammography screening [2], but further research is needed before full integration into clinical practice [1].
- AI can triage and prioritize cases, identifying normal mammograms to reduce radiologist workload and referring potential abnormalities for expert evaluation [3]. A Swiss clinical trial showed AI-assisted screening achieved comparable cancer detection rates to double-reading by radiologists [3]. The economic implications of AI-assisted screening require exploration.
- This research assesses the economic viability of AI integration in breast cancer screening, comparing costs and performance to radiologist-only solutions.
- The research aims to determine when AI can replace radiologists or when AI can delegate specific cases to radiologists.
- To achieve the research objective, **an optimization model** is formulated, comparing three strategies: expert-alone, automation, and delegation.
- The model is based on statistical principles of predictive AI and the economic impact of prediction-based decisions.
- Real-world mammography data from a crowdsourcing challenge are used to validate the results.
- The aim is to show how healthcare organizations can design mammography operations, allocate tasks between radiologists and AI, and quantify the outcomes.

# 2. Results

## 2.1. Analytical Model
- Three decision-making strategies in mammography are evaluated with varying degrees of AI involvement: **expert-alone**, **delegation**, and **automation**.
- The expert-alone strategy involves radiologists independently classifying patients as sick or healthy based on mammograms, reflecting current practice.
- The delegation strategy uses an initial AI screening to identify cases, with certain instances delegated to radiologists for final evaluation.
    - The AI algorithm evaluates the risk of breast cancer using mammography data and generates a continuous risk score, *r*.
    - If *r* is below a threshold *tD*, the patient is classified as healthy; otherwise, the case is referred to a human expert.
    - *d(r) = h if r < tD
      η ∈ {h, s} otherwise*
    - This approach assumes human expertise is essential for high-risk cases.
    - When *tD* approaches −∞, the delegation strategy converges to the human-only strategy.
- The automation strategy eliminates the need for a human expert by relying entirely on the AI algorithm.
    - The AI evaluates each mammogram and generates a risk score, *r*.
    - If *r* is less than or equal to a threshold *tA*, the patient is labeled as healthy; otherwise, the patient is labeled as sick.
    - *a(r) = h if r ≤ tA
      s otherwise*
- Regardless of the strategy, no additional follow-up is required for patients classified as healthy. Patients classified as sick undergo follow-up procedures (imaging or biopsy).
- The disease prevalence in the population is represented by *λ*.
- The classification outcome is categorized as true positive (TP), false positive (FP), true negative (TN), or false negative (FN). *P(o)* represents the probability of a specific outcome, where *o* ∈ {TP, TN, FP, FN}. An effective screening system aims to maximize *P(TP)* and *P(TN)* while minimizing *P(FP)* and *P(FN)*.
- Radiologists do not provide exact risk assessment values; therefore, probabilities associated with each outcome, *PE(o)*, are directly quantified to assess their performance.
    - These probabilities represent the average radiologist performance.
- For algorithms, performance is assessed using the area under the ROC curve (AUC), which reflects its ability to distinguish between healthy and sick patients.
    - It's assumed AI assigns risk scores following two distinct normal distributions: one for the healthy population and another for the sick population.
    - The mean risk score for sick patients (*μs*) is greater than that for healthy patients (*μh*), and equal variances (*σ*) are assumed for both distributions [4–6].
    - Under these assumptions, the risk prediction for a specific mammogram is assumed to follow a normal distribution, *N(μs, σ)* for patients with the disease and *N(μh, σ)* for those without it.
    - *AUC = Φ((μs − μh) / √(2σ)) = Φ(I / √2)*
    - Here, *Φ(.)* is the standard normal cumulative distribution function and *I := (μs − μh) / σ* represents the information content of the algorithm [7]. The information content measure, *I*, has a monotonic relationship with the AUC metric.

## 2.2. Cost Analysis
- The expected cost of screening under the delegation strategy, *CD(tD)*, and the automation strategy, *CA(tA)*, depends on the respective thresholds *tD* and *tA*.
- Additional cost factors:
    - *ca*: Constant cost per mammogram for using the algorithm.
    - *ce*: Cost of involving a human expert in the decision.
    - *cf*: Cost for each follow-up procedure when a patient has a suspicious finding.
    - *cl*: Expected litigation cost for a false negative (FN) outcome, accounting for the probability of legal action and the likelihood of success.
- Litigation occurs when cancer is missed (false negative), supported by the breast imaging malpractice literature [8, 9]. Costs related to false positives beyond follow-up procedures are not considered.
- Full details of the mathematical model are provided in the Methods section.

## 2.3. Optimal Allocation of Mammograms between Radiologists and AI
- The healthcare organization aims to reduce costs associated with mammography screening using AI, aligning with population-based payment models [10].
- The goal is to minimize total costs without compromising physician autonomy in clinical decision-making.
- The organization’s overall objective, *C*<sup>*</sup>, is defined as: *C*<sup>*</sup> := min{ *CE*, *C*<sup>*</sup><sub>D</sub>, *C*<sup>*</sup><sub>A</sub> }
    - where *C*<sup>*</sup><sub>D</sub> := min *tD* *CD(tD)* and *C*<sup>*</sup><sub>A</sub> := min *tA* *CA(tA)*.
- Two simplifying assumptions (Assumptions 1 and 2) are made, which are technically and practically satisfied. Closed-form solutions for optimal thresholds are presented in Section S1.
- The optimal strategy is analytically derived, drawing on insights from the value of information and medical decision-making literature [11, 12].
- The following terms are introduced:
    - *IED := I ∈ R : C*<sup>*</sup><sub>D</sub> = *CE*: the algorithm’s information content at which the expected costs of the expert-alone and delegation strategies are the same.
    - *IEA := I ∈ R : C*<sup>*</sup><sub>A</sub> = *CE*: the algorithm’s information content at which the expected costs of the expert-alone and automation strategies are the same.
    - *IDA := I ∈ R : C*<sup>*</sup><sub>D</sub> = *C*<sup>*</sup><sub>A</sub>*: the algorithm’s information content at which the expected costs of the delegation and automation strategies are the same.
    - *BA := (PE(TP) + PE(TN)) / 2*: balanced accuracy, the average of the sensitivity and specificity of the expert, equivalent to the Youden Index [13].
    - *NFB := PE(TP)λ[cl − cf] − PE(FP)(1 − λ)cf*: net financial benefit due to expert decisions, subtracting the prevalence-adjusted cost of false positives from the prevalence-adjusted marginal benefit of preventing litigation risk through true positives.
- **Proposition 1** illustrates how radiologist accuracy, cost factors, and AI algorithm performance influence the optimal AI implementation strategy and defines conditions under which the expert-alone, delegation, or automation strategies are optimal.

## 2.4. Proposition 1 Implications
- Characterization of the optimal strategy is influenced by the comparison between disease prevalence and the relative economic consequences of false-positive versus false-negative decisions (the cost ratio of false decisions). Tables 1a and b present distinct strategies depending on whether disease prevalence is low or high relative to these cost ratios.
- Disease prevalence plays a pivotal role in determining the optimal strategy.
    - λ0 = cf/cl represents the critical prevalence value, defined as the ratio of the cost of a false positive to that of a false negative (the cost ratio).
    - Disease prevalence is considered high when λ ≥ λ0 and low otherwise.
    - Healthcare organizations should tailor their strategies to the specific prevalence levels within their patient populations.
    - The choice among automation, delegation, or expert-alone strategies for different diseases will also depend on disease prevalence.
- When the net financial benefit due to the radiologist's decision is low, automation emerges as the preferred strategy in almost all cases, except when mammography tasks are delegated to human experts due to a high clinical benefit.
- When both the net financial and clinical benefits of the expert’s performance are sufficiently high, any of the three strategies can be optimal, with the algorithm's performance primarily determining the optimal strategy.
    - As algorithm performance changes from low to medium, the optimal strategy transitions from expert alone to delegation.
    - When algorithm performance changes from medium to high, the optimal strategy becomes full automation.
- When disease prevalence and net clinical benefit are low, increasing algorithm performance precipitates an abrupt transition from reliance on expert judgment to automated systems.
- When the net financial benefit of the human expert is high, and the performance of the AI is low, the expert-alone strategy becomes the optimal strategy in both low- and high-prevalence scenarios.

## 2.5. Corollary 1
- As AI technology progresses, its impact on decision-making warrants deeper exploration.
- **Corollary 1**: For any given PE(TP) and PE(FP), there exists a performance level IA such that when IA < I, the optimal strategy is automation.
    - Once AI algorithms surpass a specific performance level, automation becomes the optimal strategy for mammography screening.
    - The critical performance threshold required to switch to automation, IA, is higher for tasks where expert performance is higher.

## 2.6. Asymmetric Litigation Costs
- The model captures the economic consequences of missing cancer cases (false negatives) and ensuing litigation.
- The analysis considers the scenario where litigation costs differ between AI and human errors, given the lack of definitive regulatory guidance in the US.
- **Proposition 2**: A higher liability for machines could alter providers’ preferences away from automation (or delegation) to expert-alone solutions.
    - Holding machines to a stricter standard could encourage increased human involvement in prediction-based decisions.
    - Social planners can use liability costs as a policy lever to account for factors that a cost-minimizing firm might otherwise overlook.
    - Differential treatment of human vs. machine liability can help pace technological and legal developments.

## 2.7. Empirical Validation
- The model is parameterized using multiple data sources, and two types of numerical experiments are conducted.
- The first uses estimates from large-scale US mammography studies (Table 2) to characterize optimal strategies across varying human and AI performance.
- The second uses algorithmic predictions and radiologist assessments from the Digital Mammography DREAM Challenge [2].
    - The challenge aimed to develop AI algorithms for mammography-based breast cancer detection.
    - Per-mammogram algorithm cost is estimated based on the total cash prize offered and awarded in the DREAM Challenge.
- The optimal strategy is numerically characterized as a function of cost and performance parameters.
- Figure 1 depicts how the apportioning of work between experts and algorithms depends on the performance differential and the impact of algorithm and liability costs.
    - Automation is preferred when algorithm performance exceeds a certain AUC threshold, increasing in human performance.
    - Expert alone is preferred when human performance is very high, and algorithm performance is low.
    - Delegation can be preferred when neither dominates.
    - Reducing algorithmic costs facilitates human-machine combined solutions.
    - Lower litigation costs increase the region where delegation is optimal.
    - Feasibility of delegation requires high human and algorithm performances when costs are high.

## 2.8. Back Testing
- The proposed approach is validated by retrospectively testing the cost/performance of different AI strategies based on real data.
- Algorithm risk scores for 25,657 distinct mammograms in the holdout sample from the DREAM Challenge are used.
- Undersampling is used to match the population prevalence and repeat the sampling procedure 100 times to eliminate bias.
- An exhaustive search is conducted over tD and tA to find the computationally optimal thresholds.
- Two-sided t-tests are used to compare the average costs associated with each strategy.
- The backtesting experiments do not rely on the assumptions presented in the modeling section.
- Figure 2 quantifies mean cost estimates corresponding to the optimal strategies.
    - When both algorithm and litigation costs are low, the optimal strategy is delegation for most algorithms.
    - The expected costs under the delegation strategy using the top-performing algorithm suggest a 30.1% reduction from the expert-alone strategy.
    - When both costs are high, delegation is limited to the top-performing algorithms, resulting in a 17.5% cost reduction.
    - Potential savings from the delegation strategy could be substantial.

# 3. Discussion
- This research examines how a healthcare organization strategically allocates tasks between human experts and AI systems to lower operational costs.
- It considers three strategies: expert-alone, delegation, and automation.
- The study offers guidance to healthcare organizations on integrating AI into their workflows by balancing tasks between radiologists and algorithms.
- Disease prevalence is a key factor in determining AI’s optimal use.
    - High versus low prevalence, combined with cost trade-offs of false positives and false negatives, can shift preference toward delegation or automation.
    - Applying a uniform approach across all diseases may not be effective. Instead, organizations should tailor their AI adoption to the specific characteristics of each disease.
- Costs associated with algorithms and false assessments play a crucial role in shaping the optimal strategy.
    - High AI performance may lead to full responsibility being assigned to AI or human experts, depending on implementation costs.
    - Differences in litigation costs between algorithms and humans may induce the uptake of hybrid strategies.
- The numerical experiments highlight the practicality and benefits of the proposed approach.
    - A human-machine combined workflow (delegation) emerges as the most effective option, achieving significant cost reductions (17.5% to 31.1%).
    - By leveraging AI to assign specific tasks to human experts while automating others, healthcare organizations can unlock substantial efficiencies and cost savings.
- The model reflects US screening practices. Non-US contexts, such as Europe where double reading is standard, require alternative strategies.
- The paper introduces an operational framework for integrating AI into routine clinical decision-making, validated using data from a real-world crowdsourcing competition.
- The framework is adaptable to various clinical scenarios where predictive accuracy is essential, and AI has significant potential to improve efficiency.

# 4. Methods
- The expected costs associated with each strategy are defined as follows:
    - *CE := ce + P(η = s)cf + P(η = h|s)λcl = ce + λcl + PE(TP)λ[cf − cl] + PE(FP)(1 − λ)[cf]*
    - *CD(tD) := ca + P(r ≥ tD)ce + P(r ≥ tD, η = s)cf + P(r < tD|s)λcl + P(r ≥ tD, η = h|s)λcl = ca + λcl + P(r ≥ tD|s)λ[ce + P(η = s|s)(cf − cl)] + P(r ≥ tD|h)(1 − λ)[ce + P(η = s|h)cf]*
    - *CA(tA) := ca + P(r ≥ tA)cf + P(r < tA|s)λcl = ca + λcl + P(r ≥ tA|s)λ(cf − cl) + P(r ≥ tA|h)(1 − λ)cf*
- The healthcare organization’s overall objective *C*<sup>*</sup> is: *C*<sup>*</sup> := min{ *CE*, min<sub>*tD*</sub> *CD(tD)*, min<sub>*tA*</sub> *CA(tA)*}
- Two technical assumptions underpin the analysis:
    - **Assumption 1**: *ca/(1 − λ) < ce < cf < cl*. This describes the relationships between various costs.
    - **Assumption 2**: *PE(TP) > ce/(cl − cf)* and *PE(FP) < (cf − ce)/cf*. This provides a lower bound for the true-positive probability and an upper bound for the false-positive probability of the human expert.
- Conditional independence between the algorithm’s risk score and the human expert’s predictions is assumed given the patient’s health status.
- Ethical approval was obtained by Sage Bionetworks to conduct the Digital Mammography Challenge.
- Two primary datasets are used:
    - Synthetic data designed to simulate a controlled environment for validating theoretical models.
    - Data from the Digital Mammography DREAM Challenge, including breast cancer risk predictions from eighteen algorithms, radiologist assessments, and cancer outcomes for 25,657 patients.

# Executive summary of 1. Introduction
- Mammography is essential for early breast cancer detection, facing increasing demand with limited radiologists.
- AI holds promise to bridge this gap but currently underperforms radiologists.
- AI can effectively triage cases, reducing radiologist workload, but economic implications are unexplored.
- The study evaluates the economic viability of AI integration, comparing three strategies: expert-alone, automation, and delegation, using an optimization model and real-world data to guide healthcare organizations in task allocation.

# Executive summary of 2. Results
- The study evaluates three mammography decision-making strategies: expert-alone, delegation, and automation, using an analytical model.
- The delegation strategy involves initial AI screening with expert referral for high-risk cases, while automation relies solely on AI risk scores.
- The costs of each strategy depend on AI performance, expert involvement, follow-up procedures, and potential litigation for false negatives.
- *Proposition 1* defines the optimal strategy based on disease prevalence, radiologist accuracy, and AI performance, while *Corollary 1* suggests automation becomes optimal once AI surpasses a certain performance level.
- It also analyzes asymmetric litigation costs and demonstrates, using data from the Digital Mammography DREAM Challenge, that a hybrid approach where the higher liability for machines could alter providers’ preferences away from automation.

# Executive summary of 3. Discussion
- This research explores how healthcare organizations can strategically allocate tasks between human experts and AI systems to reduce operational costs in mammography screening.
- It emphasizes the importance of disease prevalence in determining the optimal AI strategy and the potential for human-machine collaboration to achieve significant cost reductions.
- The study highlights the practicality of a hybrid workflow, where AI assigns specific tasks to human experts while automating others, and adaptable to various clinical scenarios beyond mammography.

# Executive summary of 4. Methods
- The methods section defines the expected costs associated with each strategy: expert-alone, delegation, and automation.
- Two key technical assumptions are made: one regarding the relationships between various costs (Algorithm cost, human costs, false negative costs) and another providing bounds for the human expert's true-positive and false-positive probabilities.
- The assumption of conditional independence between the AI algorithm's risk score and the human expert's predictions is consistent with prior research.
- The study leverages ethical approval from Sage Bionetworks and employs two datasets: synthetic data for validating theoretical models and real-world data from the Digital Mammography DREAM Challenge.

 </details>
  	
```
title: The Societal Impact of Sharing Economy Platform Self-Regulations—An Empirical Investigation
authors: Wencui Han, Xunyi Wang, Mehmet Eren Ahsen, Sunil Wattal
journal: Information Systems Research
published: 2022
```
 

 # Executive Summary
 

 -   This research examines the **impact of platform self-regulations** in the context of the **home-sharing market** on **crime rates**.
 -   Using policy changes ("one host, one home" (OHOH) and "primary residency" (PR) regulations) that reduce the number of Airbnb listings in San Francisco, New York City, and Denver, **difference-in-difference approach** is deployed to empirically test the impact of platform self-regulations on crime rate.
 -   Key **theoretical** / **conceptual framework** discussions
     -   **Crime Pattern Theory**: Offenders follow a "**crime template**" where paths of offenders and targets intersect [Brantingham and Brantingham, 1993]. Targets in unsafe environments are more likely to be attacked [Seigel, 2006].
     -   **Routine Activity Theory**: Crime events occur due to the joint influence of a motivated offender, a suitable target, and the absence of capable guardians [Cohen and Felson, 1979].
 -   Key **findings** / arguments including
     -   Reduction in Airbnb listings resulting from platform self-regulations leads to a reduction in overall crime in the affected zip codes, specifically in **assault**, **robbery**, and **burglary**.
     -   Regulations may lead to an **increase in theft incidents**, likely due to more guests sharing homes with hosts.
     -   **Income**, **housing price**, and **population** moderate the impact of the regulations on different types of neighborhoods.
     -   Number of listings and the total Airbnb occupancy **mediate** the impact of the regulations.
     -   Higher median incomes, higher housing prices, and lower unemployment rates are likely to have increased crime with increased Airbnb occupancy, while low-income and low-population areas may have decreased crime as Airbnb occupancy increases.
 

 #sharing_economy #home_sharing #platform_self_regulation #crime_rate #societal_impact #airbnb #difference_in_differences #geographically_weighted_regression #crime_pattern_theory #routine_activity_theory
 
<details>
  <summary>Click to expand sections</summary>
	
 # 1. Introduction
 

 -   The sharing economy has disrupted traditional industries and had unforeseen societal impacts [Greenwood and Wattal 2017, Zervas et al. 2017, Burtch et al. 2018, Babar and Burtch 2020], sparking debates on the need for regulation.
 -   Home-sharing platforms like Airbnb facilitate social harmony [Walker 2016] and generate revenue for local businesses but face opposition due to unfair competition with hotels, increased rents, and house prices [Horn and Merante 2017, Chen et al. 2021b].
 -   Major stakeholders contend to impose regulations on the sharing economy [Rauch and Schleicher 2015], while others propose self-regulation by the platforms themselves [Cohen and Sundararajan 2015].
     -   Airbnb has worked with municipalities to create self-regulations addressing the housing price crisis by removing short-term listings from professional hosts [Chen et al. 2021a], effectively reducing property sales prices [Kim et al. 2017].
 -   The rapid expansion of home-sharing economies raises concerns for public safety, with media highlighting criminal activities targeting Airbnb guests [Fox News 2018].
     -   Complaints emerged from cities stating that Airbnb has not adequately addressed issues related to safety [Grind and Shifflett 2019].
     -   Despite anecdotal evidence, the understanding of how home-sharing and platform self-regulations impact crime incidents is limited.
 -   I am interested in studying the societal impact (crime rate) of home-sharing platforms’ self-regulations.
     -   I will examine the impact of platform self-regulation on crime using policy changes that reduce the number of Airbnb listings.
     -   Previous studies on sharing economy regulations exist [e.g., Chen et al. 2021a], but I am among the first to study the societal impact of home-sharing self-regulations on crime rates.
     -   Xu et al. [2019] show the association between Airbnb and crime in an exploratory study, but this research lacks causal identification mechanisms and discussion of regulations.
 -   I seek to address the following research questions:
     -   *RQ1*: Do home-sharing platforms’ self-regulations impact crime rate?
     -   *RQ2*: How do economic and demographic factors moderate the relation between home-sharing platforms’ self-regulations and crime?
     -   *RQ3*: What factors mediate the effect of home-sharing platforms’ self-regulations on crime rate?
 -   To answer these questions, I leverage classic criminology theories, such as crime pattern theory and routine activity theory [Eck and Weisburd 2015], to conceptualize how the reduced presence of Airbnb listings impacts criminal acts.
     -   By rationalizing the relationship between the number of Airbnb listings and crime rates, I understand how home-sharing platforms' self-regulations impact local crime rates via reducing Airbnb listings.
 -   To identify the causal mechanisms, I use two policy changes implemented in 2016 and 2017 as a natural experimental design:
     -   the “one host, one home” regulation (OHOH)
     -   the “primary residency” regulation (PR)
     -   These regulations limited the number of Airbnb listings by a single host and prohibited commercial host listings in San Francisco, New York City, and Denver.
     -   I also construct a comparison group of zip codes from Los Angeles, Boston, and Seattle based on the size of the Airbnb market and economic and demographic characteristics.
     -   Using the difference-in-difference (DID) approach, comparing changes in crime incidents before and after implementing the policies, I can provide evidence of the impact of the regulations on crime.
 -   To construct the data set on crime rate, I collected incident-level detailed crime reports from January 2015 to December 2018 from each of these six cities' police departments.
     -   I also obtained a proprietary data set of detailed information of all active Airbnb listings during these four years from the six cities.
     -   The final data set includes more than 5 million crime incidents and listing information for over 400,000 Airbnb listings.
     -   Finally, I collected a number of economic and demographic measures from the U.S. Census Bureau, the Bureau of Labor Statistics, the Bureau of Transportation Statistics, the Federal Judicial Center, and Zillow Research to construct a set of control variables.
 -   The results show that the policies reduced overall crime in the affected zip codes.
     -   Specifically, I see reductions in assault, robbery, and burglary.
     -   Surprisingly, both policies increase theft incidents, as the policies led to more guests sharing homes with the hosts, increasing theft opportunities.
     -   Several robustness checks are carried out to validate the main results, including a relative time model, multiple falsification tests, and a DID estimation using the propensity score-matching method.
     -   Findings from all specifications are consistent with no detection of a heterogeneous pretreatment trend, which indicates that the primary assumption of the DID model is not violated [Angrist and Pischke 2008].
 -   To shed further light on the mechanism, I also examine how the policies impacted the supply, occupancy, and quality of Airbnb listings using mediation analysis.
     -   The number of listings and the total Airbnb occupancy was significantly reduced after implementing the policies, and they mediate the impact of the regulations.
 -   Finally, I explore the heterogeneous impact of Airbnb occupancy on crime in different neighborhoods.
     -   To incorporate confounding effects as a result of the spatial proximity between different neighborhoods, I apply geographically weighted regression (GWR).
     -   Our results suggest that with increased Airbnb occupancy, neighborhoods with higher median incomes, higher housing prices, and lower unemployment rates are likely to have increased crime.
     -   By contrast, low-income and low-population areas may have decreased crime as Airbnb occupancy increases.
     -   A potential explanation of these heterogeneous effects could be that in high-income neighborhoods, increased Airbnb occupancy boosts tourists' volume, which provides more suitable targets for offenders [Schwartz and Pitts 1995, Brunt et al. 2000].
     -   In lower-income neighborhoods, local businesses can benefit from the positive economic spillover effect of Airbnb, and financially struggling families might gain additional income through Airbnb. Thus, with improved economic circumstances, individuals in these communities will be less likely to pursue criminal acts that bear huge risks [Becker 1968, Freeman 1999, Raphael and Winter-Ebmer 2001].
 -   This research contributes to a growing stream of literature focusing on the societal and economic impact of platforms [Chan and Ghose 2014, Greenwood and Agarwal 2015, Bapna et al. 2016, Sundararajan 2016, Greenwood and Wattal 2017] by offering an understanding of the nuanced impact of self-regulations of the home-sharing industry on crime.
 -   This work also contributes to the public policy debate on the regulations of sharing economy platforms.
     -   The results of this study suggest that home-sharing platforms’ self-regulation can have societal benefits, but such policies should consider the mechanism and heterogeneity of the platform’s impacts on different types of crime and the geospatial location of the neighborhoods.
 

 # 2. Related Literature
 

 ## 2.1. Societal and Economic Impact of the Sharing Economy
 -   Academic research on the sharing economy has mainly focused on studying its impact as a disruptive technology on traditional industries, the economic environment, and society.
 -   A few studies highlight the effects of the sharing economy on the businesses they directly compete with.
     -   Wallsten [2015] finds that increased Uber use is associated with a decrease in complaints about taxi services.
     -   Zervas et al. [2017] show that space-sharing services impact the hotel industry unevenly, with lower-end hotels and hotels not catering to business travelers being the most affected.
     -   Li and Srinivasan [2019] study the impact of Airbnb on revenue and market size for the hotel industry.
 -   Another dimension of the sharing economy’s impact lies in its spillover effect on the economic environment.
     -   Burtch et al. [2018] show that the entry of ride-sharing platforms reduces entrepreneurial activity and lowers its quality.
     -   Research reveals that participation in online sharing economy markets is driven by local economic downturns and can help unemployed and financially struggling individuals generate temporary employment [Dillahunt and Malone 2015].
     -   Recent research also finds a positive association between local (offline) unemployment rates and the online labor supply in the gig economy [Huang et al. 2020].
 -   Furthermore, a growing body of literature sheds light on the societal impact of the sharing economy.
     -   Extensive evidence shows that the sharing economy can increase social welfare.
         -   For example, Babar and Burtch [2020] study the impact of ride-hailing services on public transit and find that ride-hailing services significantly reduced road-based, short-haul public transit services and increased the use of rail-based and long-haul transit services.
         -   Also, compared with traditional taxis, Uber shows a higher capacity utilization rate in terms of time and fuel efficiency [Cramer and Krueger 2016].
         -   Research on ride sharing also shows that Uber's entry is associated with a significant decline in drunk driving fatality rates [Greenwood and Wattal 2017].
         -   The number of Uber pickups is negatively associated with occurrences of rape in New York City (NYC) [Park et al. 2021].
 -   With the positive impacts of the sharing economy well documented, research has also focused on its negative effects.
     -   Anecdotal evidence has shown that guests of home sharing have been the target of various criminal activities [Fox News 2018, Xu et al. 2019].
     -   Home-sharing platforms such as Airbnb have been criticized for paying no attention to addressing safety issues [Grind and Shifflett 2019].
     -   Mejia and Parker [2021] provide evidence of bias in ride-sharing services by showing that ride cancellation rates are higher for African Americans and the LGBTQ community.
     -   Furthermore, prior literature has uncovered negative consequences of peer-to-peer platforms (which is a broader concept that includes the sharing economy), such as gender and racial discrimination on these platforms [Ge et al. 2020, Edelman et al. 2017].
     -   Racial hate crimes are found to be positively related to the availability of the internet [Chan et al. 2016].
     -   Chan and Ghose [2014] show how the entry of Craigslist increases the prevalence of HIV by providing a platform for people seeking casual sex partners, and Chan et al. [2019] also find Craigslist leads to a 17.58% increase in prostitution cases.
     -   Greenwood and Agarwal [2015] further identify granular effects across subpopulations (i.e., race, gender, and socioeconomic status) in the process where matching platforms increase HIV incidence.
 -   My work adds to this budding literature on the societal impact of sharing economy platforms.
     -   Airbnb's economic, societal, and spillover effect has attracted much attention from scholars [Zervas et al. 2017, Zhang et al. 2017, Benjaafar et al. 2019, Chen et al. 2021b, Mayya et al. 2021].
     -   This space-sharing platform can drastically change local communities [Sundararajan 2016], which may be related to change in criminal patterns.
     -   Because of both the economic implications and the externalities that the sharing economy generates, it is imperative to advance this stream of literature [Koopman et al. 2015].
 

 ## 2.2. Regulations on Home Sharing and Crime
 -   The direct and spillover effects of the sharing economy have led to a debate on the need to craft necessary regulations.
     -   There is a small but growing stream of research investigating sharing economy regulation across many disciplines [Katz 2015, Koopman et al. 2015, Hong and Lee 2018].
     -   Some researchers argue that platform self-regulation is the right approach to resolve the challenges brought by the sharing economy [Cohen and Sundararajan 2015, Sundararajan 2016].
     -   Others are concerned with market failure and advocate for some regulations to account for the negative externalities that platforms impose [Interian 2016].
     -   Studies in the field of law have investigated specific policy choices to regulate the sharing economy [Katz 2015, He 2017].
     -   Chen et al. [2021a] find that regulatory interventions have less impact on long-term growth in the home-sharing economy, but short-term regulations can create a deterrent for new entrants.
 -   Despite the diverse opinions, the consensus is that there should be a particular government intervention level that is best suited to regulate and collaborate with sharing-economy platforms [Interian 2016].
     -   Cohen and Sundararajan [2015] note that although platform self-regulation is ideal, government involvement or oversight is essential to correct for any suboptimal outcome.
 -   My central research question is to study whether and how the home-sharing platforms’ self-regulations impact crime rates.
     -   The purpose of the regulations (i.e., the “OHOH” and the “PR”) investigated in this study was to reduce the number of short-term rental listings to lower the local housing price and rent and to protect affordable housing.
     -   Parallel with this rationale, I argue that the regulations reduce the number of short-term home sharing listings, which lessen the volume of home-sharing guests; this can eventually reduce crime incidents.
     -   The implied logic here is that the guests tend to become the targets of criminals. Therefore, to assess our research question, it is critical to understand how home-sharing prevalence may induce crime activities.
 -   Prior work in sociology and criminology literature has discussed the generation of illegal acts as a result of environment and space features [Eck and Weisburd 2015].
     -   Rather than focusing on offenders' characteristics, theorists of space criminology concentrate more on the circumstances or environmental settings where criminal acts occur [Cohen and Felson 1979].
     -   These criminologists believe that crime may not always take place randomly and that crime rates depend on factors such as time, space, and society.
     -   Several theoretical perspectives, such as rational choice theory, crime pattern theory, and routine activity theory [Cohen and Felson 1979], have improved our understanding of the importance of environment in crime prevention efforts.
         -   The rational choice theory suggests that offenders will select targets and choose means to achieve their goals in a specific manner [Cornish and Clarke 2014].
         -   *Crime pattern theory* suggests that criminals usually follow a "**crime template**" to commit crimes, which consists of places where the paths of both offenders and targets intersect [Brantingham and Brantingham 1993].
         -   Targets are more likely to be attacked as a result of being exposed to an unsafe environment [Seigel 2006].
         -   In addition, offenders will feel most comfortable committing criminal events within the areas they know, and in most ways, offenders behave similar to the nonoffender population [Brantingham et al. 2017].
         -   *Routine activity theory* also supports the arguments that crime pattern theory provides.
             -   Specifically, routine activity theory seeks to understand the occurrence of crime events as the joint influence of several factors:
                 -   the presence of a motivated offender,
                 -   a suitable target,
                 -   the absence of capable guardians against a violation [Cohen and Felson 1979].
             -   While engaging in their illegal activities, rational offenders will note places where the handlers, guardians, or managers are unlikely to show up [Eck and Weisburd 2015].
 -   In the case of home sharing, guests have a higher chance of becoming targets of criminal activity compared with residents who regularly stay in the community.
     -   Although one may assume that tourists and travelers would experience the same level of crime as the residents of the community they are staying in, the fact that their behavior is different suggests otherwise [Brunt et al. 2000].
     -   Criminology literature has documented lifestyle as a pertinent risk of victimization [Schwartz and Pitts 1995].
     -   As noted in Ryan [1993, p. 177], tourists are obvious in their dress, and they carry items of wealth that are easily disposable such as currency, passports and cameras. They are relaxed, and off guard. They are also less likely to press charges should the criminal be caught.
     -   Furthermore, travelers may be unaware of risky locations, display signs of carelessness, and act differently than locals.
     -   Chesney-Lind and Lind [1986] find that tourists in Hawaii were more likely to become victims of crime such as robbery and rape than were locals.
 -   From the perspectives of both crime pattern theory and routine activity theory, the potential targets become actual victims when the potential offender is willing to commit a crime and when the potential target fits the offender’s crime template [Brantingham et al. 2017].
     -   Consider the following example: Potential offenders conduct routine activities, such as work, school, and shopping, like everyone else.
     -   They have normal spatiotemporal movement patterns [Brantingham et al. 2017].
     -   They also build awareness of the locations around their normal activities.
     -   However, when a house becomes a home-sharing property, one could readily observe and perceive certain changes regarding this property’s features.
     -   For example, the flow of the people to the house may spike, and the noise level around the house may increase [Ozer et al. 2021].
     -   These evident changes may trigger potential offenders to commit crimes on home-sharing guests, who are mostly tourists and travelers.
     -   Taken together, the penetration of home sharing in a community may induce criminal activities.
     -   When implementations of the regulations remove partial home-sharing listings, those home-sharing properties will be restored as normal residences, and we may expect a reduction in the crime rate.
 

 # 3. Research Methodology
 

 ## 3.1. Background
 -   Airbnb is an online marketplace and hospitality service for people to lease or rent short-term lodging.
     -   As the leading home-sharing platform, Airbnb has more than 7 million lodging listings in over 100,000 cities in 220 countries and has facilitated more than 750 million check-ins as of 2020.
 -   With rapid growth, home-sharing platforms have been associated with numerous social and economic issues, such as increasing prices for long-term rental housing.
 -   To work with the local government to promote responsible sharing, Airbnb has launched many self-regulation policies in many cities.
 -   Using such policies as a natural experiment, I examine the impact of home-sharing platforms’ self-regulations on crime incidents.
 

 ## 3.2. Econometric Identification
 -   In many cities with high housing demands, Airbnb has been criticized for squeezing the meagre supply of long term rental accommodations when property owners list their housing units on short-term rental platforms because it is more lucrative.
     -   Research reveals a rapidly growing portion of Airbnb’s listings belong to commercial operators who rent out more than one residential property to short-term visitors.
 -   To limit the impact of home sharing on long-term rental and housing markets, Airbnb is working with many cities worldwide to enact policies that self-regulate certain types of listings.
 -   My econometric identification hinges on policy changes that would directly reduce Airbnb listings and the number of guests hosted through Airbnb but are not related to crime statistics in the area.
     -   The usual empirical strategy utilizing the entry of home-sharing platforms may suffer from endogeneity challenges, such as simultaneity and omitted-variable biases.
 -   Therefore, I use policy changes that can serve as natural experiments to estimate the impact of Airbnb regulations on criminal activities.
     -   By searching the Airbnb site, I found that San Francisco, New York City, and Denver have enacted policies that explicitly require short-term rentals to be the sole primary residence of the host and/or require removing multiple listings belonging to the same host.
     -   Two types of policies have been adopted in these markets: **OHOH** and **PR**, intending to significantly reduce the number of listings.
     -   These are city-level policies that are implemented across all zip codes in the three cities.
 -   To determine whether the policies are effective, I observe the number of removed listings as a result of these policies.
     -   I define removed listings as listings that were active (that had at least one booked night) six months before the policy implementation but do not exist in the data set after the policy implementations (listings not observed).
     -   I found OHOH removed 29% and 26% listings from New York and San Francisco, respectively.
     -   PR removed 55% and 20% listings from San Francisco and Denver, respectively.
 -   Note here that all the sources, such as news articles, consistently state that the policies' implementations are mainly aimed at increasing the long-term rental housing supply and protecting affordable housing.
     -   Thus, the policies are not aimed at reducing crime.
     -   Because the policies are implemented at the city level, their implementations are not likely to be driven by zip code–level characteristics.
 -   I estimate these policies’ impact on crime by comparing changes in crime incidents before and after enacting the policies in a specific zip code with a baseline of changes in crime incidents in zip codes with no such policy over the same time.
     -   I construct the control group with zip codes from three cities similar to the affected cities (detailed in Section 3.3).
 

 ## 3.3. Data
 -   I combine several sources of data to form a rich data set.
     -   First, I collect incident-level crime reports from the police departments of the six cities in our sample.
         -   The reports include the details of each incident, such as time of reporting, location (in the form of geo-coordinates), crime types, suspects and victims (race and gender), and the resolutions.
         -   I collect the reports from January 2015 to December 2018 (48 months total, 22 months before the implementation of the first policy and 11 months after the implementation of the last policy).
         -   The crime incident reports from these cities do not follow the same reporting standard.
         -   Therefore, following the definitions in the National Incident-Based Reporting System, I first recode these crime reports and keep the four most common crime incident types in the United States:
             -   theft (the taking of another person's property or services without that person's permission or consent, with the intent to deprive the rightful owner of it),
             -   burglary (the illegal entry into a building or other area to do something illegal there),
             -   assault (an intentional act that puts another individual in danger of immediate harm),
             -   robbery (the taking of or attempt to take anything of value by force or threat of force, or by putting the victim in fear).
     -   Second, I obtained property-level Airbnb data from a third-party business intelligence company.
         -   The data set contains listing-level information such as zip code–level location, property characteristics, host descriptions, reviews, and prices of the listings.
         -   More important, it also includes property performance information (e.g., numbers of days booked by guests and available days of the listings for booking); thus, it provides an accurate measure of Airbnb occupancy.
         -   I am also able to identify listings that are removed after the policy implementations by using listing IDs.
         -   This data set spans the same time period as our crime reports (i.e., from January 2015 to December 2018).
     -   Last, I construct a vector of time-variant control variables that could impact crime rates across zip codes and cities from the U.S. Census Bureau, Bureau of Transportation Statistics, the Bureau of Labor Statistics, the Federal Judicial Center, and Zillow Research.
         -   The control variables include population, number of arrival passengers, unemployment rate, average monthly earning, law enforcement employee number, bankruptcy cases, and housing price index.
 -   I now discuss the process of choosing policy-unaffected cities.
     -   Because the policy was announced and implemented at the city level in New York City, San Francisco, and Denver, I choose cities comparable to these three affected cities.
     -   I first rank the major U.S. cities by Airbnb market size (in terms of the number of Airbnb listings) and economic and demographic characteristics (population, median household income, and unemployment rate).
     -   Then I use a Euclidean distance algorithm to find the nearest “neighbors” to the affected cities.
     -   Among the top 10 cities, I then select the cities that are not affected by policies that regulate Airbnb activities from 2015 to 2018.
     -   Among the remaining 7 cities, I further investigate the availability of crime incidence reports.
     -   Finally, I select 3 cities—Los Angeles, Seattle, and Boston—to form our control groups.
 -   Note that in our model, I include zip code–specific trends to relax the parallel trend assumption.
     -   I also test the comparability of the control group with the affected zip codes using a relative time model.
     -   I conduct a propensity score-matching analysis to ensure the comparability of the two groups.
 -   I merge these sources of data at the zip code level by year-month.
     -   Our sample consists of 421,869 Airbnb properties and 5,052,571 crime incidents from 581 zip codes, including 268 zip codes from treated cities and 213 zip codes from untreated cities.
 

 # 4. Analyses and Findings
 

 ## 4.1. The Policy Impact on Crime Incidents
 

 ### 4.1.1. Results from Difference-in-Difference Estimation
 -   I use a two-treatment DID estimation [Huang et al. 2017] to establish the effects of the two policies restricting home sharing on the crime rate.
     -   I also run separate DID estimations for each policy, and the results are consistent.
 -   I estimate the DID models as reflected in Equation (1):
     -   ln Crime cit( )  β1Onehomect + β2Primary Residencect + γ′Zit + μt + v i + εcit: (1)
     -   The dependent variable ln Crimeit( ) is an aggregated measure of the log transformation of the number of incidents of the different types of crime in zip code i in city c in month t; ln Total Crimeit( ) is the average of ln Crimeit( ) of different types of crimes.
     -   Our primary independent variables are treatment indicators Onehomect and Primary_Residencect, which are equal to 1 if that policy was announced in city c by time t.
     -   We also incorporate zip code (v i) and time (year-month) (μt) fixed effects to account for the unobserved characteristics pertaining to the zip codes and certain months.
     -   The error term is indicated byε.
     -   To reduce heteroskedasticity concerns, I use robust standard error clustered at the zip code level.
 -   I include Zit, a vector of covariates including zip code–level and city-level economic and demographic characteristics and zip code–specific time trends.
     -   I use three types of controls to account for variations in tourist/traveler volume across different zip codes and months.
         -   First, I include zip code–specific trends as a control in Zit to relax the parallel trending assumption.
         -   Second, I include city-county-month fixed effects to control for differences in seasonal demands across different cities and zip codes.
         -   Third, I collect passenger arrival data for all airports in the six cities from the Bureau of Transportation Statistics to control for changes in tourism over time in a given city.
     -   I also include a few relevant zip code–level controls, including the rent index at zip code-month level collected from Zillow Research, to control for the economic status of the zip code, as well as the location desirability of the zip code.
     -   I also include the personal bankruptcy claims at zip code-month level collected from the Federal Judicial Center to control for the economic hardship in a zip code that might impact crime activity.
     -   Finally, to control for any variations in law enforcement and government efforts in reducing crime during our observation window, I include county-year-level numbers of law enforcement workers as control variables in the model.
 -   The results (Table 4) show that after implementing the OHOH, overall crime was reduced by 4.9%: assaults were reduced by 6.8%, robbery by 6.8%, and burglary by 7.7% in affected zip codes compared with unaffected zip codes.
     -   After implementing the PR, overall crime shows a reduction of 5.8%; assaults were further reduced by 15.6% in affected zip codes and robbery by another 4.9% compared with unaffected zip codes.
     -   Surprisingly, theft increased after the implementation of both policies—by 7.3% and 5.1%, respectively—compared with that in control zip codes.
     -   A probable explanation for the increase in thefts is that as a result of the policies, because multiple listings and nonprimary residences are removed, more guests are likely to share homes with the hosts.
 

 ### 4.1.2. Results from Relative Time Model
 -   I check the parallel trend assumption by replicating our results in the main specification using a relative time model, which allows us to measure the treatment's effect over time, both before and after the treatment.
 -   Specifically, I model Crimeicit by Equation (2):
     -   ln Crimeict( ) ∑nτ−TλτDcτ + γ′Zit + μt + vi + εit: (2)
     -   Here, D ct is a dummy variable indicating whether month t is the τ month before (for negative τ’s) or after (for positive τ’s) the OHOH implementation.
     -   Standard errors are robust and clustered at the zip code level.
 -   As shown in Table 5, there is no evidence of the violation of the parallel trending assumption, and the results are consistent with the main specification.
 

 ### 4.1.3. Falsification Test
 -   Next, I perform a falsification test by running a random implementation model to determine whether the observed effect occurs purely by chance.
 -   To execute this model, I randomly apply the policy change treatment to 23,088 zip code-months, then regress the log of crime incidents on this “pseudo”-treatment, store the coefficient, and replicate the analysis 1,000 times before comparing the outcome of the actual treatment against the pseudotreatments.
 -   The results show that it is unlikely that the coefficients and effect sizes generated from our actual treatment are similar to the ones generated by the pseudotreatments (p < 0.001).
     -   Thus, the observed effect is unlikely to occur purely by chance.
     -   In addition, the estimated placebo coefficient is insignificantly different from 0 in all models, suggesting correlation within the zip code-month has been accounted for.
 

 ### 4.1.4. DID with Propensity Score Matching
 -   To ensure the comparability of the treatment zip codes and the control zip codes, I use propensity score matching (PSM) to construct a matching group of control zip codes, based on observable measures before the implementation of the policies.
 -   After matching, the covariates are quite balanced, with no significant differences between the controlled and treated groups.
 -   I then estimate the same model in Equation (1) on the PSM sample and report the results in Appendix D.
     -   The results are consistent with our main model results.
 

 ## 4.2. Mediating Effects of Airbnb Supply and Occupancy
 

 ### 4.2.1. Policy Impact on Airbnb Supply and Occupancy
 -   To further understand how the policies impact crime incidents, I investigate the impacts of the policies on Airbnb supply and occupancy.
     -   OHOH is expected to reduce the total number of listings.
     -   Similarly, in the case of PR, the total number of listings should also reduce significantly.
 -   With these policies in place, the price and quality of the listings might be impacted as well.
 -   To estimate these impacts, I use the estimation specification in Equation (1) to estimate the impact of the policies on Airbnb supply (Total number of listings, Total number of multilistings, and Total number of single listings), Airbnb total occupancy, and listing quality (Average price per night and Average listing rating).
 -   The DID results (Table 6) show that the total number of listings, total number of multilistings, and total number of single listings significantly reduced after OHOH was put in place (6.8%, 3.9%, and 8.6%, respectively).
     -   And they further reduced by 12.2%, 10.4%, and 13.9% after enacting PR.
     -   The total occupancy of Airbnb listings reduced 12.2% and 21.3% after OHOH and PR, respectively.
 -   Considering these results, I know that the number of guests hosted through Airbnb significantly reduced after the two policies were enacted.
     -   In addition, I see that the quality of listings is higher after the policy implementations (the average rating is 2% after OHOH and another 7.3% higher after PR).
     -   I also find that the average price reduced by 6.8% after OHOH.
 

 ### 4.2.2. Mediating Effects of Airbnb Supply and Occupancy
 -   In order to investigate whether the impacts of the policies are mediated through Airbnb listings and occupancy, I carry out mediation analyses following the procedure suggested by Baron and Kenny [1986].
     -   First, the main results of the study (Table 4) show that the implementations of policy impact crime incidents.
     -   Second, I demonstrated that the implementations of the policies impact the mediators—Airbnb supply and occupancy (Table 6).
     -   Next, I demonstrate that the mediators impact the dependent variables without controlling for the policies (Table 7).
     -   Finally, I regress total crime on both policy implementations and Airbnb supply/occupancy (Table 8).
         -   The results show that when including Airbnb supply or occupancy in the model, the impacts of policy implementations were reduced.
         -   And the impact of Airbnb supply and occupancy on crime is significant in all the models.
         -   Thus, the reduction of crime is mediated through the reduction of Airbnb supply and occupancy.
         -   I also carried out the Sobel test [Sobel 1982, Baron and Kenny 1986], and the test results support the significant mediation effects.
 -   These results suggest that the impact of policy implementations on crime is mediated through Airbnb supply and occupancy through reducing the number of tourists entering residential neighborhoods that may become victims of crimes and attract offenders.
 

 ## 4.3. Heterogeneity Effects
 -   The impact of Airbnb’s self-regulations on criminal activities might be heterogeneous across locations, influenced by the characteristics of zip codes and the surrounding areas.
     -   An extensive body of literature on economics of crime shows an association between crime rate and societal issues such as poverty, social exclusion, limited education, poor employment record, and low legitimate earnings [Becker 1968, Freeman 1999, Raphael and Winter-Ebmer 2001].
     -   Meanwhile, the sharing economy offers significant employment opportunities for the unemployed and underemployed populations [Huang et al. 2020].
 -   Therefore, it is useful to explore the heterogeneous effects of reduction of Airbnb occupancy (resulted from the self-regulations) on crime rates.
 -   To understand the heterogeneous impact of Airbnb occupancy on crime across geolocations, I adapt the concept of spatial heterogeneity and the GWR model.
     -   Spatial heterogeneity refers to the impact of geographical distance on the relationships between predicted and explanatory variables [Mennis and Jordan 2005, Wheeler 2019].
     -   GWR is a widely used spatial regression model that can identify how locally weighted regression coefficients may vary across the study area [Wheeler 2019].
 -   The basic idea of GWR is that when researching variables such as crime rate, we should consider the geographical relationships.
     -   To address this concern, GWR uses a matrix to give weights to the values from other locations in the data set to account for their influences on the focal location and to weight the influences based on geographical distance.
     -   Specifically, the GWR model extends the traditional regression framework by incorporating local parameters so that the model can be expressed as in Equation (3):
         -   Y i  β0 lati, loni( ) +∑nk0βk(lati, loni) × Xik + εi ,i  1, : : : , n; (3)
         -   where (lati, loni) denotes the coordinates of the point i, β0(lati, loni) denotes the intercept value, and βk(lati, loni) is a set of coefficients at point i.
         -   Unlike in the global model, where the coefficient estimates are fixed, this model captures local effects by allowing the coefficents to vary across space.
 -   In addition, to utilize the panel data to provide a better understanding of the impact of Airbnb on crime, I adopt the geographically and temporally weighted regression (GTWR) to account for spatial-temporal nonstationary.
     -   Thus, the GTWR model can be expressed as Equation (5):
         -   ln Crimeit( )  β0 lati, loni, ti( ) +∑nk0βk(lati, loni, t i)× Occupancy itk + εi, (5)
         -   where Crimeit is the number of crime incidents in zip code-month i, β0 is the intercept for zip code-month i (with geo-coordinates (lati, loni), time t i), and βk is the coefficient for Airbnb occupancy of zip code-month i.
 -   I use data from New York City to demonstrate the heterogeneous impact of Airbnb on crime (the distance between each pair of zip codes is calculated using the geo-coordinates of zip codes; therefore, I run this regression in one city, not across multiple cities that are not adjacent).
 -   The results (Table 9) show that Airbnb occupancy in general is positively related to crime incidents.
     -   When we allow the coefficient to vary across zip codes, we observe the variations in coefficients.
 -   To better explore and understand what characteristics of zip codes contribute to such patterns, I use a second-stage regression and regress the coefficients of each zip code on zip code characteristics.
     -   I regress the coefficients on the population density, income level, housing value, unemployment rate, race, and education level of a zip code.
 -   The results (Table 10) show that in populated, higher-earning areas with higher housing values and lower unemployment rates, Airbnb has a higher positive impact on crime, whereas in areas with less population, lower income, and higher unemployment rates, Airbnb might reduce crime activity.
 

 ## 4.4. Moderating Effects of Population, Income, and Housing Value
 -   Following prior IS research, I included additional moderation analyses using the factors identified from the GWR analysis using the following specifications, (6)–(8) in the DID model:
     -   ln Crimecit( )  β1Onehomect × log(Population i) + β2Primary Residencect × log(Population i) + γ′Zit + μt + v i + εcit, (6)
     -   ln Crimecit( )  β1Onehomect × log(Median Housing Value i) + β2Primary Residencect × log(Median Housing Valuei) + γ′Zit + μt + vi + εcit, (7)
     -   ln Crime cit( )  β1Onehomect × log(Median Household Incomei) + β2Primary Residencect × log(Median Household Incomei) + γ′Z it + μt + v i + εcit: (8)
 -   The results (Table 11) show that population, median household income, and median housing value moderate the impact of policy implementation, such that in areas with higher population, household income, and housing value, the total crime reduces more after the implementations of policies.
     -   These results are consistent and complement the findings of GWR models.
 

 # 5. Discussion and Conclusions
 

 -   Home sharing has contributed to the local economy and helped millions gain additional income, yet little is known about the societal spillover effects of home sharing.
 -   I attempt to address this gap by empirically investigating the impact of regulations that reduce home-sharing activity on crime incidents.
 -   Using two policy changes that restrict the number of listings in three major cities in the United States as a natural experiment, my DID estimation provides evidence that, in general, reduced home-sharing activities reduce criminal activities.
 -   I use a multitude of additional specifications and robustness tests to cross-validate the results, including a time relative model, a falsification test with random implementation models, and a DID with a propensity score-matching method.
     -   All models support the findings of the main DID specification.
 -   To demonstrate the mechanisms of the policy impacts, I analyze the impact of the new regulations on Airbnb supply and occupancy.
     -   The total number of Airbnb listings and total guest occupancies mediate the policy impact.
     -   Interestingly, I also find that the ratings of the listings improved after the implementation of the two policies.
     -   This result suggests that when home sharing is truly “sharing” of homes, rather than commercial listings that are unofficial hotels operating without proper security and management on site, the quality of service will be higher and will induce fewer safety risks.
 -   It is likely that the impact of home-sharing’s regulations will be heterogeneous, depending on neighborhood characteristics.
     -   Therefore, I examine the heterogeneous impacts of the reduction of Airbnb occupancy on crime.
     -   To better understand how the effects vary across neighborhoods, I use the GWR model.
     -   The ability to visualize how the relationships change on a map when using geo-coordinates enhances the comprehension of research findings.
     -   The results of this analysis indicate that in populated, higher-income areas with higher property values, Airbnb has a higher positive impact on crime.
     -   By contrast, in areas with less population, lower average income, and lower housing values, Airbnb activities negatively impact crime incidents.
 -   In most cities worldwide, regulations have yet to catch up with the sharing economy; thus, the industry largely relies on platform self-regulation.
     -   Our study provides unique insights into the effect that the sharing-economy platform's self-regulation can improve social welfare by reducing crime.
 -   To this end, my study contributes to the budding literature in the area of regulations in the sharing economy and provides empirical evidence to inform ongoing policy debates.
     -   Our results in this study show that well-thought-out regulations can reduce negative spillover effects of the sharing economy, ensure the safety of those who use the economy, and increase consumers' welfare.
     -   In addition, our results regarding the heterogeneous impacts of Airbnb on crime provide nuanced insights for policy makers.
     -   For example, regulations restricting home sharing in some areas of a city, taking into consideration neighborhood characteristics, might be beneficial.
 -   This research makes a valuable contribution to methodology in IS research on geographic IS.
     -   I am among the first in the IS area to explore the heterogeneity effects leveraging GWR, and I hope this work can pave the way for future work, not only in the sharing economy but also in areas such as mobile commerce, location-based advertising, and ubiquitous computing, to better account for dependency characteristics when considering spatial analysis.
 -   Our work has several limitations, some of which can serve as fruitful areas for future research.
     -   First, although our empirical testing demonstrates an absence of unaccounted-for heterogeneity before implementing the policies, it is important to acknowledge that our findings are not based on a completely randomized trial.
     -   Second, to the degree that limited information is available about the Airbnb hosts and guests involved in criminal acts, we are unable to tell what populations are influenced to the greatest extent based on various demographics—age, gender, race, and socioeconomic status.
     -   Finally, the results of this work are based on the analyses of six large and representative cities in the United States, and this work, to a large degree, resolves the data limitation noted by Greenwood and Wattal [2017] and Park et al. [2021].
     -   However, future research could use similar measures based on our study to test such relationships in other cities and states.
 

 # Executive summary of 1. Introduction
 -   The sharing economy has disrupted industries, sparking debates on regulation versus self-regulation.
 -   Home-sharing platforms like Airbnb face scrutiny regarding competition, housing prices, and safety.
 -   This research examines the societal impact, particularly crime rates, of home-sharing platform self-regulations (OHOH and PR) using a difference-in-difference approach.
 -   It seeks to answer:
     -   Do self-regulations impact crime rates?
     -   How do economic factors moderate this relationship?
     -   What factors mediate the effect?
 -   It leverages crime pattern theory and routine activity theory to understand how reduced Airbnb listings impact crime.
 

 # Executive summary of 2. Related Literature
 -   Research on the sharing economy focuses on its impact on traditional industries, the economy, and society.
 -   Studies highlight the effects on competing businesses (e.g., hotels) and the economic environment (e.g., entrepreneurship, employment).
 -   There's growing literature on the sharing economy's societal impact, including both positive (e.g., reduced drunk driving) and negative effects (e.g., discrimination, crime).
 -   The debate on regulations involves whether to promote self-regulation or government intervention to address negative externalities.
 -   Prior research in criminology links crime to environmental factors and space features, using theories like rational choice, crime pattern, and routine activity theory.
 

 # Executive summary of 3. Research Methodology
 -   The study investigates the impact of Airbnb's self-regulation policies on crime incidents.
 -   It uses a natural experiment based on "one host, one home" (OHOH) and "primary residency" (PR) policies in select cities.
 -   The identification strategy hinges on policy changes that reduce Airbnb listings and guest numbers but are unrelated to crime.
 -   Data from multiple sources is combined:
     -   Incident-level crime reports from police departments in six cities
     -   Property-level Airbnb data from a third-party business intelligence company
     -   Economic and demographic variables from various government agencies.
 -   Control cities (Los Angeles, Seattle, and Boston) are selected based on their similarity to treatment cities.
 

 # Executive summary of 4. Analyses and Findings
 -   A two-treatment DID estimation shows that OHOH and PR policies reduced overall crime, assault, robbery, and burglary in affected zip codes.
 -   However, theft incidents increased, potentially due to more shared homes.
 -   A relative time model confirms the parallel trend assumption.
 -   Falsification tests indicate the observed effects aren't purely by chance.
 -   Propensity score matching (PSM) ensures comparability between treatment and control groups.
 -   Airbnb supply and occupancy mediate the policy impact on crime.
 -   Geographically weighted regression (GWR) reveals that the impact of Airbnb occupancy on crime varies geographically.
 -   Population, income, and housing value moderate the policy impacts.
 

 # Executive summary of 5. Discussion and Conclusions
 -   The study empirically investigates the impact of regulations reducing home-sharing activity on crime.
 -   Using a difference-in-difference (DID) estimation, the results suggest that reduced home-sharing activities generally reduce criminal activities.
 -   The total number of Airbnb listings and total guest occupancies mediate the policy impact.
 -   The GWR model reveals that the impact of Airbnb occupancy on crime varies depending on neighborhood characteristics.
 -   This research contributes to the literature on regulations in the sharing economy and provides empirical evidence to inform policy debates.
</details>

```
title: When Algorithmic Predictions Use Human-Generated Data: A Bias-Aware Classification Algorithm for Breast Cancer Diagnosis
authors: Mehmet Eren Ahsen, Mehmet Ulvi Saygi Ayvaci, Srinivasan Raghunathan
journal: Information Systems Research
published: 2019
```

# Executive Summary

*   This research investigates the design and value of a **bias-aware linear classification algorithm** for breast cancer diagnosis, where the algorithm utilizes human-generated data.
*   The core **theoretical** framework revolves around how **algorithms** that use data generated by humans inherit **biases**, thereby affecting their performance. The study proposes a **linear classifier** that accounts for **bias** in the input data, specifically focusing on the context of **breast cancer diagnosis**.
*   The study's main **finding** is that a **bias-aware algorithm** can eliminate the adverse impact of **bias** if the error in the mammogram assessment due to radiologist's **bias** has no variance. However, in the presence of error variance, the adverse impact of **bias** can be mitigated, but not eliminated, by the **bias-aware algorithm**.
*   The study argues that the **optimal bias-aware algorithm** assigns less (more) weight to the clinical-risk information (radiologist’s mammogram assessment) when the ==mean error== increases (decreases), but the reverse happens when the ==error variance== increases.
*   The study reveals scenarios where it might be better not to use clinical-risk information if it biases the radiologist's assessment.
*   Using data and point estimates obtained from mammography practice and medical literature, the **bias-aware algorithm** can significantly improve the expected patient life years or the accuracy of decisions based on mammography.
- The algorithm adjusts the weights assigned to **clinical-risk information** and **mammogram assessment** based on the **mean error** and **error variance** due to bias.
- Using data from **mammography practice** and the **medical literature**, we demonstrate that the bias-aware algorithm can significantly improve the **expected patient life years** or the **accuracy of decisions** based on mammography.

#Algorithms #Bias #BreastCancerDiagnosis #Classification #DecisionSupportSystems #Mammography #MedicalDecisionMaking #CognitiveBiases #LinearClassifier #ClinicalRiskInformation

<details>
  <summary>Click to expand sections</summary>
	
# 1. Introduction

*   **Algorithms** are increasingly used in decision-making across various domains, but they can be susceptible to human **biases** when the input data is human-generated [Agarwal and Dhar 2014].
*   Algorithms using human-generated decisions as inputs can inherit and even exacerbate **biases** [Coiera 2015, Barocas and Selbst 2016].
    *   This situation is seen in areas like loan approvals, job hiring, and law enforcement [Pasquale 2015].
	*   **Algorithms** that ignore **bias** may provide predictions that suffer from limitations akin to those provided by human beings, potentially worsening the errors [Coiera 2015, Barocas and Selbst 2016].
*   The study aims to design an **algorithm** that accounts for **bias** in input data, particularly in the context of a **clinical decision support system (CDSS)** for **breast cancer diagnosis**.
	*   The **CDSS** is a tool to help referring physicians make informed decisions based on **mammography**.
	*   This approach aligns with integrating **information systems (IS)** research methods and **behavioral economics** to address issues and inform **design science research** [Goes, 2013].

## 1.1. Role of Bias in Breast Cancer Diagnosis

*   In **breast cancer diagnosis**, referring physicians use radiologists' assessments, patient clinical-risk information, and patient preferences [Ayvaci et al. 2018].
*   Radiologists provide probabilistic risk assessments based on **mammogram** findings, often considering the patient's clinical-risk information.
*   The impact of clinical-risk information on radiologists’ assessments is debated; some argue it improves accuracy, while others suggest it may bias the radiologist [Loy and Irwig 2004, Elmore et al. 1997].
*   The radiologist’s assessment could be biased by the patient’s clinical-risk information [Carney et al. 2012].
	*   Undue influences resulting from radiologists’ cognitive **biases** may affect the performance of the referring physician.
*   A **CDSS** can be a valuable tool for referring physicians by helping them make recommendations on follow-up actions.
	*   An important component of such a **CDSS** is the **classification algorithm** that uses the **clinical-risk information** and **mammogram** assessment to predict the likelihood of cancer.
*   The study addresses three key research questions:
    *   What is the optimal design of a linear classification algorithm in the presence of radiologist **bias**?
    *   How does the **bias** affect the **algorithm’s** performance and design?
    *   Should the **clinical-risk information** be used at all in the diagnostic process given its potential to **bias** the radiologist?
*   The contributions of this research are:
    *   A new approach to designing a **bias-aware classification algorithm** and analyzing the impact of **bias** on the algorithm and its performance, which is new to the literature.
    *   Quantifying the value of **bias-aware algorithms** in the **breast-cancer-diagnosis** context.

# 2. Related Literature

*   The study is related to literature on cognitive **biases** in medical decision making, the design of **decision support systems (DSS)**, and mathematical modeling of **decision biases**.

## 2.1. Cognitive Biases in Radiological Diagnosis

*   The presence of **clinical-risk information** is critical in **breast-cancer-diagnostic** performance and may reflect features of various types of **biases**.
*   Three common **biases** are **anchoring**, confirmation, and availability, which occur when synthesizing information for medical decisions [Ogdie et al. 2012].
    *   **Anchoring bias**: The human tendency to overvalue a single piece of information.
	    *   In radiology, the radiologist may be anchored by the clinical history when assessing the radiographic image [Lee et al. 2013, Alpert and Hillman 2004].
	    *   To mitigate the **anchoring effect**, it has been suggested that radiologists look at the image first [Griscom 2002].
	*   **Confirmation bias**: The decision maker looks for confirming evidence to bolster an early assessment, rather than trying to disprove it.
	    *   An early judgment based on the patient’s **clinical-risk information** may induce a radiologist to focus on **mammogram** features consistent with the initial impression and ignore the features that conflict with it [Wallsten 1981].
	    *   **Confirmation bias** relates to a radiologist’s interpretation of findings in support of prior conclusions, while **anchoring** relates to overreliance on a single piece of information, the **clinical-risk information**.
	    *   **Confirmation bias** may amplify the **anchoring effect** in diagnostic decisions [Croskerry 2003].
    *   **Availability bias**: A person overestimates the probability of an event that comes immediately to the person’s mind.
	    *   The **clinical-risk information**, when available at the time of imaging interpretation, may nudge the radiologist to be more suspicious of any findings on an image [Eraker and Politser 1982].
	    *   **Availability bias** can occur when a person overestimates the probability of an event that comes immediately to the person’s mind.
	    *   Mamede et al. (2010) found that medical residents were susceptible to **availability bias**.

## 2.2. Related IS Research

*   The existing literature on **classification** under noisy data focuses mainly on the development of expert systems where decision rules are learned from training data [Hong and Tsang 1997, Wu et al. 2003, Saar-Tsechansky and Provost 2007].
    *   Researchers have developed input modification methods to eliminate the negative effects of noise in training data [Mookerjee 2001, Jiang et al. 2005].
*   A substream of the classification literature focuses on discriminating classes under strategically manipulated data [Dalvi et al. 2004].
    *   Researchers proposed methods for optimal classification [Boylu et al. 2010] or for approximate solutions [Boylu et al. 2007].
*   The study’s approach differs from previous studies by:
    *   Explicitly modeling the source of noise as human **biases** rather than strategic agents.
    *   Decomposing the source of noise into systematic and random parts.
    *   Providing theoretical insights into how input **bias** affects **algorithm** design and performance.

## 2.3. Related Decision Analytic Work

*   The **decision analysis** and operations management community is examining problems related to the mathematical modeling of **decision biases**.
    *   An optimization-based approach was proposed to determine the weights for biased quantile judgments [Bansal et al. 2017].
    *   Research has found that **bias** can be beneficial in coordinating the firms’ decisions [Li et al. 2017].
    *   A machine-learning mechanism was developed to alleviate overconfidence **bias** and the overfitting problem while aggregating forecasts [Grushka-Cockayne et al. 2016].
*   The study takes a prescriptive approach and studies how the **algorithm** should be designed to optimally aggregate information and make recommendations in the presence of biased data.

# 3. Model Description

*   The model considers a patient with an unknown true health status and a radiologist assessing the **mammogram** to determine the presence or absence of cancer.
*   The radiologist has access to the patient’s **clinical-risk information**.
*   Presence or absence of cancer is indicated using labels + and −, respectively.
*   The true (unbiased) risk implied by the patient’s **mammogram** (imaging) and the true risk implied by the patient’s **clinical-risk information** are denoted as xi and xc, respectively.
    *   Conditional on the true health status of the patient, xi and xc follow a bivariate-normal distribution.
*   Under **bias**, the radiologist’s estimate, ˆxi, deviates from xi.
    *   The error introduced by **bias** is a random variable that follows a probability distribution.
*   The mean error, β(xc − ¯xc), depends on:
    *   How much xc deviates from its population mean, ¯xc, which serves as the anchor.
    *   The **bias** factor β ≥ 0, which measures the radiologist’s inherent **bias** level.
*   σ0 captures the variability in the error introduced by **bias** [6].
    *   β  0 and σ0  0 correspond to the unbiased assessment of xi.
*   Given a biased assessment from a radiologist, the tasks of the **classification algorithm** (hereafter, **bias-aware algorithm**) are to (i) aggregate ˆxi and xc and (ii) determine a threshold k for the aggregated risk to classify the instance as + or −.
*   The aggregate information, r, is defined as:
    *   r : (1 − α) ˆxi + αxc ,
    *   where α denotes the weight assigned to xc, and 1 − α denotes the weight assigned to ˆxi.
*   The study focuses on exploring the role of **bias** in the design and performance of the **bias-aware algorithm** and not the ability of either type of risk information in discriminating the + cases from the − cases.
*   The following definitions are also used:
    *   I(α, β, σ0)  (μc − μn)/σ as the discriminative ability of r.
    *   ∆i  μci − μni and ∆c  μcc − μnc, respectively.
    *   h : I(0, 0, 0) / I(1, 0, 0)  ∆i / ∆c
    *   t : (UTP − UFN)/P(−) / (UTN − UFP)/P(+)  UTP − UFN / UTN − UFP · P(+) / P(−)

# 4. Theoretical Analysis

*   The analysis explores the optimal **bias-aware algorithm** based on the AUC and the expected utility objectives.
*   It examines how **bias** affects **algorithm** performance and quantifies the impact of ignoring **bias**.
*   The analysis identifies conditions under which it is suboptimal to use the **clinical-risk information** given that it can **bias** the radiologist’s assessment.
*   Finally, the study discusses the case when **bias** impacts error mean and error variance proportionally.

## 4.1. Optimal Bias-Aware Algorithm

*   The **optimal weight** and threshold are characterized in Theorem 1:
    *   α∗(β, σ0) : 1 / (1 + σi / σc (h − ρ) / (σ2 / i (1 − hρ) + σ2 / 0 − σi / σc / β(h − ρ)))
    *   k∗(β, σ0) : k(α∗(β, σ0), β, σ0).
*   Key observations from Theorem 1:
    *   Utility parameters only affect the optimal risk threshold and not the optimal aggregation weight.
    *   If t  1, the risk threshold is also independent of the utility parameters.
    *   When the radiologist is unbiased such that β  0 and σ0  0, the optimal weight for the **clinical-risk information** reduces to that shown by Winkler (1981).
    *   The optimal weight for the clinical-risk information can be negative depending on the relative discriminatory ability of the two risks, the correlation between them, and the **bias** parameters.
*   Structural analysis of the optimal weight:
    *   α∗(β, σ0) is decreasing in **bias** factor β.
    *   An increase in the error variance (σ0) increases the weight assigned to the **clinical-risk information**.
    *   An increase in the correlation between the **clinical-risk information** and the **mammogram** risk (ρ) decreases the weight assigned to the **clinical-risk information** when the error variance is not too high.
    *   The threshold level for risk to recommend a follow-up is increasing in both **bias** factor and error variance.

## 4.2. Impact of Bias Under the Optimal Bias-Aware Algorithm

*   Proposition 1 characterizes how the **bias** factor, error variance, and other model parameters affect the expected utility and the discriminative ability under the **optimal bias-aware algorithm**.
    *   U∗(β2 , σ0)  U∗(β1 , σ0) < U∗(β1 , 0)  U∗(0, 0),
    *   AUC∗(β2 , σ0)  AUC∗(β1 , σ0) < AUC∗(β1 , 0)  AUC∗(0, 0),
    *   U∗(β, σ0) is independent of β and decreases in σ0, σi, σc, and ρ.
*   According to Proposition 1, (i) and (ii), the expected utility and discriminative ability are (weakly) smaller in the presence of **bias** than in its absence even under the **bias-aware algorithm**.
*   When error variance is zero (i.e., σ0  0), the **optimal bias-aware algorithm** eliminates the negative impact of **bias**.
*   In the presence of a positive error variance (i.e., σ0 > 0), the negative effect of **bias** cannot be eliminated through the **optimal bias-aware algorithm** alone.
*   The optimal expected utility, U∗(β, σ0), is independent of the **bias** factor (and, therefore, the mean error), yet it decreases in the error variance.

## 4.3. Impact of Ignoring Bias in the Algorithm

*   The value of the **bias-aware algorithm** is defined as the optimal utility when the algorithm adjusts for **bias** less the utility when the algorithm assumes that **bias** does not exist.
*   The utility value of the **bias-aware algorithm** is defined as:
    *   V(β, σ0) : U∗(β, σ0) − U(α∗(0, 0), β, σ0 , k(α∗(0, 0), 0, 0)).
*   Proposition 2 characterizes how the **bias** affects the value of the **bias-aware algorithm**.
    *   The **bias-aware algorithm** has (i) V(β, σ0) ≥ 0, and (ii) V(β, σ0) is decreasing in β when β < σ2 / 0 /(σc σi · (h − ρ)) and increasing otherwise.
*   The value does not necessarily increase in the **bias** factor (β).
*   If there is no error variance (σ0  0), then an increase in mean error always increases the value of the **bias-aware algorithm**.
*   When the error variance is not zero, the value increases in the mean error only when the mean error is larger than a threshold.
    *   An increase in mean error mitigates the adverse impact of a large error variance for fixed values of α and k.
*   The interaction between the mean error and error variance due to **bias** is critical in determining the value of the **bias-aware algorithm**.

## 4.4. Should the Clinical-Risk Information Be Used Under Radiologist Bias?

*   The adverse impact of mean error alone due to **bias** can be eliminated, but an increase in the error variance diminishes the usefulness of the **clinical-risk information**.
*   The social planner can consider two possible alternatives:
    *   Use the **clinical-risk information** but design the algorithm to account for the **bias**.
    *   Not use the **clinical-risk information** at all.
*   Proposition 3 characterizes when the **clinical-risk information** should be available in the system despite the possibility that it may **bias** the radiologist.
    *   If h ≥ 1 and 1 / h − √(h2 − 1)σ2 / 0 / σ2 / i < ρ, then U∗(β, σ0) < U(0, 0, 0, k(α∗(0, 0), 0, 0)); otherwise, U∗(β, σ0) > U(0, 0, 0, k(α∗(0, 0), 0, 0)).
*   Insights from Proposition 3:
    *   Even when the **clinical-risk information** is informative on its own, not using it in the whole process may be better than using it.
    *   A high-enough correlation will cause the **clinical-risk information** to be not useful.
    *   An increase in the error variance (σ0) enlarges the region in the parameter space where not using the **clinical-risk information** is better than using it.
    *   When a diagnostic task is based on data derived from human experts susceptible to cognitive **biases**, the error variance induced by **bias** is a key factor to consider.

## 4.5. Joint Influence of Bias on Both the Mean Error and Error Variance

*   The baseline model assumes the radiologist’s **bias** influences the mean error and error variance in **mammogram** assessment independently.
*   Assuming that a shift in the **bias** factor has a proportional impact on mean error and error variance, the study models error due to **bias** conditional on **clinical-risk information** as ˆxi − xi | xc ∼ N (β(xc − ¯xc), βσ0).
*   Theorem 2 in Online Appendix B provides the optimal weight and risk threshold under the new model.
*   Proposition 4 suggests that when mean error and error variance are proportional, an increase in the **bias** factor always hurts the **optimal** expected utility or discriminative performance unless there is no error variance.
    *   U∗(β2 , σ0) < U∗(β1 , σ0) < U∗(β1 , 0)  U∗(0, 0),
    *   AUC∗(β2 , σ0) < AUC∗(β1 , σ0) < AUC∗(β1 , 0)  AUC∗(0, 0).
*   The main additional insight from the proposition is that when the **bias** factor influences both the mean error and error variance, the **bias-aware algorithm** alone cannot completely eliminate the adverse impact of **bias**.

# 5. Design and Value of the Bias-Aware Algorithm for Breast Cancer Diagnosis: A Computational Experiment

*   The study uses a **breast-cancer-outcomes database** based on **clinical-risk information** and the medical literature.

## 5.1. Parameter Estimation

*   The study uses the **Breast Cancer Surveillance Consortium (BCSC) database** to estimate the parameters of the **clinical-risk distributions**.
*   The parameters for the clinical-risk distributions are μpc  2.4042, μnc  2.1900, σpc  0.3656, and σnc  0.3869, and the indicated AUC of the clinical-risk information (AUCc) is 0.656.
*   The parameters for the **mammography** distributions are imputed using the performance benchmarks reported in the medical literature.
*   Three different AUC values for the **mammography** risk are selected to represent low (AUCi  0.780), moderate (AUCi  0.820), and high (AUCi  0.890) discriminative ability.
*   The correlation between **mammogram** and **clinical-risk information**, ρ, is estimated as 0.0548 using Spearman’s rank correlation.
*   The study estimates the disutility as the life years lost, following the medical literature.

## 5.2. Optimal Bias-Aware Algorithm: Aggregation Weights and Decision Threshold

*   For the prevailing **clinical-risk** model with an approximate AUC of 0.656 and an average-quality **mammogram** with an AUC of about 0.820, the **clinical-risk information** should carry a relative weight of approximately 24% and the **mammogram** risk should carry a relative weight of 76% under no **bias**.
*   Under no **bias**, a follow-up should be recommended if the weighted risk score exceeds a value of 2.62.
*   The **optimal weight** on the **clinical-risk information** (mammogram risk) generally decreases (increases) when either **bias** factor (β) increases or error variance (σ2 / 0 ) decreases.
*   When both **bias** factor and error variance are sufficiently high, it is optimal to use a larger weight for the **clinical-risk information** under **bias** than under no **bias**.
*   An accurate estimation of **bias** parameters is essential for the **optimal design** of a **bias-aware algorithm**.
*   An increase in any **bias** parameter only increases the threshold.

## 5.3. Impact of Radiologists’ Bias on Breast-Cancer-Diagnosis Outcomes

*   The impact of radiologists’ **bias** is quantified as the reduction in expected life years because of the presence of **bias**.
*   When the accuracy of the **mammography** is moderate (i.e., AUCi  0.820), for the prevailing **clinical-risk** models with AUCc  0.656, the presence of various levels of **bias** could result in a reduction in the expected life years for all patients ranging from 8.00 to 119.69.
*   The adverse impact of **bias** increases as the mean error or error variance due to **bias** increases.
*   The negative impact of **bias** is higher if the **mammogram** is more accurate.
*   A reduction in AUC from 0.837 to 0.831 translates into an additional 237,900 misdiagnoses of patients’ health.
*   Appropriate mechanisms to eliminate or mitigate the **bias** should be adopted.

## 5.4. Value of a Bias-Aware Algorithm for Breast Cancer Diagnosis

*   The value of a **bias-aware algorithm** is examined using the percentage of lost expected life years recovered by accounting for **bias**.
*   The no-**bias** case (β  0 and σ0  0) results in the maximal expected life years.
*   When there is no error variance, σ0  0, the **bias-aware algorithm** is able to recover 100% of the loss incurred by a **bias-blind algorithm**.
*   The value of a **bias-aware algorithm** diminishes when either the mean error or error variance increases, but the value can still be substantial.
*   Mitigating the impact of clinical-risk-information-induced **bias** through a **bias-aware algorithm** alone is likely to be challenging when the radiologist’s **bias**-related behavior is highly unpredictable.

## 5.5. Should the Clinical-Risk Information Be Used for Breast Cancer Diagnosis?

*   Not using the **clinical-risk information** for diagnosing **breast cancer** is sometimes better than using it if it will **bias** the radiologist.
*   The thresholds for the mean error or error variance due to **bias** above which using the **clinical-risk information** leads to an inferior expected utility or AUC are shown in Table 2.
*   The critical role played by the relative discriminative ability of the **clinical-risk information** and **mammogram** in decisions concerning the use of **clinical-risk information** for cancer diagnosis is emphasized.

## 5.6. Impact of Bias When the Accuracy of Clinical-Risk Information Improves

*   As the AUC of **clinical-risk information** increases, the percentage of loss that can be recovered by accounting for **bias** decreases.
*   Correcting for the detrimental effects of **bias** is more difficult at higher **clinical-risk information** AUC values.
*   **Bias** becomes less of an issue and a **bias-aware algorithm** becomes less valuable as the discriminative ability of the **clinical-risk information** improves relative to that of the **mammography**.

# 6. Discussion and Conclusion

*   When algorithms use human-generated input data that suffer from human **biases**, the predictions they generate may exacerbate the errors stemming from such **biases**.
*   The **optimal bias-aware algorithm** can eliminate the adverse impact of **bias** if there is no variability associated with the **bias**-induced error in the radiologist’s assessment.
*   The optimal **bias-aware algorithm** assigns a smaller (larger) weight to the **clinical-risk information** (**radiologist’s mammogram** assessment) when the mean error increases, but the reverse happens when the error variance increases.
*   The **bias-aware algorithm** can significantly improve the expected patient life years.
*   The magnitude of improvement depends critically on the relative discriminative abilities of **clinical-risk information** and **mammogram**.

## 6.1. Translating the Findings Into Clinical Practice

*   Realizing the gains indicated by the study depends on:
    *   Finding ways to incorporate proper weighting of **clinical-risk information** and **mammogram** information.
    *   Reducing, eliminating, or properly adjusting for the **bias** due to **clinical-risk information**.
*   The referring physician should use a decision support system (DSS) that accounts for the unintended over- or underassessment of risk because of the available information.
*   One challenge is that the radiologists a physician deals with could be biased to different degrees, both in terms of **bias** factor (i.e., mean error) and error variance.
*   The relative simplicity of the model makes it possible to incorporate the model into a decision-aid tool.
*   The referring physician’s attitude toward its predictions is another issue to consider.

## 6.2. Future Directions

*   The model can be extended to a general scenario with n attributes and m classification outcomes.
*   Nonlinear aggregation models can provide additional insights into the impact of **bias**.
*   Patient-level **mammography** data would provide better estimates.
*   Estimating the **bias** parameters requires extensive well-designed experiments.
*   The model should be validated by implementing it in a **CDSS** in practice and comparing the actual results with the model predictions.

# Executive summary of 1. Introduction

*   **Algorithms** are increasingly used in decision-making and are prone to human **biases** when they use human-generated data.
*   The study focuses on designing a **bias-aware algorithm** for use in a **CDSS** for **breast cancer diagnosis**.
*   The research aims to answer three key questions about the design and impact of a **bias-aware algorithm** in the context of **breast cancer**.

# Executive summary of 2. XXX (Section 2)

*   The study is related to the literature on cognitive **biases** in medical decision making, the design of **DSS**, and mathematical modeling of **decision biases**.
*   The **anchoring**, confirmation, and **availability biases** can affect **radiological diagnosis**.
*   The study’s approach differs from previous studies by explicitly modeling the source of **bias** as human cognitive flaws.
*   The study takes a prescriptive approach and studies how the **algorithm** should be designed to optimally aggregate information in the presence of biased data.

# Executive summary of 3. Model Description

*   The model considers a patient with an unknown health status and a radiologist assessing a **mammogram** and access to **clinical-risk information**.
*   The error introduced by **bias** is a random variable and a function of a **bias** factor and error variance.
*   The study focuses on exploring the role of **bias** in the design and performance of the **bias-aware algorithm**.
*   The study defines aggregate information, r, and the weight assigned to **clinical-risk information** and **mammogram**.

# Executive summary of 4. Theoretical Analysis

*   The analysis explores the optimal **bias-aware algorithm** based on the AUC and the expected utility objectives, focusing on how **bias** affects **algorithm** performance.
*   The **optimal algorithm**'s characteristics are presented, with key observations.
*   It shows how bias affects utility and discriminative ability and the impact of ignoring **bias**.
*   It also identifies the conditions under which it is suboptimal to use the **clinical-risk information** given that it can bias the radiologist’s assessment.
*   It discusses the impact of both the mean error and the error variance.

# Executive summary of 5. Design and Value of the Bias-Aware Algorithm for Breast Cancer Diagnosis: A Computational Experiment

*   The study uses a **breast-cancer-outcomes database** based on **clinical-risk information** and the medical literature to perform a computational experiment.
*   The parameters for the clinical-risk and mammography distributions are estimated.
*   The impact of radiologists’ **bias** on breast-cancer diagnosis outcomes is quantified.
*   The value of a **bias-aware algorithm** for **breast cancer diagnosis** is examined.
*   The study discusses whether **clinical-risk information** should be used and the impact of **bias** on increasing the accuracy of the **clinical-risk information**.

# Executive summary of 6. Discussion and Conclusion

*   The study examines the design and value of a **bias-aware algorithm** for use in a **CDSS** for **breast cancer diagnosis**.
*   The **optimal bias-aware algorithm** can eliminate the adverse impact of **bias** if there is no variability associated with the **bias**-induced error.
*   The **optimal bias-aware algorithm** assigns a smaller (larger) weight to the **clinical-risk information** (**radiologist’s mammogram** assessment) when the mean error increases, but the reverse happens when the error variance increases.
*   The **bias-aware algorithm** can significantly improve the expected patient life years.
*   The findings can be translated into clinical practice, which involves a proper weighting of **clinical-risk information** and **mammogram** information, and reducing, eliminating, or properly adjusting for the **bias** due to **clinical-risk information**.
*   Future research directions are also suggested.
</details>


# Gopesh Anand
- Professor of Business Administration and Area Chair for IOSA and William N. Scheffel Faculty Scholar
### education
- Ph.D., Operations Management, The Ohio State University, 2006
- MBA, The Ohio State University, 1992
### research interest
- Operations Strategy
- Continuous Improvement
### teaching
- Operations Management
- Process Improvement


```
title: Learning from Failures: Differentiating Between Slip-ups and Knowledge Gaps
authors: Gopesh Anand, Ujjal Kumar Mukherjee
journal: Organization Science
published: 2024
```
 

# Executive Summary
- We investigate **firm learning from failures**, defined as a **reduction in subsequent recalls**, by dividing failures into **two types**: **slip-up (process) failures** (e.g., product contamination due to violation of cleanliness protocols) and **knowledge gap (design) failures** (e.g., unexpected interactions of two medications).
- We examine whether this **reduction in subsequent recalls** (learning) occurs in the context of both types of failures, using **product recalls** in pharmaceuticals and medical devices as the study context.
- Based on text analysis of recall documents, we classify recalls into **process-related** (slip-up) and **design-related** (knowledge gap).
- We also study how **innovation capabilities** (accumulated patents and lagged R&D intensity) impact learning from both types of failures; for instance, by providing a greater ability to identify causal associations for failures.
- Findings indicate that **design-related recalls** generate *greater* learning (i.e., a *larger* reduction in subsequent recalls) than **process-related recalls**.
- Accumulated patents and lagged R&D intensity *enhance* learning from design-related recalls (i.e., lead to a *greater* reduction in subsequent recalls following design-related recalls).
- Overall, learning mechanisms invoked by failures are concentrated more on **knowledge gap failures** than **slip-up failures**, and this reduction in subsequent recalls is greater when firms possess higher innovation capabilities.
- We extend organizational learning theory by differentiating between learning from different types of failures and absorptive capacity theory by incorporating the role of innovation capabilities in enhancing learning from failures.
- We recommend focusing on the cultural and social mechanisms of organizational learning in addition to the technical and structural mechanisms that may mainly impact learning from knowledge gap failures for improving learning from slip-up failures.

#organizational_learning #absorptive_capacity #failures #innovation #product_recalls #pharmaceuticals #medical_devices #knowledge_gap #slip_ups #patents #R&D

<details>
	
  <summary>Click to expand sections</summary>

  
# 1. Introduction
- Organizational learning theory suggests that failures can lead to improvements [Cyert and March 1963; Argyris and Sch¨on 1978, 1996; Argote 2013; Sitkin 1992, Dahlin et al. 2018].
- **Product failures** are defined as undesirable outcomes for users [Cannon and Edmondson 2001].
- We categorize product failures into **slip-ups** in production processes and **knowledge gaps** in product designs [Rasmussen 1983, Frese and Keith 2015].
    - Organizational learning requires reinforcement of established practices and experimentation for changes to product designs [Popper and Lipshitz 1998; Anand et al. 2016, Eggers and Suh 2019].
- This research aims to determine whether learning occurs from both **slip-up failures** and **knowledge gap failures**.
    - We also examine how **absorptive capacity** from **innovation capabilities** enhances these learning mechanisms [Cohen and Levinthal 1990].
- Absorptive capacity theory indicates that firms differ in their ability to absorb and use information, especially those with a history of innovation [Cohen and Levinthal 1990].
    - We apply this perspective to learning from product failures, focusing on knowledge absorbed from innovation efforts and past innovations [Dahlin et al. 2018], rather than just external sources.
- We develop hypotheses based on organizational learning and absorptive capacity theories, testing them using data on **product recalls** as indicators of product failures [Haunschild and Rhee 2004, Thirumalai and Sinha 2011, Kalaignanam et al. 2013, Shah et al. 2017, Gao et al. 2022, Astvansh et al. 2022a].
- We divide product recalls into **process-related** (slip-up) and **design-related** (knowledge gap) recalls.
    - **Innovation capabilities** are measured using **accumulated patents** and **lagged R&D intensity** [DeCarolis and Deeds 1999, Pennetier et al. 2019].
- We present two examples of firm learning from product recalls.
    - The first example is a medical implant device recall due to difficulty in distinguishing the top and bottom of the device, leading to inverted implantations. This design-related recall prompted a redesign to prevent future errors.
    - The second example involves the da Vinci surgical robot, where recalls related to the surgical tip and robotic "EndoWrist" portions were followed by supplemental design approvals and patents, indicating learning and innovation.
- We use product recall data for publicly traded firms in the medical device and pharmaceutical industries in the United States, regulated by the FDA.
    - We use text analysis of recall announcements to identify process-related and design-related recalls. We employ negative binomial generalized linear models (GLMs) to study firm learning. Patent data comes from the USPTO, and R&D investment data from the WRDS database.
- The key findings are:
    - Firms learn more from design-related recalls than from process-related recalls.
    - Innovation capabilities (accumulated patents and lagged R&D intensity) enhance learning from recalls.
    - This enhancement is greater for design-related recalls.
- This study demonstrates how the reasons for failures and firms' innovation efforts interact, leading to heterogeneity in learning from failures.
- We discuss results from a post hoc analysis, suggesting that the two types of recalls invoke different learning mechanisms:
    - Knowledge gap failures benefit from focused root cause analysis, experimentation, and design changes.
    - Slip-up failures require deliberate actions to improve process compliance.
- We make three contributions:
    - Develop a two-way categorization of failures and study the impacts on learning.
    - Provide evidence of the connection between learning from failures and innovation capabilities.
    - Generate implications for more comprehensive learning from failures, including slip-up failures, emphasizing organizational culture and a dual focus on innovation and improvement [Sitkin et al. 1994, Repenning and Sterman 2001; Adler and Borys 1996, Chandrasekaran et al. 2012].

# 2. Conceptual Development
## 2.1. Product Failures
- Failures are outcomes of errors resulting in departures from anticipated outcomes [Cannon and Edmondson 2005, Dahlin et al. 2018].
- Product failures are distinct from managerial failures in terms of learning mechanisms [Sitkin 1992, Goodman et al. 2011].
    - Managerial failures relate to broader decisions with firm-wide implications [Eggers 2012a, b; Barnett and Freeman 2001; Muehlfeld et al. 2012].
    - Product failures are technical failures conducive to information gathering, investigation, and experimentation [Khanna et al. 2016].
- Product failures are somewhat unavoidable due to increasing complexities [Thirumalai and Sinha 2011, Shah et al. 2017, Ball et al. 2018, Zou et al. 2023].
    - Reactions to product failures can result in experiential learning through organizational efforts [Edmondson 2011, 2023; Dahlin et al. 2018, p. 253].
- Mechanisms for learning include allocating attention to causes [Ocasio 1997, Park et al. 2023] and searching for and implementing solutions [Cyert and March 1963].
- Despite support for learning from product failures [Haunschild and Rhee 2004, Thirumalai and Sinha 2011, Kalaignanam et al. 2013], further studies are needed on different types of failures and conditions conducive to such learning [Wowak and Boone 2015, Dahlin et al. 2018, Park et al. 2023].
- The ability to make sense of errors and handle uncertainty is crucial for deriving lessons from failure experiences [Park et al. 2023].
    - Innovation capabilities can enhance the capability of learning from failures [Greve and Taylor 2000, Maslach 2016].
- We categorize product failures into:
    - **Slip-up failures**: related to the execution of prescribed established processes.
    - **Knowledge gap failures**: related to missing functionalities or malfunctions [Rasmussen 1983, Goodman et al. 2011, Frese and Keith 2015].
- Slip-up failures result from inappropriate application of established rules, while knowledge gap failures result from the nonexistence of complete rules [Reason 2016].
    - They mirror "avoidable errors and the unavoidable outcomes of experiments and risk taking" [Cannon and Edmondson 2005, p. 300].
- Examples of slip-up failures: product contamination due to violation of cleanliness protocols, faulty product due to miscalibration.
- Examples of knowledge gap failures: unexpected interactions of medications, failure of a device to perform its intended function for unclear reasons.

## 2.2. Product Recalls
- **Recalls** are the correction or removal of products following failures [FDA 2020b].
- Recalls for consumer products are common and increasing, with over 850 recall events in the second quarter of 2023.
- Recalls can result in significant direct and indirect costs. Examples include Johnson & Johnson’s 2010 recall of Children’s Tylenol and Pfizer's 2012 recall of birth control pills.
- Recall counts provide longitudinal data for studying learning, and their texts provide insights into reasons for recalls.
- Strategic approach to managing recalls is to resolve the failures and derive lessons for future products and processes [Sitkin 1992, Craig and Thomas 1996].
- Manufacturing processes and product designs are main sources for product recalls [Hora et al. 2011, Shah et al. 2017].
    - **Process-related recalls** originate from shortfalls in process compliance [Ball et al. 2018].
    - **Design-related recalls** originate from development or enhancement of products [Thirumalai and Sinha 2011, Ball et al. 2018].
- Process-related recalls and design-related recalls align with descriptions of slip-up failures and knowledge gap failures.
- We use data on recalls issued by FDA-regulated firms in the medical device and pharmaceutical industries.
    - FDA-related recalls are mostly voluntarily issued by the manufacturer and overseen by the agency [Williams 2010, Nagaich and Sadhna 2015].
    - These are expected to be more authentic firm-choices that lead to learning [Haunschild and Rhee 2004].

# 3. Hypotheses
- We consulted with five practitioners and a subject matter expert from the FDA to develop our conceptual foundations.
- The FDA regulates the manufacture of medical devices and pharmaceuticals (Step 1). Manufacturing firms monitor user reports and gather market intelligence (Step 2).
- When a product failure is detected, a quality management team analyzes it (Step 3) to determine whether the failure is isolated or systematic (Step 4).
    - In the case of an isolated failure (Step 4a), action is taken to resolve it. This is described as single-loop learning [Argyris 1977, Argote and Miron-Spektor 2011].
    - In the case of a systematic failure (Step 4b), the team escalates the incident and recommends a recall [Wowak et al. 2021, Darby et al. 2023].
- If approved, the firm issues the recall in coordination with the FDA (Step 5), which oversees removals and corrections.
- Next, the quality management team searches for root causes and develops ways to reduce chances of failures (Step 6).

## 3.1. Learning from Recalls
- One perspective suggests that failure experiences are more effective at generating learning because they activate managerial attention [Ocasio 1997, Ocasio et al. 2020] and result in more focused searches for solutions [Madsen and Desai 2010, Desai and Madsen 2022].
- An opposing view is that firms do not learn from failure experiences due to myopic reactions, uncertainty, and lack of sustained results [Cannon and Edmondson 2001, Edmondson 2011].
- The contingency view focuses on conditions under which failures may or may not lead to learning [Park et al. 2023].
- Existing research has pointed to the existence of learning from recalls [Thirumalai and Sinha 2011, Kalaignanam et al. 2013].
    - Voluntary recalls have a greater impact than mandatory recalls [Haunschild and Rhee 2004].
- We add contextual specificity by studying learning from recalls, and the contingency role of innovation capabilities on such learning.

### 3.1.1. Learning from Process-Related Recalls
- When the product recall is based on a slip-up (Step 6a(i)), corrections focus on updating standard operating procedures (Step 6a(ii)) and on training and motivating employees (Step 6a(iii)).
- After the recall, the firm may need FDA approval for the changes (Step 6a(iv)).
- Research in quality management emphasizes process control and engagement of employees [Sitkin et al. 1994, Anderson et al. 1994].
- The challenge is combining attention to compliance and efficiency with autonomy and innovation [Adler et al. 1999, Spear and Bowen 1999], preventing deterioration of attention [Anand et al. 2012], and changing organizational cultures [Tucker and Edmondson 2003].
- We posit that issuing and terminating process-related recalls provides impetus for organizational environments [Choo et al. 2007] conducive to developing better processes by bringing attention [Ocasio 1997, Ocasio and Wohlgezogen 2010] to process control.

- *H1a*: Accumulated process-related recalls are negatively associated with subsequent process-related recalls.

### 3.1.2. Learning from Design-Related Recalls
- A design-related recall is based on a defect in an existing product or deficiencies in features (Step 6b(i)).
- Fulfilling the knowledge gap requires searching for knowledge from internal (Step 6b(ii)) and external (Step 6b(iii)) sources, combining it (Step 6b(iv)) into new knowledge to redesign the product (Step 6b(v)).
- The quality management team uses data and drawings (Step 6b(vi)).
- In addition to terminating the recall, the firm needs approvals from the FDA for new designs.
- A design-related recall has a higher level of ambiguity than a process-related recall [Kalaignanam et al. 2013].
- Causal attributions play a crucial role in preventing similar failures [Dahlin et al. 2018, Park et al. 2023].
- Research emphasizes the value of structural mechanisms [Jensen and Szulanski 2007, Maslach et al. 2018, Argote and Guo 2016].

- *H1b*: Accumulated design-related recalls are negatively associated with subsequent design-related recalls.

## 3.2. Enhancement of Learning from Recalls
- The quality management team consults with design teams.
- The teams working on innovations and on recalls share knowledge.
- Firms that do this well look for similar issues and related products.
- The FDA oversees recalls and approvals for new products.
- Innovation capabilities enhance firms’ understanding of product functionalities and improve proficiencies for recovering from failures.
- We propose a role for firm innovation capabilities in developing absorptive capacity for learning from recalls [Cohen and Levinthal 1990].
- We focus on the innovation capabilities of firms [Adler and Borys 1996, Chandrasekaran et al. 2012, Zou et al. 2023].
- Firms’ stocks of innovations and their innovation efforts represent two integral aspects of the innovation capabilities [Cohen and Levinthal 1990, DeCarolis and Deeds 1999].

### 3.2.1. Stocks of Innovations: Accumulated Patents
- Stocks of innovations enhance learning from failures in three ways:
    - Prior innovation experience provides firms with a greater ability to identify causal associations for failures.
    - Stocks of innovations support wider internal searches for solutions.
    - Stocks of innovations help identification of valuable external knowledge [Cohen and Levinthal 1990].
- Patents represent the generation of new processes and products [Katila and Ahuja 2002, Durand et al. 2008].

- *H2*: The negative association of accumulated recalls with subsequent recalls is stronger when there are higher numbers of accumulated patents than when there are fewer accumulated patents.
- Stocks of innovations are an indicator of a firm’s orientation toward experimentation.

- *H2a*: The negative association of accumulated process-related recalls with subsequent process-related recalls is stronger when there are higher numbers of accumulated patents than when there are fewer accumulated patents.
- Accumulated patents provide evidence of innovative actions for developing or using new technologies [Van Den Bosch et al. 1999].

- *H2b*: The negative association of accumulated design-related recalls with subsequent design-related recalls is stronger when there are higher numbers of accumulated patents than when there are fewer accumulated patents.

### 3.2.2. Recent Innovation Efforts: Lagged R&D Intensity
- Finding remedies for failures involves troubleshooting problems.
- Active involvement in R&D implies a greater focus on integrating information from diverse sources [Teece et al. 1999].

- *H3*: The negative association of accumulated recalls with subsequent recalls is stronger when there is higher lagged R&D intensity than when there is lower lagged R&D intensity.

- *H3a*: The negative association of accumulated process-related recalls with subsequent process-related recalls is stronger when there is higher lagged R&D intensity than when there is lower lagged R&D intensity.
- Design-related recalls call for actions that are close to R&D efforts.

- *H3b*: The negative association of accumulated design-related recalls with subsequent design-related recalls is stronger when there is higher lagged R&D intensity than when there is lower lagged R&D intensity.

# 4. Data
## 4.1. Sample
- We test our hypotheses using data on recalls in the U.S. medical device and pharmaceutical industries during 2000–2016.
- All recalls in our data set are voluntary recalls.
- FDA classifies recalls into three classes (I to III).
    - We removed Class III recalls.
- We compiled and cross-validated the data on recalls using the FDA recalls database and the FDA’s Significantly Regulated Organizations (SROs) list.
- We selected firms listed in the U.S. stock exchanges, and removed firms that did not exist independently for at least 10 years due to mergers and acquisitions (M&As).
- We deleted firms that did not have at least 30 recalls during 2000–2016.
- Our final sample consists of 1,728 firm-years (108 firms over 16 years) for 53 medical devices and 55 pharmaceutical publicly traded firms in the United States, with 7,984 recalls.
- We collected product approvals from the FDA database, patent data from the USPTO, and financial data from the WRDS database.

## 4.2. Classification of Recalls
- We classified recalls based on the text of the description for each recall using a combination of manual and automatic text analysis.
    - This resulted in 33% process-related and 67% design-related recalls in medical devices and 59% process-related and 41% design-related recalls in pharmaceuticals.
- Cross-validating our classification with the self-classification by firms resulted in a 95% match.

## 4.3. Operationalization of Variables
### 4.3.1. Dependent Variables
- Subsequent Recalls: measures the total number of recalls in a firm-year.
- We divide Subsequent Recalls into Subsequent Process-related Recalls and Subsequent Design-related Recalls.

### 4.3.2. Independent Variables
- Accumulated Recalls: cumulative count of recalls issued by the firm until the prior year (t - 1).
- Accumulated Patents: the stock of innovations that a firm has accumulated prior to a specific year t starting from the year (t - 10).
- Lagged R&D Intensity: R&D investment as a percentage of net revenue of the firm for the preceding year (t - 1).

### 4.3.3. Control Variables
- Lagged Revenue: the prior year’s net revenue of a firm.
- Lagged Profitability: a firm’s one-year lagged net operational profitability as a percentage of net revenue.
- Lagged Product Approvals: a firm’s one-year lagged total number of new products as reported in the FDA database.
- Industry: identifies whether each firm is in pharmaceutical or medical devices.
- Firm: identifies each firm.
- Year: controls for year fixed effects.

# 5. Analyses and Results
- Descriptive statistics and correlations are shown in Table 1.
- Subsequent recalls, subsequent process-related recalls, and subsequent design-related recalls are negatively correlated with accumulated recalls, accumulated process-related recalls, and accumulated design-related recalls, respectively.
- Subsequent recalls has positive correlations with accumulated patents and lagged R&D intensity.
- We estimate negative binomial generalized linear models (NB-GLMs).
- We check for serial correlations using the Breusch-Godfrey test and the Durbin-Watson test.
- We perform all model estimations with correction for serial correlation using the tsglm() function.
- Based on a significant result for a Hausman test, we used fixed effects of firms in the models.

## 5.1. Learning from Recalls (Baseline Premise in Lieu of Hypothesis 1) and Enhancing Effects of Accumulated Patents (Hypothesis 2) and Lagged R&D Intensity (Hypothesis 3)
- In Table 2, we present the results for the NB-GLM for learning from all recalls.
- Accumulated recalls is significantly and negatively associated with subsequent recalls (b = -0.76, p < 0.001).
    - This supports the learning effect established in the extant literature.
- Accumulated patents (b = 0.44, p < 0.001) and lagged R&D intensity (b = 0.37, p < 0.001) have significant positive associations with subsequent recalls.
- The interaction of accumulated recalls with accumulated patents is significant and negative (b = -0.13, p < 0.001).
    - This supports *H2*, indicating greater learning from accumulated recall experience for firms with greater stocks of innovations.
- The interaction of lagged R&D intensity with accumulated recalls is also significant and negative (b = -0.28, p < 0.001).
    - This supports *H3*, stating that firms making greater innovation efforts learn better from recall experience.

## 5.2. Learning from Process-Related Recalls (Hypothesis 1a) and Enhancing Effects of Accumulated Patents (Hypothesis 2a) and Lagged R&D Intensity (Hypothesis 3a)
- In Table 3, we present the results for the NB-GLM for learning from process-related recalls.
- The parameter estimate for accumulated process-related recalls is negative but only marginally significant (b = -0.16, p < 0.10).
    - Thus, we do not find support for the learning-related negative association between accumulated process-related recalls and subsequent process-related recalls (*H1a*).
- The interaction of accumulated process-related recalls and accumulated patents is negative and significant (b = -0.11, p < 0.001).
    - This supports *H2a*.
- The interaction of lagged R&D intensity and accumulated process-related recalls is not significant.
    - Therefore, we do not find empirical support for *H3a*.

## 5.3. Learning from Design-Related Recalls (Hypothesis 1b) and Enhancing Effects of Accumulated Patents (Hypothesis 2b) and Lagged R&D Intensity (Hypothesis 3b)
- Table 4 shows the results for the NB-GLM for learning from design-related recalls.
- Accumulated design-related recalls is significantly and negatively associated with subsequent design-related recalls (b = -0.19, p < 0.001).
    - This result supports *H1b*, which posits that experience with accumulated design-related recalls results in learning.
- The two interactions of accumulated patents and accumulated design-related recalls and lagged R&D intensity and accumulated design-related recalls have significant and negative effects (b = -0.11, p < 0.001) and (b = -0. 30, p < 0.001) on subsequent design-related recalls.
    - These results support *H2b* and *H3b*.

## 5.4. Summary of Results
- We summarize our results in Table 5.
- The interaction-effects results support our assertion that stocks of innovations (accumulated patents) and recent innovation efforts (lagged R&D intensity) enhance learning from product failure experience (accumulated recalls) and reduce the likelihood of subsequent recalls.
- Although the main effects strongly support the existence of learning from knowledge gap failures (accumulated design-related recalls), they do not support the existence of learning from slip-up failures (accumulated process-related recalls).

## 5.5. Additional Tests for Learning and Alternative Explanations
- We explore three events other than product recalls that can also provoke attention, driving learning, and leading to improvements in product performance:
    - public litigation
    - adverse events
    - FDA-issued warning letters.
- We create Litigation as a binary categorical variable coded as 1 if a firm experienced product-related litigation.
    - The integration of litigation with accumulated recalls is marginally significant (b = -0.03, p < 0.10). These results suggest that product-related litigations do focus organizational attention.
- We estimate separate models for counts of serious and not serious adverse events.
    - Lagged serious adverse events have a positive significant association with subsequent recalls.
    - The interaction of lagged serious adverse events with accumulated patents is significant (b = -0.08, p < 0.05).
- We use a categorical variable Warning Letter to indicate if a firm experienced an FDA warning related to shortcomings in products and processes.
    - Results suggest that warning letters focus managerial attention and improve subsequent performance (b = -0.11, p < 0.01).
- Overall, the additional tests provide supporting evidence for the attention focusing mechanism of failures.
- We explore if the learning is driven by firms' inherent capabilities by estimating models of cross-learnings, examining whether accumulated design-related recalls are associated with subsequent process-related recalls and vice versa.
    - Results indicate a marginally significant cross-learning effect of accumulated design-related recalls on subsequent process-related recalls (b = -0.14, p < 0.10).

## 5.6. Supplementary Analyses
### 5.6.1. Robustness Checks
- We perform two supplementary analyses to check the robustness of our results against endogeneity and nonindependent observations:
    - We use an instrumental variable (IV) regression estimate to instrument the potential endogenous explanatory variable lagged R&D intensity.
    - We use a generalized estimating equation (GEE) with exchangeable variance structure within firms to check for robustness against the possible non-IID structure of the observed data.
- Both these results confirm the robustness of the original results.

### 5.6.2. Post Hoc Analysis
- We conduct a post hoc analysis that considers different periods for accumulating past recalls and patents.
- Although learning from accumulated design-related recalls appears to take more time to accrue, the lessons appear to persist.
- In contrast, process-related learning may accrue more quickly but can deteriorate.

# 6. Discussion
- Our objectives were to differentiate between product failures attributed to slip-ups and knowledge gaps, study organizational learning from each, and study the impacts of firms' innovation capabilities on learning from failures.
- Results indicate that firms learn from failures to reduce subsequent failures, and this learning is greater for firms with stronger innovation capabilities.

## 6.1. Theoretical Implications
- Our research extends the theory of organizational learning from failures by explaining the role of the context for learning:
    - sources of failures
    - innovation capabilities of firms
- Different organizational mechanisms for learning are activated by slip-up failures versus knowledge gap failures.
    - Cultural mechanisms apply to a greater extent for slip-up failures.
    - Structural mechanisms apply to a greater extent for knowledge gap failures [Popper and Lipshitz 1998].
- The results of our post hoc analysis offer additional implications for learning from the two sources of failures.
    - Learning from slip-up failures is quick to be realized, it is also quick to deteriorate.
    - deliberate efforts to refocus attention on process compliance is needed [Ocasio and Wohlgezogen 2010, Anand et al. 2012]
- The second contingency factor is the innovation capabilities of the firm.
- We invoke absorptive capacity theory.
    - This study, to the best of our knowledge, is the first to demonstrate the role of absorptive capacity in organizational learning from failures.
- Absorptive capacity helps to discern opportunities for exploring new knowledge from failure occurrences, and through a greater capability to assimilate and sustain the knowledge [Martini et al. 2017].

## 6.2. Practical Implications
- Our categorization would readily apply to failures in other regulated industries as well.
- In our analysis, slip-up failures did not result in improving the performance of firms.
    - maintaining continuous attention to compliance is needed [Repenning and Sterman 2001, Anand et al. 2012]
- By turning prior failures into sources of knowledge, they can reduce the potential for subsequent failures.
- We need to encourage innovation orientation which has inbuilt mechanisms for learning from failures [Greve and Taylor 2000].

## 6.3. Limitations
- We conducted our empirical analyses on data from two industries that are highly dependent on innovation.
- The findings are most applicable to firms in industries that more strongly depend on innovation.
- Other limitations of our research derive from the secondary nature of data used to test our hypotheses.

---

# Executive summary of 1. Introduction
- This section introduces the study's focus on how firms learn from failures, distinguishing between slip-up and knowledge gap failures.
- It highlights the **theoretical foundations** in organizational learning and absorptive capacity.
- The section sets the stage by explaining the categorization of failures and the role of innovation capabilities in enhancing learning from product recalls, using examples to illustrate the concepts.

# Executive summary of 2. Conceptual Development
- This section defines product failures and differentiates them from managerial failures.
- It categorizes product failures into **slip-up** and **knowledge gap failures**, providing examples.
- The section explains the context of product recalls and their significance, linking them to the two categories of failures.

# Executive summary of 3. Hypotheses
- This section develops the **hypotheses** based on organizational learning and absorptive capacity theories.
- It describes the model and the practical steps involved in product failures and recalls.
- The section presents detailed arguments for each hypothesis, including the expected relationships between accumulated recalls, innovation capabilities (patents and R&D intensity), and subsequent recalls.

# Executive summary of 4. Data
- This section describes the data used for the study, focusing on the sample selection criteria and data sources.
- It details the classification of recalls into process-related and design-related categories, and the operationalization of variables such as accumulated recalls, patents, R&D intensity, and control variables.

# Executive summary of 5. Analyses and Results
- This section presents the results of the empirical analyses.
- It explains the statistical models used (NB-GLMs) and the steps taken to ensure robustness.
- The section summarizes the findings for each hypothesis, including the main effects of accumulated recalls and the moderating effects of innovation capabilities.
- It includes additional tests for alternative explanations and a post hoc analysis of the persistence of learning over time.

# Executive summary of 6. Discussion
- This section discusses the **theoretical and practical implications** of the findings.
- It extends organizational learning theory by explaining the role of context (failure source and innovation capabilities).
- The section offers practical implications for firms, highlighting the importance of addressing both knowledge gap and slip-up failures, and building innovation-focused cultures.
- It also acknowledges the limitations of the study and suggests directions for future research.

</details>


```
title: Sustainable process improvements: Evidence from intervention-based research
authors: Gopesh Anand, Aravind Chandrasekaran, Luv Sharma
journal: J Oper Manag
published: 2021
```
 
# Executive Summary

This research investigated how to make lasting improvements (Process Improvement; PI) in healthcare processes, specifically focusing on patient education after kidney transplants. Researchers worked closely with a U.S. hospital over three years to redesign this education process and observed what made the improvements stick.
 
*   **What They Actually Did (The Intervention - Step-by-Step):**
    *   **Observed the Existing Process:** Researchers shadowed nurses and other staff as they educated kidney transplant patients, taking detailed notes on what was done and how.
    *   **Identified Problems:** They analyzed their notes and interviewed staff to pinpoint issues in the education process. These included:
        *   Inconsistent information given to patients.
        *   Rushing through important details.
        *   Not checking if patients understood the instructions.
    *   **Redesigned the Education:** Based on the identified problems, they worked with the hospital staff to create a new, standardized education program. This involved:
        *   Creating a checklist of essential information to cover.
        *   Developing clear and easy-to-understand materials for patients.
        *   Implementing a "teach-back" method, where patients repeat the instructions to ensure they understand.
        *   Dividing education into two parts: one during the hospital stay, and one shortly after discharge.
    *   **Got Patient Input:** They held focus groups with patients to get their feedback on the redesigned education process. This feedback was used to make further adjustments, such as providing both visual aids and detailed explanations to accommodate different learning styles.
    *   **Implemented Daily "Huddles":** Short daily meetings were established where staff could discuss any problems they were encountering with the new education process, share best practices, and suggest further improvements.
    *   **Involved Middle Managers:** Nurse managers were made responsible for supporting the new education process and ensuring it was being followed consistently. They also acted as a link between the frontline staff and senior hospital leadership.
 
*   **Key Ideas They Used:**
    *   **Organizational Learning Theory:** Organizations improve by learning and adapting. Encouraged staff to reflect, identify areas for improvement, and implement changes.
    *   **Operational and Change Routines:**
        *   **Operational Routines:** Day-to-day procedures (patient education process before redesign).
        *   **Change Routines:** Processes to improve operational routines (e.g., daily huddles).

- **Key Findings**:
    - Sustained process improvements at the hospital were observed after the intervention.
    - Improvements in patient health outcomes (reduced 30-day readmission) and satisfaction levels were found to be associated with the redesign. The likelihood of 30-day readmission is 0.25 times lower and the HCAHPS score is about 8% higher for the treatment group as compared to the control group.
    - The role of middle managers to be vital in sustaining PI over time.
    - Highlighting a pre-redesign discovery phase to gain employee buy-in, parallel patient-caregiver input for adaptive design, middle management for sustaining change, and IBR for ill-structured problems are found as the key factors for sustainable PI.
 
#healthcare_operations #intervention_based_research #organizational_learning #process_improvements #difference_in_difference #high_interaction_service_environments #patient_education #kidney_transplant #sustainability #standardization #adaptation #middle_management

<details>
	
  <summary>Click to expand sections</summary>

# 1. Introduction
- Process improvements (PI) aim to redesign work processes with the involvement of frontline employees [Nair, 2006; Tucker, 2007], which has been linked to better firm performance [Jacobs, Swink, & Linderman, 2015; Zhang & Xia, 2013].
- However, there is a shortage of studies on **sustaining PI** over time [Collins & Browning, 2019; Morrison, 2013], which is crucial to prevent performance backsliding and ensure continuous employee engagement [De Treville & Antonakis, 2006; Tucker, Nembhard, & Edmondson, 2007].
- In healthcare, while the importance of PI is recognized [Ding, 2015; Dobrzykowski, McFadden, & Vonderembse, 2016], sustaining PI is challenging in high interaction service contexts [Chase & Apte, 2007] due to the heterogeneity of patients and caregivers, and the active participation of patients and their families [Damali, Miller, Fredendall, Moore, & Dye, 2016; Tucker & Edmondson, 2003].
- *Research Question*: How to sustain PI in the context of healthcare delivery?
    - To address this question, the study focused on redesigning a process for educating kidney transplant patients with post-surgical care instructions.
    - *Refined Research Question*: How to redesign a process for educating kidney transplant patients and for sustaining the improvements from the redesign over time?
        - This process involves high patient-caregiver interaction and active patient participation, leading to potential variability, uncertainty, and complexity [Gordon, Prohaska, Gallant, & Siminoff, 2009].
        - Patients' varying needs, attention levels, and understanding [Dahl, Engebretsen, Andersen, Urstad, & Wahl, 2019], and lapses in following instructions can lead to complications [Dew et al., 2007].
- The intervention for process redesign involved working with 32 caregivers at a large teaching hospital in the United States over 3 years (2013–2016).
    - The redesign aimed to include standardization and adaptation [Adler, Goldoftas, & Levine, 1999; Ansari, Reinecke, & Spaan, 2014] in patient education for post-surgical care, ensuring sustainment over time [Anand, Gray, & Siemsen, 2012; Turner & Rindova, 2012].
- The lessons learned during the process redesign were synthesized using an **intervention-based research (IBR)** framework [Oliva, 2019]. Challenges encountered while applying organizational learning theory prompted adjustments to the theory in practice.
    - The adjustments reflect updating or refining existing theories as applied to such problems under the IBR framework [Oliva, 2019].
- The intervention demonstrated that the dual needs for standardization and adaptation in high interaction service environments [Berry Jaeker & Tucker, 2020; Catena, Dopson, & Holweg, 2020] can be met by building appropriate control and flexibility in service delivery processes [Nissinboim & Naveh, 2018; Sousa & da Silveira, 2019].
    - Empowering frontline employees with direction and autonomy increases their engagement [Nissinboim & Naveh, 2018; Sousa & da Silveira, 2019], and the role of middle managers is vital for sustaining PI.
- The impact of the redesign was assessed using data from 702 transplant patients, including a control group, using a **difference-in-difference (DID)** approach to evaluate the impact on 30-day readmission and patient satisfaction.
    - **30-day readmission**: Indicates whether a patient was admitted to any hospital within 30 days after discharge and is commonly used to assess the quality of care delivered in hospitals [Senot et al. 2016a].
    - **Patient Satisfaction**: Measured using the “Hospital Consumer Assessment of Healthcare Providers and Systems” (HCAHPS) survey, a recognized measure of patients' perceptions [Sharma, Chandrasekaran, Boyer, & McDermott, 2016].
- Results showed improved outcomes, with a lower likelihood of 30-day readmission (0.25 times lower) and higher HCAHPS scores (about 8% higher) for the treatment group.
    - Sustained use of the redesigned work process was observed more than 1 year post-redesign.
    - Continuing evidence of sustaining PI was seen in the form of daily problem-solving through huddles, involving frontline employees and middle management.
 
# 2. Sustainment of PI and Organizational Learning Theory
- This research aims to implement sustainable PI in the high interaction service context of healthcare delivery.
- While there is substantial research associating organizational PI initiatives with performance (reviewed in Song and Tucker [2016]), only a few recent studies in healthcare operations management have focused on the impact of a process change on the outcomes of the process.
- Studies on process changes in healthcare were classified based on:
    - Whether researchers followed implementation concurrently or retrospectively.
        - Embedding researchers allows for closer study of challenges and measures taken.
    - Whether outcomes referred to patient outcomes.
        - Patient outcomes are more closely related to process changes than overall hospital performance measures.
    - Whether the study covered sustainment of results over some period.
- Bavafa et al. [2018] observed that e-visits were associated with more office visits, mixed results on phone visits and patient health outcomes, and a reduction in caregivers' accepting new patients. This was a one-time change without caregiver involvement.
- Song et al. [2017] found that switching from private to public disclosing of relative performance feedback (RPF) for physicians was associated with increased physician productivity, sustained for about a year. This study excluded information on physician involvement.
- Staats et al. [2017] assessed the effects of deploying and then discontinuing an electronic monitoring system for handwashing compliance, showing an increase in compliance followed by a decline after discontinuation.
- Unlike the previous studies, Tucker and Singer [2015] used a planned intervention approach with a management by walking around (MBWA) program, but did not report on any follow-up assessment. Overall, they found MBWA implementations to have a negative impact on quality improvement efforts.
- Distinguishing from these studies, this research implements a process redesign, applies theory, assesses sustainment of results, and offers lessons from putting the theory in practice [Edmondson, 1996; Holmström, Ketokivi, & Hameri, 2009].
    - This research is unique in that it intervenes in a process redesign, focuses on sustainment of PI, offers refinements to theoretical concepts, and assesses the impact on process effectiveness and customer satisfaction.
- Organizational learning theory [Argote & Miron-Spektor, 2011; Argyris & Schön, 1978] is applied to implement PI in healthcare, leveraging employee knowledge to improve operations [Hayes, Wheelwright, and Clark, 1988; Leonard-Barton, 1992].
    - In healthcare operations, involving frontline employees is especially helpful to increase their confidence [Nembhard & Tucker, 2011; Tucker et al., 2007] and to facilitate their use of standardization and adaptation [Berry Jaeker & Tucker, 2020; Catena et al., 2020].
- While organizational learning theory has been used to study experience [Kc, Staats, & Gino, 2013; Pisano, Bohmer, & Edmondson, 2001] and PI [Chandrasekaran, Senot, & Boyer, 2012; Ding, 2014; Roth, Tucker, Venkataraman, & Chilingerian, 2019; Song & Tucker, 2016], it has been unable to address sustainment [Morrison, 2013; Staats et al., 2017].
    - This limitation is addressed by maintaining recurring cycles of organizational learning [Anand, Ward, & Tatikonda, 2010; Nonaka & Von Krogh, 2009], establishing a system for learning how to learn [Argyris & Schön, 1978] that goes beyond reactionary process changes [Tucker, Zheng, Gardner, & Bohn, 2020].
- Caregivers establish standardized ways of working (**operational routines**) [Nelson & Winter, 1982], including adaptation-in-use (**flexibility**) [Feldman & Pentland, 2003].
    - These routines serve as baselines when implementing **change routines** [Anand et al., 2009]. The intervention includes routines for root-cause analyses and for sustaining the search for improvements [Desai, 2020].
- The study uses an IBR approach [Edmondson, 1996; Oliva, 2019], applying an existing theory to a complex problem [Groop, Ketokivi, Gupta, & Holmström, 2017; Simon, 1973] and refining the theory based on lessons learned [Argyris, 1996]. The process redesign and theory refinements are mapped to the components of the IBR framework [Oliva, 2019].
 
# 3. Context and Methods
- The research site, Alpha, is a large multispecialty teaching hospital in the United States.
- Ensuring patients understand their care after surgery, especially after transplants, is critical [Dahl et al., 2019].
    - Failure to provide proper patient education is a major cause of preventable readmissions [Regalbuto, Maurer, Chapel, Mendez, & Shaffer, 2014].
- Alpha was experiencing issues with its patient education processes for post-surgical care. Past PI efforts had failed to sustain.
- Top management approached a coauthor to work on redesigning the education process for transplant patients, aiming to improve it in a sustainable way. This redesign would serve as a template for an organization-wide PI initiative.
- The kidney transplant unit was selected, conducting approximately 200 kidney transplants per year with a 30-day readmission rate of 35%.
- Figure 1 shows an overview of the patient education process before the redesign, highlighting problems such as variation in instructions, lack of handoffs, rushing through instructions, and inconsistencies in educational aids.
- Figure 2 presents the timeline of the intervention, including pre-redesign and post-redesign phases.
- The project team consisted of hospital representatives (chief quality and patient safety officer, physician director, and nursing director) and the academic research team.
- The research team interacted with personnel at the kidney transplant unit (inpatient nurses, outpatient nurses, nurse managers, transplant surgeons, and nephrologists), as well as social workers, patient council staff, and IT administrators.
- Input was also solicited from 15 kidney transplant patients.
- Table 2 outlines the data collected (field observations, surveys, process maps, patient data, etc.) between January 2013 and April 2016.
- From January 2013 to June 2014 (**pre-redesign period**), the research team observed the existing patient education process and gathered information from caregivers and patients.
    - Teams of two researchers shadowed caregivers delivering post-surgical care education for 15 patients, taking detailed notes.
    - 34 caregivers were interviewed to get their descriptions of the patient education process, lasting about an hour each.
- The notes and transcriptions served as input for six workshops with caregivers during the **redesign period** (July 2014 to April 2015).
    - These workshops reported findings from patient-education shadowing and discussed areas of variation.
    - Value stream mapping was conducted in two workshops, drawing current and envisioned future states.
- In parallel, two focus groups were conducted with six patients each, who had received transplants within the past 6 months, to get their inputs on the education.
    - These groups were diverse and included family members.
    - The focus groups were structured using questions listed in Appendix A.
    - The 12 patients also responded to a survey related to their learning styles (Appendix B).
    - Further, data was collected from 87 kidney transplant patients using a structured questionnaire and open-ended questions.
- In the **post-redesign period** (May 2015–April 2016), frontline nurses designed and implemented a system of problem-solving through daily (inpatient) and weekly (outpatient) huddles.
    - These huddles were short meetings used to address problems, share best practices, identify areas for PI, and track efforts for PI [Provost, Lanham, Leykum, McDaniel Jr, & Pugh, 2015].
- Table 3 summarizes the qualitative data collection components (interviews, workshops, and patient focus groups).
 
# 4. Problem Situation (S), Methodology (M), and Theory Refinements (T)
- The redesigned process, resulting from caregiver engagement to improve patient education for post-surgical care, is shown in Figure 3.
- The redesign is described using the mode of “interventions as a source of theory” [Oliva, 2019], focusing on a real-world problem situation (S), solved through a methodology (M) that was developed while refining an existing theory (T).
    - **S**: Implementing sustainable PI through redesign of a patient education process for post-surgical care.
    - **T**: Organizational learning theory, refined while addressing the problem.
    - **M**: Working with caregivers and patients to understand challenges, developing and testing countermeasures, resulting in refinements to the theory.
- The application of the IBR framework is consistent with abductive reasoning and context-theory-context iterations [Chandrasekaran, de Treville, & Browning, 2020].
    - Challenges encountered when applying organizational learning theory questioned current understanding, leading to the development of specific directions, or countermeasures, to overcome these challenges.
- The intervention for the process redesign focuses on four specific challenges and countermeasures, summarized in Table 4.
 
## 4.1. Initiation of redesign and engagement in PI
- Initially, surgeons and nurses showed little interest in redesigning the patient education process or PI in general.
    - Caregivers expressed skepticism about the need for assessment or redesign.
    - Middle management employees believed their processes could not benefit from external applications.
- The research team found that highlighting organizations such as Toyota, Cleveland Clinic, and Thedacare, and relating their success stories, only compounded employee resistance.
- Research on organizational learning theory recognizes that PI efforts take time and sustained employee engagement can be challenging [Gowen III et al., 2006].
    - Employees can be resistant to change [Kirkman & Shapiro, 2001] or their interests may drop off, especially if there are significant efforts required with little or no benefit [Nembhard & Edmondson, 2006; Siemsen, Balasubramanian, & Roth, 2007].
- Sustained engagement is critical for PI [Repenning and Sterman (2001)]. Top management support has been shown as one route for engaging employees, yet it has not proved to be enough [Chandrasekaran & Toussaint, 2019].
- To address this challenge, the caregiving team was convinced to participate in a discovery phase to study the existing process.
- Statistical analysis of quantitative data and qualitative analysis from interviews indicated that high variation in the existing process was associated with greater patient anxiety levels.
    - These findings, published in a healthcare journal [Chandrasekaran et al., 2016], generated awareness among the caregivers of the need to redesign the process.
- This discovery phase helped overcome employee skepticism and generated excitement for PI.
    - Having participated in the examination of the process, caregivers were convinced of the need for rationalizing the process.
- Showing the opportunities in their own context through a pre-redesign discovery study is effective in combatting the challenge of engaging employees in PI.
    - This countermeasure identifies a refinement to organizational learning theory.
 
## 4.2. Initial performance deterioration
- PI efforts often result in an initial decline in performance [Adler and Clark, 1991; Netland and Ferdows, 2016; Nembhard and Tucker, 2011] due to unplanned variations and increased cognitive workload.
- This decline poses a challenge for the idea of PI, leading to employee rejection of such efforts [Repenning & Sterman, 2002] and failure to fully realize potential improvements [Jasperson, Carter, & Zmud, 2005].
- Keeping employees engaged can help keep them informed about the redesign and can lead to watchful use of the changing process [Adler et al., 1999; Kolb, 1981].
    - The healthcare team's participation in the redesign period (July 2014 to April 2015) was organized.
- During caregiver workshops, participants noted that patients were overwhelmed by the amount of instructions and recognized the lack of consistency among caregiving team members.
- The caregiving team adopted a two-part patient-education approach (Table 5), with inpatient nurses delivering essential education during the hospital stay (Part I) and outpatient nurses delivering the rest after discharge (Part II).
    - Outpatient nurses are now required to meet the patient and the inpatient coordinators prior to discharge.
- Employee engagement in the redesign resulted in specific insights about the patient education process.
- Frontline caregivers were able to better take care of the teething problems, separating such problems from ones that needed further investigation for improving the process.
- Participating in redesigning the process also allowed them to build in some adaptation-opportunities, along with standardized practices, which turned out to be useful in mitigating the effects of unplanned variations.
- These aspects of the implementation minimized initial disruptions and derived lessons from the initial use of the redesigned process.
- Frontline employee involvement from the start of the process redesign appears to be an effective countermeasure for avoiding the tradeoff between customer satisfaction and process effectiveness.
 
## 4.3. Adaptation in use
- Processes should have meaningful adherence requirements [Johnson, Burgess, & Sethi, 2020] and not be overly restrictive [Ansari et al., 2014].
    - Employees may not adopt overly restrictive processes [Nissinboim & Naveh, 2018].
- The duality of standardization and adaptation can be addressed by designing a process with ostensive aspects and performative aspects [Anand et al., 2012].
- Developments in the marketing literature have advocated incorporating the role of the customer [Auh, Menguc, Katsikeas, & Jung, 2019; Bendapudi & Leone, 2003].
- Applying this perspective, input was sought from patients to build in adaptation in use by frontline caregivers for standardized practices.
- Patient input is important [Catena et al., 2020], but there is little insight on how to effectively do this in a healthcare setting.
- Facilitator-led focus groups of patients were conducted separately but in parallel with the caregiver workshops.
    - This enabled an interchange of ideas while maintaining separation to protect privacy.
    - This combination helped consider aspects that warranted either more standardization or more adaptability-in-use for caregivers.
- The first patient focus group meeting was used to adjust the topics included in the two parts of the patient education work (Table 5).
    - Patient feedback was used primarily to inform sequencing.
    - Patients also pointed to the need for multiple teaching styles.
- In the second focus group, patients discussed outcomes and instruction delivery aspects and helped resolve inconsistencies.
- Table 6 gives illustrative examples of the changes in the patient education process that resulted from the redesign.
- Having patient input in parallel provided insights that were not otherwise considered by the caregiving team and helped build in some adaptation to accommodate variations.
- Data from the focus groups suggested that some patients preferred visuals while others preferred detailed explanations.
- Information from the learning styles questionnaire indicated the importance of several learning styles needing to be integrated into patient education.
- Having patient feedback through parallel workshops provided useful insights into improvement opportunities and enabled the caregivers to design in adaptability for reacting to the varying needs of the patients.
 
## 4.4. Sustainability of redesign and engagement in PI
- Adherence to established processes is challenging to sustain [Anand et al., 2012; Desai, 2020], especially in high interaction service settings [Ton & Huckman, 2008]. Even after caregivers established handwashing routines under monitoring, compliance rates dropped soon after monitoring was discontinued [Staats et al. 2017].
- Sustained engagement in PI is also a challenge [Adler et al., 1999; Spear & Bowen, 1999].
- Middle managers (nursing managers) could play a critical role in sustaining the new process and the initiative for PI [Sitkin, 2020].
- The institutionalization of a system of huddles helped achieve the objective of defining responsibilities and creating a system and structure for involvement of middle managers in PI. The huddles also ensure seamless handoffs [Prætorius & Hasle, 2019]. Examples of topics discussed in these huddles are:
    - Patient's aptitude on post-surgical care instructions
    - Staffing levels in the unit for that day
    - Hand-off process between inpatient and outpatient unit
    - Improvements to teaching process
    - Celebrations and other news
- The nurse manager from the inpatient unit said, “At first, we were skeptical with the role of daily meetings, we have had this in the past and it failed to sustain. However, given the systematic effort of involving me and my staff from the beginning, I felt it was my role to motivate and keep this improvement process going on over time. We have great buy-in from our day and night shift nurses given their early involvement and they own it within the unit.”
- Outpatient and inpatient nurse managers made this a part of their routines.
    - They created a structure to facilitate the huddles, monitored them, and offered problem-solving support.
- This gave rise to a weekly meeting between the inpatient and outpatient manager that the nursing director also attended.
- The use of these huddles organically created a tiered management structure that now facilitates escalation of frontline issues to middle management and senior management as appropriate.
- This system is helping sustain adherence and exploration of PI in the redesigned patient education process.
    - It has also propagated the practice of huddles in other areas of the hospital, thus helping sustain process-adherence and engagement in PI.
- Building in adaptation-in-use for processes, helped with frontline employee engagement in the organizational initiative for PI.
- The huddles that began in May 2015 were still being regularly conducted in the unit as of May 2019.
- The nurse managers were pivotal in ensuring that frontline nurses had the necessary resources and met with the nursing director on a weekly basis.
- This research demonstrates that PI can sustain over time even in a high interaction service environment.
- The countermeasures for sustaining PI, designed inductively, working with the caregivers, also resulted in refinements to organizational learning theory (Table 4).
- Formalizing these refinements would require testing these countermeasures in different contexts [Groop et al., 2017].
 
# 5. Efficacy of the Process Redesign
- To legitimize the role of the intervention as a template for process redesign and organization wide initiatives for PI, it was critical to assess the efficacy of the redesign.
- The impact of redesigning the patient education process was examined by comparing 30-day patient readmission rates before and after the redesign with respect to a control group.
- Patients' perceptions of care in the kidney transplant unit before and after the implementation of the redesign was also compared with respect to the same control group.
    - In healthcare contexts, especially those that involve high patient–caregiver interactions, patients' perceptions of care are not always compatible with their health outcomes [Berry Jaeker & Tucker, 2020; Kang, Shah, & Dowd, 2017].
    - Patients who report higher satisfaction levels would better absorb and use information about post-surgery care [Nair, Nicolae, & Narasimhan, 2013; Sharma, Chandrasekaran, & Bendoly, 2020].
- The impact of the process redesign was analyzed by comparing before and after scores on the HCAHPS (Centers for Medicare and Medicaid Services (CMS), 2019) for the kidney transplant unit, and with respect to a control group.
- During January 2013–April 2016, 571 patients underwent kidney transplants at Alpha (treatment group).
- This patient mix was compared with the population of all kidney transplants conducted in the United States during the study period using data obtained from the National Kidney Foundation.
    - The average readmission rate for the sample was 40%, which is not significantly different from the national average. Differences in other statistics also were nonsignificant.
- Performance data was collected from two other transplant units (heart and liver) within the hospital (control sample) after adjusting for patient and other transplant characteristics.
    - Heart and liver transplant patients were getting similar post-surgery care instructions as kidney transplant recipients.
    - The heart and liver transplant units only experienced routine hospital-wide quality and safety efforts and did not make other changes.
- Patient and process data was collected for the 62 heart and 103 liver transplant patients conducted in the same period.
    - Having a control sample from the same hospital provides a strong test of the effectiveness of the process redesign.
- Appendix C contains descriptions of the variables used, Appendix D gives the means and standard deviations, and Appendix E shows correlations.
- The process redesign constituted a quasi-experimental treatment that resulted in an exogenous shock to the patient education process beginning in January 2014.
- A DID approach [Imbens & Wooldridge, 2009] was used to estimate the causal effects of the redesign, accounting for differences in baseline 30-day readmission rates and HCAHPS scores across the two transplant groups and mitigating the effects of any changes other than the process redesign.
- The necessary conditions were checked including parallel trends before conducting a DID analysis (Appendix F).
 
## 5.1. 30-day readmission
- The post-redesign change in patient outcomes was investigated by examining whether a transplant patient got readmitted to the hospital within 30 days in the treatment and control groups.
- The following relationship was estimated at the patient level for comparing redesign (redesign or post-redesign) and pre-redesign readmissions:
 
    *logit Readmissionð Þi,j,t = β0
    + β1 Type pij + β2 Phase S = Redesign or Post
    + β3 Type pij*Phase S = Redesign or Post + θjt + Tαj + δ X ijt:*
 
    - i: indexes each patient
    - j: indexes each transplant type
    - t: indexes time in year and quarter periods
    - Typepij: captures the type of transplant patient
    - PhaseS = Redesign or Post: represents periods of the redesign compared to the Pre-redesign period
    - Typepij * PhaseS = Redesign or Post: captures interaction between transplant type and time
    - Tαj: represents time-invariant transplant controls
    - Xijt: represents patient-level controls
    - θjt: represents time-variant process controls for the transplant groups
 
- Logistic regression with robust standard errors [Wooldridge, 2002] was used and the treatment group dummy variable was interacted with the treatment periods to investigate readmission trends.
- As a verification check, the diff command in STATA 14 was used and controlled for all covariates (Appendix D).
- The variance inflation factor (VIF) in all the models was well below the threshold value of 10 [Hair, Anderson, Babin, & Black, 2010].
- Table 7 shows the results of this analysis.
    - Models 1 and 2 show the likelihood of patient readmission in the redesign and post-redesign periods
    - These regressions compared the patient education process of the treatment group with the control group in the redesign and post-redesign periods with their pre-redesign characteristics.
- The results of Model 1 (Table 7) suggest that the patient's likelihood of readmission in the treatment group during the redesign period is no different from that in the control group and pre-redesign period (b = − 0.07).
    - The benefits of the process redesign can only be seen after the patient education process was fully implemented.
    - However, no deterioration in performance was found in this period compared to the patients in the control group.
    - Readmission across both the treatment and control group, however, was worse than in the other periods,  suggesting the likelihood of readmission within 30 days was significantly higher in this period.
- Model 2 (Table 7) provides the results for the likelihood of readmission post-redesign.
    - As seen from Model 2, the coefficient of the interaction term between redesign (post) and Kidney is negative and significantly associated with readmissions (b = − 1.39).
        - Kidney transplant patients' likelihood of readmission within 30 days was lower post-redesign compared to the control group and to the pre-redesign period.
        - Post-redesign odds of readmission within 30 days were 0.25 times lower when compared to the control group and pre-treatment periods.
- The patient controls and time-relevant process controls remain consistent across both regression models.
 
## 5.2. Patient satisfaction
- Monthly HCAHPS data was accessed for both the kidney transplant unit and the liver 1 unit in the hospital.
- The following relationship was estimated between the process redesign periods and the overall satisfaction ratings at the unit level.
 
    *Overall Patient Satisfactioni,t = β0
    + β1 Type pi + β2 Phase S = Redesign or Post
    + β3 Type pi*Phase S = Redesign or Post + θjt + Tαj*
 
    - i: indexes the transplant type
    - t: indexes time in year and months periods
    - Type pi: captures the type of transplant patient
    - PhaseS = Redesign or Post: represents the periods of the process redesign compared to the pre-redesign period
    - Typepi * PhaseS = Redesign or Post: captures transplant type and time interaction
    - Tαj: represents time-invariant transplant controls
    - θjt: represents time-variant process controls for the transplant groups
 
- Robust standard errors were used.
- Table 8 gives the results of these analyses.
    - Process-level controls were included, along with relevant time intervals when comparing the two units.
    - Models 3 and 4 provide results for redesign and post-redesign periods, respectively.
- Model 3 (Table 8) compares the trends between the baseline period and the redesign period.
    - The interaction term is negatively associated with HCAHPS score (b = − 0.11), suggesting a decline in HCAHPS performance for the kidney transplant unit between the redesign and baseline periods, when compared to a similar change in HCAHPS scores for the control group.
- Model 4 gives the HCAHPS results for the post-redesign period.
    - The interaction term between Redesign (Post) and Kidney is positively associated with the overall patient satisfaction ratings (b = 0.09).
        - The overall change in HCAHPS scores for the kidney unit between the post-redesign and baseline period was higher when compared to a similar change for the control group.
- The robustness of the empirical results was verified for both sets of analyses, the 30-day readmission and patient satisfaction.
    - The robustness checks comprised changing the time window for impact of process redesign from 12 months to 9 months, and repeating the main analyses examining only cadaveric kidney transplants as the treatment group, and comparing it with the heart and liver patient control group.
    - The interpretation of the results remained unchanged.
 
# 6. Discussion
- The purpose of this study was to develop a methodology for sustaining PI in high interaction service organizations such as hospitals.
- There is limited research in operations management and organizational science that has offered concrete recommendations on how to sustain PI.
- The four challenges to sustaining PI that were identified had been discussed in the literature without guidelines for overcoming them. The research offers preliminary evidence on ways to overcome these challenges and sustain PI.
- The four main contributions of this research are:
    - First, conducting a process discovery prior to considering process redesign assures employee buy-in for initiating PI.
        - While senior management support was present, the nurses and surgeons at the hospital were uncommitted to the process redesign.
        - The results of the pre-redesign study, however, generated a shift in the attitudes of nurses and surgeons toward the redesign, leading to their enthusiastic participation.
        - By demonstrating the effectiveness of a pre-redesign study and analysis on caregiver acceptance of the need for process redesign, the research adds specificity that can help organizations better initiate process redesign and efforts for PI that are sustained over time.
    - Second, insights are offered on implementing process redesign in high interaction service settings.
        - Patient input was included in the redesign of the process in a parallel fashion.
        - Studies suggest surveying customers to gauge their perceptions of the process and using them for the design.
        - The research delineates a new way for incorporating patient feedback in improving healthcare delivery.
        - The initially sequential then separate but simultaneous collection of caregiver and patient viewpoints can be useful in synthesizing information and counterbalancing process features.
            - This combines the benefit of standardization with the ability to adapt certain features based on the patient needs and preferences and achieve better patient health outcomes and higher customer satisfaction.
    - Third, insights are offered on how to maintain organizational initiatives for PI and create a mindset of continuous improvement.
        - The success in sustaining the initiative introduced through the intervention can be attributed to the midlevel management's continuous engagement in huddles.
        - This middle-up-down method also helps frontline nurses and senior leadership connect in a timely and sustained manner.
        - Middle managers are critical elements in sustaining PI following process redesign.
    - The fourth contribution comprises a demonstration of the IBR approach for studying ill-structured problems such as sustaining PI.
        - The research began inductively, with the intervention targeting the ill-structured problem of initiating, executing, and sustaining PI.
        - Using deductive logic, the effects of the process redesign were examined, controlling for specific unobservable factors pointed out as relevant by the caregivers.

</details>

# Aravinda (Ari) Garimella
- Assistant Professor of Business Administration and Ethics and Professional Responsibility Scholar
### education
- PhD, Business Administration, University of Washington, 2018
- M.S., Information Systems, University of Washington, 2015
- B.E., Computer Science, Birla Institute of Technology, 2005

### research interest
- Digital Platforms, Online Communities, Human-AI Interaction

### teaching
- **Database Design and Management (BADM 352)** Introduce the modern concepts, techniques and management practices when dealing with data and use of data in organizations.
- **AI in Business & Society (BADM 590)** Special topics in the general area of business. 
- **Proseminar in Informat Systems (BADM 591)** 


```
title: When Top-Down Meets Bottom-Up: Legislative Signals and Online Crowdfunding
authors: Anqi Wu, Aravinda Garimella, Ramanath Subramanyam
journal: Information Systems Research
published: 2025
```
 
# Executive Summary
- This study investigates the impact of legislative signals on online giving behavior, focusing on the ratification of the Every Student Succeeds Act (ESSA) and its effects on donations to public schools through DonorsChoose.org.
- The research questions are: (1) What are the effects of a legislative signal on overall and local donation behavior online? (2) What are the mechanisms underlying these effects?
- Key **theoretical** / **conceptual framework** discussions:
    -   **Signaling Theory**: Legislative signals, such as ESSA ratification, convey information about governmental attention to public causes, influencing donor perceptions and actions.
    -   **Crowding Out/In**: Legislative signals can lead to crowding out (decrease) of overall donations as donors perceive reduced need for their contributions, while also causing crowding in (increase) of local donations due to heightened awareness of local needs.
    -   **Information Push and Pull**: These mechanisms explain how donors become aware of legislative signals, with information push (media coverage) and information pull (Internet searches) influencing donation behavior differently.
-   Key **findings** / arguments:
    -   ESSA ratification led to an 8% decrease in total donation amounts and a 4% decrease in the proportion of funded projects.
    -   Local donations increased by 11.9%, while nonlocal donations decreased by 15.7% following ESSA ratification.
    -   The shift towards local donations was stronger among donors with higher awareness of ESSA through information pull, whereas the decrease in nonlocal donations was more influenced by information push.
    -   Schools with students of lower socioeconomic status (SES) experienced a sharper decline in the proportion of projects funded compared to schools with students of higher SES.
-   **Managerial implications**: Platform designers should use targeted nudges (like focused campaigns) to prevent inequitable resource distribution. Teachers need to communicate to donors to fill the gaps left by policy implementations. Policymakers should acknowledge market failures in funding and incentivize philanthropy to meet the resource gaps.
 
#legislative_signals #online_crowdfunding #public_education #signaling_theory #crowding_out #crowding_in #information_push #information_pull #socioeconomic_status #digital_platforms #familiarity_bias #home_bias #essa #quasi_experiment #difference_in_differences

<details>
	
  <summary>Click to expand sections</summary>
  
# 1. Introduction
- Buchanan [1950], Augenblick et al. [1997], and Baker [2021] argue that equity and adequacy in funding are prerequisites for equal educational opportunity, but public schools in the U.S. are underfunded and inequitably funded [Farrie and Johnson, 2015].
- Teachers often spend their own money on classroom supplies, especially in high-poverty schools [Walker, 2019], so they seek outside funds for classroom projects due to budget limitations.
- Online crowdfunding platforms have emerged to help alleviate these resource shortages [Burtch et al., 2013, 2014; Hong et al., 2018; Burtch and Chan, 2019; Geva et al., 2019; Kim and Viswanathan, 2019; Lin et al., 2022].
    -   These platforms, like DonorsChoose.org, provide teachers with efficient access to resources, mitigating market failures and transcending geographic constraints.
    -   DonorsChoose has raised over $1.6 billion since 2000 for teacher-led projects.
-   This bottom-up resource mobilization occurs within a context of top-down legislative activity, such as the Every Student Succeeds Act (ESSA) of 2015 [McGuinn, 2016; Egalite et al., 2017; Jacob, 2017; Hess and Eden, 2021].
    -   ESSA is a devolution law, decentralizing public sector responsibilities [Senate Committee on Health, Education, Labor, and Pensions].
    -   The announcement of such laws can signal societal needs and shift public perceptions [Tankard and Paluck, 2016], even before implementation.
-   Despite growing IS research on charitable crowdfunding, there's limited work on the interaction between top-down signals and bottom-up efforts to address resource gaps [He et al., 2024].
    -   This paper examines how the ratification of ESSA affected donor behavior and the fundraising ecosystem.
    -   It draws from theories in philanthropy, public goods research, and information economics, contributing to the IS literature on crowdfunding success [Burtch et al., 2013, 2014, 2015; Younkin and Kuppuswamy, 2018; Burtch and Chan, 2019; Yang et al., 2020; Lin et al., 2022].
- Legislative signals could alter overall giving or realign donor interests to local initiatives [Wagner and Wheeler, 1969; Bekkers and Wiepking, 2011].
- Research Questions:
    -   *RQ1*: What are the effects of a legislative signal on overall and local donation behavior online?
    -   *RQ2*: What are the mechanisms underlying these effects?
- A 20-quarter (2015Q1–2019Q4) panel data set, representing 73,303 schools (nearly 70% of all U.S. K-12 public schools) and totaling 4,40,617 observations, was collated to study the research questions.
    -   These data were combined with school demographics and characteristics from the National Center for Education Statistics (NCES), as well as ESSA-related Internet search data and media coverage data from Google Trends and LexisNexis.
- The study adopts a staggered difference-in-differences (DD) research design, consistent with methodological approaches [Seamans and Zhu, 2014; Zhang et al., 2024] used to examine exogenous policy changes, focusing on donor behavior changes following state ESSA ratifications.
    -   The state plan sign-off by the U.S. Department of Education is the tangible event signaling legislative action.
    -   The geographic and temporal variation in ratification events helps disentangle the legislative signal's effect from macro trends [Parvin and Beruvides, 2021]. An event-study analysis is also employed for methodological triangulation.
- Key findings:
    -   Donation patterns shifted substantially following state ESSA ratifications, becoming more local.
    -   Local donations increased by 11.9%, while nonlocal donations decreased by 15.7%.
    -   There was a net negative effect, with an 8% decline in total donation amounts ($2.45 million per quarter for the sample schools). Low-SES schools experienced a sharper funding drop.
    -   Two awareness mechanisms were identified: information push and information pull, with donors showing higher ESSA awareness exhibiting stronger donation behavior shifts.
- This study contributes to IS literature and practice by:
    -   Examining how signals from the external institutional environment influence crowdfunding platforms [He et al., 2024].
    -   Combining theoretical perspectives on familiarity bias and signaling to establish geographically asymmetric effects of legislative signaling.
    -   Examining two distinct awareness channels, information push and information pull.
    -   Highlighting the resource-divide problem and suggesting platform interventions like targeted nudges and improved matching processes.
 
# 2. Research Context
- The data were sourced from DonorsChoose.org, a platform where individuals donate directly to public school classroom projects.
    -   By early 2024, over $1.6 billion USD had been raised, with more than 80% of U.S. public schools (Pre-K to 12) having posted projects.
    -   The majority of requests come from low-resource schools.
- Project page descriptions on DonorsChoose include needs, school information, location, poverty level, subject, grade level, number of students, and donor contributions.
- DonorsChoose uses an "all-or-nothing" model: teachers receive donations only if the funding goal is met.
    -   If a project isn't fully funded within four months, donations are refunded as campaign credits.
- The Every Student Succeeds Act (ESSA), signed into law on December 10, 2015, is a major reauthorization of the 1965 Elementary and Secondary Education Act (ESEA).
    -   ESSA decentralizes public education by granting states and local districts greater authority in setting standards.
    -   It attracted substantial local and national media coverage, described as "the largest devolution of federal control to the states in a quarter-century" [Wall Street Journal, 2015].
- Content analysis of over 10,000 news articles from Lexis Nexis revealed that over 50% emphasized devolution as the most important theme.
    -   Interviews with education policy researchers and teachers corroborate devolution's salience.
- While ESSA was signed in December 2015, state ESSA plans took effect at different times during the 2017–2018 school year, based on U.S. Department of Education approval.
 
# 3. Literature Review
- This work builds upon and contributes to two streams of IS literature: online crowdfunding and signaling in online communities.
 
## 3.1. Online Crowdfunding
-   Online Crowdfunding is a widely studied outcome in the IS literature [Burtch et al., 2014; Burtch and Chan, 2019; Geva et al., 2019; Yang et al., 2020].
-   Prior research has examined factors like crowdfunder characteristics [Lin and Viswanathan, 2016; Younkin and Kuppuswamy, 2018], reward structures [Burtch et al., 2015; Yang et al., 2020; Lin et al., 2022], risk disclosure [Kim et al., 2022], social networks [Hong et al., 2018], historical data [Burtch et al., 2013; Kim and Viswanathan, 2019; Jiang et al., 2022], algorithm choices [Song et al., 2022], and charity metrics [Kessler and Milkman, 2018; Exley, 2020].
-   A relevant sub-stream focuses on public education crowdfunding [Meer, 2014; Wash and Solomon, 2014; Weinmann and Mishra, 2019; Gao et al., 2021; Xiao and Yue, 2021; Keppler et al., 2022].
    -   **Geographical proximity** between lenders and borrowers positively influences outcomes, suggesting a home bias [Galak et al., 2011; Burtch et al., 2014; Lin and Viswanathan, 2016]. Proximity generates trust even without economic benefits [Lai and Teo, 2008].
-   The work contributes by:
    -   Identifying conditions that intensify home bias and explaining underlying reasons.
    -   Highlighting that home bias is dynamic and influenced by external signals.
    -   Addressing a research gap by examining how external signals influence crowdfunding activities [He et al., 2024].
 
## 3.2. Signaling in Online Communities
- A substantial body of IS literature has examined the effects of signaling on online platform user behavior.
- Examples include website informativeness [Pavlou et al., 2007] and website quality [Wells et al., 2011] in e-commerce, as well as signals in crowdsourcing [Majchrzak and Malhotra, 2016], social media [Nian and Sundararajan, 2022], matching platforms [Shi and Viswanathan, 2023], and online service marketplaces [Zheng et al., 2023].
- Crowdfunding studies examine signaling from resource providers and seekers.
    -   Resource seekers provide personal information to build trust [Ahlers et al., 2015; Geva et al., 2019].
    -   Project pitches contain signals like readability and quality of visuals [Mollick, 2014].
    -   Resource provider signals include word-of-mouth [Qiu, 2013], expert investment [Kim and Viswanathan, 2019], and recommendations. Signals from investors in the external environment also matter [He et al., 2024].
-   This paper contributes by:
    -   Examining the effect of institutional signals on online donor behavior, an area previously unexplored in the online world.
    -   Examining the two mechanisms of how institutional signals are received: information push and information pull [Cybenko and Brewington, 1998].
    -   Focusing on how legislative signaling can impact the perceived worthiness of an entire class of initiatives, rather than just individual initiatives.
 
# 4. Theory and Hypotheses
- Digital platforms exist within social, cultural, economic, political, technological, and regulatory contexts. Formal institutions (e.g., governments) take measures aimed at societal problems, transmitting signals that shape individuals’ beliefs [Tankard and Paluck, 2016].
- The study examines the effect of legislative signals on online donor behavior, focusing on governmental action and the informational content of legislation (e.g., ESSA's devolutionary focus).
- Donor awareness is essential for any change in giving behavior. Donors can receive signals through information push and pull [Cybenko and Brewington, 1998].
    -   Information pull is when individuals request information (e.g., Internet search).
    -   Information push is when information is sent without being solicited (e.g., newspaper).
- Efficacious signals are characterized by observability and frequency [Janney and Folta, 2003, 2006; Park and Mezias, 2005].
 
## 4.1. Overall Crowding Out
- Legislative signals can indicate governmental attention to a public cause, potentially reducing overall online giving.
- The **substitution-based view of volunteering** suggests that donors see their actions as substitutes for government action [Dekker and Halman, 2003; Prodi et al., 2023].
    -   If the government is devoting resources to a cause, donors may discount their contributions [Frey and Jegen, 2001; Nyborg and Rege, 2003; Bekkers and Wiepking, 2011].
- **Public goods theory** predicts that the nonprofit sector is larger when governments fail to meet societal needs [Weisbrod, 1978; Salamon and Toepler, 2015]. When governments step in, the perceived necessity for nonprofit intervention diminishes.
- Economic crowding-out theories have mixed evidence [Warr, 1983; Roberts, 1987; Andreoni, 1990]. An overall crowding out effect is proposed even without explicit dollar-for-dollar tradeoffs.
    -   Economic models assume full information on the part of donors, which empirical data often contradict [Handy, 2000; Horne et al., 2005; Shah et al., 2015; Lergetporer et al., 2016].
    -   Citizens rely on coarse signals and simple cues [Simon, 1990; Kahneman and Tversky, 2013]. Information push, through media coverage, shapes public perception.
- *H1*: A legislative signal pertaining to a public cause will lead to an overall decrease in online donations to the cause.
 
## 4.2. Local Crowding In
- The informational content of a signal matters for decision-making [Arrow, 1973]. Information pull is likely to encourage potential donors to move from hesitation to active contribution.
- *H2*: A legislative signal with a devolutionary focus (informational content) will lead to a redistribution of online donations toward local causes.
- With devolutionary legislation, information-seeking points donors to greater delegation of control to local authorities. IS literature notes the existence of home bias [Galak et al., 2011; Burtch et al., 2014; Lin and Viswanathan, 2016].
- Devolutionary legislative signaling has the potential to motivate donors to respond more enthusiastically to requests from teachers in their state and local communities.
- Specifically, the message of devolution will lead to:
    -   Draw public attention to the challenges faced by local schools, as donors become more aware of local needs.
    -   Donors are more likely to give when they feel that their individual actions make a difference, believing that their own actions will make a bigger difference.
    -   Familiarity effects, where donors are more inclined to support causes they know well [Jiang et al., 2019], are likely to be amplified as they gain more knowledge about local schools.
- This aligns with the **complementarity theory** [Dekker and Halman, 2003; Dahlberg, 2005] of voluntary action, where volunteers see their efforts as complementary to governmental action. Donors perceive a diminished federal role and an increased local role in education.
 
# 5. Data
- The study systematically assembled a multifaceted data set from four independent sources.
 
## 5.1. Data Sources
- Donation data was obtained from DonorsChoose.org via their API, including details about classroom projects, teachers, schools, donors, and project donations.
- School demographic information came from the NCES, the Department of Education’s database on public Pre-K to Grade 12 education.
- Donor awareness was measured using:
    -   Data on Internet search traffic for ESSA-related terms from Google Trends.
    -   News and media releases about ESSA from Lexis Nexis.
- The school-quarter-level data set included all active U.S. public Pre-K to Grade 12 schools in both DonorsChoose and the NCES system.
    -   The final sample consisted of 73,303 unique schools, covering nearly 70% of U.S. public schools.
- The main analysis focused on 20 quarters from 2015 to 2019, ensuring that ESSA ratifications fell within the central part of the study period, with quarters as the time unit.
 
## 5.2. Variable Definitions
-   Dependent Variables:
    -   *Amount Raised*: Total dollar amount raised by the school in a given quarter [Younkin and Kuppuswamy, 2018; Cornelius and Gokpinar, 2020].
    -   *Proportion Funded*: Proportion of fully funded projects.
    -   *Local Amount*: Contribution amount from local donors (same first three zip code numbers).
    -   *Nonlocal Amount*: Contribution amount from donors outside this area.
-   Independent and Control Variables:
    -   *ESSA*: Binary variable indicating the date on which the respective state’s ESSA plan had been approved.
    -   A broad set of control variables were included, such as Amount Requested, Prop. Basic Projects, and Minority [Meer, 2014; Gao et al., 2021].
- School and time fixed effects were used to control for unobserved heterogeneity. Standard errors were clustered at the state level to control for potential correlations.
 
# 6. Empirical Analysis and Results
- The effects of state ESSA ratifications on school donations were estimated using a model specification. The empirical findings on overall fundraising success, local and nonlocal giving were presented.
 
## 6.1. Empirical Strategy
- The individual ESSA plans for different states went into effect at different time points, providing a quasi-experimental setting to examine the legislative signal's effect [Seamans and Zhu, 2014].
- A staggered difference-in-differences (DD) model was adopted using an ordinary least squares (OLS) estimation with fixed effects.
- The model specification for the DD analysis is as follows:
 
    *Y<sub>ijt</sub> = β · ESSA<sub>ijt</sub> + γ · X<sub>ijt</sub> + School<sub>i</sub> + Quarter<sub>j</sub> + Year<sub>t</sub> + ε<sub>ijt</sub>*
 
    - *Y<sub>ijt</sub>* denotes the dependent variables: Amount Raised, Proportion Funded, Local Amount, and Nonlocal Amount. The logs of the dependent variables measured in dollar amounts were used.
    - *ESSA<sub>ijt</sub>* is the treatment indicator variable.
    - *X<sub>ijt</sub>* represents a vector of time-variant control variables.
    - *School<sub>i</sub>*, *Quarter<sub>j</sub>*, and *Year<sub>t</sub>* are school and time fixed effects.
- Preliminary checks to verify the empirical framework:
    - Placebo treatment simulations were conducted by generating random dates during the ratification period [Eftekhari et al., 2023; Dobrescu et al., 2024].
    - Event-study analyses and a Goodman-Bacon decomposition were performed to check the validity of the staggered DD specification [Goodman-Bacon, 2021].
    - Falsification tests were conducted to rule out demand-side shifts in teacher requests.
 
## 6.2. Results
- Most schools experienced declines in crowdfunding success after their respective state plan approvals.
 
### 6.2.1. Negative Overall Effect on Crowdfunding Success
- The negative and significant coefficients of the variable ESSA indicate that schools in states with ratified ESSA plans saw significantly inferior crowdfunding outcomes (corroborating Hypothesis 1).
-   Following their state’s ESSA ratifications, schools saw a drop of 8% in the amount raised and a 4% drop in the proportion of projects that met their funding goals.
 
### 6.2.2. Increase in Local Donations and Decrease in Nonlocal Donations
- A positive coefficient of ESSA for Local Amount and a negative coefficient Nonlocal Amount was observed, with both estimated coefficients statistically and economically significant.
- Schools raised 11.9% more from local donors and 15.7% less from nonlocal donors (corroborating Hypothesis 2). ESSA ratifications redistributed online donations, leading to increased localization in crowdfunding.
 
## 6.3. Mechanism Examination: Information Push and Information Pull
- The assumption that citizens are aware of the legislative signal and examine information push and information pull was tested.
- To capture information push, a binary variable, Push, was constructed to indicate whether a school’s donations primarily originated from states with intensive ESSA-related news coverage.
- To examine information pull, Google Trends was used to track ESSA-related search traffic, and a binary variable, Pull, was created to indicate whether a school’s donations primarily originate from states displaying higher than median interest in the search terms.
- When jointly considering the effects of information pull and push, distinct results for local and nonlocal donations were observed.
- The increase in local donations induced by legislative signals is shaped mainly through information pull via active search, although the decrease in nonlocal donations is more likely to be driven by information pushed through news and media releases.
 
## 6.4. Post Hoc Analyses
- Impacts on equity were further investigated by focusing on low-SES schools.
 
### 6.4.1. Impacts on Low-SES and Title I Schools
- Funding success (proportion of projects funded) decreased to a larger extent for low-SES schools following state ESSA ratifications.
- The coefficients of the interaction terms involving the three variables, ESSA × Low-Income, ESSA × Minority, and ESSA × Title I, are all negative and significant at the 5% level.
- Title I schools and schools with a higher percentage of low-SES students were disproportionately impacted by the shift in donor behavior following the ESSA ratifications.
 
### 6.4.2. Impacts on Number of Donors vs. Per-Donor Amount
- The number of overall and local donors remained stable, although the average amount contributed by donors both overall and locally was impacted.
- This implies that the legislative signal did not substantially change the pool of donors but significantly changed donation behavior.
 
### 6.4.3. Evaluation of Alternative Explanations and Additional Robustness Checks
- A series of additional robustness checks were performed to confirm that possible confounds or alternative explanations did not drive the main results.
 
# 7. Discussion
- This study finds that legislative signals have substantial effects on online donor behavior, corroborating the hypothesis that ESSA leads to greater localization of online donations.
- The net effect is an 8% drop in the average amount raised by schools through the platform, totaling an average of $2.4 million per quarter.
- The effects manifested through donors’ behavioral shifts, aligning consistently with information pull (Internet search) and information push (media coverage). Schools with a greater percentage of low-SES students were more adversely affected.
 
## 7.1. Theoretical Implications
- Platforms make information accessible beyond geographical constraints [Gefen and Carmel, 2008; Chen and Horton, 2016].
- This study contributes to IS crowdfunding literature.
    -   This is the first study to hypothesize and demonstrate geographically asymmetric effects of external signals on fundraising outcomes, contributing to the understanding of familiarity bias and home bias [Lin and Viswanathan, 2016; Sabzehzar et al., 2023].
    -   Top-down legislative signaling, that has not been previously examined. It provides evidence of hyperlocalization in online crowdfunding.
    -   The institutional environment effect on crowdfunding platforms. The signal shapes donors’ perceived worthiness of initiatives.
 
## 7.2. Managerial Implications
- Given factors observable in other domains, results can provide predictions about online crowdfunding efforts in other domains (healthcare, financial services, utilities, poverty, small business growth, entrepreneurship, etc.).
- Interface design can be used to align user behavior with platform goals.
- Timed and deliberate nudges, such as targeted campaigns, reprioritized project placement, and email promotions, become increasingly important surrounding major policy announcements.
- Platforms might consider explicitly exposing donors from wealthy areas to projects from schools in poorer areas to counteract the potential “rich-get-richer” problem.
- Teachers may need to remind donors that the effects of federal measures such as ESSA trickle down to the schools only gradually and that many day-to-day classroom needs still remain unmet.
- Policymakers need to engage with platforms to help them serve as counter-forces against inefficiencies. Policymakers could seek to mitigate such problems by, for example, providing tax-based incentives for certain forms of philanthropy that promote the equitable distribution of resources.
 
---
# Executive summary of 1. Introduction
-   Public schools face underfunding and inequitable funding, pushing educators to seek external funds through online crowdfunding.
-   Online crowdfunding platforms have facilitated fundraising efforts to alleviate resource shortages and mitigate geographic constraints.
-   The study examines the interaction between top-down legislative signals, such as the Every Student Succeeds Act (ESSA), and bottom-up efforts on crowdfunding platforms.
-   The research questions are: (1) What are the effects of a legislative signal on overall and local donation behavior online? (2) What are the mechanisms underlying these effects?
-   The study finds a shift in donation patterns following ESSA ratification, with increased localization and a net decrease in overall funds raised.
 
# Executive summary of 2. Research Context
-   The data were sourced from DonorsChoose.org, a leading online crowdfunding platform for public school classroom projects.
-   DonorsChoose operates on an "all-or-nothing" model, and the majority of requests come from low-resource schools.
-   The Every Student Succeeds Act (ESSA) decentralizes public education and attracted substantial media coverage.
-   State ESSA plans took effect at different times during the 2017–2018 school year, based on U.S. Department of Education approval.
 
# Executive summary of 3. Literature Review
-   The study builds upon literature in online crowdfunding and signaling in online communities.
-   Prior research has examined factors influencing crowdfunding outcomes, including crowdfunder characteristics, reward structures, social networks, and charity metrics.
-   Geographical proximity between lenders and borrowers has a positive influence on online crowdfunding outcomes, suggesting a home bias.
-   Signaling in online communities has been studied in various contexts, including e-commerce, crowdsourcing, and social media.
-   This study contributes by examining the effect of institutional signals on online donor behavior and exploring the mechanisms of information push and pull.
 
# Executive summary of 4. Theory and Hypotheses
-   Formal institutions transmit signals that shape individuals’ beliefs about societal needs.
-   The study examines the effect of legislative signals on online donor behavior, focusing on governmental action and the informational content of legislation.
-   Donor awareness is essential for changes in giving behavior, with signals received through information push and pull.
-   The hypotheses are:
    -   *H1*: A legislative signal pertaining to a public cause will lead to an overall decrease in online donations to the cause.
    -   *H2*: A legislative signal with a devolutionary focus will lead to a redistribution of online donations toward local causes.
 
# Executive summary of 5. Data
-   A multifaceted data set was assembled from four independent sources: DonorsChoose.org, NCES, Google Trends, and Lexis Nexis.
-   The data set included 73,303 unique schools, covering nearly 70% of U.S. public schools.
-   The main analysis focused on 20 quarters from 2015 to 2019.
-   Dependent variables included Amount Raised, Proportion Funded, Local Amount, and Nonlocal Amount.
-   The main independent variable was ESSA, and control variables were included based on prior research.
 
# Executive summary of 6. Empirical Analysis and Results
-   The study uses a staggered difference-in-differences (DD) model to estimate the effects of state ESSA ratifications on school donations.
-   Preliminary checks were performed to verify the empirical framework, including placebo treatment simulations, event-study analyses, and falsification tests.
-   The results show a negative overall effect on crowdfunding success, with a decrease in total donation amounts and the proportion of funded projects.
-   Local donations increased, while nonlocal donations decreased following ESSA ratification.
-   Information pull strengthened the effects on local donations, while information push magnified the reduction in nonlocal donations.
-   Low-SES schools experienced a sharper decline in funding.
 
# Executive summary of 7. Discussion
-   Legislative signals have substantial effects on online donor behavior, leading to greater localization of online donations.
-   The net effect is a decrease in the average amount raised by schools.
-   The effects manifest through donors’ behavioral shifts, aligning with information pull and information push.
-   This study contributes to IS crowdfunding literature by examining geographically asymmetric effects of external signals and the interaction between top-down policy and bottom-up crowdfunding efforts.
-   The study provides managerial implications for platform designers, public school administrators, teachers, and policymakers.

</details>

```
title: Transformation of the Transaction Cost and the Agency Cost in an Organization and the Applicability of Blockchain—A Case Study of Peer-to-Peer Insurance
authors: Ruo-Ting Sun, Aravinda Garimella, Wencui Han, Hsin-Lu Chang and Michael J. Shaw
journal: Frontiers in Blockchain
published: 2020
```

# Executive Summary

- The study investigates the impact of **blockchain technology** on **organizational economics**, specifically focusing on **transaction costs** and **agency costs**.
- The paper argues that blockchain introduces **trust** and **transparency**, enhancing internet-based business services and **corporate governance**.
- **Smart contracts** are highlighted for improving **transaction speed** and **volume**.
- The study employs **transaction cost theory** and **agency theory** to analyze how blockchain affects **economic activities** and **organizational boundaries**.
- A **case study** of **InsurePal**, a **peer-to-peer insurance** platform, illustrates the transformation of costs.
- The introduction of blockchain **pushes organizational structures** further from hierarchies toward **electronic markets**.
- **Search and information costs** decrease due to **transparency** and **traceability**, but could increase depending on the **maturity of validation technology**.
- **Bargaining costs** increase due to the need for more detailed agreements codified in **smart contracts**.
- **Policing and enforcement costs** decrease as smart contracts automate compliance monitoring.
- **Monitoring, bonding, and residual costs** associated with **agency problems** all decrease with blockchain adoption.
- Blockchain **shifts monitoring efforts** from **internal to external sources**, potentially increasing **search and information costs**.
- The transformation of costs occurs in **two stages**:
    - Initially, the organization structure remains unchanged (permissioned blockchain).
    - Subsequently, the structure evolves towards a **Decentralized Autonomous Organization (DAO)** (permissionless blockchain).
- The study contributes to understanding blockchain from an **exchange technology perspective**, predicting **organization boundaries**, and highlighting the **shifting role of third parties**.
- Key **findings** / arguments include:
    - Blockchain will push further the structure of an **organization from hierarchies toward electronic markets** (*P1*).
    - The **search and information cost** will decrease in an organization by adopting blockchain technology to the existing business process (*P2a*).
    - The **bargaining cost** will increase in an organization by adopting blockchain technology to the existing business process (*P2b*).
    - The **policing and enforcement cost** will decrease in an organization by adopting blockchain technology to the existing business process (*P2c*).
    - The **monitoring cost** of an organization will decrease by adopting blockchain technology to the existing business process (*P3a*).
    - The **bonding cost** of an organization will decrease by adopting blockchain technology to the existing business process (*P3b*).
    - The **residual loss** of an organization will decrease by adopting blockchain technology to the existing business process (*P3c*).
    - Blockchain **shifts the monitoring efforts from internal to external sources** and thus **increases the search and information costs in transaction cost** (*P4*).

#blockchain #transaction_cost #agency_cost #exchange_technology #organization_costs_evolution #blockchain_applicability #peer_to_peer_insurance #smart_contracts #decentralization #trust #transparency #corporate_governance


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction

- Blockchain technology supports **decentralized applications** with **immutable**, **decentralized**, and **trustless** properties.
- Despite its potential, there are concerns about **overpromising and underdelivery**, with some arguing that alternative technologies could be more efficient.
- The paper notes corporates are investing in **blockchain technology** but little progress has been made.
- The necessity and impact of blockchain vary across organizations and business models, leading to **complex blockchain products**.
- The focus on **system functionality** may overshadow **organizational variables**.
- The study adopts an **Exchange Technology (ET)** perspective to examine **blockchain** at the **firm level**.
    - ET emphasizes the **efficiency** and **scope of transactions** in market exchanges.
- **Transaction costs** (market imperfections) and **agency costs** (conflict of interest and information asymmetry) are used as theoretical lenses.
- The study aims to provide **managerial insights** into blockchain's applicability to existing business processes.
- The research questions are:
    - *RQ1:* What effects does the introduction of blockchain technology have on the **boundaries of an organization** with respect to IT infrastructure?
    - *RQ2:* How does the introduction of blockchain **transform the transaction costs** of a firm?
    - *RQ3:* How does the introduction of blockchain **transform the agency costs** of a firm?
    - *RQ4:* How do the **new transaction costs and agency costs interact** and what effects does this have on the organization when adopting blockchain technology to the existing business process?
- The study uses **transaction cost theory** and **agency theory** to analyze the interaction between determinants and blockchain properties, impacting cost variables.
- **InsurePal**, a blockchain company, is used as a **case study** to explore cost transformations.

# 2. Background and Related Work

## 2.1. Properties of Blockchain

- Blockchain has four main properties: **transparency**, **immutability**, **programmability**, and **decentralization** [Bahga and Madisetti, 2016; Peters and Panayi, 2016; Cachin and Vukoli´c, 2017; Zheng et al., 2017; Abadi and Brunnermeier, 2018; Davidson et al., 2018].
    - It's a **distributed ledger** without central control.
    - Each participant has an identical copy, providing **transparent shared data**.
    - **Transaction data** is encrypted and grouped into blocks, creating an **immutable transaction trajectory**.
    - **Smart contracts** enable registered peers to validate transactions automatically and anonymously [Peters and Panayi, 2016].
    - **Consensus mechanism** ensures security by making data immutable [Baliga, 2017; Cachin and Vukoli´c, 2017].
    - A blockchain can be viewed as a **permanent database** that keeps track of all transaction records [Davidson et al., 2018].
    - In the event of a **malicious attack**, a blockchain system can continue working to a satisfactory level within this decentralized setup [Bahga and Madisetti, 2016].

## 2.2. Types of Blockchain

- The level of **decentralization** in blockchain applications is determined by a participant’s ability to freely enter the network [Abadi and Brunnermeier, 2018].
- There are three types of blockchain ledger systems: **private**, **permissioned**, and **permissionless** [Zheng et al., 2017].
    - A **private blockchain** can be deployed by a centralized institution.
    - A **permissioned blockchain** is formed by a consortium that can regulate the system.
    - A **permissionless blockchain** enables users to freely enter the market.
- Blockchain adaption depends on its **configuration and business context**.
- **Permissionless blockchains** require participants to contribute computational power, necessitating incentive design [Peters and Panayi, 2016].
- **Permissioned blockchains** serve as a transit product from private centralized blockchains to decentralized permissionless ones.
- The different contexts of blockchain ledger systems imply **different cost structures** of blockchain implementation.

## 2.3. Transaction Cost Theory

- Transaction costs are classified into three types [Mahoney, 2004]:
    - **Search and information costs**: incurred to reduce uncertainty before a transaction.
    - **Bargaining costs**: incurred during negotiations before reaching an agreement.
    - **Policy and enforcement costs**: incurred during the supervision of a contract.
- The core concept of transaction cost theory aims to enhance **economic efficiency** within the process of product or service exchange through the market.
- Williamson proposed three determinants of transaction costs: **frequency**, **asset specificity**, and **uncertainty** [Williamson, 1975].
- **Asset specificity** has four types: **Site**, **Physical**, **Human** [Williamson, 1983], and **Time** [Malone et al., 1987].
- Extant studies have demonstrated the **transformation of transaction costs** when a new technology emerges [Malone et al., 1987; Lajili and Mahoney, 2006].

## 2.4. Agency Cost Theory

- Within an organization, the **principal-agent problem** stems from conflicts of interests between managers and shareholders and information asymmetry.
- A contract is fulfilled to supervise and restrict the agent’s behavior, creating **monitoring costs**, **bonding costs**, and **residual losses** [Mahoney, 2004].
    - **Monitoring cost**: principal reduce information asymmetry.
    - **Bonding cost**: the agent is also required to commit to contractual obligations.
    - **Residual losses**: the interests between principals and agents are still misaligned due to uncertainties.
- Two determinants are crucial to mitigate the agent-principal problem: **incentive** and **accountability**.
- **Incentives** motivate engagement with the agent and mitigate the **moral hazard** issue [Jensen and Meckling, 1976; Ang et al., 2000].
- **Accountability** of individuals can reduce the misalignment of interests between the principal and the agent [Khan, 2011].
- Task **programmability** (how specifications can be formulated) [Ouchi, 1979] and **non-separability** (traceability of a task to an individual) [Alchian and Demsetz, 1972] help clarify accountability.
- Several studies discuss the **potential of blockchain** to reduce auditing effort, facilitate decision-making, and improve the involvement of shareholders [Chedrawi and Howayeck, 2018; Lafarre and Van der Elst, 2018; Kaal, 2019].

# 3. Research Proposition

- The interplay of four determinants (frequency, time specificity, human specificity, and uncertainty) in **transaction theory**, and four blockchain properties (programmability, decentralization, transparency, and immutability).

## 3.1. The Impact of Blockchain on Transaction Cost

- Blockchain's core values (consensus mechanism and smart contract) may change **time specificity** and **human specificity** [Malone et al., 1987; Williamson, 1983].
- *P1*: Blockchain will **push further the structure of an organization from hierarchies toward electronic markets**.
    - High-frequency transactions between specific trading partners will increase the likelihood of vertical integration [Williamson, 1985].
    - Blockchain's programmability saves the transaction time and boosts the frequency of a transaction. The backbone consensus allows the participants to execute the growing number of transactions in a decentralized manner.
- *P2a*: The **search and information cost will decrease** in an organization by adopting blockchain technology to the existing business process.
    - The transparency and traceability of the blockchain digital ledger will significantly reduce the cost of validating trading partners [Catalini and Gans, 2016].
    - The programmability of blockchain facilitates the exchange of digital information [Peters and Panayi, 2016].
- *P2b*: The **bargaining cost will increase** in an organization by adopting blockchain technology to the existing business process.
    - Opportunistic actions may occur due to information asymmetry, leading to the increase of behavior uncertainty [Williamson, 1985].
    - More negotiations are required to reduce uncertainty and fulfill the needs of each participant and corresponding agreement [Mahoney, 2004].
    - The immutability of blockchain mitigates the possibility of an inaccurate data registry by reducing the uncertainty of each pre-transaction.
- *P2c*: The **policing and enforcement cost will decrease** in an organization by adopting blockchain technology to the existing business process.
    - Smart contracts automatically check agreements, reducing uncertainty of contract enforcement [Catalini and Gans, 2016].

## 3.2. The Impact of Blockchain on Agency Cost

- Blockchain encounters **moral hazard risks** when relying upon third parties to verify the registry of assets, as blockchain is heavily dependent on third parties to verify the fidelity of assets registered [Catalini and Gans, 2016].
- *P3a*: The **monitoring cost** of an organization will **decrease** by adopting blockchain technology to the existing business process.
    - Incentives motivate the agent to align with the principal’s interest [Jensen and Meckling, 1976].
    - Smart contract faculty of blockchain is a new opportunity to improve the alignments of interests in a decentralized manner [Tapscott and Tapscott, 2016].
    - Immutable data ledger provides traceable trajectories of each input and output, which enables a fair distribution of incentives.
- *P3b*: The **bonding cost** of an organization will **decrease** by adopting blockchain technology to the existing business process.
- *P3c*: The **residual loss** of an organization will **decrease** by adopting blockchain technology to the existing business process.
- *P4*: Blockchain **shifts the monitoring efforts from internal to external sources** and thus **increases the search and information costs in transaction cost**.

# 4. Research Methodology

- This study uses a **case study** [Thomas, 2011] to analyze a **blockchain-based startup company**.

## 4.1. Subject, Object, and Purpose

- **Subject**: Key example is selected from a free Initial Coin Offering (ICO) rating platform—ICObench.
    - **InsurePal**, a blockchain-based, decentralized insurance platform, is selected as a successful case.
- **Object**: Shift in **business process** due to the introduction of **blockchain intervention**.
    - The hypothesized cost transformations are explored and described in detail in the later section.
- **Purpose**: The exploratory elements such as variables of transaction and agency costs are used to examine our propositions (**theory-testing**).
    - Aims to investigate a single and retrospective study to thoroughly analyze the business model of InsurPal.

## 4.2. Approach and Process

- **Approach**: A narrative approach involved theoretical elements to develop a detailed business process of our selected insurance commodity.
    - Car insurance, and explore a simple use case of an overpayment issue as the foundation of our case analysis.
- **Process**: Use variables of transaction and agency costs as key scenarios to collect the information of the corresponding business processes.
    - Map out the associated costs from the interviews and description of use cases in the whitepaper and blog posts.
    - The agent and the principal can be mapped to the manager and the stakeholder of an insurance company.

## 4.3. Background Information—InsurePal

- InsurePal is a provider of a global, blockchain-based, decentralized insurance platform powered by the **social proof mechanism**.
- There are three main players in the ecosystem—the **insurer**, the **policy holder** and the **blockchain platform provider** (InsurePal).
- InsurePal serves as a two-sided platform that connects the two parties together and provides both parties with network benefits.
- The platform partners with the insurer and uses social endorsements patent to help the policy holder reduce fraud and enable the fair pricing of premiums.
- On the other side, insurance companies (the agent) take the advantage of the platform to facilitate client segmentation, help risk profiling, reduce fraud and save costs.

# 5. Transformation of the Transaction Costs by Blockchain Intervention

- This section supports our *P2* and *P4* by exploring three types of transaction costs.

## 5.1. Search and Information Cost

- The insurance provider’s **search and information costs** include the collection of personal information and the evaluation of risk factors.
- The distributed and encrypted data ledger shared across the insurance provider, the policyholder, and InsurePal enable **immutable and transparent data**.
- InsurePal utilizes a unique algorithm to calculate a policyholder’s **TrustScore**, which facilitates the process of risk evaluation by introducing a new dimension—**social trust**.
- The segmentation can help the insurer position the target clients, providing them with more accurate and proper insurance premiums.
- The trusted network mitigates the insurance provider’s effort to validate a policyholder’s risks.
- As a consequence, the **search and information cost will reduce**, which supports proposition *P2a*.
- From the perspective of **agency theory**, the agent confronts the new challenge of how to evaluate the social proof mechanism. This may increase the **search and information costs** to assess the validity of the algorithm.
- Evaluating the performance of a third party becomes a new challenge, and corresponding managerial problems would emerge.
- The alternative efforts made on the evaluation would increase the search and information cost.
- The maturity of a validation technology/method is crucial in leveraging the changes in the search and information cost.
- Therefore, proposition *P4* is supported when the validation technology is mature.

## 5.2. Bargaining Cost

- Blockchain intervention through InsurePal requires that the relationship among the participants should be defined in detail since the **smart contract with automatically execute the agreements by lines of code**.
- While building a contract among the policyholder, the endorser and InsurePal, more negotiations are required.
- Attributed to blockchain’s **immutability**, recovering from adverse effects becomes more costly since more parties are involved.
- Therefore, the **bargaining cost increases** in order to reduce transaction uncertainty, which supports the proposition *P2b*.

## 5.3. Policing and Enforcement Cost

- Smart contracts are automatically executed within blockchain applications when the “if-then” conditions are met.
- In terms of blockchain’s programmability, compared to the traditional process of supervising the enforcement of contracts, the required **human efforts can be reduced substantially through smart contracts** so long as the tasks can be digitized and codable.
- The uncertainty of contract enforcement becomes lower due to blockchain’s programmability.
- As a consequence, the backbone of blockchain—smart contract—will **reduce policing and enforcement costs**.
- The proposition *P2c* is hence supported.

# 6. Transformation of the Agency Costs by Blockchain Intervention

- This section supports our *P3* by exploring three types of agency costs that introduced in section Agency cost theory.

## 6.1. Monitoring Cost

- Through a third-party platform, InsurePal, the insurance provider can rely on the TrustScore to mitigate the effort on risk evaluation of each policyholder.
- The traceability of blockchain makes the accountability of the agent (an insurance company) and a third-party platform (InsurePal) clearer.
- The principal can always trace an agent’s actions by examining records that are registered on-chain. This reduces the effort the principal must exert to control for an agent’s potential opportunistic behavior.
- As a consequence, the **monitoring costs of the principal will reduce**, which supports the proposition *P3a*.
- Moreover, the role of a **third-party (InsurePal) becomes more crucial** in the decision-making process.

## 6.2. Bonding Cost

- Since blockchain intervention improves **transparency**, the reduction in information asymmetry can mitigate the agent’s opportunistic behavior.
- The agent is less likely to act against the principals’ interests.
- Blockchain’s traceability plays a role in clarifying the accountability of each participant.
- The agent no longer receives the same **bonding cost** from traditional structures since the traceability of all the actions can prevent the agent from evidence alteration.
- The principal can clearly identify the accountability of each party, resulting in a **reduction in the bonding cost** by the agent.
- Hence, the proposition *P3b* is supported.

## 6.3. Residual Loss

- Loss may incur when the misalignment of interest between the agent and the principal exists.
- Due to the reduction in both the monitoring cost by the principal and the bonding cost by the agent, the resulting **residual loss will further reduce**.
- The derived cost transformation supports the proposition *P3c*.

# 7. A Two-Stage Cost Transformation in Blockchain Transition

- The ultimate status of a business entity triggered by blockchain is a network with properties of permissionless (peer-to-peer) blockchain applications that is highly autonomous.
- Every participant can access the network, broadcast new transactions, verify transactions, and append new data blocks on the chain.
- With the level of decentralization increased and the power of smart contract enhanced, the centralized system will initially grow to the transition stage—Permissioned Blockchain with a hybrid structure of both private and permissionless.
- When the aggregated benefits outweigh the overall loss, adopting a blockchain-based system can be worthwhile.
- This section answers the research question: What effects does the introduction of blockchain technology have on the boundaries of an organization with respect to IT infrastructure? and supports our proposition 1.
- InsurePal is an example of a permissioned blockchain service provider that aims to reach the ultimate goal of peer-to-peer insurance (permisionless blockchain).
- The first stage of transformation is captured by our case analysis and the second stage of transformation is analyzed by reviewing the scenario of InsurePal’s ultimate business goals.

## 7.1. First-Stage Transformation

- Moving from a private to permissioned blockchain mitigates the agent-principal problem since the organization structure has not yet transformed; organizations that intend to utilize blockchain technology desire access to its properties like the transparent and immutable ability of recording data to reduce information asymmetry between the agent and principal.
- In our case study, the focal organization (insurance companies) does not change the organization structure although the business process has been altered by adopting InsurePal.
- The maturity of the validation technology moderates the search and information costs.
- From the lens of transaction cost theory, the search and information cost will decrease (Proposition 2a) since the process of gathering agreements.
- InsurePal allows the policyholder to reduce unreasonable insurance premiums and helps the insurer (our focal organization) facilitate the process of risk assessment by their patent of social endorsement.
- From the agency theory perspective (auditing/monitoring), third parties become more crucial in the blockchain setup.
- The search and information cost increases (Proposition 4) because additional efforts are required for the agent to validate a third party (i.e., the use of multiple oracles to reduce errors).
- In the first transition stage, the maturity of a third-party validation is a key determinant of examining the overall impacts of search and information costs.
- When the validation technology is not mature, the impact of agency theory will outperform that of transaction theory on search and information cost.
- When the validation technology is mature, the impact of transaction theory will outperform that of agency theory on search and information cost.

## 7.2. Second-Stage Transformation

- The ultimate goal of InsurePal is to create an environment for a fully decentralized peer-to-peer insurance.
- Decentralized autonomous organizations (DAO) mature while networks transition from a permissioned to a permissionless blockchain.
- Every participant, the insurer or the policyholder can freely join the network and automatically interact with each other through smart contracts.
- Tokens, a new incentive mechanism, can be embedded in smart contracts to provide the principal with new indicators to measure the agent’s performance.
- The fundamental changes in the principal-agent relationship shift further the boundaries of an organization from hierarchies toward electronic markets (Proposition 1).
- However, there are debates over the idea of the DAO.
- Corporations can make more efficient decisions through centralized management.
- Decision making is inefficient and hence, slower for the DAO, resulting in a sub-optimal situation [Lafarre and Van der Elst, 2018].
- Transaction costs reduction: Mart contracts facilitate the decrease of transaction costs by providing a new channel for bargaining between autonomous organizations.
- At the second stage of transformation, when the validation technology becomes mature, search and information costs will be lowered, along with transaction costs.

# 8. Discussion

- Extant studies forward different understandings on the usage of blockchain, leading to the diverse frameworks available for developing blockchain-based services.

## 8.1. Clarification of Blockchain’s Firm-Level Impact

- The contributions of this study to literature are 3-fold.
    - First, this study contributes to the existing literature on the understanding of blockchain from an economic prospective.
    - Second, we observe the changes in organizational and managerial structures in different stages of blockchain transition.
    - Finally, our findings contribute to transaction cost theory [Williamson, 1985].

## 8.2. Implications on the Blockchain Business Model

- This study implies the potential to facilitate the development of blockchain business models and contributes to the practice.
- Applying a single managerial methodology to different Blockchain configurations is risky.
- Especially, how an organization interacts with new business partners or whether a new channel change the existing customer relationships determines the shift in an existing business model.

# 9. Conclusion and Limitations

- Our study proposes that blockchain technology brings two more benefits, trust and transparency, to the existing Internet-based business services, and helps improve corporate governance.
- Smart contracts improve the execution time of transactions significantly and increase transaction volume rapidly.
- To answer our research questions 2 and 3, we argue the transformation of costs in different stages of the blockchain transition has different managerial implications for the organization structure and the role of third parties.
- Third parties play an important role in reporting the truth to the common ledger and leverage the impact of transaction costs on organizations.
- The transition from a permissioned to a permissionless blockchain implies changes in organizations.
- Essential shifts in the principal-agent relationships push the organization structures from hierarchies toward more electronic markets.
- There are several limitations to this study that should be considered.
    - One of the potential limitations is the selection of blockchain use cases.
    - Second, our motivation stems from the diversity of blockchain architecture across organizations.
    - Also, we only consider a single use case, which limits the possibility of different scenarios that might conflict with our findings and restrict the generalizability of this study.

---

# Executive summary of 1. Introduction

- Introduces **blockchain technology** and its potential for **decentralized applications**.
- Highlights concerns about **overpromising** and the need to understand blockchain's impact on **organizational variables**.
- Presents **transaction cost** and **agency cost** as theoretical lenses to examine blockchain's effects.
- Outlines the study's **research questions** regarding the impact of blockchain on organizational boundaries, transaction costs, and agency costs.
- Mentions the use of **InsurePal** as a **case study**.

# Executive summary of 2. Background and Related Work

- Defines the four main **properties of blockchain**: **transparency**, **immutability**, **programmability**, and **decentralization**.
- Describes the three **types of blockchain ledger systems**: **private**, **permissioned**, and **permissionless**.
- Summarizes **transaction cost theory**, including types of costs (**search, bargaining, policy**) and determinants (**frequency, asset specificity, uncertainty**).
- Explains **agency cost theory**, focusing on the **principal-agent problem**, costs (**monitoring, bonding, residual**), and key determinants (**incentive, accountability**).

# Executive summary of 3. Research Proposition

- Proposes that blockchain's **consensus mechanism** and **smart contracts** change **time** and **human specificity**.
- *P1*: Blockchain will **push organizations toward electronic markets**.
- *P2a*: **Search and information costs will decrease**.
- *P2b*: **Bargaining costs will increase**.
- *P2c*: **Policing and enforcement costs will decrease**.
- *P3a*: **Monitoring costs will decrease**.
- *P3b*: **Bonding costs will decrease**.
- *P3c*: **Residual losses will decrease**.
- *P4*: Blockchain **shifts monitoring efforts externally, increasing search and information costs**.

# Executive summary of 4. Research Methodology

- Uses a **case study** approach to analyze **InsurePal**.
- Focuses on the **shift in business process** due to blockchain intervention.
- Aims to **test theory** by examining variables of **transaction** and **agency costs**.
- Collects data from InsurePal's **whitepaper**, **official website**, and **blog posts**.
- Describes InsurePal as a **blockchain-based insurance platform** that uses **social proof mechanisms**.

# Executive summary of 5. Transformation of the Transaction Costs by Blockchain Intervention

- Explores the impact of blockchain on **search and information costs**.
- Explains how blockchain facilitates **risk evaluation** and **segments insurance customers** by risk levels.
- Discusses the **contrary view** from the perspective of **agency theory**, where evaluating the **social proof mechanism** increases search and information costs.
- Examines how blockchain affects **bargaining costs** due to the need for more detailed agreements in **smart contracts**.
- Analyzes the reduction of **policing and enforcement costs** through the use of **smart contracts**.

# Executive summary of 6. Transformation of the Agency Costs by Blockchain Intervention

- Explores the impact of blockchain on **monitoring costs**, as insurance providers rely on **TrustScore** to mitigate risk evaluation efforts.
- Discusses the impact of blockchain on **bonding costs** by improving **transparency** and clarifying **accountability**.
- Analyzes the decrease in **residual loss** as both **monitoring** and **bonding costs** are reduced.

# Executive summary of 7. A Two-Stage Cost Transformation in Blockchain Transition

- Explains the two stages of transformation:
    - **First-stage**: Organization structure remains unchanged (**permissioned blockchain**).
    - **Second-stage**: Structure evolves towards a **Decentralized Autonomous Organization (DAO)** (**permissionless blockchain**).
- Discusses how the **maturity of validation technology** moderates **search and information costs** in the first stage.
- Analyzes the impact of **keepers** and **tokens** in the second stage.
- Explains how the principal-agent relationship pushes organizations toward more **electronic markets**.

# Executive summary of 8. Discussion

- This study contributes to the existing literature on the understanding of blockchain from an **economic prospective**.
- We observe the changes in **organizational** and **managerial structures** in different stages of blockchain transition.
- Our findings contribute to **transaction cost theory**

# Executive summary of 9. Conclusion and Limitations

- Discusses how blockchain brings **trust** and **transparency** to Internet-based business services.
- Highlights the reduction of **transaction** and **agency costs** in different stages of blockchain transition.
- Mentions the limitations of the study, including the **selection of blockchain use cases** and the **single use case approach**.

</details>


# Abhijeet Ghoshal
- Associate Professor of Business Administration and Office of Risk Management and Insurance Research Faculty Scholar
### education
- Ph.D., Management Information Systems, University of Texas at Dallas, 2011
- Bachelor of Technology, Indian Institute of Technology (BHU) at Varanasi, 2002
### research interest
- Recommendation and Personalization Systems, Software support, Data privacy, Quantum Computing applications
### teaching interest
- Business Analytics, Visual Basic, Java, Database Management Systems, Introduction to Management Information Systems

```
title: From Stars to Dogs – Identifying “Out-Of-Favor” Products on E-Commerce Platforms? A Data Analytic Approach to System Design
authors: A. Ivanov, A. Ghoshal, & A. Kumar
journal: Production and Operations Management, Sage Publishers
published: Forthcoming
```

# Executive Summary

- This study tackles the crucial problem of identifying **"out-of-favor" products** (those experiencing sales decline) on e-commerce platforms.
- The core argument is that proactively detecting these products enables platforms to implement **corrective strategies** (e.g., promotions, inventory adjustments) to bolster overall performance and minimize losses.
- **Key conceptual framework:** A novel approach combining the **Bayesian Structural Time Series (BSTS) model** with **association rule mining** is employed to forecast sales slumps and decipher underlying causes.
    - **BSTS model:** Predicts sales based on historical data considering **trend**, **seasonality**, and the impact of **external events** like promotions and price changes. Products whose actual sales fall significantly below the BSTS forecast are flagged as potentially out-of-favor.
    - **Association Rule Mining:** Uncovers relationships between product **attributes** (e.g., category, price, rating, number of reviews) and their likelihood of becoming out-of-favor.  This helps to understand which *characteristics* or *co-occurring events* correlate with sales declines.
- **Key findings:**
    - The proposed system achieves high **accuracy** in identifying out-of-favor products. In experiments, the system demonstrated an **F1-score of 85%**, indicating a strong balance between precision and recall.
    - Association rule mining reveals specific product characteristics and external factors that significantly increase the likelihood of a product becoming out-of-favor.  For example, products in the electronics category with ratings below 3.5 stars and lacking recent promotional campaigns were found to have a **70% higher chance** of being flagged as out-of-favor.
- The research contributes a practical, data-driven methodology for product lifecycle management on e-commerce platforms, offering actionable insights for improving sales performance and mitigating losses linked to declining products.

#ecommerce #productlifecycle #salesforecasting #bayesianstructuraltimeseries #associationrulemining #outoffavorproducts #datanalytics #systemdesign #productmanagement


<details>
    
  <summary>Click to expand sections</summary>


# 1. Introduction

- E-commerce platforms face the challenge of managing a vast and dynamic product portfolio, where some products inevitably become "out-of-favor" due to changing consumer preferences, competition, or other factors. These products experience a decline in sales and can negatively impact the platform's overall revenue and profitability [Anderson, 2006; Elberse, 2008].
-  Proactively identifying these "out-of-favor" products is crucial for e-commerce platforms to take timely corrective actions.
    - Such actions might include implementing targeted promotions, adjusting inventory levels, or even removing the products from the platform altogether [Kotler & Armstrong, 2016].
    - Early detection and intervention can help mitigate losses and improve overall platform performance.
- Existing literature often focuses on predicting overall product sales or identifying popular products [e.g., Chevalier & Mayzlin, 2006; Godes & Mayzlin, 2004]. However, there is a gap in research specifically addressing the problem of identifying products that are *losing* popularity or are at risk of becoming "out-of-favor."
- Therefore, this study aims to develop a data-driven system for identifying "out-of-favor" products on e-commerce platforms and to understand the factors that contribute to their decline.
    - By combining **sales forecasting** with **association rule mining**, the study provides a comprehensive approach for both *predicting* and *understanding* product decline.
- This research offers valuable insights for e-commerce platform managers seeking to improve product lifecycle management and optimize their product portfolios.

## 1.1. Research Questions

- The research addresses the following questions:
    - **RQ1**: Can we accurately identify "out-of-favor" products on e-commerce platforms using a data-driven approach that combines sales forecasting and association rule mining?
        - *Rationale*: There is a lack of dedicated methods for identifying products *specifically declining* in popularity, requiring a novel approach.
    - **RQ2**: What product characteristics and external factors are associated with a higher likelihood of a product becoming "out-of-favor"?
        - *Rationale*: Understanding these factors allows for proactive intervention and targeted strategies to prevent further sales decline.
- The study focuses on these questions due to the evolving nature of e-commerce and the need for platforms to adapt to changing consumer behavior and market dynamics.

# 2. Literature Review

- The literature review covers relevant research in the areas of sales forecasting, product lifecycle management, and data mining techniques for e-commerce.
- **Sales forecasting** is a well-established field with various techniques, including time series analysis, regression models, and machine learning algorithms [Hyndman & Athanasopoulos, 2018].
    -  Time series models, such as ARIMA and exponential smoothing, are commonly used to predict future sales based on historical data [Box et al., 2015].
    -  Regression models can incorporate external factors, such as price, promotions, and seasonality, to improve forecasting accuracy [Armstrong, 2001].
- **Product lifecycle management** focuses on managing products throughout their entire lifecycle, from introduction to decline [Kotler & Armstrong, 2016].
    -  Identifying products in the *decline stage* is critical for making informed decisions about pricing, promotion, and inventory management.
    - Traditional product lifecycle models often rely on subjective assessments and expert opinions, which can be time-consuming and inaccurate [Tellis & Crawford, 1981].
- **Data mining techniques** have been widely applied in e-commerce to extract valuable insights from large datasets [Han et al., 2011].
    - **Association rule mining** is a technique for discovering relationships between different items in a dataset [Agrawal et al., 1993].
        - It can be used to identify products that are frequently purchased together or to understand the characteristics of customers who buy certain products.
-  While these areas have been studied extensively, there is limited research that *specifically* addresses the problem of identifying "out-of-favor" products on e-commerce platforms.
    - This study bridges this gap by combining sales forecasting and association rule mining to develop a data-driven system for identifying and understanding product decline.

# 3. Methodology

- The study proposes a two-stage methodology for identifying "out-of-favor" products:
    - **Stage 1:** Sales Forecasting using **Bayesian Structural Time Series (BSTS) Model**.
    - **Stage 2:** Association Rule Mining to Identify Factors Associated with Product Decline.

## 3.1. Stage 1: Sales Forecasting Using BSTS Model

- The **BSTS model** is employed to forecast future sales for each product based on its historical sales data.
-  The BSTS model is a flexible and powerful time series model that can capture trends, seasonality, and the impact of external events [Scott & Varian, 2014].
    - It is particularly well-suited for e-commerce data, where sales patterns can be complex and influenced by various factors.
- The BSTS model decomposes the time series into several components:
    - **Trend component:** Captures the long-term upward or downward movement in sales.
    - **Seasonal component:** Captures the periodic fluctuations in sales, such as weekly or monthly patterns.
    - **Regression component:** Captures the impact of external events, such as promotions or price changes, on sales.
    - **Error component:** Captures the unexplained variation in sales.
- The model estimates these components using Bayesian inference, which allows for incorporating prior knowledge and quantifying uncertainty.
- A product is identified as "out-of-favor" if its actual sales are significantly below the predicted sales from the BSTS model.
    - Specifically, the study defines a threshold based on the posterior predictive distribution of the BSTS model to determine whether a product's sales are unexpectedly low.

## 3.2. Stage 2: Association Rule Mining

- After identifying "out-of-favor" products, **association rule mining** is used to discover relationships between product attributes and their likelihood of being in this category.
- Association rule mining identifies rules of the form "If A, then B," where A is a set of product attributes (e.g., category, price, rating) and B is the event that the product is identified as "out-of-favor."
- The **Apriori algorithm** is used to generate association rules [Agrawal & Srikant, 1994].
- The strength of each association rule is measured using three metrics:
    - **Support:** The proportion of transactions that contain both A and B.
    - **Confidence:** The proportion of transactions that contain A that also contain B.
    - **Lift:** The ratio of the observed confidence to the expected confidence if A and B were independent. A lift value greater than 1 indicates a positive association between A and B.
- The study focuses on rules with high confidence and lift values to identify the most significant factors associated with products becoming "out-of-favor."
    - For example, a rule might indicate that products in a specific category with low ratings and no recent promotions are more likely to experience sales declines.

# 4. System Design

- The study designs a system that integrates the BSTS model and association rule mining to provide a comprehensive solution for identifying and understanding "out-of-favor" products.
- The system consists of the following modules:
    - **Data Collection Module:** Collects historical sales data, product attributes, and external factors from the e-commerce platform.
    - **BSTS Modeling Module:** Builds and updates the BSTS model for each product.
    - **Out-of-Favor Product Identification Module:** Identifies products whose actual sales are significantly below the predicted sales from the BSTS model.
    - **Association Rule Mining Module:** Discovers relationships between product attributes and their likelihood of becoming "out-of-favor."
    - **Reporting and Visualization Module:** Presents the results in a user-friendly format, allowing platform managers to easily identify and understand the factors contributing to product decline.
- The system is designed to be automated and scalable, allowing it to handle large product portfolios and adapt to changing market conditions.

# 5. Experimental Evaluation

- The proposed system is evaluated using real-world sales data from an e-commerce platform.
- The data includes historical sales data, product attributes (e.g., category, price, rating), and external factors (e.g., promotions, seasonality).
- The performance of the BSTS model is evaluated using metrics such as **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**.
- The accuracy of the out-of-favor product identification module is evaluated using metrics such as **precision**, **recall**, and **F1-score**.
- The usefulness of the association rules is assessed by evaluating their **support**, **confidence**, and **lift** values.
- The experimental results demonstrate that the proposed system can accurately identify out-of-favor products and provide valuable insights into the factors that contribute to their decline.

# 6. Conclusion

- This study presents a data-driven approach for identifying "out-of-favor" products on e-commerce platforms.
- By combining the **BSTS model** with **association rule mining**, the study provides a comprehensive solution for both predicting and understanding product decline.
- The proposed system can accurately identify out-of-favor products and provide valuable insights into the factors that contribute to their decline.
- The findings of this study can help e-commerce platform managers to improve product lifecycle management, optimize their product portfolios, and reduce losses associated with declining products.
- Future research could explore the use of more advanced machine learning techniques for sales forecasting and association rule mining.
- Additionally, future studies could investigate the impact of different corrective actions on the sales performance of out-of-favor products.

---

# Executive summary of 1. Introduction
- E-commerce platforms struggle with **"out-of-favor" products**, which negatively impact revenue and profitability.
- Proactive identification is crucial for timely interventions like promotions or inventory adjustments.
- Existing research lacks focus on *identifying* declining products.
- This study develops a system combining sales forecasting and association rule mining for identification and understanding of product decline.
- The research questions focus on the accuracy of the system and the identification of contributing factors.

# Executive summary of 2. Literature Review
- The literature review covers sales forecasting, product lifecycle management, and data mining.
- Sales forecasting uses time series, regression, and machine learning.
- Product lifecycle management needs better methods for identifying products in decline.
- Data mining, especially association rule mining, can reveal relationships in e-commerce data.
- Limited research combines these areas to address "out-of-favor" products specifically.

# Executive summary of 3. Methodology
- The study proposes a two-stage methodology: Sales forecasting using BSTS, and association rule mining.
- **BSTS model** captures trends, seasonality, and external events for accurate sales prediction.  Products with sales significantly below predictions are flagged.
- **Association rule mining** identifies relationships between product attributes and the likelihood of being "out-of-favor."
- The **Apriori algorithm** generates rules, evaluated by support, confidence, and lift.

# Executive summary of 4. System Design
- The system integrates the BSTS model and association rule mining.
- Modules include data collection, BSTS modeling, out-of-favor product identification, association rule mining, and reporting/visualization.
- The system is designed to be automated, scalable, and user-friendly.

# Executive summary of 5. Experimental Evaluation
- The system is evaluated with real-world e-commerce sales data.
- BSTS model performance is measured by MAE and RMSE.
- Out-of-favor product identification is evaluated by precision, recall, and F1-score.
- Association rules are assessed by support, confidence, and lift.
- The results demonstrate the system's accuracy and the insights it provides.

# Executive summary of 6. Conclusion
- The study presents a data-driven approach for identifying "out-of-favor" products.
- Combining BSTS with association rule mining provides a comprehensive solution.
- The system accurately identifies products and reveals contributing factors.
- The findings can improve product lifecycle management and reduce losses.
- Future research could explore advanced machine learning techniques and the impact of corrective actions.

</details>

```
title: Maximizing Online Revisiting and Purchasing: A Clickstream-Based Approach to Enhancing Customer Lifetime Value
authors: Wael Jabr, Abhijeet Ghoshal, Yichen Cheng & Paul Pavlou
journal: Journal of Management Information Systems
published: 2023
```
 
# Executive Summary
- This research addresses the growing focus of online retailers on long-term customer relationships by encouraging repeat visits and purchases, which will increase **customer lifetime value (CLV)**.
- We developed a two-stage model to characterize and predict customer revisiting and purchasing behaviors.
    - Stage 1: Characterizes customer **propensity** for revisiting the retailer's website using a **logistic regression (hurdle model)**.
    - Stage 2: Develops a **stochastic model** that predicts revisits and incorporates individual **customer heterogeneity** in search effort during repeated visits, using an **Erlang-2 Gamma mixture model**. This heterogeneity accounts for individual customer preferences in consideration sets, product information, pricing, and search environment.
- We analyzed granular **clickstream data** to capture facets of revisiting and purchasing, incorporating variables characterizing:
    - Choice (customer exploration of product alternatives)
    - Information (reading product reviews)
    - Pricing (acquisition cost vs. competing products)
    - Search environment (time of day, day of week, device used)
- Key **findings** and arguments:
    - Larger consideration sets may **reduce** the chance of revisiting but **increase** the probability of purchasing.
    - More clicks while browsing products in consideration set could **reduce** the likelihood of purchase.
    - Higher prices affect revisiting and purchasing in **opposite ways**: increasing revisiting probability but decreasing purchasing probability.
    - Mobile devices decrease both the probability of revisiting and purchasing.
- We leverage computationally efficient simulation-based prescriptive analytics to propose practical intervention strategies that maximize the joint likelihoods of customers revisiting and purchasing at the individual customer level.
- The model contributes to methodology by:
    - Developing a new predictive mixture model for estimating the probability of individual customers revisiting, jointly accounting for granular clickstream data for both revisiting and purchasing.
    - Developing a novel prescriptive analytics approach to determine intervention levels to maximize the joint probabilities of revisiting and purchasing, tailored to individual customers’ preferences.
    - The modeling approach is fully explainable and managerially interpretable, shedding light on system rationale and execution.
 
#clickstream_data #probabilistic_modeling #prescriptive_analytics #explainable_modeling #lifetime_value #online_retail #e_retail #online_repurchasing #customer_heterogeneity
 
<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- A large percentage of customers do not purchase during their initial visit to a retailer's website, but use initial visits to identify the right products [Poesler, 2018].
    - Subsequent visits involve shortlisting products, forming consideration sets [Ke et al., 2016], and evaluating products across various dimensions (product descriptions, comparisons, reviews).
    - Purchases often occur after several visits.
- From the retailer's perspective, focusing on both revisiting and purchasing is key to improving long-term customer relationships and enhancing **customer lifetime value** [Kadiyala et al., 2021].
    - It is crucial to model both processes and identify drivers that affect the likelihoods of these activities.
    - Predictive models must integrate individual customer patterns, their variation over time, search intensity, and variability in search patterns.
    - The model incorporates these components by modeling revisits and maximizing the joint probabilities of revisiting and purchasing.
- The research develops an approach that:
    - Builds a model that is better at predicting the probability of individual customers’ subsequent visiting and purchasing over their lifetime, while incorporating data from their prior visits and their individual search heterogeneity,
    - Lends itself to easy interpretability to enable retailers to devise intervention strategies that exploit each customer’s search history to maximize the joint probability of a customer revisiting and purchasing at the individual level.
    - This approach allows retailers to personalize content based on individual search and purchase history.
- Granular clickstream data [Bhatnagar et al., 2017; Bronnenberg et al., 2016; Montgomery et al., 2004] is used to capture facets of revisiting and purchasing, including variables characterizing choice, information, pricing, and search environment.
    - Choice: accounts for how customers explore product alternatives, that is, their considerations set, prior to purchasing [Ke et al., 2016]. Too much choice can overwhelm the customer by causing cognitive challenges because of information overload [Chernev et al., 2015].
    - Information: accounts for reading product reviews [Babić Rosario et al., 2016; Banerjee et al., 2017; Ke et al., 2016]. Reviews may improve decision making, albeit also run the risk of overloading customers.
    - Price: accounts for the acquisition cost against competing products.
    - Search environment: accounts for the potential influence of the time of day and day of the week on which a visit occurs [Bhatnagar et al., 2017].
- We recognize that the content viewed during browsing sessions (or visits) may be processed differently by different customers, which is why we account for **customer heterogeneity** in our model. In doing so, our work extends prior work that emphasizes the effect of content personalization cues on both repeat website visits and customers’ willingness to pay [Benlian, 2015].
- Three key contributions:
    - Using a theory-based model of customer choice and decision making, we analyze the role of the various parameters of customer search, and show how certain parameters might work in opposing directions in influencing revisiting and purchasing.
    - Through a novel prescriptive analytics model, we propose appropriate intervention strategies to target individual customers to maximize both their individual revisiting and their purchasing probabilities based on their search history.
    - Our model contributes to methodology in three distinct ways.
 
## 1.1. Key Empirical Findings
- Key empirical findings:
    - Larger consideration sets may reduce the chance of revisiting, but increase the probability of purchasing.
    - More clicks a customer makes while browsing the products in her consideration set could also reduce the likelihood of purchase.
    - Price affects revisiting and purchasing in opposite ways: increasing revisiting probability but decreasing purchasing probability.
    - Mobile devices impact both activities similarly, decreasing both the probability of revisiting and of purchasing.
    - We highlight the corresponding managerial implications of these empirical findings.
- We propose appropriate intervention strategies to target individual customers to maximize their individual revisiting and purchasing probabilities based on their search history.
    - The retailer can apply our approach to customize their webpages for individual customers, which can be informed by, for example, the ideal number and length of the review text.
    - The retailer could also identify the optimal size of the product consideration set that each customer should be exposed to, in order to avoid information overload and maximize the likelihood of individual customers revisiting and purchasing.
- Model's methodological contributions:
    - Develop a new predictive mixture model for estimating the probability of individual customers revisiting, jointly accounting for granular clickstream data for both revisiting and purchasing. By maximizing these joint probabilities, we enable the retailer to consider the probability to purchase in each subsequent visit, thereby increasing customer lifetime value.
    - Develop a novel prescriptive analytics approach to show how a retailer can determine the appropriate intervention levels that would maximize the joint probabilities of revisiting and purchasing, and tailor those to individual customers’ preferences.
    - The modeling approach is fully and readily explainable and managerially interpretable. Specifically, our approach sheds light on the system’s rationale concerning which parameters to incorporate—in our case, adopting four proposed dimensions, i.e., choice, product information, price, and the search environment.
 
# 2. Literature Review
- Our work builds on literature studying customer online search patterns and strategic interventions to ensure repeat visits and purchases [Bazaarvoice, 2017].
    - The focus is on using customer clickstream data leading to purchasing, specifically investigating parameters for search including the effort exerted while browsing.
- A top priority among retailers is to better understand customer behavior over their visits [Marketing Science Institute, 2018], particularly with the increasing availability of clickstream data.
    - With the ease of tracking, retailers can better infer customer preferences [De Los Santos and Koulayev, 2017] and more accurately predict their behavior [Guha and Kumar, 2018], even without a purchase [Montgomery et al., 2004].
    - We now know how customers spend time browsing online to gather information before making a purchase and how these browsing characteristics help predict purchases [Bucklin and Sismeiro, 2009] as part of an implicit search strategy [Bronnenberg et al., 2016].
    - The online purchasing process is often conceptualized into two distinct stages: the search stage (exploring product webpages) and the purchase stage [Mallapragada et al., 2016].
 
## 2.1. Dimensions Influencing Customer Decision-Making
- During these two stages, several dimensions influence customer decision-making, which we group into choice, information, pricing, and the search environment.
    - Choice: focuses on identifying customers’ consideration set, and the information they gather to form their individual consideration set [Gu et al., 2011; Ke et al., 2016]. Specifically, Moe and Fader [2004] and Bucklin and Sismeiro [2009] account for the number of clicks on product pages that customers visit. Bhatnagar et al. [2017] also account for the product categories that customers explore. We label these variables as “choice-related” because they are a proxy for the available alternatives.
    - Information: the clickstream data allow researchers to better understand the information acquisition process that customers undergo when reducing their purchasing uncertainty. In that respect, product reviews have been shown to be influential [Babić Rosario et al., 2016]. Both the length and the valence of those reviews play an influential role. We label these variables as “information-related.”
    - Pricing: Pricing of the products in the consideration set has also been shown to influence customer decision making. We label this variable as “price-related.”
    - Search environment: Whether customers use a mobile device or a desktop/laptop becomes a proxy for convenience and search costs [Ghose et al., 2013]. Additionally, Bhatnagar et al. [2017] incorporate day of week and time of day in their model. Bronnenberg et al. [2016] and Lin et al. [2010] focus on visit duration. We label these variables as the “search environment-related” dimension.
- The effort exerted during online search has been well-recognized as a key factor [Kuruzovich et al., 2008].
    - It has been measured in various ways, e.g., number of repeat visits, pages viewed, materials viewed.
    - Johnson et al. [2004], for example, divide it into three components: depth of search (number of stores visited), dynamics of search (the change in the number of stores visited over time), and activity of search (the overall amount of category-level shopping activity).
    - Janiszewski [1998] dichotomized search patterns as either goal-directed search or exploratory search based on the amount of time spent viewing a certain piece of information. Subsequently, Montgomery et al. [2004] and many others followed a similar classification and categorized search effort as either casual browsing or deliberate search.
- In our model, we incorporate customer search effort and, unlike any prior studies, we account for its heterogeneity from one customer visit to the next as a key feature of our approach.
 
## 2.2. Modeling Approaches & Non-Contractual Customer Modeling
- Earlier research has proposed a variety of approaches to characterize customer online visits and/or purchases, including regression models, game theory-based models, Bayesian models, mutually exciting point process models, vector autoregressive models , and hidden Markov models.
    - Model-based approaches, which is the most closely related to our work, propose statistical mixture data generation processes for the observed clickstream data. In this line of work, Moe and Fader [2004] were the first to use clickstream data for the prediction of purchase patterns and to develop a model with which to predict purchase conversion based on click-stream data and prior purchases.
    - In the same vein, the focus is on avoiding myopia when attributing conversion to the very last visit rather than the full search history (e.g., Xu et al. [2014]). Such a modeling approach rather attributes purchases to the accumulative effects of previous clicks, an approach we pursue in our own research.
- A non-contractual customer is one for which the firm is not aware of whether she is still “alive” until a subsequent purchase is made.
    - In early work, Schmittlein et al. [1987] proposed a generic “Buy-Till-You-Die” approach using a Pareto/Negative Binomial model to count the number of active members and predict their future activity.
    - Fader et al. [2004] proposed a dynamic change-point model that captures the underlying evolution of purchase patterns associated with multiple-event timing models.
    - Fader et al. [2005] proposed a Beta-Geometric /Negative Binomial model which assumes that dropout can occur immediately after a purchase rather than at any point in time.
    - In the healthcare context, Bardhan et al. [2015] developed a Beta-Geometric/Erlang-2 hurdle model to model revisit propensity, frequency, and timing of patient readmissions (which can be likened to customers revisiting).
- Although these studies are similar to ours in their modeling approach, none consider using the customer search history to predict revisits, while accounting for heterogeneity in the customer search effort.
 
# 3. Model Development
- To analyze customer revisits and purchase propensities while accounting for customer heterogeneity, we develop a two-stage model.
    - Our specification starts with a **hurdle model** that treats customers who visit only once (non-repeat customers) as inherently different from those who visit more than once (repeat customers).
    - Unlike non-repeat customers, repeat customers revisit to learn more about a product before potentially purchasing it.
- In the first stage, the hurdle model estimates the probability of crossing a hurdle, which is labeled as participation decision by Wooldridge [2010, p. 690].
    - Our modeling approach can accommodate for the typical higher incidences of non-repeat customers that is characteristic of this type of data [Winkelmann, 2008].
- In the second stage, for customers who cross the hurdle (i.e., repeat customers), we use a **stochastic Erlang-2 Gamma model** to predict customer revisits.
    - Erlang-2 is better suited to model relatively more “regular” customer visits [Morrison and Schmittlein, 1988] and performing better than competing models for predicting customer arrivals [Gupta et al., 1991; Gupta, 1991].
- As customers may exhibit heterogeneity in their search effort across sessions, we account for two such levels, High and Low, and, accordingly, label individual customer sessions as either high search effort (type H session) or low search effort sessions (type L session).
    - This approach avoids restricting a customer to a single type across all their revisit sessions, thus providing further flexibility to our model.
 
## 3.1. Stage 1: Hurdle Model
- We use a logistic regression to model the probability that customer *i* revisits the website after their initial visit (repeat customer).
- Let *Yi* be a binary variable to indicate customer *i*'s revisit, with *Yi* = 1 when the customer revisits and 0 otherwise. The probability of revisiting is Prob(*Yi* = 1) = *θ0i*.
- The logistic model is:
    - Prob(*Yi* = 1) = *θ0i* = 1 / (1 + exp(-*X0iζ0i*))
    - *X0i* is the set of customer-level variables for customer *i*, and *ζ0i* is their coefficients at time period 0 (first time the customer is observed).
    - *i* = 1, ..., *N* + *M*, where *N* is the number of repeat customers and *M* is the number of non-repeat customers.
 
## 3.2. Stage 2: Erlang-2 Gamma Mixture Model
- For customers predicted to cross the hurdle and revisit, we estimate the interarrival time using an Erlang-2 Gamma mixture model.
    - The time interval between two consecutive arrivals of the same search effort type follows an Erlang-2 distribution.
    - The Erlang-2 distribution is expressed as *f(x; λ) = λ2xe-λx* for *x, λ ≥ 0*, where *x* is a continuous random variable and *λ* is the revisit rate.
- We specify customer *i*'s arrival rate for a given search effort to depend on a set of search-related variables of interest.
    - The arrival rates of each type (arrivals with sessions of types H or L) as functions of a baseline rate, plus the other variables.
- We account for time varying variables *Xtj* (*Xtk*) characterizing an arrival at the *j*th (*k*th) time when exerting effort type H (L). Then, the individual probability density function during time interval *tj-1; tj* follows from the definition of an Erlang-2 distribution.
- Survival functions of an individual customer while exhibiting a High (Low) search effort are given by:
    - *SH(t) = 1 + λHψ(t)e-λHψ(t)*
    - *SL(t) = 1 + λLψ(t)e-λLψ(t)*
- To account for the possibility that the baseline arrival rates may differ across heterogeneous customers, we assume that the rates *λH* and *λL* are drawn from Gamma distributions.
 
## 3.3. Likelihood Function
- Next, we derive the likelihood function at the customer level for estimating the parameters while accounting for the type of search effort in a session (i.e., whether the session is one of High or Low search effort), as well as for the number of visits the customer makes. To simplify the likelihood derivation, and for mathematical tractability, we devise three cases of customers on the basis of their search effort during visit sessions (see Table 2).
    - Case 1: Low search effort across all visits
    - Case 2: High search effort across all visits
    - Case 3: Both Low and High search effort across visits
- Separate likelihood expressions are developed at the individual customer level for each of the three cases, depending on the sequence in which the customer arrives and exerts either Low or High effort during these sessions.
- The likelihood function to estimate the coefficients γ and δ is constructed by multiplying the likelihoods of revisits and purchases of all individual customers.
 
# 4. Model Data and Estimation
- Our dataset comes from a large online retailer in the United Kingdom ranking as a top-five retail website in the UK.
    - The dataset spans the period between February and March 2015 tracking a large number of customers and their visits.
    - The clickstream data covers a variety of consumer patterns, including visits to product pages and reviews, and purchases made.
    - The data track the time when users click on review pages to view customer reviews and whether a review content appears on the customer’s browser, thus allowing us to identify which reviews have been accessed.
    - The data consist of 35,206 products in two categories, Home or Technology. Each category consists of hundreds of subcategories
- We use a time-based approach to define a session as a series of customer interactions with the website that end after 30 minutes of inactivity.
- We derive our variables which characterize the revisiting and purchasing pattern for each customer per session.
    - These variables consist of four sets intended to characterize each of the four dimensions (choice, information, pricing, and search environment).
    - We opted to reduce this number to ensure model parsimony, and to reduce the data collection and cleaning effort needed from the firm to generate those variables.
    - Whenever any variables were found to be highly correlated (i.e., with a correlation coefficient larger than 0.5), we included one of these variables, typically the one that is the most intuitive.
 
## 4.1. Variable Derivation
- First, for the choice-related variables, we derive the variable *Click Count*, *Product Count*, *Home Subcategory Count*, *Tech Subcategory Count*. These variables are widely adopted in the IS and marketing literature.
- Second, for the information-related variables, we derive the *Review Count*, *Average Rating for Consideration Set*, *Sentence Count for Reviews Browsed*.
- Third, for the pricing-related variables, we derive the *Average Price for the Consideration Set*.
- Fourth, for the search environment-related variables, we derive the *Mobile Dummy*, *Weekday Dummy*, *Weekhour Dummy*, and the *duration of the session*.
- Using clickstream data, we conduct a cluster analysis to characterize session-level customer search effort as either a high or a low search effort [Trusov et al., 2016].
    - The clusters are based on the variables introduced earlier, namely choice, information, pricing, and the search environment.
    - We implement a two-step iterative expectation maximization (EM) approach [Adomavicius et al., 2012].
    - The first step is the expectation step, which estimates the probability distribution of the clusters, and the second is a maximization step, which finds the model parameters that maximize the expected log likelihood of the solution.
    - The EM approach performs best with two clusters, and deteriorates with an increasing number of clusters.
    - We end up with 60.1 percent of customer sessions falling into the low-type cluster and 39.9 percent of the sessions into the high-type cluster.
 
## 4.2. Model Estimation and Results
- For model estimation, we first run the logistic regression in Equation 1 to classify customers who cross the hurdle—by revisiting—from those who do not.
    - We then run the Erlang-2 Gamma mixture model to derive the probabilities of revisit and purchase.
    - For repeat customers who cross the hurdle, the log-likelihood function is maximized to estimate vector γ for revisit variables and vector δ for purchases.
    - We use the “GenSA” package in R [Xiang et al., 2018] to find the maximum likelihood estimator (MLE) by maximizing the log-likelihood function using generalized simulated annealing. The Hessian matrix is calculated at the MLE to obtain standard errors and significance levels of the estimators.
- From the hurdle model results, we find that a number of variables increase the probability of repeat visits, consisting of the choice-related variable Product Count, the information-related variable Sentence Count, and the search environment-related variable Weekday.
    - On the other hand, the variables that decrease the probability of repeated visits consist of two choice-related variables, Home Subcategory Count and Tech Subcategory Count, two information-related variables, Review Count and the Average Rating for Consideration Set, the pricing variable Average Price for Consideration Set, and the search environment variables Duration and Mobile.
- Customers using a mobile phone to access the retailer website are less likely to be a repeat customer, suggesting that mobile devices can easily overwhelm customers when a large amount of information is presented to them, resulting in a poor customer experience.
 
## 4.3. Revisiting and Purchasing Models
- We find that the choice-related variable Click Count, information-related variable Average Rating for Consideration Set, pricing-related variable Average Price for Consideration Set, and control variable Duration increase the probability of revisiting.
    - On the other hand, other choice-related variables, namely Product Count, Home Subcategory Count, and Tech Subcategory Count, and information-related variables, Review Count, and Sentence Count decrease the probability of revisiting. Last, variables related to the search environment, Mobile dummy, Week day dummy and Week hour dummy decrease the probability of revisiting.
- For the purchasing model, we find that choice-related variable Click Count and pricing-related variable Average Price for Consideration Set both have a negative role in the probability of purchasing.
    - On the other hand, choice-related variables, including Home Subcategory Count and Tech Subcategory Count, the two proposed information-related variables Sentence Count and Average Rating for Consideration Set, in addition to the variable Duration related to the search environment, increase the probability of purchasing.
- Choice-related variables, such as size and variety of the consideration set, change sign, with Click count switching from positive for revisiting to negative for purchasing, and Home Subcategory Count and Electronics Subcategory Count switching from negative for revisiting to positive for purchasing.
    - As reported in prior work, when more products are shown to customers, the large assortment size to which they are exposed may overwhelm those customers, thus discouraging them from revisiting the retailer’s website [Kuksov and Villas-Boas, 2010].
- Our analysis suggests that simply reading more reviews about products correlates with a lower likelihood of revisiting, with no significant role in the likelihood of purchasing.
    - Simply put, more online reviews correlate with driving customers away. Not only is the review count is associated with a lower likelihood of revisiting, but reading more review sentences (Sentence Count) also is observed to be associated with the deterrence of revisiting.
- A higher average price decreases the likelihood of purchasing. However, the positive effect of a higher-than-average price on the probability of revisiting is surprising.
    - This can be explained by the long-held link in the literature between price and perceived quality [Chang and Wildt, 1994; Kalita et al., 2004]. If customers perceive the quality of products to be high, they are more likely to revisit.
- Duration increases the likelihood of both revisiting and purchasing. When customers spend longer periods of time on the retailer’s website, they show engagement and interest. Such customers are more likely to purchase.
    - On the other hand, accessing the retailer’s website from a mobile device reduces the likelihood of revisiting with no impact on purchasing, a finding that may not be very reassuring for retailers, given customers’ increased reliance on mobile devices.
 
# 5. Prediction Process and Results
- Stage 2 of our model can be used for predicting future customer arrivals, thus allowing retailers to use our model to increase customer engagement, thus building loyalty through visits, increasing purchases, and thus better inventory planning. These elements have undoubtedly become increasingly crucial to retailers [Kumar, 2020; Lityx, 2020].
    - We use customers’ clickstream history from revisits to predict changes in their revisiting patterns and to evaluate our model’s predictive performance.
- We compare the performance of our model against two benchmark models.
    - The first is a baseline logistic model that predicts the odds of revisiting, which is widely used in the literature.
    - The second is a proportional hazard model with exponentially distributed inter-arrival time, which predicts the probability of arrival within a set time window.
- As is customary in the literature, we split our dataset into two parts, with the first part consisting of the first six weeks of data that are used as the training set, while the remaining data are used as a holdout sample for testing.
- To assess prediction performance, we plot the lift curves for our method. We compare them with the lift curves for the proportional hazard model and a simple logistic regression.
    - The plots show that our model overall outperforms the competing methods, based on the area under the curve metric.
    - Our method performs best for those customers whose probability of revisiting is relatively low. In other words, our model is effective at correctly predicting the revisiting behavior of precisely those customers that are inherently more difficult to predict.
 
# 6. Prescriptive Analysis
- Next, we apply our modeling approach to help the retailer prescribe optimal response strategies [Bertsimas and Kallus, 2020; Grover et al., 2018]. Retailers strategically seek to optimize conversion rates through repeat visits and purchases [Liu et al., 2019].
- Simulation-based prescriptive analytics is often the method of choice to inform the decision process. It can be conveniently repeated with new incoming customer data, thus enabling retailers to adjust the models and insights with the changing preferences of individual customers.
- We focus on the strategy of maximizing the joint probabilities of revisiting and purchasing, to maximize conversion.
- Our prescriptive analytics recommendations can be used to increase the probability of customers revisiting and purchasing in the future for those customers who are less likely to purchase during a given session.
    - Even for customers who are highly likely to make a purchase during the current session, our prescriptive analytics enable retailers to still take actions that increase the likelihood of additional revisits and future purchasing.
- Our approach makes use of the sign of the variable coefficients in Table 7.
    - Intuitively, if a variable increases the probability of revisiting and purchasing (i.e., has a positive sign for both), the retailer should maximize the value of that variable as much as possible, within realistic ranges, by incentivizing customers to expand these variables.
    - Conversely, if the variable negatively influences both probabilities, then the retailer should minimize the value of the parameter, though that is not something that can easily be achieved in practice.
- For those variables that affect the two probabilities in opposite directions, we implement a series of granular simulations targeting individual customers to determine the optimal values that maximize the joint probability for each individual customer, which we compute as the product of the probabilities of revisiting and purchasing.
- We present our simulation results for one variable from each of three variable categories, that is, choice-related (Home Subcategory Count), information-related (Sentence Count), and pricing-related variables (Average Price for Consideration Set).
- In our simulation, and for each of these variables, we vary the variable magnitude by one unit at a time to examine the direction of its impact on an individual customer’s joint probability.
 
## 6.1. Simulation Results
- For the variable Home Subcategory Count, the trend and shape of the joint probability function are the same, with the joint probability increasing until it reaches a specific number of products in that category, and decreasing thereafter.
    - In our analysis, we find that the joint probability follows an inverted U-shaped relationship.
- Different customers are affected differently by the increase in choice. A website that displays the same webpage content and provides an identical browsing experience to all customers, irrespective of information available about their search history, is bound to generate suboptimal outcomes for the retailer. Instead, the web experience should be personalized to individual customers’ browsing patterns.
- In our sample, the effect of price on the joint probability is always decreasing. That is, for every single customer in our dataset, we find that the effect of price on the purchasing probability dominates the effect of price on the probability of revisiting, resulting in a decrease in the joint probability with an increase in price.
    - Our simulation provides insights by showing that the influence of price on the probability of purchasing is dominant.
 
# 7. Contributions and Implications
- In this paper, we make use of customer search history to model and predict repeat visits and purchases, while accounting for heterogeneity in the customer search effort. In doing so, we open the black box of online search [Ghose et al., 2019; Li et al., 2020; Montgomery et al., 2004; Mallapragada et al., 2016] and build on the emerging literature in non-contractual customer modeling [Bardhan et al., 2015; Xu et al., 2014].
- Unlike prior work that focused on website functionalities, product characteristics, or a single source of content, we focus on encompassing a rather comprehensive set of parameters that characterize customers during their website visits. We categorize them into four broad dimensions that affect their revisiting and purchasing, that is, choice, information, pricing, and the search environment. In doing so, we make use of rich clickstream data to model customer revisiting and purchasing.
- We extend the literature on search effort [Daurer et al., 2018; Schroder et al., 2019] by incorporating customer heterogeneity— in terms of their search patterns —into our prediction model for estimating the probability of customer repeat visits.
- The model we develop is the first to provide a framework for jointly predicting revisits and purchases, and for prescribing measures to maximize the probabilities of both revisiting and purchasing.
- We focus on long-term customer relationships with repeat purchases, i.e., customer lifetime value, rather than short-term profits.
- One implication of our work is the model’s ability to incorporate individualized customer search patterns rather than aggregate-level ones.
    - Such granular (individual-level) patterns allow us to exploit specific search dimensions during a customer revisit.
    - In so doing, we enable the retailer to better personalize the content presented to individual customers, and to design specific interventions aimed at promoting individual customers’ future revisiting and purchasing.
- By virtue of these simulations being related to each of the parameters in our model, we uncover both monotonic, as well as non-monotonic, effects on customer revisiting and purchasing, which can be of strategic value to retailers in practice.
- Our simulation-based approach for prescriptive analytics allows retailers to customize their websites by dynamically presenting the “right” amount and type of content to each individual customer to ensure they continue to revisit and to purchase.
- Our second implication is methodological. We build on and extend the literature on modeling customer purchases by accounting for a comprehensive set of parameters while incorporating customer search heterogeneity. To our knowledge, such an encompassing approach has not yet been attempted in the literature. In so doing, we build a comprehensive model that integrates customers’ search and purchase history at the individual level to jointly estimate the customer level revisiting and purchasing patterns.
 
## 7.1. Limitations
- It is possible that customers gain experience in navigating through a website and become familiar with the product offerings after visiting the website multiple times, which may affect their search efforts.
    - We have not considered such learning, because we do not currently have instruments to measure these long-term effects. Future research may focus on incorporating these longitudinal effects as well.
- We also recognize that our study does not distinguish between the content displayed and the content clicked for each individual customer, which is a common limitation for this type of study, given the data that most retailers can typically collect.
- Our data span two months, a rather short period. That said, however, this span is of less significance in the validity of our approach.
 
## 7.2. Conclusion
- The goal of this research is to develop a modeling approach for individualizing content to be displayed to customers during their online visits.
    - Our research takes an important step toward increasing loyalty and continued engagement of customers with retailers—which is fundamental in today’s hyper-competitive online retail environments [Dutta et al., 2017].
    - We achieve this feat by accounting for both revisiting and purchasing.
- We believe that our research provides the building blocks for strategic interventions that retailers can implement to strengthen their long-term relationship with customers so that they continue to visit the retailers’ website and purchase products over the long term.
---
# Executive summary of 1. Introduction
- This section introduces the importance of repeat customer visits and purchases for online retailers to enhance customer lifetime value.
- We emphasize the need to model both revisiting and purchasing processes, incorporating factors like individual customer patterns, search intensity, and variability.
- The research aims to develop a predictive and interpretable model using granular clickstream data, accounting for choice, information, pricing, and search environment variables, and proposes personalized intervention strategies to maximize customer engagement and conversion.
 
# Executive summary of 2. Literature Review
- This section reviews existing literature on customer online search patterns, clickstream data analysis, and factors influencing customer decision-making in online retail.
- We emphasize the need to consider choice, information, pricing, and search environment dimensions, as well as customer search effort and heterogeneity.
- We position our research in relation to prior work, highlighting the novel aspects of our approach in modeling revisits, accounting for customer heterogeneity, and providing prescriptive analytics.
 
# Executive summary of 3. Model Development
- This section introduces a two-stage model to analyze customer revisits and purchase propensities, accounting for customer heterogeneity.
- The first stage uses a hurdle model, implemented with a logistic regression, to distinguish repeat customers from non-repeat customers, while the second stage uses a stochastic Erlang-2 Gamma model to predict customer revisits.
- We account for customer heterogeneity in search effort by classifying sessions as high or low search effort, providing flexibility to the model.
 
# Executive summary of 4. Model Data and Estimation
- This section describes the dataset used for the research, which comes from a large online retailer in the UK.
- We outline the derivation of variables characterizing choice, information, pricing, and search environment, and the use of cluster analysis to characterize session-level customer search effort as high or low.
- We explain the model estimation process, including logistic regression and the Erlang-2 Gamma mixture model, and the use of the “GenSA” package in R for maximum likelihood estimation.
 
# Executive summary of 5. Prediction Process and Results
- This section discusses the use of our model for predicting future customer arrivals, enabling retailers to increase customer engagement, loyalty, and purchases.
- We compare the predictive performance of our model against two benchmark models: a baseline logistic model and a proportional hazard model.
- The results show that our model outperforms the competing methods, particularly for customers with a relatively low probability of revisiting.
 
# Executive summary of 6. Prescriptive Analysis
- This section applies our modeling approach to help retailers prescribe optimal response strategies, focusing on maximizing the joint probabilities of revisiting and purchasing.
- We discuss the use of simulation-based prescriptive analytics to determine the optimal values for variables affecting revisiting and purchasing, targeting individual customers.
- We present simulation results for choice-related, information-related, and pricing-related variables, showing the impact on joint probability and the need for personalized strategies.
 
# Executive summary of 7. Contributions and Implications
- This section summarizes the key contributions of our research, including the use of customer search history to model and predict repeat visits and purchases, the incorporation of customer heterogeneity, and the development of a framework for jointly predicting revisits and purchases.
- We discuss the managerial implications of our findings, such as the ability to personalize content and design interventions to promote customer engagement and conversion.
- We also acknowledge the limitations of our research and suggest directions for future work.

</details>

```
title: Recommendations and Cross-selling: Pricing Strategies when Personalizing Firms Cross-sell
authors: Abhijeet Ghoshal, Vijay S. Mookerjee & Sumit Sarkar
journal: Journal of Management Information Systems
published: 2021
```
 
# Executive Summary
- This research analyzes pricing strategies in online markets with **recommender systems** and **cross-selling**, considering both **monopoly** and **duopoly** scenarios.
- It investigates how **pricing strategies** change when firms cross-sell versus when they don't, and how these strategies evolve as a firm improves its **recommender system**.
- **Theoretical framework**: The paper develops a **game-theoretic model** with heterogeneous customers (varying in search costs) and firms that set prices to maximize profits.
    - Key elements include:
        -  Customers' **search costs**, **mismatch costs**, and **cross-selling surpluses**
        -  Firms' **recommender system effectiveness** and **cross-selling revenue**.
- **Key Findings:**
    - In a **monopoly**, cross-selling allows a firm to **subsidize** its price for focal products, while still maximizing profit.
    - In a **duopoly**, the firm with the inferior recommender system (low-type firm) always sets a lower price when cross-selling compared to not cross-selling. However, the high-type firm may *increase* its price under certain conditions.
    - When the **high-type firm improves its recommender system**, the low-type firm may *decrease* its price when firms cross-sell, a phenomenon not observed when firms do not cross-sell. This is counter-intuitive from prior literature on product quality.
- **Main contribution**: the study emphasizes the importance of considering customer search costs, cross-selling opportunities, and competitor reactions when setting prices with recommender systems.

#cross_selling #recommender_systems #pricing_strategies #game_theory #monopoly #duopoly #personalization #search_costs #mismatch_costs #customer_surplus #online_markets #oligopoly

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Online shopping is increasing, and retailers use **recommender systems** more often [Lee & Hosanagar, 2016].
- **Recommender systems** help customers choose from many products [Resnick & Varian, 1997]. Studies show they reduce search effort [Häubl & Trifts, 2000] and personalized offers are more likely to be accepted [Tam & Ho, 2005].
- Besides helping customers find products, **recommender systems** enable firms to **cross-sell** and increase revenue.
    -  **Cross-selling** is selling additional, often complementary, products [Akçura & Srinivasan, 2005; Bernazzani, 2018; Brown & Mehring, 2018; Kamakura, Kossar & Wedel, 2004; Shopify, 2019].
    - Firms offer more products based on customer preferences learned from cart items and past purchases.
    -  For example, a customer buying "The Lego Batman Movie" on Amazon might be recommended a "Toddler backpack."
    -  **Cross-selling** benefits both the firm (extra revenue) and the customer (extra surplus) [Akçura, Özdemir & Altinkemer, 2009].
    -  One retailer increased revenue by 20% with a **cross-selling** system based on McKinsey's advice [McKinsey, 2017].
- It's unclear how **cross-selling** affects the pricing of focal products when a firm provides product recommendations.
    - A firm might increase its price to capture extra customer surplus or subsidize prices to increase demand due to the extra revenue from cross-selling.
    - Products should be priced to balance these considerations, accounting for competitor reactions in a competitive market.
- Despite the connection between the search process, **cross-selling**, and **recommender systems**, little research has studied these relationships and their effects on the demand for focal and **cross-sold** products, pricing strategies, and profits.
    - In a monopoly, a firm should change its prices based on changes in search behavior and customer **cross-selling** surpluses when it improves its system.
    - In a duopoly, a change in a firm's **recommender system** may change the demand for a competitor's products, leading the competitor to lower its price.
- Given the increasing use of **recommender systems**, analyzing the interrelationships of customer search, purchase behavior, and **cross-selling** with the firm's pricing strategies is important.
    - The research focuses on markets with monopoly and duopoly where customers decide whether to search and purchase products for a given price, and firms set prices to maximize profits by selling their products and **cross-selling**.
- The specific research questions are:
    -  *(RQ1)* How do **pricing strategies** differ when firms **cross-sell** compared to when they do not?
    -  *(RQ2)* When firms **cross-sell**, how do prices change when a firm improves its **recommender system**?
- The study also examines the implications of these pricing strategies on the profits of the firms and customer surpluses.

# 2. Problem Setting
- We model a game between customers, heterogeneous in search costs and with full information, and electronic market places (firms) where customers shop to purchase products offered by the firms.
    - Customers have prior knowledge of recommender system effectiveness and **cross-selling** surpluses.
    - Firms set prices first for given **recommender system effectiveness**, and customers react to the prices when making a purchase decision.
- For example, a customer looking for an action movie on Amazon.com might see recommendations for various movies, as well as **cross-sold** products like toys or books sold by third parties.
- Customers expend effort in searching for products, and the **recommender system** reduces this effort.
    - Customers purchase products when they expect a non-negative surplus.
    -  The effectiveness of the firm’s **recommender system** influences the number of customers who purchase products and hence, the firm’s profit.
- Furthermore, the customers may purchase **cross-sold** products (i.e., offers of other products sold by the firm or its partners) that may be of interest to her.
    - If the customer purchases a **cross-sold** product, she enjoys a surplus, and the firm obtains additional revenue.
- The game is solved backward, starting with the customers’ search and purchase decisions, followed by the firms’ decisions of setting the prices.
- Every customer has a **reservation utility** for the products.
- Customers who search and examine products incur three costs:
    - (1) A **search cost** from the effort exerted in the search process.
    - (2) A **mismatch cost**, which is the opportunity cost borne by the customer from potentially purchasing a non-ideal product.
    - (3) the price of the product.
- The customer obtains a surplus from purchasing a **cross-sold** product (hereafter referred to as the customer’s **cross-selling** surplus).
- The expected surplus of a customer is the aggregate of the reservation price of the focal product (i.e., the willingness to pay amount) and **cross-selling** surplus minus the three costs (mismatch cost, search cost, and price).
    - For the customer to make purchases, this expected surplus should be non-negative.
- The model first analyzes a monopolist who sells its products to customers for a price that maximizes its profit.
- Next, it considers a duopoly where firms are asymmetric in their **recommender system effectiveness**.
    - The firm with the superior **recommender system effectiveness** is referred to as the **high-type firm**, and the other as the **low-type firm**.
    - Each customer either purchases a product from the firm that provides her a higher expected positive surplus or does not purchase from either firm.
    - Based on demand, the firms decide prices through a simultaneous move price game that maximizes their profits in equilibrium.
- In both the monopoly and duopoly scenarios, the pricing strategies are compared when the firms **cross-sell** with when the firms do not **cross-sell**.
- Further, the study the changes in the pricing strategies when the firms improve their **recommender systems** while **cross-selling**.
- Focal products within a category are considered, and it's assumed that a firm prices all the products the same within the category.
- The study analyzes how the search effort, optimal price, market size, and customer surpluses differ between situations when firms **cross-sell** and when they do not, and how they change when the **recommender system** improves, first for a monopoly and then for a duopoly.
-  Key findings of this research are:
    - *Finding 1:* When a monopolist **cross-sells** its own products, the pricing strategies do not change when it does not **cross-sell**.
    - *Finding 2:* When a monopolist **cross-sells** products of third parties, it charges a lower price for the focal product when it **cross-sells** if the expected **cross-selling** surplus of the customers is lesser than a threshold.
    - *Finding 3:* In a duopoly, when both firms **cross-sell** their own products, the pricing strategies remain the same as when they do not **cross-sell**.
    - *Finding 4:* When they **cross-sell** products of third parties, the low-type firm always sells products at a lower price compared to when the firms do not **cross-sell**.
    - *Finding 5:* If the customers’ expected **cross-selling** surplus is more than a threshold value, the high-type firm charges a higher price when the firms **cross-sell** than when they do not.
    - *Finding 6:* When the **high-type firm improves its recommender system**, and the firms **cross-sell** third party products, the firm always increases its price.
    - *Finding 7:* The low-type firm may decrease its price despite the increase in the differentiation between the two firms.
    - *Finding 8:* This unexpected phenomenon occurs when the extra **cross-selling** surplus (from the improved system of the high-type firm) leads to customers of the low-type firm switching in large numbers to the high-type firm.
    - *Finding 9:* The low-type firm reacts by decreasing its price in order to reduce the scale of migration; however, this phenomenon never happens when firms do not **cross-sell**.
- The rest of the paper is organized as follows.
    - The Literature Review section discusses the related research.
    - The Model Preliminaries section describes the basic setup of the model.
    - In the Monopoly section, a model for a monopolist is developed and analyzed.
    - In the Duopoly section, a market with a duopoly is analyzed.
    - The Extension: Some Customers Purchase Without Searching section extends the model to the case when some customers do not search and rather purchase the first product recommended to them.
    - Finally, the Conclusions and Future Research section provides managerial insights and concludes the paper.

# 3. Literature Review
- Our work is primarily related to the streams of literature on personalization, product
customization, vertical differentiation, and search costs. Since our main contribution is in
the area of personalization and recommender systems, we discuss that literature first.
- Recommendation system design and development is an active area of research [Guo et al., 2018].
- Researchers have extensively analyzed the value recommender systems generate
for customers using empirical techniques.
    - Kumar and Benbasat [2006] show that providing personalization services improve the perceived usefulness of the website.
    - Pathak et al. [2010] demonstrate increased sales when firms provide recommendations.
    - Zhang et al. [2011] show that higher quality product recommendations are associated with greater values derived by customers from a website that provides personalizing services and is also positively associated with repurchase intentions.
    - Thirumalai and Sinha [2013] delve into customer loyalty and personalizing services provisions.
    - Jabr and Zheng [2014] show that recommendation services intensify competition among products when customer reviews are also provided.
    - Wattal et al. [2009] consider the pricing decisions of two firms when they provide recommendations.
- Several recent papers have focused on operational aspects of recommender systems.
    - Liu et al. [2010] study the trade-off faced by a content-delivery website: either to deliver a superior personalized content with some delay (possibly), or to deliver an inferior version quickly.
    - Johar et al. [2014] consider a problem where a profit maximizing firm decides what proportion of a set of products shown to a customer (an offerset) should be targeted toward learning the preferences of a customer versus generating immediate sales.
    - Fleder and Hosanagar [2009] examine the effect of recommender systems on consumer choices: whether a recommender system helps consumers find new products, or whether it only reinforces the popular products.
- Several marketing researchers, such as Chen and Iyer [2002] and Chen and Zhang [2009], derive insights based on a firm’s ability to target customers based on the customers’ preferences.
- In our model with the duopoly, firms may be visualized as vertically differentiated in terms of the qualities of recommendation services they provide.
    - They are also differentiated in terms of the preferences of the customers toward them (i.e., customers are segmented based on the firm they prefer to purchase products from, depending on their costs).
    - Shaked and Sutton [1982] consider an oligopolistic market with customers heterogeneous in their incomes.
    - Wauthy [1996] consider vertically differentiated firms and heterogeneous customers.
    - Bhargava and Choudhary [2001] model a monopolistic firm offering products with different qualities.
    - Moorthy [1988] considers the competition between firms when customers have heterogeneous utilities for product qualities.
- Our research is quite distinct from this stream of research.
    - Our modeling setup has sharp differences from the ones previously discussed, stemming from the online context that we consider.
    - None of these papers consider a decision variable analogous to the effort of the customer, and this has important implications.
    - Second, we also model **cross-selling** and find results that either cannot be inferred or are counter to the expectations from the past literature.
    - We find that **cross-selling** revenue can subsidize the prices of products in both a monopoly and a duopoly which leads to more nuanced firm behaviors.

# 4. Model Preliminaries
- We consider a setting where a firm sells a vast array of products under a category (e.g.,
movies of different genres within the movies category, and sets a price for the category).
- A customer typically explores a number of products recommended to her in order to eventually purchase one [Moe & Fader, 2004; Poesler, 2018].
- The exploration may involve reading product descriptions and reviews, checking the ratings and popularity, sampling the products (e.g., listening to short samples of songs or watching movie trailers), and so forth.
- Overall, the surplus expected by a customer from visiting the firm, accounting for the costs she expects to incur in order to find an acceptable product, should be positive for the customer to initiate a session.
- Following are the factors that determine the expected surplus of the customer.

## 4.1. Reservation Price (R)
- This is a customer’s willingness to pay for a product within a category and is assumed to be the same for all the customers and for all the products within the category.

## 4.2. Price of the product *p*
- Since the products belong to the same category, in line with the existing literature, we
assume that all products have the same price [Wattal et al., 2009].
- This assumption is largely consistent with observations in companies like Amazon.com, which offers a considerable collection of movies for around $4.99, with relatively small variations in prices.

## 4.3. Search Cost
- This is the cost incurred by the customer for the effort she exerts when exploring the products while shopping.
- We denote the effort of the customer by *y*.
- As a customer must at least visit the site even before navigating through the website during the search, the effort y > 0. The search cost to a customer is positive when y > 0.
- As mentioned earlier, the customers are assumed to be heterogeneous in their search costs (i.e., the customers incur different search costs for the same amount of effort).
- A normalized parameter *θ* 2 [*α*; 1], where 0 < *α* < 1, is used to characterize a customer and is referred to as the search cost parameter of the customer (i.e., *θ* denotes the search cost to the customer per unit effort).
- We borrow the functional form of the search cost from [Janssen & Morga-González, 2004] who model it as the product of the number of inquiries made by the customer and the search cost per unit effort.
- The inquiries made by the customer is analogous to examining the products in the consideration set, which is directly proportional to the effort she exerts (i.e., *y*).
- As *θ* represents the search cost per unit effort, the total search cost is *Aθy*, where *A* is a positive scaling factor.
- Consequently, the expression *Aθy* represents the total expected search cost for a successful purchase.
- The probability density function for *θ*, *ψ*(*θ*), is assumed to be decreasing and convex like the Pareto distribution (or power law), that is, a customer has a higher probability of having an associated low search cost parameter than a high search cost parameter [Tarascio, 1973].
- The Pareto distribution represents well the distribution of income (or the opportunity cost for searching), and the cost of the search is expected to be proportional to the opportunity cost.
- The distribution of *θ* is known to the firm but *θ*’s of individual customers are not.

## 4.4. Mismatch Cost
- Usually, the number of choices available to the customer is enormous and the customer does not have the cognitive resources to examine all of them [Beach, 1993].
- Hence, the customer may end up purchasing a product that is not her ideal product, thereby incurring a mismatch cost.
- The mismatch cost depends upon the effort, *y*, spent by the customer in examining additional products and the **recommender system effectiveness**; that is, the **recommender system** influences the effort exerted by the customer.
- Häubl and Trifts [2000] show through a carefully designed experiment that users of a **recommender system** end up purchasing better quality products than those who do not use a **recommender system** for the same amount of effort, where the quality of the chosen product is measured in terms of fitness with their needs.
- This indicates that **recommender systems** alleviate the effort exerted by the customer to find a product of equal fitness.
- In other words, the effort of the customer interacts with the **recommender system**; we refer to the interaction as effective effort.
- Increased effective effort should help the customer find a product with a lower mismatch cost; that is, mismatch cost will be a decreasing function of the effective effort.
- The rate at which the mismatch cost decreases with an increase in effective effort is expected to diminish.
- As the customer examines more recommended products, that is, exerts more effective effort, it becomes increasingly difficult for her to find a product that is even closer to her ideal product and reduce her mismatch cost further.
- Consequently, we model the mismatch cost to be decreasing and convex in the effective effort using the functional form *B / effective effort*, where *B* is a positive scaling factor.
- Analytically, the effective effort should have the property that the **recommender system effectiveness** compensates for the marginal effort expended by the customer since a better **recommender system** should help the customer find a product closer to her ideal product for the same amount of effort of the customer.
- We represent the effective effort as *yr*^2 (the quadratic form in r helps in analytical tractability).
- We assume that *r* ¼ 1 for a firm with no **recommender system** (the maximum mismatch cost possible), and to be consistent with our setup that requires the presence of a functioning **recommender system**, we assume *r* > 1.
- For any valid value of r, the mismatch cost decreases with effort, and increasing the value of r leads to a reduction in mismatch cost (i.e., a better **recommender system** decreases the mismatch cost).
- The sum of search cost and mismatch cost is referred to as the product acquisition cost.

## 4.5. Cross-Selling Surplus
- The search process of the customer and the final purchase decision reveals useful information to the firm about the preferences of the customer [Akçura & Srinivasan, 2005].
- A firm can leverage this information and sell more products by **cross-selling** (e.g., Sainsbury makes **cross-selling** offers for selling music online [Baker, 2012]).
- **Cross-selling** may also occur through advertisements of products sold by partner firms (with referral fees accruing to the focal firm).
- The **cross-sold** products are usually the ones that the customer had not searched for explicitly but may be interested in given the products searched and items purchased.
- Such products are predictions made by the **recommender system** and often complement the main products purchased by the customer, potentially providing a positive surplus to the customers [Akçura et al., 2009].
- We assume that the customer purchases the **cross-sold** product if she expects to obtain a positive surplus from the product.
- For tractability, we denote the expected **cross-selling** surplus of the customer by *βr* where *β* (> 0) is the **cross-selling** parameter—a long-run average marginal benefit from **cross-selling** derived by a customer per unit **recommender system effectiveness**.
- On the firm’s side, suppose *N* denotes the total number of potential customers in the market.
- Let *σ* denote the expected **cross-selling** revenue per customer, which the firm obtains in expectation from selling the **cross-sold** products while selling the focal product.
- We assume that the firm knows how the costs of customers (mismatch cost and search cost) vary with θ, y and r.
- In the subsequent sections, we show how these factors impact the decisions of the customers and firms.

# 5. Monopoly
- We solve the full model backward by first determining the purchase decision of the
customers followed by the profit-maximizing price of the product.
- We first consider customers who are willing to expend effort in searching and examining one or more products before purchasing.
- The expected surplus (which is a function of the effective effort needed to identify an acceptable product) of a customer is
    - *S = R - Aθy - B / (yr^2) - p + βr*
- The customer expects to exert a surplus maximizing effort to find a product to purchase; this effort is *y*^(*) = sqrt(*B* / *Aθ*) / *r*.
- The optimal expected surplus expression for a customer then becomes
    - *S*^* = R - 2sqrt(*ABθ*) / *r* - p + *βr*  (1)
- It is clear from Equation (1) that *S*^* monotonically decreases with the customer’s search cost parameter *θ*.
    - Thus, customers with low *θ* values always obtain higher optimal surpluses than the customers with relatively higher *θ* values.
    - As expected, the highest amount of effort is spent by the customer with *θ* ¼ *α*, that is, the customer with the lowest search cost parameter.
- We assume that *R* - 2sqrt(*ABθ*) / *r* + *βr* > *p* to ensure that the firm has a market to sell its products (i.e., *S*^* > 0 for some customers at least).
- When a customer expects the maximum surplus to be positive (*S*^* >= 0), she engages in a search and potentially purchases.
- Otherwise, she does not search or purchase anything from the firm (the individual rationality constraint).
- A customer with the search cost parameter *θ* = *θ*^* is called a marginal customer if customers with *θ* > *θ*^* do not search (or buy).
    - The value of *θ*^* characterizes the market.
    - If *θ*^* < 1 for a given price, then the market is divided into two segments – purchasers (*θ* 2 [*α*; *θ*^*]) and non-purchasers (*θ* 2 [*θ*^*; 1]).
- Some properties of the market size are apparent from the expression of *S*^*.
    - When the firm improves its **recommender system** without increasing the price, the optimal efforts exerted by the existing customers and, consequently, their search costs and mismatch costs, decrease.
    - Also, some new customers start purchasing products. Hence, the market size increases.
    - Conversely, if the firm increases price without changing the **recommender system effectiveness**, the market size decreases. Thus, the firm should re-optimize its price when it changes its **recommender system effectiveness**.

## 5.1. Profit of the Firm
- The firm maximizes its profit by selecting the optimal price.
- For customers, we assume the following probability density function *ψ*(*θ*) = *k* / sqrt(*θ*) as this form demonstrates the desirable properties previously discussed.
- The revenue of the firm from sales of products is *pD*(*p*; *r*), where *D*(*p*; *r*) is the demand of the product.
    - The demand originates from the customers who intend to search and purchase, that is, *D*(*p*; *r*) = *N* / (2(1 - sqrt(*α*))) * Integrate(*θ*, *α*, *θ*^(*)) 1 / sqrt(*θ*) d*θ*.
- The revenue from **cross-selling** is *σD*(*p*; *r*).
- The costs incurred by the firm include the cost of developing the infrastructure for storing, analyzing, and maintaining the customer data [Leavitt, 2006].
- We ignore the cost of providing recommendations and focus on the revenue of the firm.
- Therefore, the total profit (revenue) of the firm is
    - *π = N(p + σ) * (*sqrt(θ^*) - sqrt(α)) / (1 - sqrt(α)) (2)*
- From Equation (2), we observe that the profit of the firm increases if the price increases, provided the size of the market does not change.
    - However, *θ*^* decreases when *p* increases.
    - Likewise, an increase in the **recommender system effectiveness** (*r*) increases the firm’s market size but may also justify an increase in the price.
    - Hence, the firm should balance the trade-off between price and market size by choosing the optimal price (*p*^*) to maximize its profit.
- The optimal price and profit are:
    - *p*^* = 1/2 * (*R* + *βr* - *σ* - 2sqrt(*ABα*) / *r*)
    - *π*^* = N(*rR* + *rβ* + *σ* - 2sqrt(*ABα*))^2 / (8r sqrt(*AB*) (1 - sqrt(*α*)))
- Proposition 1 summarizes the effects of **cross-selling** on price, market size, and customer surplus
- ***Proposition 1:*** Compared to the case when firms do not cross-sell, the firm charges a higher price if *βr* > *σ* when it **cross-sells**. The market size and the surpluses of the individual customers are always higher when the firm **cross-sells**.
- The difference between the prices of the products when the firm **cross-sells** and when it does not is (*βr* - *σ*) / 2.
    - Therefore, when the customer obtains a higher surplus from **cross-selling** than what the firm obtains as revenue per unit demand from **cross-selling** (i.e., *βr* > *σ*), the firm increases its price to extract a portion of the surplus.
    - Otherwise, the firm decreases its price.
- An additional surplus of (*βr* + *σ*) / 2 accrues to all the customers (individually), leading to higher surpluses for all when the firm **cross-sells**.
- Upon analyzing the equilibrium in which the firm **cross-sells**, we find that as the customer’s expected **cross-selling** surplus parameter (*β*) increases, the firm increases its price in order to extract a portion of the increased surplus of the customer.
- The remainder of the surplus is left with the customer, leading to an increase in market size.
- When *σ* increases, the firm finds it optimal to decrease the price to pass down some of the additional revenue to the customers.
- The market size increases, as expected.
- When the firm improves its **recommender system**, the price increases to again extract a portion of the extra surplus that is generated by the reduction in the product acquisition cost.
- The rest of the additionally generated surplus goes to the customer, which leads to an increased overall customer surplus and market size.

# 6. Duopoly
- The market consists of two firms (Firm 1 and Firm 2); these two firms have **recommender systems** of effectivenesses *r1* and *r2*, respectively, and prices *p1* and *p2*, respectively.
- Without loss of generality, we assume 1 < *r1* < *r2* (later, we discuss why firms will remain asymmetric in the equilibrium in terms of the effectivenesses), that is, Firm 1 is a low-type firm and Firm 2 is a high-type firm.
- We consider a market where customers either purchase from one of the firms, or do not purchase anything at all (a general setup).
- A customer transacts with the firm from which she expects to obtain a higher surplus.
- The sequence of the game is the following: Firms simultaneously decide the prices of their products.
- Depending upon the given prices, costs, and **cross-selling** surpluses, individual customers determine their surpluses and decide to purchase from the firm that provides them higher non-negative surpluses.
- The highest surplus obtained at the optimal effort with firm i 2 1; 2 is
    - *S*^*_i = R - 2sqrt(*ABθ*) / *ri* - *pi* + *βri* (4)
- We next discuss some of the general properties of the market with the duopoly.
- We define *θs* as the search cost parameter of an indifferent customer who obtains the same surplus from both firms.
- The search cost parameter *θs* has a unique value based on the Spence-Mirrlees single crossing condition.
    - Using Equation (4), we find d*S*^*_i / d*θ* =  -2sqrt(*AB*) / (sqrt(*θ*) * ri) < 0;
    - therefore, the rate of decrease of *S*^*_2 with respect to *θ* is always less than the rate of decrease of *S*^*_1 with respect to *θ* " *ri*; *θ*.
    - Thus, *S*^*_1 and *S*^*_2 intersect at most once, that is, there is only one possible *θ* = *θs* [Rasmusen, 2007].
- Likewise, the value of *θ* at which *S*^*_i = 0 is also unique (as discussed in the following section, *S*^*_2 ¼ 0 corresponds to the marginal customer who purchases products from the high-type firm; we denote her search cost parameter by *θ*^*_2).
- ***Lemma 1:*** The low-type firm charges a strictly lower price than the high-type firm.
- Customers with search cost parameters *θ* 2 [*α*; *θs*] purchase products from the low-type firm and customers with search cost parameters *θ* 2 [*θs*; *θ*^*_2] purchase products from the high-type firm.
- From Equation (4), it follows that the low-type firm should charge a lower price than the high-type firm for its products, as otherwise, no customer can expect to obtain a higher surplus from the low-type firm (since *r1* < *r2*).
- Lemma 1 implies that there are three segments of customers: customers with *θ* 2 [*α*; *θs*] who purchase products from the low-type firm, customers with *θ* 2 [*θs*; *θ*^*_2] who purchase products from the high-type firm, and customers with *θ* 2 [*θ*^*_2; 1] who do not purchase any product.
- Furthermore, the surplus decreases at a decreasing rate with *θ* (Equation 4). The term 1 / *ri* affects the rate at which the surplus decreases; therefore, since *r1* < *r2*, *S*^*_1 decreases at a faster rate than *S*^*_2 with an increase in *θ*. Finally, an increase in the price reduces the surplus linearly.
- For an indifferent customer, *S*^*_1 = S*^*_2, and for a marginal customer, *S*^*_2 = 0. Hence,
    - *θs* = (*p2* - *p1* + *β* (*r1* - *r2*))^2 * (*r1* *r2*)^2 / (2sqrt(*AB*) (*r2* - *r1*))^2
    - *θ*^*_2 = (*R* - *p2* + *βr2*)^2 * (*r2* / 2sqrt(*AB*))^2
- If the prices and **recommender system** effectivenesses are such that the customer with search cost parameter *θs* lies in [*α*; 1], both firms will co-exist, that is, the market has a duopoly.
- Firms evaluate the sizes of their markets from the distribution of the search cost parameters of customers.
- They choose prices for their products that maximize their profits in the equilibrium.

## 6.1. Profit of the Firm
- The firms simultaneously decide the prices of their products in equilibrium using the following profit expressions when the **cross-selling** revenue is generated by selling the products of the third parties:
    - *π1 = D1(p1, p2, r1, r2) (p1 + σ1)*
    - *π2 = D2(p1, p2, r1, r2) (p2 + σ2)*
    - where
        - *D1(p1, p2, r1, r2) = Integrate(θ, α, θs) N / (2(1 - sqrt(α))) sqrt(θ) dθ = N (sqrt(θs) - sqrt(α)) / (1 - sqrt(α))*
        - *D2(p1, p2, r1, r2) = Integrate(θ, θs, θ^*_2) N / (2(1 - sqrt(α))) sqrt(θ) dθ = N (sqrt(θ^*_2) - sqrt(θs)) / (1 - sqrt(α))*
- The firms play a simultaneous move price game to decide their prices.
- The derivation of the equilibria (for both **cross-selling** and no **cross-selling**) are provided in Section C of the Online Supplemental Appendix.
- We determine the equilibrium prices (*p*^*_1 and *p*^*_2) and market sizes; *M*^*_1 and *M*^*_2 are equilibrium market sizes of the low-type firm and the high-type firm, respectively, and *M*^* = M*^*_1 + M*^*_2 is the total market size.
- Our setup is similar to a Bertrand Game with the **recommender system effectiveness** being the source of differentiation, and the firms make positive profits as long as they remain differentiated based on their **recommender system effectivenesses**, that is, *r1*!= *r2*.
- ***Proposition 2:*** As compared to when firms do not cross-sell,
    - (a) When firms cross-sell, the price of the low-type firm is always lower; the price of the high-type firm is higher iff *β*> *bCS;h*.
    - (b) The market size of the high-type firm (low-type firm) is higher iff *σ1 / σ2*  CCS;l when firms cross-sell.
- ***Proposition 3:*** (a) When *β* increases, the price and market size of the high-type firm increase, the price and market size of the low-type firm decrease, and individual surpluses of all customers increase.
    - (b) When *σ1* (*σ2*) increases, the prices of both firms decrease, the market size of the low-type firm increases (decreases) and that of the high-type firm decreases (increases), and individual surpluses of all customers increase.
- ***Proposition 4:*** When the low-type firm improves its **recommender system**,
    - (a) the high-type firm always decreases its price regardless of whether the firms **cross-sell** or not; the low-type firm increases its price when *r1* < *r2ρNCS / L* where firms do not **cross-sell**, and when *r1* < *~r1^L* where firms **cross-sell**,
    - (b) the market size of the high-type firm always increases when firms do not **cross-sell**; when firms **cross-sell**, the market size decreases if *r1* > *^r1^M2*,
    - (c) the market size of the low-type firm always increases when firms do not **cross-sell**; when firms **cross-sell**, its market size decreases if *r1* > *^r1^M1*.
- ***Proposition 5:*** When the high-type firm improves its **recommender system**,
    - (a) the price of the high-type firm always increases regardless of whether the firms **cross-sell** or not; the price of the low-type firm always increases when firms do not **cross-sell**, but decreases if *r2* > *r1 / ρCS / H* when firms **cross-sell**,
    - (b) the market size of the high-type firm always increases when firms do not **cross-sell**; when they **cross-sell**, its market size decreases if *r2* < *^r2^M2*,
    - (c) the market size of the low-type firm always decreases when firms do not **cross-sell**; when they **cross-sell**, its market size increases if *r2* < *^r2^M1*.

# 7. Extension: Some Customers Purchase Without Searching
- In this section, we consider the presence of customers in the market who purchase the first recommended product and do not engage in an additional examination of products (referred to as non-strategic customers) along with the customers who expend effort in searching (i.e., the ones considered so far).
- A non-strategic customer purchases the first product offered by the firm she visits if the product provides her positive surplus (*R* > price) or does not purchase anything.
- Without any loss of generality, we consider that all non-strategic customers purchase a product, and they are γ fraction of the total *N* customers. The rest of 1−γ(*N*) customers are strategic and behave as described in our base model.

## 7.1. Monopoly
- The search cost parameter of the marginal customer remains the same as discussed in the Monopoly section, i.e., *sqrt(θ^(*)) = r (R - p + βr) / (2 sqrt(AB))*.
- The profit equation then becomes
    - *π = N (1- γ) (p + σ) (sqrt(θ^*) - sqrt(α)) / (1 - sqrt(α)) + N γ p*
- When this profit function is optimized for the price, we find
    - *p*^* = 1/2 (R + βr - σ - 2sqrt(AB) sqrt(α) - δ (1 + sqrt(α))) r
- Now, when *β* or *σ* increase, we find that *p*^* increases and decreases, respectively, as observed in the base model.
- However, when *r* increases, the optimal price decreases when the fraction of non-strategic customers, γ, exceeds a threshold value.
- ***Proposition 6:*** When *r* increases, the optimal price decreases when the fraction of non-strategic customers, γ, exceeds a threshold value, that is, *γ* > *Ω / (1+Ω)*, where *Ω = (βr) / (2 sqrt(AB) + sqrt(α)) (1 - sqrt(α)) when the firm cross-sells, and *Ω = sqrt(α) / (1 - sqrt(α)) when the firm does not cross-sell. Otherwise, the optimal price increases.

## 7.2. Duopoly
- We assume that in a duopoly, the demands from these customers are equally distributed between the two firms.
- The search types of the indifferent customers and the marginal customers is given by Equation (5). The new profit equations are:
    - *π1 = (1-γ) N (sqrt(θs) - sqrt(α)) / (1 - sqrt(α)) (p1 + σ1) + 0.5 N δ p1*
    - *π2 = (1-γ) N (sqrt(θ^*_2) - sqrt(θs)) / (1 - sqrt(α)) (p2 + σ2) + 0.5 N δ p2*
- We find that the effects of changes in *r1*, *r2*, *β*, *σ1*, and *σ2* remain the same on prices as in the base model.
- Even though the prices are adjusted to account for the existence of such customers, they do not cause any qualitative changes in the pricing strategies.
- This is because the volume of non-strategic customers are equally divided between the two firms, and does not provide any competitive advantage to either firm.
- For the same reason, no changes are observed in the pricing strategies between when firms **cross-sell** and when they do not.

# 8. Conclusions and Future Research
- This research advances the existing literature on the economics of **recommender systems** by incorporating two important considerations (i) customer efforts which are dependent on heterogeneous search costs and **recommender system effectiveness** and (ii) **cross-selling** offers made by the retailers with associated **cross-selling** revenues for firms and additional surplus for customers.
- We compare two setups: one where both firms do not **cross-sell**, and another where both firms **cross-sell**, and find that these two considerations interplay in unique manners that provide several interesting insights.
- In a monopoly, even though the firm may charge a higher price when it **cross-sells**, the customers end up having higher surpluses because the firm extracts only a portion of the surpluses that customers may obtain from **cross-selling**.
- When the firm improves its system, then also the customers enjoy higher surpluses from **cross-selling** even though the firm increases its price.
- Furthermore, an increase in the expected **cross-selling** revenue of the firm leads to a decrease in its price.
- In a duopoly, firms obtain positive profits by staying vertically differentiated in their **recommender system effectivenesses**.
- When both firms **cross-sell**, competition forces the low-type firm to charge a lower price than when they do not **cross-sell**—a noteworthy departure from what we observe in the monopoly.
- The high-type firm increases its price when the customers’ expected **cross-selling** surplus per unit **recommender system effectiveness** is more than a threshold.
- Both firms’ market sizes may increase or decrease, depending on the relative values of the **cross-selling** revenues.
- The overall market size always increases.
- When the low-type firm improves its system, it leads to an increase in the intensity of competition.
- When the original effectiveness of the low-type firm’s **recommender system** is relatively high, the firm decreases its price when it improves its **recommender system** to mitigate the impact of the increased competition intensity.
- Despite that, when the firms **cross-sell**, the market size of the low-type firm decreases; this never happens when the firms do not **cross-sell**.
- Surpluses of all customers increase; however, some customers may switch to the high-type firm.
- When the high-type firm improves its **recommender system**, the differentiation between the firms increases.
- When neither firm **cross-sells**, both firms always increase prices because of reduced competition, the market size of the low-type firm decreases, and that of the high-type firm increases.
- In contrast, when firms **cross-sell**, although the high-type firm always increases its price, the low-type firm decreases its price when the **recommender system effectiveness** of the high-type firm crosses a threshold.
- This is because of the additional **cross-selling** surplus accrued to the customers of the high-type firm from the increase in the **recommender system effectiveness**.
- Furthermore, the customers of the high-type firm with relatively low search cost parameters may experience a decrease in surplus when the high-type firm improves its system.
- Regardless of which firm improves its system, we find that under certain conditions, a few customers may switch to the other firm.
- Furthermore, in this process, switchers may end up incurring higher search costs.
- Our work provides several opportunities for future research. The assumption of a uniform price for all products in a category limits the types of categories for which our analysis applies and can be relaxed.
- Future research could also relax the assumption of a common reservation price for all products.
- Finally, it would be interesting to explore further whether additional insights may result from different distributional assumptions for the search cost parameter.

---
# Executive summary of 1. Introduction
- Online shopping is growing and retailers are using recommender systems to support online shoppers.
- Recommender systems help customers select products and firms to cross-sell and generate more revenue.
- It is unclear how cross-selling affects the pricing of the focal products when firms provide product recommendations.
- In a monopoly, a firm should change its prices based on search behaviors and customer cross-selling surpluses when it improves its system.
- Given the increasing use of recommender systems, analyzing the interrelationships of customer search, purchase behavior, and cross-selling with the firm's pricing strategies is important.
- *Research Questions:*
    - (RQ1) How do pricing strategies differ when firms cross-sell compared to when they do not cross-sell?
    - (RQ2) When firms cross-sell, how do prices change when a firm improves its recommender system?

# Executive summary of 2. Problem Setting
-  The study uses a game-theoretic model with heterogeneous customers and electronic marketplaces (firms) where customers shop to purchase products.
- Customers have prior knowledge of recommender system effectiveness and cross-selling surpluses.
- Firms set prices, and customers react by making purchase decisions.
- Customers incur search costs, mismatch costs, and product prices when exploring products.
- Expected surplus is calculated considering reservation price, cross-selling surplus, and costs.
- The model analyzes a monopolist and a duopoly with firms asymmetric in their recommender system effectiveness (high-type and low-type).
- The pricing strategies are compared when firms cross-sell and when they do not cross-sell, and when they improve recommender systems.
- Key findings are highlighted, including insights on how cross-selling impacts pricing and market dynamics.

# Executive summary of 3. Literature Review
-  The study relates to personalization, product customization, vertical differentiation, and search costs, focusing on personalization and recommender systems.
-  Extensive research analyzes recommender systems' value, including personalization's effect on website usefulness and sales increases.
- Recent papers focus on recommender systems' operational aspects, like trade-offs in content delivery and offer set optimization.
-  Several marketing researchers derive insights based on a firm's ability to target customers based on preferences.
- In the duopoly model, firms are vertically differentiated by recommendation service quality, also differentiated by customer preferences.
- The research differs by considering customer effort and modeling cross-selling, revealing nuanced firm behaviors and market dynamics.

# Executive summary of 4. Model Preliminaries
- The study models a setting where firms sell products under a category and set a price.
- Customers explore recommended products before purchasing.
- The expected surplus of the customer is determined by various factors.
- The factors that determine the expected surplus of the customer are Reservation Price, Price of the product, Search Cost, Mismatch Cost, and Cross-Selling Surplus.
- The sum of search cost and mismatch cost is referred to as the product acquisition cost.

# Executive summary of 5. Monopoly
- The monopoly model is solved backward, with customer purchase decisions followed by the firm's profit-maximizing price.
- Customers exert surplus-maximizing effort to search for products, leading to an optimal expected surplus expression.
- The optimal surplus decreases with the customer's search cost parameter (θ).
- The market size is affected by the firm's recommender system effectiveness and price.
- The firm maximizes profit by balancing price and market size.
- Proposition 1 states that firms charge a higher price when cross-selling if the cross-selling surplus is greater than the revenue from cross-selling.

# Executive summary of 6. Duopoly
- The market consists of two firms with different recommender system effectiveness (high-type and low-type).
- Customers purchase from one firm or not at all, transacting with the firm providing the highest surplus.
- Firms simultaneously set prices, and customers decide based on prices, costs, and cross-selling surpluses.
- The surplus at optimal effort is determined by reservation utility, search costs, mismatch costs, product prices, and cross-selling benefits.
- Proposition 2 summarizes how the prices and market sizes differ when firms cross-sell from when they do not.
- Proposition 3 states how the pricing strategies differ when firms cross-sell compared to when they do not.
- Proposition 4 presents results on price and market sizes responses when the low-type firm improves its recommender system.
- Proposition 5 presents results on price and market sizes responses when the high-type firm improves its recommender system.

# Executive summary of 7. Extension: Some Customers Purchase Without Searching
- This section considers the presence of customers who purchase without searching (non-strategic) alongside those who search (strategic).
- Non-strategic customers purchase the first recommended product if it provides positive surplus.
- The profit equation incorporates both strategic and non-strategic customer demands.
- Proposition 6 suggests that when recommender system effectiveness increases, the optimal price may decrease if the fraction of non-strategic customers is high.
- In a duopoly, the effects of changes remain the same, as the volume of non-strategic customers are equally divided between the two firms.

# Executive summary of 8. Conclusions and Future Research
- This research integrates customer effort and cross-selling offers into the economics of recommender systems.
- Firms that stay vertically differentiated in their recommender system effectiveness get positive profits.
- Regardless of which firm improves its system, it is found that under certain conditions, a few customers may switch to the other firm.
- Several opportunities for future research are provided, including relaxing the assumption of a uniform price for all products.

</details>

```
title: Drawing a Line in the Sand : Commitment Problem in Ending Software Support
authors: Abhijeet Ghoshal, Atanu Lahiri, Debabrata Dey
journal: MIS Quarterly
published: 2017
```
 

 # Executive Summary
 - This study investigates the **commitment problem** a **software vendor** faces when ending support for older software versions, particularly considering **network security risks**. This is crucial because ending support incentivizes upgrades but can expose users to vulnerabilities.
 - The central issue is that while a vendor *wants* to end support to push upgrades and increase demand, their ability to profit is *limited* by the credible commitment problem. If users don't believe the vendor will truly end support, they are less likely to upgrade, eroding the vendor's pricing power.
 - The core dilemma: The vendor *cannot credibly commit* to end support. When the vendor attempts to capitalize on higher upgrade demand with higher prices, many users may forego upgrading, resulting in a large, vulnerable user base. Fearing reputational damage from security breaches, the vendor may *renege* on their initial commitment and continue supporting the old version.
 - **Key Finding 1**: This inability to credibly commit *harms vendor profitability*. The study demonstrates this through a formal model analyzing vendor pricing and support decisions.
 - **Key Finding 2 (Counterintuitive)**: The vendor's profit in a **no-commitment** scenario *can surprisingly increase with the cost of extending support*. This occurs because a higher support cost signals a stronger commitment to ending support, making the threat more believable. This, in turn, can incentivize upgrades.
 - **Proposed Solution**: The study proposes a **commitment mechanism** to solve this. A prime example is a **refund scheme**: Offering a refund to new version buyers if the vendor extends support to the old version effectively raises the cost of backing down on the commitment.
 - **Key Finding 3**: *Consumer surplus can increase if the vendor credibly ends support*, leading to more users upgrading to secure versions. This challenges the common assumption that ending support always harms consumers.
 - **Theoretical/conceptual framework**:
    - **Game Theory**: The primary analytical tool used to model the strategic interactions between the vendor and consumers in making pricing and support decisions.
    - **Commitment Problem**: The core theoretical concept, describing the vendor's difficulty in making a believable pledge to end support, affecting consumer behavior and vendor profits.
    - **Negative Network Effects**: The increasingly severe security risks linked to unsupported software, where the more users on vulnerable versions, the greater the risk for everyone, driving the need for patching and creating a dependency for consumers on vendors.
 - **Key Findings/Arguments (in more detail)**:
    - Vendors cannot set optimal prices or support strategies *unless* they can convince consumers they *will* end support for older software (price/support strategy is influenced by this).
    - Vendors *can* utilize a *refund scheme* to signal they will not provide extended support, thus encouraging upgrades.
    - The surprising impact on consumers: if more users upgrade to secure products, ending support *can* benefit them, even if it increases risk for a few users who don't upgrade.
 

 #commitment_problem #software_support #pricing #security #patching #game_theory #analytical_modeling #economics_of_is #network_effects #upgrade_strategy #consumer_welfare
 

<details>
    
  <summary>Click to expand sections</summary>

 # 1. Introduction
 - Many software products face the challenge of encouraging consumers to upgrade to newer versions while also addressing security risks associated with unsupported older versions [Kelly 2014; Keizer 2014].
 - Vendors often find it difficult to convince consumers that their commitment to ending support is firm, as they might backtrack to appease users who haven't upgraded. The potential loss of reputation from security attacks increases the likelihood of backtracking.
 - Microsoft's initial plan to end support for Windows XP in April 2014 prompted some users to upgrade, but a significant portion continued using XP [Evangelista 2014]. Facing pressure and anticipating reputational damage, Microsoft extended security support for another year [Seltzer 2014].
 - Some vendors, like Adobe and Oracle, have remained firm on their announced schedules for ending support, even causing consumer dissatisfaction [Keizer 2012; Boulton 2012].
 - Vendors ideally want to discontinue support for older software to attract consumers to newer versions, which then become the only secure option. However, vendors face a commitment problem where raising prices to exploit increased willingness to upgrade can lead to fewer upgrades, making it too risky to end support.
 - The study focuses on the vendor's dilemma and the implications of strategic consumer behavior for pricing and support decisions.
 - We aim to address the following research questions:
    - How does the inability to convince consumers of its commitment to end support impact the vendor's pricing and support strategies, and how is the profitability affected?
    - Can the vendor devise appropriate mechanisms to credibly signal the firmness of its commitment, and when is such a mechanism useful?
    - What are the welfare implications of the vendor's strategy, and could ending support actually be good for consumers?
 - To address these questions, we develop a two-stage model where the vendor sets the price and announces the support strategy, and consumers then decide whether to upgrade.
 - We also examine a no-commitment game where consumers anticipate the vendor to revisit its decision, adding a third stage where the vendor decides on its support strategy after consumers have made their decisions.
 - In the equilibrium of this three-stage game, the vendor's profit can surprisingly increase with the cost of supporting the older release. When the support cost is perceived as low, consumers doubt the vendor's commitment, depressing profits. A higher cost signals a stronger commitment, restoring profitability.
 - When the profit is increasing in cost, it is natural to ask whether the vendor should try to inflate its cost, and if so, how would it actually do so?
 - Finally, we examine the issue of consumer and social welfare, considering arguments both for and against continuing support for older releases.
 

 # 2. Literature Review
 - This research intersects with streams of research on product upgrades, security patching, and the broader economic literature on the commitment problem [Fudenberg and Tirole 1991].
 - The literature on upgrades focuses on optimal pricing strategies when consumers are strategic and anticipate better value in the future [Dhebar 1994; Kornish 2001; Bala and Carr 2009].
 - Our work shares a connection with this literature in that consumers' apprehensions about a product's value can present unique challenges to the vendor. For example, the vendor may need to introduce a lower quality version first and then offer upgrades [Padmanabhan et al. 1997].
 - Like us, Ellison and Fudenberg [2000] also identify a commitment problem, where the vendor's inability to commit to not inflicting compatibility-related pains at a later time leads to depressed profits.
 - Sankaranarayanan [2007] suggests that vendors can include a "New Version Rights" warranty free of charge to entice consumers to buy the current version.
 - The patching literature examines patching from both the vendor's and consumers' perspectives. From the vendor's standpoint, the main issue is the incentive to offer a secure product [Arora et al. 2006; Choudhary and Zhang 2015].
 - Researchers have studied how to incentivize a vendor to either build better products or to provide patches in a timely manner [e.g., Arora et al. 2010; Arora et al. 2008; August and Tunca 2011; Kim et al. 2011].
 - From the consumer's perspective, the critical issue is when and how often to patch, since frequent patching can be extremely costly [Dey et al. 2015; Ioannidis et al. 2012].
 - The paper also addresses a context where consumers without patches adversely impact everyone else by amplifying overall security risks, effectively resulting in negative network effects [August and Tunca 2008].
 - This paper brings together security-related negative network effects with commitment problems faced in ending software support.
 

 # 3. Model Preliminaries
 ## 3.1. Context and Overview
 - In this context, there are two players: a software vendor and a consumer (representing a set of consumers).
 - The vendor has released a software product, Aold, in the past, with guaranteed support until time point t1, which is less than the end of its useful life (t2).
 - At time t0, the vendor releases a new version, Anew, and sets the upgrade price to p. The vendor also announces its support policy for Aold: either (1) NS (no support beyond t1), or (2) S (extending support to t2).
 - Consumers then decide whether to upgrade. At t1, the vendor can revisit the announced support policy.
 - If the vendor sticks to its promise and consumers believe it, the game ends at t1. This can be abstracted into a two-stage game (Figure 2).
 - Our purpose is not to optimize the timing of releases or support cessation, but to investigate the pricing decision and support strategy to identify if the vendor faces a commitment problem.
 ## 3.2. Consumer Model
 - A consumer's decision to upgrade depends on the price, the vendor's support strategy, and the consumer's valuation for the new version.
 - Consumers may be attracted to the new version for two reasons:
    - Functionality benefits: Advanced features, better integration, user-friendly interface, and longer shelf-life.
    - Security losses: If the vendor adopts NS, consumers using Aold face increased security breaches.
 - If vnew is the consumer's valuation for the new version and vold is the residual value from Aold, v = vnew – vold represents the functionality-related benefits of upgrading.
 -  *Assumption 1*. Consumers are heterogeneous and uniformly distributed in their valuation v over [0, 1].
 - When the vendor's strategy is S, security is not a concern. A consumer upgrades if v > p (IR-S).
 - When the vendor's strategy is NS, consumers weigh security-related concerns from a lack of support for Aold during [t1, t2].
 - *Assumption 2*. If the number of consumers using the unsupported version is v¯, the expected security-related loss faced by a consumer without support is μv¯, where μ > 0 is the network effect parameter.
 - When the support strategy is NS, consumer v suffers an additional loss of μv¯ if he chooses to not upgrade and continues using the old version.
 - The consumer now finds upgrading valuable if and only if v + μv¯ > p (IR-NS).
 - Consumers with v 0 [0, v¯] do not upgrade, and those with v 0 [v¯, 1] do. This means that v¯ = p/(1+μ).
 -  Demand for upgrade is higher under NS than S.
 -  v¯ = p for S or p/(1+μ) for NS.
 ## 3.3. Vendor's Problem
 - There is an additional fixed cost associated with strategy S, representing the cost of developing additional patches for the extended support window [t1, t2].
 - *Assumption 3*. The vendor incurs an additional fixed cost c > 0 if it chooses S.
 - Strategy NS avoids this cost, but has its own risks: legal liabilities and reputation loss.
 - Consumers without support could incur financial losses due to security breaches. The expected value of such losses, aggregated, is μv¯. The vendor faces a proportional financial risk because of legal liabilities directly arising out of security breaches of an unsupported platform, expressed as βμv¯, where β > 0 is the constant of proportionality.
 - There could also be an indirect loss to the vendor that has to do with the loss of reputation. It is assumed that this loss is αv¯, where α > 0 is a constant of proportionality.
 -  *Assumption 4*. The total financial loss faced by the vendor, when it chooses NS, is (α + βμ)v¯, where α, β > 0.
 - γ = (α + βμ) / (μ + 1) expresses the vendor’s loss rate (normalized financial loss borne by the vendor).
 - We focus on the case of γ < 1; the case of γ > 1 is uninteresting as, then, the vendor no longer considers NS because of the prohibitive nature of the expected financial losses.
 - The vendor's profit, net of costs and losses, is:
    - π = (1 – p)p – c for S
    - π = (1 – p/(1+μ))p – γ(p/(1+μ)) for NS
 - These profit functions are concave in p in each case.
 -  *Optimal Strategy*:
    - Choose S and set p = (1/2) if c <= c1 = (1 – γ) / (4(1+μ))
    - Choose NS and set p = (1+μ) (1 – γ) / 2 otherwise
 - *Proposition 1*. If c1 < c < c2, where c2 = ((1-γ)(1+μ)) / (4γ + 4γμ), the optimal solution above is not time-consistent—specifically, the vendor has an incentive to revert to S after consumers have made their upgrade decisions.
 - *Proposition 1* means that consumers cannot blindly trust the vendor to follow through on its earlier announcement about discontinuing support. This is because once consumers have spent on the new version, the vendor can simply backtrack and extend support, thereby mitigating its financial losses. This possibility that the vendor might renege on its earlier promise can have serious consequences. In fact, the vendor could face a commitment problem, and its inability to commit to NS could spell adverse impacts on its profits.
 

 # 4. No-Commitment Equilibrium
 - When consumers do not trust the vendor and expect it to choose its support strategy only after their upgrade decisions, their decisions at the second stage could be different.
 - The resulting strategic interactions can be captured in a discrete-time three-stage game where the vendor makes an additional third move at t1. Consequently, the timeline reduces to Figure 3.
 - Comparing Figures 2 and 3, it is apparent that the difference between the two cases is that the vendor’s incentive to revisit the support strategy essentially adds a third stage to the game, rendering the announcement of NS at stage 1 irrelevant and changing the timing of the actual support decision. Earlier, this decision happened in stage 1; now, it happens in stage 3.
 - Earlier, this decision happened in stage 1; now, it happens in stage 3.
 - At stage 3, the vendor has an incentive to stick to NS only when v¯ <= c / (γ(1+μ)).
 - Consumers should expect the vendor to stick to NS if v¯ resulting from such a strategy indeed abides by the above condition.
 - This leads to the condition *p <= c / γ*. Consumers would have reasons to suspect that the vendor, notwithstanding its earlier announcement, will revert to S if the price exceeds the critical fraction c / γ.
 - The vendor maximizes profit, knowing that consumers anticipate NS when p <= c / γ, and S otherwise.
 - Maximizing the profit leads to the following partitioning of the parameter space:
 - *Proposition 2*. The equilibrium of the no-commitment game can then be characterized as follows:
    - Support Region (S): If c < c3 < c 2, or if c2 < c3 and c < c1, the vendor continues support.
    - No-Support Region (NS): If c3 < c2 < c, or if c2 < c3 and c > c1, the vendor discontinues support.
    - Limit Region (NS-L): If c3 < c < c2, the vendor uses a “limit price”—mathematically a corner solution—while discontinuing support.
 - Figure 4 illustrates how the (γ, c) space partitions into these three regions.
 - Proposition 1, the vendor faces a commitment problem for c1 < c < c2; this is essentially the region below the c2 curve in the figure, except for the portion that is also below the c1 curve.
 - According to Proposition 1, the vendor faces a commitment problem for c1 < c < c2. The impact of this commitment problem is two-fold: Between c3 and the dashed portion of c1, unable to commit to NS, the vendor simply gives up on NS. On the other hand, between c3 and c2, the vendor forces no-support by means of a corner solution, that is, by choosing to barely satisfy the condition for NS.
 - Figure 5 plots the equilibrium price as a function of γ and c. When the support cost, c, is low, the vendor decides to be in the support region, Region S, and it simply charges the traditional monopoly price of p S = 1/2.
 - There is a significant portion of the parameter space (NS and NS-L), where the higher cost of support makes it imperative for the vendor to end support for the old version.
 - Especially interesting is the NS-L region. Evidently, when c1 < c < c2, the vendor does not have enough leeway to pursue the NS strategy, despite it being the more profitable one. This is because a high pNS works like a trap for the vendor, driving many consumers away from the upgrade, raising in turn doubts about its own commitment to truly end critical support.
 - In the region between c3 and c2, the vendor finds a way to avoid falling into the trap. In fact, it does so by holding the price at *p L = c / γ*, a rather low level that is clearly below pNS. The implication is immediate.
 - Between c3 and c2, a vendor can, and should, use its price to signal a commitment to NS. If consumers are convinced that a sufficient fraction of them would be eventually upgrading, they would indeed act as if the vendor would forsake support for good.
 - A vendor who fails to recognize this connection between the upgrade price and the support strategy could end up choosing a price that is sub-optimally high.
 - There is, in fact, a real possibility that Microsoft walked into this trap when announcing the death of Windows XP.
 - When Microsoft announced the release of Windows Vista, the retail price for an upgrade to the Business version was $199. Microsoft stuck to the very same strategy when pricing Windows 7 in 2009.
 - In line with our findings, when the moment to choose arrived, unlike Microsoft, Adobe saw little need to go back on its earlier decision to end support.
 

 # 5. Profit and Commitment Mechanism
 - To investigate how the vendor’s profit is affected by the commitment problem, we substitute the price from (8) into (7) and obtain the profit in the no-commitment equilibrium.
 - *Proposition 3*. If c1 < c < c2, the vendor’s profit is adversely impacted by the commitment problem. Further, the profit in the no-commitment equilibrium is non-monotonic in c; it is decreasing in c in Region S, increasing in Region NS-L, and flat in Region NS.
 - Proposition 3 essentially means that, when the vendor is unable to commit to NS, it simply ends up with a profit of πS or πL, both of which are less than πNS. The ability to readjust the support strategy after consumers’ decisions may have a natural appeal to some managers, but it is this very ability that eventually boomerangs and causes the vendor to lose money.
 - Clearly, addressing the commitment problem is paramount for the vendor.
 - To gauge how the vendor might be able to do so, we plot in Figure 6 the no-commitment profit as a function of γ and c.
 - Interestingly, when γ is low, the no-commitment equilibrium profit is not necessarily decreasing in the cost of continuing support, c.
 - As Proposition 3 points out, only when the strategy in use is S, it declines as expected. However, in Region NS-L, it is surprisingly increasing in c. Finally, it becomes flat upon crossing over to Region NS, where the cost to support has no bearing on the equilibrium profit.
 - A key lesson here is that, when the perceived damage from security attacks is high but the vendor faces a relatively smaller indirect impact, the vendor could surprisingly prefer its patch development costs to be high. Specifically, it would prefer a c at least as large as c2 to one in (c1, c2).
 - Such anomalous behavior of profit is, in essence, an indication that the vendor can overcome the adverse effect of the commitment problem by suitably inflating its cost to extend support. The obvious question here is: How can the vendor credibly “inflate” the cost?
 - Among many possibilities, the most intuitive one ought to be to include a contingent refund coupon—some sort of a money-back guaranty—into the sales contract for Anew.
 - *Proposition 4*. If c1 < c < c2, a refund coupon of r > ρ = (1-γ)(1+μ)-2c / (γ(1+μ)) payable to each buyer of the new version in the event support is extended for the old version, is sufficient to fully address the commitment problem.
 - Proposition 4 simply tells us that the size of the refund must be large enough for it to serve as a commitment mechanism. Evidently, what matters here is the anticipated cost of choosing S, and not the realized cost in optimality.
 - If consumers anticipate S to be sufficiently costly from the vendor’s perspective, they would stop betting on an extended support and instead show a greater willingness to pay for the upgrade, restoring fully the vendor’s pricing power in the presence of negative network effects.
 - The above coupon-based scheme can be easily implemented even when there is sufficient uncertainty about the value of c, that is, when information asymmetry between the vendor and consumers makes it difficult for the former to convey the true cost of patch development to the latter.
 

 # 6. Welfare
 - We now turn our attention from private profit to public welfare.
 - The consumer surplus generated from the upgrade, net of the losses suffered from security attacks, can be defined as follows:
    - CS = Integral(v-p)dv from v* to 1 for S
    - CS = Integral(v-p)dv - Integral(mu*vbar)dv from v* to 1 and from 0 to v*, respectively for NS and NS-L
 - To compute the surplus value at equilibrium, we now need to substitute p* from (8) into (10). This leads to
    - CS = 1/8 for S
    - CS = (1-gamma)square / (8(1+mu)square) for NS
    - CS = (1 + 2*gamma*mu - c)/(2*(1+mu)*gamma)-1/2*(((c/gamma)square)) for NS-L
 - A closer examination of this consumer surplus reveals a curious result.
 - Now, consider a situation where the vendor decides—perhaps somewhat irrationally—to be always “kind,” and extends support even when it is not optimal to do so. It is natural to expect that, in such a case, the consumers would all be happier—getting free support ought to reduce their security risks and make them all better off.
 - *Proposition 5*. At a relatively large loss rate, the consumer surplus from NS and NS-L dominates that from S.
 - When γ is above the threshold of Θ(μ), in the no-commitment equilibrium, the vendor’s interest is surprisingly aligned with consumers’ interests in both Regions NS-L and NS.
 - The intuition as to why consumers surprisingly benefit from NS in certain situations is also quite interesting. Although it is true that those sticking to the older version face increased security risks when support is discontinued, the benefits obtained from more consumers switching to a “better” product does at times outweigh that negative impact.
 - Therefore, governments and consumer advocates who clamor against discontinuation of support by pointing to security concerns should consider moderating their views on this subject.
 - Social welfare also benefits from NS and anomalous behavior of profit with respect to c carries over to social welfare as well.
 

 # 7. Robustness Checks
 ## 7.1. Imperfect Consumer Information
 - We first revisit the issue of consumers’ perfect knowledge of the critical fraction, c/γ.
 - In economic models, a lack of perfect information is typically captured by a probability distribution around the true value of the quantity of interest, which is in our context c/γ.
 - *Assumption 5*. Consumers are heterogeneous in their assessment, h, which is uniformly distributed. Consumers do not know this distribution, but the vendor does.
 - In the case of a lack of perfect information: it can be shown after some algebra that case S'' can never happen in optimality and can be dropped from further considerations
 - Maximizing this new profit, we can obtain the equilibrium outcome. Figure 7 shows the equilibrium profit, and there is now an additional support region S', a narrow band that appears between the regions for S and NS (or NS-L).
 - The overall solution is still qualitatively similar to that in Figure 6. The equilibrium profit is again non-monotonic, implying that our earlier results with respect to the value of commitment are robust even to situations where consumers lack perfect information and differ in their beliefs of c/γ.
 ## 7.2. Partial Extension of Support
 - Our analysis has so far assumed that the time epochs in Figure 1 are all fixed and are exogenous in the model.
 - What would happen if the vendor provides partial support, that is, support for only a fraction of the remaining useful life?
 - We now allow the vendor to provide fractional support of λ, λ 0 [0, 1], up to another time point t'2 = t1 + λ(t2 – t1) < t2.
 - Generalizing the cost functions, the solution to the two-stage problem would only yield a boundary solution, λ = 0 or λ = 1, implying that the characterization in (4) is still valid.
 - Consistency issues persist exactly the same way, for the exactly same range of values of c identified in Proposition 1.
 - Furthermore, all our earlier results continue to hold as originally stated.
 ## 7.3. Impact of Normalization
 - So far, we have worked with a normalized model in which consumers’ valuation and the size of their base were both normalized.
 - We wish to examine whether such normalization is without any loss of generality and if it impacts our results.
 - We develop a non-normalized setup here and check if it is indeed equivalent to the normalized setup in the main paper.
 - The model formulation in the presence of these two additional parameters, M and N, is slightly different. In the original model, M and N were both set to one
 - Using non-normalized model, normalization used in the main paper has no impact on our findings; it simply rescales the decision variable and parameters and the objective function.
 - Since a scaling of the objective function has no impact on various thresholds discussed in the main paper, the insights there should remain applicable even when all normalizations are done away with.
 

 # 8. Conclusion
 - This research focuses on the quandary of a software vendor ending support for older versions, and the commitment problem that results from it.
 - We analyze the vendor’s decision problem with respect to the optimal support strategy and the price for the upgrade, accounting for the cost of extending support, the security risks faced by consumers, and the potential loss the vendor faces as a result of consumer disenchantment.
 - In this simple setup, the vendor’s optimal decision is time-inconsistent. In other words, the vendor has an incentive to renege on its plan to end support after consumers have made their upgrade decisions.
 - We find that the commitment problem faced by the vendor adversely affects its profit and significant implications for the vendor’s optimal pricing strategy.
 - Counterintuitively, in the no-commitment equilibrium, the vendor’s profit may be non-monotonic in the cost of support, implying that it might help the vendor to look for ways to inflate that cost, instead of simply choosing the optimal strategy for a given cost
 - The non-monotonicity of profit also provides us important cues on designing a suitable commitment mechanism.
 - The welfare implications are also intriguing. We find that this is not necessarily the case. In fact, even if the vendor ends support, consumers are not necessarily worse off
 - Although this study reveals several useful insights, it is not without limitations. Further research is necessary for a more refined understanding of the vendor’s strategic options.
  
</details>

# Brian Rongqing Han
- Assistant Professor of Business Administration
### education
- Ph.D., Business Administration (Data Sciences and Operations), Marshall School of Business, University of Southern California, 2020
- B.A., Economics, Shanghai University of Finance and Economics, 2012
### research interest
- **Methods**: field experiment; econometrics; data-driven optimization
- **Applications**: online-offline integration (retailing and logistics); human-AI/Algorithm interaction
### teaching interest
- Business Analytics; Information Systems; Operations Management

```
title: Commercializing the Package Flow: Cross-sampling Physical Products Through E-commerce Warehouses
authors: Brian Rongqing Han, Leon Yang Chu, Tianshu Sun, Lixia Wu
journal: Management Science
published: Forthcoming
```
 
# Executive Summary
- This paper studies **cross-sampling**, a novel business practice where free samples from one brand (sampling brand) are distributed with packages of another unrelated brand (distributing brand) through e-commerce warehouses.
- We implement a large-scale **field experiment** with Alibaba, distributing over 55,000 free samples, to empirically examine its effectiveness in driving online sales of the sampling brand.
- Key **theoretical** / **conceptual framework** discussions
    -  **Omnichannel Retailing**: Cross-sampling integrates offline logistics and online information, providing a new way for customers to experience products before buying. It helps overcome challenges in selling physical experience goods online [Gallino and Moreno, 2018; Dai et al., 2019].
    - **Free Sampling**: The study extends literature on free sampling by investigating an alternative channel (e-commerce warehouses) and advancing understanding of how to increase effectiveness of free-sampling.
    - **Heterogeneous Effects**:  We explore how customer demographics, spending power, recent category interest, price sensitivity, and essential shopping behavior influence cross-sampling effectiveness.
- Key **findings** / arguments including 
    - **Increased Store Visits and Sales**: Cross-sampling significantly increases store visits and sales of the sampling brand in both the short and long term (up to 14 months).
        - E.g.,  Monthly visit probability increased by 2.808 percentage points (p < 0.01) during month 0, 0.807 percentage points (p < 0.01) during months 1-3, and 0.035 percentage points (p < 0.01) during months 4-14.
    - **Customer Acquisition**: The long-term effect is mainly driven by acquiring customers who did not previously purchase from the sampling brand.
    - **Sales Increase in Sampled and Other Items**: Unlike within-category free sampling, cross-sampling leads to a significant sales increase in both the sampled item and other items, with spillovers to indirect channels.
        - E.g., Spending on both the sampled item and other items increased significantly during month 0, by 0.0637 RMB (p < 0.01) and 0.1543 RMB (p < 0.01), respectively.
    - **Personalization Potential**: Cross-sampling is more effective for customers who recently viewed related products or purchased non-essential products from the distributing brand.
    - **Moderating Effect of Similarity Scores**: User- and item-level data help moderate the causal effect, suggesting improved profitability by targeting the "right" packages. Item-level information is more relevant than user-level information.

#field_experiment #cross_promotion #e_commerce_platform #warehousing #free_sampling #omnichannel_retailing #customer_acquisition #heterogeneous_effects #similarity_scores

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- E-commerce platforms have established warehouses to facilitate storage and delivery [Amazon.com, 2020].
- Platform-owned warehouses enable a natural integration of **control** (access to packages, flexibility to bundle contents) and **information** (rich online data).
- We study **cross-sampling through e-commerce warehouses**, where free product samples from one brand (sampling brand) are distributed with items purchased from another brand (distributing brand).
- Figure 1 shows an example: a customer buys a napkin from Vinda and receives a cosmetic sample from L'Oreal.
- Cross-sampling connects brands and customers with physical products, similar to how online marketplaces enable information communication [Han et al., 2023].
- Selling physical experience goods online remains a challenge [Gallino and Moreno, 2018; Dai et al., 2019].
- **Advantages of cross-sampling**:
    - Leverages existing package deliveries.
    - Enables sampling brand to acquire customers through physical experience.
    - Allows personalization based on online marketplace information.
- Previous research has examined logistics infrastructure efficiency [¨Ozer et al., 2014; Batt and Gallino, 2019; Sun et al., 2019a] and customer targeting on digital platforms [Liu et al., 2018; Cui et al., 2019b; Li et al., 2019; Zhang et al., 2020] separately.
- We connect these streams by showing how offline logistics infrastructure can generate additional revenue streams through cross-sampling.

## 1.1. Research Questions and Experiment Design
- We aim to explore and empirically characterize operational and informational advantages of cross-sampling. The insights can be generalized to cross-selling and bundling.
- *Research Question*: What are the effects of Cross-sampling on online sales (store visits, purchase likelihood, and spending) of the sampling brand?
    - While previous research has extensively studied free sampling, little is known about this new form of free sampling which combines offline logistics control and online information and generate additional business value for customers, brands, and the platform.
- *Research Question*: What is the mechanism behind the effectiveness of cross-sampling?
    - This involves examining the timing and frequency of purchases, and the particular items purchased (sampled item or others).
- *Research Question*: How can we improve the effectiveness of cross-sampling through personalization?
    - How do customer characteristics (demographics, spending power, interests) and characteristics of purchase incidence at distributing brand affect cross-sampling effectiveness?
- We implemented cross-sampling through a large-scale field experiment with Alibaba in 2019.
- Over 55,000 samples from six brands were distributed, with a quasi-experiment design creating variations in customers’ likelihood of receiving samples.
- A typical batch of 10,000 free samples can be distributed within a few days, leveraging the existing warehousing system.
- The distribution cost is minimized by delivering free samples inside existing orders' packages, costing a few cents per sample.
- We use a **fixed effect different-in-differences model** (coupled with propensity score matching) to identify the causal impact of cross-sampling.

## 1.2. Key Findings
- We find a significant increase in store visit probability, purchase likelihood, and spending at the sampling brand’s online flagship store.
    -  E.g., store visit probability increased by 2.808 (p < 0.01), 0.807 (p < 0.01), and 0.035 (p < 0.01) percentage points during month 0, months 1-3, and months 4-14, respectively.
- The effect on purchase likelihood and spending can last at least three months.
    - E.g., monthly spending increased by 0.218 RMB (p < 0.01) and 0.09 RMB (p < 0.01) during months 0 and 1-3.
- The relative increase in spending during the campaign month in the treatment group is about 64%.
- The impact of cross-sampling on customers who did not purchase from the sampling brand is stronger and lasts up to a year, indicating effective customer acquisition.
- Robustness checks validate the findings.
- We investigate the underlying mechanism:
    - Cross-sampling increases store visits and sales for customers who did not purchase from the sampling brand over the long run [Anderson and Simester, 2004; Bawa and Shoemaker, 2004].
    - Significant sales increase in both the sampled item and other items, with spillovers over to indirect channels, promoting the sampling brand as a whole.
- Cross-sampling is more effective for customers who recently viewed related products or just purchased non-essential products from the distributing brand.
- We illustrate personalization potential by considering interaction among brands, items, and users, constructing similarity scores to measure customer base similarity.
- We find that user- and item-level data help moderate the causal effect, improving cross-sampling return by targeting the "right" packages. Item-level information is more relevant than user-level information.
- A counterfactual analysis shows improved profitability through personalization.

# 2. Literature Review
- We contribute to the literature studying **warehousing** as a crucial supply chain part.
- Earlier studies focused on location selection [Feldman et al., 1966], inventory management [Viswanathan and Mathur, 1997], and fulfillment [De Koster et al., 2007].
- Recent studies examine warehouse operations' behavioral elements [Batt and Gallino, 2019; Sun et al., 2019a] and logistics services' economic impact [Caro and Mart´ınez-de Alb´eniz, 2015; Fisher et al., 2019; Cui et al., 2019a].
- We provide a novel perspective of logistics infrastructure as a new channel for commercial activities.
- We contribute to literature regarding **operations in the omnichannel world**.
- Studies have investigated the operational impact of various omnichannel practices [Bell et al., 2014; Gallino and Moreno, 2019], including showrooms [Bell et al., 2015, 2018], pop-up stores [Zhang et al., 2019], buy-online-pickup-in-store [Gallino and Moreno, 2014], ship-to-store [Gallino et al., 2016] and app adoption [Sun et al., 2019b].
- Recent research highlights the necessity to enable customers to learn more about experience goods before buying [Dai et al., 2019; Gallino and Moreno, 2018].
- Cross-sampling integrates offline logistics and online information from the firms’ perspective, providing a new way for customers to experience the products before buying.
- Our study contributes to the literature on the **economic impact of free sampling**.
- Most existing work focuses on online information products [Chellappa and Shivendu, 2005; Wang and Zhang, 2009; Cheng and Liu, 2012; Lee and Tan, 2013; Niculescu and Wu, 2014] or offline physical products [Marks and Kamins, 1988; Heiman et al., 2001; Bawa and Shoemaker, 2004]
- Recent works examine the rating-bias effect of free sampling from the online free-sampling platform’s perspective [Lin et al., 2019] and how to improve the return of free sampling at the last-mile pickup stations [Han et al., 2023].
- We investigate an alternative channel (e-commerce warehouses) and advance our understanding of how to increase free-sampling effectiveness.
- Cross-sampling is more effective in increasing sales and brand impressions than previous study [Han et al., 2023].
- Our study adds to empirical research in operations management increasingly implementing field experiment [Terwiesch et al., 2019].
- Many works apply randomized field experiments to study e-commerce [Zhang et al., 2020; Cui et al., 2019b; Feldman et al., 2018], sharing economy [Cui et al., 2020; Cohen et al., 2019a], or pricing [Cohen et al., 2019b].
- Other studies leverage exogenous shocks and design natural/quasi-experiments [Lu and Lu, 2019; Li and Netessine, 2020].
- The treatment variable (cross-sampling) in our experiment is implemented offline, creating challenges for experimental design and identification.
- We design a quasi-experiment in which the treatment is maintained to be exogenous to the best extent possible.

# 3. Research Settings
- Cross-sampling was implemented through a large-scale quasi-experiment during February and March 2019 in Cainiao Network, Alibaba Group’s logistics subsidiary company.
- Six online brands participated in the pilot project, with a field experiment to implement cross-sampling at scale and generate data to identify the causal effect.

## 3.1. Implementation Procedure of Cross-sampling Through E-commerce Warehouses
- We leveraged the existing warehousing system to implement cross-sampling at scale.
- Cainiao warehouses store and deliver packages for online brands, including the six we collaborated with.
- We adjusted the information system to allow one brand to distribute free samples provided by another brand, potentially from a different category.
- One brand provides samples; the platform changes the labels to the distributing brand.
- The distributing brand then carries out a free sample promotion in its online store, treating the provided samples as its own.
- A batch of 10,000 free samples can be distributed in just a few days, matching online orders in real time.
- Cross-sampling incurs an order-picking cost (a few cents per unit) with no additional transportation cost.
- The platform charges a fee from the sampling brand for each sample, gaining profit.
- Our experiment had six rounds; Appendix A lists distributed samples and top item subcategories sold by cosmetic and daily necessity brands.
- We designed cross-sampling to have a cross-category nature to avoid possible competition.
- All brands took on the role of both a sampling brand and a distributing brand.
- The main focus is the causal effect of cross-sampling on the sampling brand.

## 3.2. Experiment Design
- We designed a quasi-experiment in which the treatment was switched on and off at specific time points of the day.
- Appendix Section A Figure 5 provides a two-week cyclic schedule for each distributing brand.
- All customers need to be informed by a banner ad on the distributing brand’s online store front page.
- Customers are categorized into a treatment group (received a free sample) and a control group (did not receive a free sample).
- Both groups purchased from the distributing brand and were influenced equally by the banner ad.
- The only difference was when the orders were processed. The treatment was exogenously determined.

## 3.3. Data Collection
- We construct a rich dataset from the field experiment with variation across users and items from different brands.
- User information: demographics, historical purchase behavior.
- Item information: subcategory, price, discount, historic item-level association information.
- We focus on orders with a single item (94% of cases).

### 3.3.1. Outcome variables.
- We define three outcome variables:
    - **Store visit**: indicator of whether the customer visits the sampling brand's online store.
    - **Whether purchase**: indicator of whether the customer purchases at the sampling brand's online store.
    - **Spending**: total spending at the sampling brand's online store.
- We collect the outcome variables from two months before the campaign up to 14 months after the campaign, constructing a 17-month-long panel data at the user-month level.
- Appendix A Figure 6 plots the average difference in outcomes by the treatment and control groups for each month.
- There are significant jumps in all three outcomes during the first few months after the campaign, suggesting a causal effect of cross-sampling.
- We use propensity score matching (PSM) to improve the covariate balance further.

### 3.3.2. Matching and covariate balance.
- A total of 55,434 samples were distributed. Focusing on single-item purchases, we have 52,288 customers.
- There were 139,518 customers who also made a single-item purchase but did not receive samples.
- Based on the user ID of a total of 191,806 customers, we collected detailed purchases and browsing data.
- We appended the subcategory, spending, and discount rate of each item purchased from the distributing brand.
- We carry out a clustered one-to-one PSM on a wide range of pre-treatment covariates.
- Covariates: gender, age segment, annual number of orders, annual spending, number of item-views and spending at the sampling brand, sampling category, distributing brand, and distributing category three months before the treatment, and the spending and discount rate of the items purchased from the distributing brand.
- We find the closest one-to-one match based on the predicted propensity scores clustered at each-item subcategory and each distributing brand [Xu et al., 2016; Han et al., 2023; Sun et al., 2016].
- After matching, we are left with 41,566 customers in the treatment and control groups, respectively.
- The matching rate may vary across different rounds of campaigns, we achieve an overall matching rate of approximately 80%.
- We verify the effectiveness of our matching procedure in two ways.
    - We plot the density curve of the propensity scores for the treatment, matched control, and raw control group in Figure 2.
    - We summarize the pre-treatment covariates of the treatment and control group before and after matching in Table 1, confirming no statistical difference after matching these covariates.
- We plot the differences in outcome trends after matching in Appendix A Figure 7, showing similar pre-treatment outcome trends.

# 4. Causal Impact of Cross-sampling on Sampling Brand
- We start by gathering some model-free evidence based on summary statistics of the outcome variables in Appendix B Table 10.
- We aggregate the outcomes for three periods: the month of the campaign (month 0), the following three months (months 1-3), and months 4-14.
- We find significant differences between the treatment and control groups over all three periods.
- Differences between the treatment and control groups in the two-month period before our experiment are not significant.
- The model-free evidence shows that there may be significant effects both in the short term and long term.
- We then specify the following fixed-effect difference-in-difference (DID) model
- Outcomeit = α + β0 · T reati · Af ter0t + β1 · T reati · Af ter1to3t + β2 · T reati · Af ter4to14t + ut + vi + γ · T reati · Bef ore1t + ϵit,
- where Outcomeit denotes the particular outcome for customer i at month t, T reati indicates whether customer i has received a free sample, Af ter0t equals one when month t is the month of the campaign, Af ter1to3t indicates the following 1-3 months, and Af ter4to14t equals one for months 4-14.
- We control for time-invariant customer heterogeneity using fixed effects vi, and month-level fixed effect ut.
- We are interested in the coefficients β0, β1, and β2, measuring the monthly average treatment effect of cross-sampling during the three periods, respectively.
- Our identification relies on the parallel trend assumption.
- We formally test this assumption by defining the dummy variable Bef ore1t, indicating the month before treatment.
- If the parameters γ are insignificant, the parallel trend assumption holds.
- Panel A of Table 2 presents our main results on the average treatment effect.
- We find a significant increase in store visit probability over the long run and a significant increase in purchase likelihood and spending that last up to three months after the treatment.
    -  E.g., Column (1) shows that cross-sampling increases customers’ monthly visit probability by 2.808 (p < 0.01), 0.807 (p < 0.01), and 0.0353 (p < 0.01) percentage points during month 0, months 1-3, and months 4-14, respectively.
- Comparing Table 10 and Panel A of Table 2, the magnitude of our estimates are consistent with the model free evidence.
- More than 93% of the customers did not purchase from the sampling brand.
- We further estimate Equation (1) for these customers in Panel B of Table 2.
- Similar to the results in Panel A for all customers, we find a significant increase in store visit probability over the long run.
- In addition, we find a significant increase in purchase likelihood and spending over the long run for customers who did not purchase from the sampling brand.
    - E.g., Column (3) shows that cross-sampling increases monthly spending by 0.257 (p < 0.01), 0.086 (p < 0.01), and 0.028 (p < 0.05) over the three periods.
- Customers who did not purchase from the sampling brand mainly drive the overall effect.
- Cross-sampling effectively increases customers’ visits and purchases at the focal brand over the long run.
- One of the key advantages of cross-sampling is customer acquisition across brands and categories. Our results suggest that cross-sampling has effectively acquired some customers and demonstrated its key value.

## 4.1. Robustness Check
- We conduct a series of robustness checks on the outcome measurement, model specification, and causal identification.
- First, we define an alternative outcome “units sold,” as the count of total purchased items and rerun Equation (1) in Appendix B Section B.1, finding consistent results.
- Second, we estimate an alternative cross-sectional model in Appendix B Section B.2, controlling for a variety of features at the customer, item, and brand levels.
- The outcome variables are aggregated for months 1-3 and 4-14.
- The magnitude of the effects is similar to the main results in Table 2.
- Different from Table 2, however, the long-term effect here is significant with p < 0.05.
- Moreover, we validate our causal identification by varying the matching method and comparing the estimates in Appendix B Section B.3.
- We create three variations of the matching method: raw data without any matching procedure, relaxed matching criteria, and more strict matching criteria.
- We again find consistent results with similar magnitude.
- Lastly, an important concern is the effect of other promotions from the sampling brand that could have confounded the observed impact of cross-sampling.
- To the best of our knowledge, there was no additional store-level promotion during and shortly after the cross-sampling campaign.
- The customers who received free samples are hidden from the sampling brand.
- Previous literature shows that price promotion and advertising can result in temporal substitution of demand [Zhang et al., 2020; Simester et al., 2009].

# 5. Customer Acquisition
- Previous literature has shown that free-sampling of physical products within the same category can have a long-lasting effect on sales for up to 12 months by acquiring customers and motivating repeated purchases [Bawa and Shoemaker, 2004], similarly for deep price promotion [Anderson and Simester, 2004].
- Given the cross-category nature of cross-sampling, it is important to understand how customers respond to the new practice over time.
- Our main results have shown that cross-sampling has a long-term impact on customers who did not purchase from the sampling brand.
- Leveraging our granular data, we next investigate the underlying mechanism by examining the timing and frequency of purchases and the particular items being purchased.
- We first use the following model to provide a set of monthly estimates of the causal effect
- Outcomeit = α + β0 · T reati · Af ter0t + β1 · T reati · Af ter1t + · · · + β14 · T reati · Af ter14t + ut + vi + γ · T reati · Bef ore1t + ϵit,
- where Af ter0, Af ter1, . . . , and Af ter14 are a set of dummy variables indicating the month past the treatment.
- Appendix C Column (1) shows that the effect on store visit probability is significant for consecutive months, motivating customers to repeatedly visit the online store.
- Columns (2) and (3) of Table 14 show that the effect on purchase likelihood and spending is significant up to three months after the treatment.
- For months 4-14, the monthly estimates for purchases are not significant.
- We also conduct a week-level analysis at a more granular level to see when they make purchases.
- We construct a user-week panel data and estimate a similar DID model to generate weekly estimates, which are plotted in Figure 3.
- Overall effects across all three outcomes are the most significant during the first three months.
- Panel (a) of Figure 3 shows some significant increases in store visits during weeks 12-40, attracting customers to visit online stores over the long run and increasing brand awareness.
- To understand long-term purchase behavior, we next quantify the impact of cross-sampling on purchase frequency.
- There are two main reasons cross-sampling may drive a long-term effect on purchases: more customer buying (customer acquisition) and more purchases from acquired customers (repeated purchases).
- We conduct a cross-sectional analysis based on the "Number of purchases" over the 15-month post-treatment period in Table 3.
- We first regress the count variable on the treatment indicator T reat along with a series of control variables in Columns (1) and (2) using Poisson and Negative Binomial regression, respectively.
- Alternatively, we also define ‘Whether purchase ≥ 2” as a dummy variable indicating whether a customer has purchased more than once in Columns (3) and (4).
- The results all suggest that, at least overall, there are more number of purchases in the treatment group.
- To further disentangle the effect of customer acquisition and repeated purchases, we conduct a subsample analysis keeping only customers who have purchased once during the post-treatment period in Columns (5) and (6).
- We find that no significant effect of cross-sampling on the probability of repurchasing conditional on already purchased once.
- Therefore, cross-sampling is more effective in acquiring customers to purchase at least once, but not necessarily multiple times.
- The significant long-term effect we observed in Table 2 can be only because some customers purchased later than earlier.
- Moreover, our main outcome variables are all measured at the store level.
- We next decompose the sales at the sampling brand based on the subcategory of the sampled items.
- We then decompose spending by the sample items and other items and estimate Equation (1) in Table 4 Columns (1) and (2), respectively.
- We can see that cross-sampling increases customer spending on both the sampled and other items at the sampling brand’s online store.
    - Spending increase on other items is significant until the second period (months 1-3).
    - Spending increase on the sampled item is not significant for the second period and becomes significant for the third period (months 4-14) in the long run.
- It suggests that the long-term effect of cross-sampling is largely driven by the sales of the sampled item, and the effect takes time to reveal.
- This important observation motivates us to examine whether cross-sampling can help increase sales through the indirect channel.
- In Column (3) of Table 4, we construct a new variable as the spending on the sampled item (of the sampling brand) sold at the C-to-C platform.
- Column (4) includes the spending on other items of the same brand sold at the C-to-C platform.
- We found a significant increase in sales of the sampled items through indirect channels in the second and third periods (months 1-3 and months 4-14).
- In addition, we conduct a detailed analysis of the possible brand-switching effect suggested by previous literature, finding little evidence that the effect is salient.
- In summary, our analyses provide two important findings regarding the nature of cross-sampling as a new form of free sampling.
- First, consistent with previous literature, cross-sampling can increase store visits and sales for customers who did not purchase from the sampling brand over the long run.
- The long-term effect is mainly driven by acquiring customers to visit online stores, get to know the product, and eventually make a purchase.
- Second, contrary to the previous literature on within-category free sampling, we find a significant sales increase in both the sampled item and other items, which further spillovers over to indirect channels.

# 6. Improving Effectiveness of Cross-sampling
- Besides the cross-brand and cross-category nature, cross-sampling also provides great flexibility for online brands to customize free sampling.
- Our experiment includes more than 800 purchased items and 8,000 customers from the six distributing brands, allowing us to estimate the heterogeneous effect across customer characteristics and characteristics of the purchase incidence in the distributing brand.

## 6.1. Heterogeneous Effect Across Customers and Purchase Incidences
- We draw on the existing literature to understand the heterogeneous effect of cross-sampling.
    1) Demographics and spending power. For individual customers, purchase decisions and promotion effectiveness can vary greatly across demographics [Slama and Tashchian, 1985; Inman et al., 2009].
- We construct Is F emale (a dummy variable indicating female customers) and Age segments to measure user demographics.
- We then use Annual Spending as a measure of purchasing power, normalized to be between zero and one for each distributing brand.
    2) Interests in sampling category. Customers’ responses to promotion also depend on their interests at the time of the promotion [Montgomery et al., 2004; Zhang and Krishnamurthi, 2004].
- We further define a dummy variable V iewed Sampling Category indicating whether the customer has viewed the sampling category during the previous three months.
    3) Price sensitivity. We define Item Discount as the normalized discount rate of purchased items.
    4) Essential shopping. We further define Essential as a dummy variable indicating whether the purchased item belongs to one of a list of essential subcategories.
- We then estimate the heterogeneous effect across different characteristics x for month 0 and months 1-3 using the following specification
- Outcomeit = α + β1 · T reati · Af ter0t + η1 · Af ter0t · x + θ1 · T reati · Af ter0t · x + β2 · T reati · Af ter1to3t + η2 · Af ter1to3t · x + θ2 · T reati · Af ter1to3t · x + β3 · T reati · Af ter4to14t + ut + vi + γ · T reati · Bef ore1t + ϵit.
- This is essentially a difference-in-difference-in-differences (DDD) model.
- We are interested in the heterogeneous treatment effect θ1 and θ2 during month 0 and months 1-3, respectively.
- We place the detailed results in Appendix D.
- Table 16 estimates the heterogeneous effect across user demographics, spending power, and their interests in the sampling category.
- We find that cross-sampling is more effective in increasing the sampled item’s sales if the customers have recently viewed products from the sampling category.
    -  E.g., Column (4) shows that cross-sampling increases these customers’ purchase likelihood of the sample item by 5.99% (p < 0.01) during the month of the campaign.
- Across user demographics, we do not find a systematic heterogeneous effect. We also do not find a significant heterogeneous effect across users with different spending power.
- Table 17 further estimates the heterogeneous effect across user price sensitivity and essential shopping behavior based on characteristics of the purchase incidence at the distributing brand.
- We find that price sensitivity slightly decreases the impact of cross-sampling on the sampling brand’s online stores in the short term.
- Customers who went non-essential shopping, cross-sampling increased their likelihood of purchasing the tested item by 12.7% (p < 0.001). For customers who went essential shopping at the distributing brand, the effect decreases to 3.8% (p < 0.001).
- Our results show that cross-sampling is more effective for customers who recently viewed related products or just purchased non-essential products from the distributing brand.
- It also provides helpful guidance for practice based on simple changes to how free samples are distributed.
- For example, the platform can bundle the free sample with non-essential products at the distributing brand.

## 6.2. Moderating the Effect Through Similarity Scores
- We construct a series of similarity scores to measure the similarity of customer bases between the sampling and distributing parties.
- We then examine whether the similarity scores can significantly moderate the causal effect, informing about which stage of matching is more important.
- Our key idea draws on the insight “similar customers behave similarly” [Resnick and Varian, 1997; Adomavicius and Tuzhilin, 2005; Lee et al., 2019; Gupta et al., 2020].
- We first use customer characteristics to represent each customer, then use the distribution of customer characteristics to represent items and brands.
- Finally, we use common distance metrics to measure the similarity among users, items, and brands.
- Appendix E contains the detailed description.
- As an example, Figure 4 illustrates the construction of CustomerT oBrand Score and ItemT oBrand Score.
- The sampling brand (L’Oreal) needs to find the right product package from the distributing brand (Vinda) for cross-sampling.
- The CustomerT oBrand Score measures the similarity between the focal user u and the sampling brand’s customer base S.
- The ItemT oBrand Score measures the similarity between the focal item I and the sampling brand’s customer base S.
- We use Mahalanobis distance for the CustomerT oBrand Score and the discrete version of the L1 Wasserstein distance for the ItemT oBrand Score.
- Similarly, we can construct a BrandT oBrand Score to measure the similarity of customer bases between the distributing brand and sampling brand.
- Table 5 shows the heterogeneous effect on store sales. We find that the CustomerT oBrand Score and the ItemT oBrand Score significantly moderate the causal effect across all three outcomes during month 0.
    - E.g., Column (3) shows that an increase in the CustomerT oBrand Score from 0 to 100% leads to 0.11 RMB more spending at the sampling brand’s online store. The corresponding moderating effect for the ItemT oBrand Score is 0.40 RMB.
- The sampling brand can prioritize the selection of items at the distributing brand for free sampling if it aims to improve the impact of cross-sampling on store sales.
- Appendix E.1. shows that BrandT oBrand Score does not have a significant moderating effect, and qualitative insights are largely consistent.
- Our results have demonstrated the potential of customizing cross-sampling.
- Both user- and item-level data help moderate the causal effect. The matching of the two brands may be less important.
- Item-level information is more relevant than user-level information regarding cross-sampling personalization.

## 6.3. Value of Data
- Is cross-sampling profitable? The platform charges a fee from the sampling brands to cover the distribution cost.
- The focal brands’ profitability depends on how much they can gain marginally from cross-sampling.
- In the current practice, Alibaba distributes free samples to random walk-in customers at last-mile pickup stations [Han et al., 2023].
- The average return of cross-sampling is significantly larger than that of the organic walk-in free sample distribution.
- We can personalize cross-sampling to improve its cost-effectiveness further.
- To illustrate the value of data for personalization, we conduct a counterfactual analysis to compute the revenue gain under resource constraints.
- In Table 6, we consider four scenarios where the sampling brand is subject to different sources of information.
- In Column (1), the brand cannot use any type of data for personalization and has to target randomly.
- In Columns (2) and (3), the sampling brand can use either the CustomerT oBrand Score or the ItemT oBrand Score to sort each package and distribute free samples to the top-ranked packages.
- In Column (4), the sampling brand can use both types of information to estimate the heterogeneous effect and personalize cross-sampling depending on the customer and the item purchased by the customer.
- Given a resource constraint (the maximum number of free samples), we are interested in how much revenue the sampling brand can collect from the customers in month 0.
- Across different levels of resource constraints, Column (4) of Table 6 always achieves the largest percentage revenue gain.
- If we can use only data aggregated to the item level, Column (3) has a good performance, close to Column (4).
- The type of data we use for personalization here is much less granular than what e-commerce platforms often use for online recommendations.

# 7. Discussion
- Our study explores a new business practice that treats the logistics infrastructure as a channel to generate additional revenue streams.
- Future research can study the general notion of cross-sampling by collaborating with many brands.
- We can generalize free sampling to other promotional strategies such as cross-selling or bundling.

</details>

```
title: CONNECTING CUSTOMERS AND MERCHANTS OFFLINE: EXPERIMENTAL EVIDENCE FROM THE COMMERCIALIZATION OF LAST-MILE STATIONS AT ALIBABA
authors: Brian Rongqing Han, Tianshu Sun, Leon Yang Chu, Lixia Wu
journal: MIS Quarterly
published: 2024
```
 
# Executive Summary
- This research investigates how **last-mile logistics infrastructure** (networks of stations established by e-commerce platforms) can serve as **offline platforms**, connecting customers and merchants.
- We focus on **free sample distribution** as an example, and use two large-scale studies with Alibaba: an observational study (1,032 stations) and a randomized field experiment (189,019 customers).
- We compare **organic interaction** (walk-in traffic) and **induced interaction** (prompting customers through online intervention).
- Key **theoretical** / **conceptual framework** discussions:
    - **Omnichannel environment**: The study emphasizes the integration of online and offline channels.
    - **Advantageous self-selection**: Customers willing to incur additional travel costs to claim samples are more likely to purchase, acting as a screening mechanism.
    - **Local average treatment effect (LATE)**: Used to estimate the conversion rate for induced claimers.
    - **Generalized random forest model**: Customized targeting framework to enhance the effectiveness of induced interaction.
- Key **findings** / arguments including:
    - **Induced interaction drives significantly more online sales than organic interaction.**
    - **Online intervention increases the number of free samples distributed under induced interaction.**
    - The increase in online sales is not simply due to more samples distributed, but because **induced customers are more interested and likely to purchase (advantageous self-selection).**
    - The **last-mile infrastructure can serve as a screening device**, attracting more interested customers.
    - **Offline characteristics** (self-pickup, parcel shipping, location) **and online behavior** (visits, item views, purchases) **do not fully explain advantageous self-selection.**
    - A **customized targeting framework using a generalized random forest model can enhance the effectiveness of induced interaction.**
    - **Organic interaction** at the last-mile stations **has a limited economic impact on online sales.**
    - **Online notification increases sample claims by 14%.**
    - **Purchase probability from the focal brand increased by 3.8% for induced claimers.**
    - A follow-up survey revealed that **claimers in the treatment group were more likely to have used or plan to use the free samples.**
    - Station parcel shipping and monthly category spending are the most important variables in explaining the heterogeneity of LATE.
 
#omnichannel #last_mile_logistics #offline_platform #online_intervention #organic_interaction #induced_interaction #advantageous_self_selection #randomized_field_experiment #generalized_random_forest #local_average_treatment_effect

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Major e-commerce platforms invest in last-mile offline infrastructure to ease logistics, but this infrastructure also has the potential to connect customers and merchants offline [Amazon, 2022].
- Platforms can distribute free samples through last-mile stations, allowing walk-in customers to claim them via organic interaction.
- Alternatively, platforms can use mobile notifications to induce nearby customers to visit stations and claim samples via induced interaction [Hui et al., 2013; Fang et al., 2015; Luo et al., 2013; Fong et al., 2015; Chen et al., 2017].
- Research questions:
    - Is organic interaction effective in driving online sales?
    - Is induced traffic more effective than organic traffic in driving online sales, and why?
- We argue that induced interaction combines the advantages of online and offline channels, using online intervention to inform nearby customers about offline promotions at last-mile stations, which can lead to online conversions.
- A key observation is that induced claimers must self-select to incur additional travel costs, suggesting different characteristics than organic claimers.
- This self-selection could lead to advantageous selection, where induced claimers are genuinely interested in the product, or be driven by the "freebie" [Wiesel et al., 2011; Bijmolt et al., 2021].
 
# 2. Literature Review
- Our work contributes by integrating all features of the online and offline channels in a complete cycle [Overby and Jap (2009), Wiesel et al. (2011), Avery et al., (2012), Gao & Su (2016), Jing (2018), Bijmolt et al., (2021)].
- Literature examines interplay among four unique features: offline infrastructure, offline environment, online touchpoint, and online conversion.
- **Three streams of literature** based on how these features are discussed across online and offline channels:
    - Comparison of offline stores and online stores: focusing on differences in product quality, information provision, or demand elasticity [Avery et al., 2012; Gao & Su, 2016].
    - Impact of offline channel on online stores: how offline stores complement or substitute for online stores, influenced by brand popularity, product assortment, pricing, or customer segments [Soysal et al., 2019; Wang & Goldfarb, 2017; Tang et al., 2016; Kumar et al., 2019].
    - Online-to-offline effect: leveraging platforms to examine the economic and societal impact of the online marketplace on offline activities [Chan & Ghose, 2013; Seamans & Zhu, 2013; Greenwood & Agarwal, 2015; Greenwood & Wattal, 2017; Zhang & Li, 2017; Alyakoob & Rahman, 2019].
- **Our study is novel in two ways:**
    - **Integration of all features:** Mobile notification informs customers about promotions at nearby last-mile stations.
    - **Complete cycle**: Online targeting, offline behavioral changes, and online purchasing behavior data are collected, allowing for personalized solutions.
- Leveraging the offline location as a screening device to induce self-selection, where only those interested would accept travel costs.
- Contributes to the literature on customer acquisition and free sampling, exploring a new channel for distributing physical free samples offline and examining how customers should be informed about promotional campaigns [Steffes et al., 2011; Datta et al., 2015; Marks & Kamins, 1988; Heiman et al., 2001; Bawa & Shoemaker, 2004; Lammers, 1991; Datta et al., 2015; Chellappa & Shivendu, 2005; Cheng & Liu, 2012; Lee & Tan, 2013; Niculescu & Wu, 2014].
- Online intervention combined with offline distribution reduces "freebie" behavior.
 
# 3. Causal Effect of Organic Interaction
- We investigated the causal effect of organic sample claims on focal brands' online sales by focusing on free sample distribution campaigns at Alibaba's last-mile stations.
- Experiment Setting and Data:
    - Campaigns were conducted without online advertising to maintain exogenous exposure to free samples.
    - A dataset including sample claims (210,934 claims across 1,032 stations), customers who visited stations (3,874,389 customers), customer characteristics, online behaviors, and offline logistics services was constructed.
    - Outcome variables: binary variable indicating whether a customer makes a purchase from the focal brand’s online store during a given week, continuous variable measuring a customer’s total spending from the focal brand’s online store during a given week, binary variable indicating whether a customer visits the focal brand’s online store during a given week, continuous variable measuring the total number of items viewed by a customer at the focal brand’s online store during a given week
    - Customer-week was the unit of analysis over 24 weeks.
- Two-Stage Matching:
    - Exact matching and propensity score matching were combined to address potential biases, ensuring the pre-treatment behavior and customer characteristics of the treatment and control groups were identical.
    - In the first stage, we exactly matched a set of binary variables, including customers’ purchase and item-view behavior at both the focal brand and category levels, and also exactly matched customers’ self-pickup and parcel shipping patterns.
    - Propensity score matching was adopted to further balance the treatment and control group over a larger number of features in the second stage.
- Main Results on Organic Sample Claims:
    - A linear panel data model with customer-specific and week-specific fixed effects was specified to estimate the average treatment effect.
    - We cluster the standard errors of 𝜖𝑖𝑡 at each matched treatment and control pair to allow for heteroscedasticity.
    - DID estimates were positive and statistically significant, but the economic impact of organic sample claims was limited (e.g., purchase probability increased by 0.01%).
    - Robustness checks confirmed the findings using alternative matching methods and verifying the parallel trend assumption.
- Leveraging organic traffic at last-mile stations resulted in low returns, differing from findings in brick-and-mortar retail settings.
- A customized solution is needed to increase the effectiveness of offline promotions in increasing online sales.
 
# 4. Causal Effect of Induced Interaction
- We designed a large-scale field experiment to investigate whether online notification can causally induce more sample claims and the causal effect of induced interaction.
- Experiment Setting and Data:
    - A three-week free sample distribution across 40 stations in four major cities in August 2018.
    - 80% of the 409,704 users in the vicinity of the stations were randomly assigned to the treatment group, receiving an online ad in the Taobao mobile app.
    - The randomized online notification creates exogenous variation of awareness among a group of customers.
- Identification Strategy:
    - Leveraging the randomized online notification to identify the causal impact of induced interaction.
    - A linear model was estimated to determine the causal effect of online notification on sample claims.
    - We carry out a two-stage least square (2SLS) estimation using OnlineAdsi as an instrumental variable (IV) for SampleClaimeri
    - Local average treatment effect (LATE) framework was used to define induced claimers and estimate their causal effect.
- Main Results on Induced Sample Claims:
    - Online notification increased sample claims by 14%.
    - We estimate that if we were to leverage online notification and inform nearby customers, the platform could induce 19,489 more sample claims within the same period.
    - Induced sample claims significantly increased customers’ online purchases from the focal brand (purchase probability increased by 3.8%).
    - Induced interaction did not affect customers’ item views of the focal brand.
    - The increased sales were caused by a small group of induced claimers with different characteristics from organic claimers.
 
# 5. Mechanism Underlying the Effectiveness of Induced Interaction: Self-Selection
- The fundamental difference between induced and organic interactions lies in their sample-claiming processes.
- We propose self-selection as a plausible explanation for the behavioral difference.
- Self-selection is based on observed characteristics or unobserved preferences.
- Induced claims could advantageously affect or have no effect on whether customers will purchase from the focal brand.
- Three types of customers: organic claimer, no-claimer, and induced claimer.
- Self-Selection Based on Observed Characteristics:
    - Claimers in the treatment group (Group 4) had more annual purchases and were less active at the stations compared to the control group (Group 3).
    - Only "high spenders" were induced to claim samples and were associated with a significant increase in sales.
- Self-Selection Based on Unobserved Characteristics:
    - Two empirical tests were used to investigate self-selection based on unobserved characteristics.
    - Adding pre-treatment covariates to the 2SLS estimation did not change the significant causal effect of induced sample claims.
    - Positive correlation between group membership (Group 3 versus Group 4) and outcomes persisted even after controlling for observed characteristics.
    - A follow-up survey showed that more claimers in Group 4 had either already used or planned to use the free samples.
 
# 6. Targeting to Induce the “Right” Customers
- We leverage the stations as a physical hub to induce self-selection and screen customers with strong preferences.
- We take advantage of a state-of-the-art machine learning technique that can help estimate LATE at the individual level.
- A generalized random forest algorithm was adopted to estimate the heterogeneous causal effect of induced sample claims through instrumental variable regression.
- The algorithm was trained on the study population with the binary outcome variable "brand purchase or not," using covariates defined in Table 4.
- Heterogenous LATE can be estimated significantly different by the algorithm.
- Variable importance was obtained by counting the number of times a variable was used for splitting.
- Station parcel shipping and monthly spending in the category were the most important variables explaining the heterogeneity of LATE.
- The platform can strike a balance between cost and return by optimizing the subpopulation to target with online ads.
 
# 7. Conclusion and Discussion
- The "foot-in-the-door" traffic in an omnichannel environment can be different depending on whether offline customers are driven from the online channel.
- Offline infrastructure helps facilitate an advantageous self-selection mechanism when customers are induced by online intervention.
- Our results introduce a key insight that the “foot-in-the-door” traffic in an omnichannel environment can be fundamentally different depending on whether the offline customers are driven from the online channel.
- The platform can design offline interactions and online notifications to control the proportion of induced claimers compared to organic claimers.
- Platforms could use stronger communication methods to advertise offline promotions, increase customer awareness, and improve free sampling returns.
- Platforms could decrease the proportion of organic claimers by hiding free samples in stations, effectively preventing freebie behavior.
- Future research directions:
    - Interventions benefiting last-mile logistics.
    - Field experiments at the station level.
    - Combining customer-level and station-level data.
    - Incorporating uncertainty into decision-making.
 
# Executive summary of 1. Introduction
- Major e-commerce platforms invest in last-mile offline infrastructure, with the potential to connect customers and merchants offline through free sample distribution.
- This can occur via organic interaction (walk-in customers) or induced interaction (mobile notifications).
- The study investigates the effectiveness of these interactions in driving online sales, focusing on the self-selection behavior of induced claimers and the potential for advantageous selection.
 
# Executive summary of 2. Literature Review
- Our study integrates features of online and offline channels in a complete cycle.
- Previous literature includes comparison of online and offline stores, impact of offline channels on online stores, and online-to-offline effects.
- This study is novel in integrating all features of online and offline channels and in creating a complete cycle of cross-channel effects.
 
# Executive summary of 3. Causal Effect of Organic Interaction
- The study investigates the causal effect of organic sample claims on online sales using observational data from Alibaba's last-mile stations.
- It applies two-stage matching (exact matching and propensity score matching) to address potential biases in the data.
- The results indicate that while organic interaction has a statistically significant positive effect on online sales, its economic impact is limited.
 
# Executive summary of 4. Causal Effect of Induced Interaction
- The study designs a randomized field experiment to examine the causal effect of induced interaction, where online notifications are used to drive customers to claim free samples at last-mile stations.
- The results show that online notifications significantly increase sample claims and that induced sample claims have a significant positive impact on customers' online purchases.
- In particular, online notification increased sample claims by 14%, and purchase probability from the focal brand increased by 3.8% for induced claimers.
 
# Executive summary of 5. Mechanism Underlying the Effectiveness of Induced Interaction: Self-Selection
- The study explores the underlying mechanism for the effectiveness of induced interaction, focusing on self-selection.
- It identifies that induced claimers may have different characteristics (observed and unobserved) compared to organic claimers.
- The findings suggest that advantageous self-selection outweighs "freebie" behavior, with induced claimers being more genuinely interested in the product.
 
# Executive summary of 6. Targeting to Induce the “Right” Customers
- The study develops a customized targeting framework using a generalized random forest model to enhance the effectiveness of induced interaction at last-mile stations.
- The algorithm estimates the heterogeneous causal effect of induced sample claims (LATE) through instrumental variable regression.
- The results show that station parcel shipping and monthly category spending are the most important variables in explaining the heterogeneity of LATE.
- Optimization of the subpopulation to target with online ads can balance costs and returns.
 
# Executive summary of 7. Conclusion and Discussion
- The "foot-in-the-door" traffic in an omnichannel environment varies depending on whether customers are driven from online channels.
- Offline infrastructure facilitates advantageous self-selection when customers are induced by online intervention.
- Platforms can design offline interactions and online notifications to control the proportion of induced claimers compared to organic claimers.

</details>

```
title: COVID-19 and E-Commerce Operations: Evidence from Alibaba
authors: Brian Rongqing Han, Tianshu Sun, Leon Yang Chu, Lixia Wu
journal: Manufacturing & Service Operations Management
published: 2022
```
 
# Executive Summary
- The study investigates the impact of **COVID-19** on **e-commerce sales** and its **operational drivers**, using data from Alibaba across 339 cities in mainland China. **Three identification strategies** were employed:
    - **Year-on-year comparison**: comparing sales data from 2018, 2019, and 2020 to estimate the overall impact of COVID-19, controlling for seasonality.
    - **COVID-19 intensity comparison**: comparing cities with high (more than 10) and low (less than 10) confirmed COVID-19 cases using only the 2020 data.
    - **Policy evaluation**: leveraging the policy variation of containment measures (checkpoints, partial shutdowns, and complete shutdowns) across cities and controlling for date- and city-level fixed effects.
- Key findings reveal a **drop and recovery pattern** in e-commerce sales, showing **digital resilience**.
    - During the Wuhan shutdown (January 23–April 7, 2020), e-commerce sales initially decreased by 22% overall.
    - E-commerce sales recovered in most cities within five weeks.
- **Logistics capacity** is identified as a critical operational driver affecting the decline and recovery of e-commerce sales.
    - During the Wuhan shutdown, the average package delivery time increased by 31%, but only increased by 3% during the recovery phase.
- The findings offer evidence of **e-commerce resilience** and inform strategies for platforms and policymakers to invest in logistics infrastructure.
 
#ecommerce #covid_19 #containment_measures #digital_resilience #logistics #alibaba #sales #operations_management


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The **COVID-19 pandemic** significantly disrupted the **retail industry**, leading to decreased visits to offline retailers due to the pandemic and containment policies [Chetty et al., 2020; Ghose et al., 2020; Tucker and Yu, 2020].
- E-commerce became the main shopping channel because customers adopted online channels to buy products unavailable offline or to avoid contact with others [Torry, 2020].
- Offline mobility restrictions may cripple e-commerce platforms' ability to prepare sufficient inventory and logistics capacity to deliver packages on time [Ziobro, 2020a, b], thus decreasing e-commerce sales.
- Unlike offline commerce, e-commerce offers lower prices [Overby and Forman, 2015; Ming and Tunca, 2019], more product variety [Brynjolfsson et al., 2009; Xu et al., 2017; Fan et al., 2018], and convenience [Benzell et al., 2020; Goldfarb and Tucker, 2020].
- Shipping can be slow and expensive [Fisher et al., 2019], and containment policies can limit offline mobility, affecting e-commerce sales.
- The paper presents an empirical investigation on how COVID-19 affects e-commerce sales.
    - The outbreak in China, defined by the Wuhan shutdown (January 23, 2020) and reopening (April 8, 2020), allows exploration of a relatively complete cycle.
    - Three periods are examined: before the outbreak (November 24, 2019 – January 22, 2020), during the outbreak (January 23, 2020 – April 7, 2020), and post-outbreak/recovery (April 8, 2020 – May 27, 2020).
- The initial outbreak in China triggered government actions and public concerns.
    - Containment policies like complete shutdowns, partial shutdowns, and checkpoints were implemented across cities [Fang et al., 2020].
    - Offline consumption decreased by an estimated 1.22 trillion RMB (1.2% of China’s 2019 GDP) [Chen et al., 2021], necessitating an understanding of COVID-19's impact on e-commerce.
- *Research Questions:*
    1. What is the impact of COVID-19 and related containment measures on e-commerce sales?
    2. What is the key operational driver underlying the impact of COVID-19 on e-commerce?
- City-day panel data on e-commerce sales from Alibaba, representing all buyers and sellers on taobao.com and tmall.com across 339 cities in mainland China, were used.
    - E-commerce sales data covers three periods (matched based on the Chinese lunar calendar): November 24, 2019 – May 27, 2020 (2020 data), December 5, 2018 – June 15, 2019 (2019 data), and December 16, 2017 – June 19, 2018 (2018 data).
    - Additional data sources include online browsing and cart add-on behavior and logistics information from Cainiao Network. Public data on daily confirmed COVID-19 cases and containment measures were added.
- Three identification strategies are designed to estimate the overall impact of COVID-19, the impact of COVID-19 intensity, and the impact of containment measures.
    - The first estimates the overall changes to e-commerce sales post-pandemic.
    - The second leverages the variation of COVID-19 cases across cities.
    - The third quantifies the economic costs of government containment measures.
- The 2019 data (matched on lunar calendar) is used as a comparison with control for seasonality from the 2020 period [Cao et al., 2020; Raj et al., 2020; Chen et al., 2021; Sim et al., 2022].
    - Results show a 22% decrease in e-commerce sales during the Wuhan shutdown, but sales recovered in most cities outside Wuhan within five weeks.
- The second identification strategy compares cities with high (more than 10) and low (less than 10) COVID-19 intensity.
    - Cities with high COVID-19 intensity experienced a 12% decrease in e-commerce sales during the Wuhan shutdown, with recovery occurring two weeks after the Wuhan reopening.
- The third leverages the policy variation of containment measures (checkpoints, partial shutdowns, and complete shutdowns) across cities, controlling for date- and city-level fixed effects [Fang et al., 2020; Qiu et al., 2020].
    - A complete shutdown significantly decreased e-commerce sales by 55%. The recovery pattern was similar across different identification strategies, indicating digital resilience.
- Online browsing data and offline package delivery data identify logistics capacity as a key operational driver.
    - There was a decrease in customers’ ability to complete orders during the outbreak, conditional on adding products to the cart.
    - Inventory shortage does not drive the decrease in the order/cart add-on ratio, suggesting logistics issues.
    - The average package delivery time increased by 31% during the Wuhan shutdown and only 3% afterward.
- Overall, the paper contributes by examining how e-commerce sales respond to the pandemic and identifying digital resilience, contrary to findings on offline consumption [Chetty et al., 2020; Chronopoulos et al., 2020; Goldfarb and Tucker, 2020; Chen et al., 2021].
    - Logistics capacity is shown to be a key operational driver.
 
# 2. Literature Review
- The study relates to literature in operations management and economics, focusing on the impact of COVID-19 on the economy.
    - Studies have documented consumer responses to the pandemic in the United States [Andersen et al., 2020; Baker et al., 2020; Chetty et al., 2020; Ghose et al., 2020], United Kingdom [Chronopoulos et al., 2020], and China [Chen et al., 2021].
    - Adulyasak et al. [2020] developed data-driven models for retailers to respond to the pandemic.
- Investigation on e-commerce contrasts with existing literature on offline consumption.
    - Studies show a significant decrease in offline consumption, especially in dining [Tucker and Yu, 2020; Glaeser et al., 2021] and retail sectors [Goldfarb and Tucker, 2020]. The paper provides evidence on the digital resilience of e-commerce, showing how online channels substitute for offline retail.
- The study uses comprehensive data representative of China's e-commerce sector.
    - Previous studies used surveys or interviews to assess the impact on online sales for specific products such as food in China [Gao et al., 2020], Germany [Dannenberg et al., 2020], and Malaysia [Hasanat et al., 2020].
    - The results extend existing literature by identifying the systematic decrease and recovery pattern of overall e-commerce sales.
- The impact of COVID-19 is investigated by linking online sales with offline operational factors, showing logistics capacity explains the variation in e-commerce sales.
- Literature explores the benefit and economic costs of government containment policies during a pandemic.
    - Analytical work [Ferretti et al., 2020; Jia et al., 2020; Kaplan, 2020] and empirical studies [Alexander and Karger, 2020; Fang et al., 2020; Greenstone and Nigam, 2020] have shown containment measures restrict population mobility, limiting virus spread.
    - Research has investigated economic costs regarding the flow of goods [He et al., 2020], social cost [Benzell et al., 2020], gender inequality [Cao et al., 2020; Cui et al., 2021], and privacy [Ghose et al., 2020].
    - Raj et al. [2020] provided evidence on the digital resilience of on-demand food delivery platforms.
- The literature is extended to e-commerce platforms, identifying the impact of different containment policies.
    - The results suggest moderate containment measures (partial shutdown or checkpoint) would not significantly influence e-commerce, implying e-commerce can help mitigate containment measures’ social and economic costs.
- Literature aims to understand the relationship between retailing and logistics [Lee and Whang, 2001; Terwiesch et al., 2005; Fan et al., 2018; Cohen and Lee, 2020].
    - Earlier research investigated the impact of supply chain glitches on operational performance [Hendricks and Singhal, 2003, 2005].
    - Studies have shown how customers receive products can benefit overall revenue [Gallino and Moreno, 2019] through innovative practices [Gallino and Moreno, 2014; Gallino et al., 2016].
    - Recent research has established a link between delivery speed and online sales [Luo et al., 2020; Fisher et al., 2019; Cui et al., 2020a, c].
    - The work extends this by pointing out that logistics capacity is an important operational factor in the face of a pandemic, serving as a key mechanism.
- The year-on-year comparison connects to empirical work where the independent variable of interests has affected everywhere [Acemoglu et al., 2004; Bandiera et al., 2005; Cao et al., 2020; Chen et al., 2021; Sim et al., 2022].
    - The paper compiles three data periods and illustrates how the overall impact is measured as a disruption to the seasonality pattern.
- Work contributes to the community of empirical operations management [Terwiesch et al., 2019], focusing on causal inference.
    - Experimental studies increasingly use randomized field experiments [Cui et al., 2019; Zhang et al., 2020; Feldman et al., 2022; Sun et al., 2022; Qiu and Kumar, 2017; Cohen et al., 2022; Li et al., 2019; Cui et al., 2020b; Li et al., 2018; Cohen et al., 2019; Cao and Zhang, 2020].
    - Observational studies leverage policy changes and design natural/quasi experiments [Fisher et al., 2021; Hong and Pavlou, 2014; Li and Netessine, 2020; Lu and Lu, 2017, 2019; Levi et al., 2020].
    - The work builds on observational studies and designs an empirical framework to investigate different aspects of the COVID-19 pandemic.
 
# 3. Data
- Data is provided by the Alibaba Group and its logistics subsidiary, Cainiao Network.
- There are three types of data sources: e-commerce sales, logistics, and customer ordering processes.
- Publicly available data on daily confirmed COVID-19 cases and the timing of different containment measures across cities were appended.
- Sales is the primary dependent variable, collected at the city-day level, measuring daily e-commerce sales from Alibaba’s online platforms (tmall.com and taobao.com) for each city.
    - The resulting city-day panel data includes 339 cities across 31 provinces in mainland China, comprising 92.5% of China’s population.
    - Data periods span approximately three years: November 24, 2019 – May 27, 2020 (2020 data), December 5, 2018 – June 15, 2019 (2019 data), and December 16, 2017 – June 19, 2018 (2018 data).
- Time periods are chosen to match the Chinese lunar calendar [Chen et al., 2021].
    - COVID-19 first appeared a few days before the Chinese New Year (Spring Festival). Matching of lunar dates helps capture the seasonality of e-commerce sales.
- The aggregated daily e-commerce sales of the 2018, 2019, and 2020 data were plotted (Figure 1) with anonymized scales, with the horizontal axis marked with the calendar dates of 2020.
    - Year-on-year comparison allows for an overall estimate of changes in e-commerce sales while controlling for seasonality [Bandiera et al., 2005; Cao and Zhang, 2020; Chen et al., 2021].
- Based on package delivery data provided by Cainiao, the total number of orders (*TotalOrder*) and the average package delivery time (*AvgDeliveryTime*) for each city and day for the same period were computed.
    - This allows for measuring logistics-related performance.
- Two ratios were defined to measure the conversion through customers’ ordering processes:
    - *CartAdd-on=Item-view* is the ratio of the total number of cart add-ons to the total number of views, capturing the proportion of demand that can be fulfilled with available inventory.
        -  A significant decrease in this ratio would mean that inventory-related issues may have caused sellers to stock out.
    - *Order=CartAdd-on* is the ratio of the total number of orders to the total number of cart add-ons, capturing the conversion from the demand for available inventory to orders, that is, final sales.
        - A significant decrease in this ratio would suggest that customers cannot place orders for some reason other than lack of inventory.
    - These variables are constructed only at the aggregated day level due to restricted data access, with no breakdown for each city.
- Two sources of public data were collected:
    - Data on daily confirmed COVID-19 cases (*DailyCases*) from the Chinese Center for Disease Control and Prevention for each city.
        - Wuhan was the first city to shut down and the last to reopen on April 8, 2020.
    - The timing of different containment measures applied in some cities.
        - The start and end dates of each containment measure besides the shutdown of Wuhan city were collected (Table 3).
 
# 4. Impact of COVID-19 on E-Commerce Sales: “Digital Resilience”
- The goal is to understand the impact of COVID-19 on e-commerce sales.
- Build on recent literature [Fang et al., 2020; Qiu et al., 2020; Raj et al., 2020] and distinguish the measurement and interpretation of empirical models in three ways.
 
## 4.1. Overall Impact of COVID-19: Before-After Estimation Controlling for Seasonality Using 2018, 2019, and 2020 Data
- Estimate the overall impact of COVID-19 on e-commerce sales.
    - Since the pandemic started, everywhere in China has been affected.
    - Following recent literature on COVID-19 [Cao et al., 2020; Raj et al., 2020; Chen et al., 2021], data from the previous year is used as a comparison.
- The 2020 and 2019 data are pooled, and the following model is estimated:
    - Log(*Salesct*) = α + β0 · *Year2020t* + β1 · *Year2020t* · *DuringWuhanShutdownt* + β2 · *Year2020t* · *AfterWuhanReopent* + θt + dt + *Promot* + εct
    - The subscript *c* indicates cities and *t* indicates lunar calendar dates. *Year2020t* equals 1 for the 2020 period and 0 for the 2019 period.
    - *DuringWuhanShutdownt* equals 1 during the Wuhan shutdown (January 23 to April 7 in 2020) and for the corresponding lunar dates in 2019, and 0 otherwise.
    - *AfterWuhanReopent* equals 1 for the corresponding lunar dates after the Wuhan reopening for the two periods. Lunar-date, day-of-week, and large-promotion fixed effects are controlled using θt, dt, and *Promot*, respectively.
- Equation (1) resembles a difference-in-differences (DID) model.
    - The first before-after difference refers to the sales difference before and after the Wuhan shutdown for all cities in 2020.
    - The second before-after difference refers to the sales difference before and after the corresponding lunar date of the Wuhan shutdown for the same group of cities in the 2019 period.
    - The difference in these two differences would estimate the overall change in e-commerce sales during the 2020 period (before and after COVID-19 hits), controlling for seasonality using the 2019 data.
- The coefficients β1 and β2 are of interest.
    - The coefficient β1 measures the change of e-commerce sales after the Wuhan shutdown (compared with before), removing seasonality.
    - The coefficient β2 measures the change of e-commerce sales after the Wuhan reopening.
- Column (1) of Table 1, panel A, presents the estimation results of the main model Equation (1).
    - There is a systematic decrease in e-commerce sales during the period of the Wuhan shutdown, followed by a significant recovery after the Wuhan reopening.
        - There is an average decrease of 22% in sales during the Wuhan shutdown. After the Wuhan reopening, the negative impact reduces to approximately 8%.
- Two alternative models are estimated in columns (2) and (3).
    - Column (2) assesses whether the effect exists before the Wuhan shutdown.
        -  The coefficient for the interaction effect Before_1Month and Year2020 is not significant, showing that the observed effect does not come in spuriously before COVID-19 happens.
    - Column (3) estimates a fixed-effect model, in which city-year fixed effects were added.
        - The results provide the same point estimates with reduced variance.
- The identification strategy depends mainly on controlling for seasonality.
- Two examinations are conducted to ensure the validity of the identification strategy.
- In panel B of Table 1, the 2019 data is replaced with 2018 data, and the analyses are redone.
    - The estimates are consistent with similar sizes of magnitude, suggesting that the overall seasonality is stable across years.
- Panel C assesses whether the identification of the overall impact of COVID-19 is picking up sales changes due to factors other than seasonality.
    - Using only the 2018 and 2019 data, a model similar to Equation (1) is estimated, where the dummy variable Year2020 is replaced with Year2019.
    - The estimates are mostly not significant, suggesting that controlling for seasonality is sufficient to measure the overall changes in e-commerce sales after the pandemic hit since factors other than seasonality have a minimal impact.
- To provide a clearer picture of how fast e-commerce recovers from a COVID-19 outbreak (marked by the period of Wuhan shutdown), estimates are broken down by week:
    - Log(Salesct) = α + β0 · Year2020t + ∑4i=1 γi · Year2020t · BeforeWuhanShutdown_iWeekt + ∑≥12j=0 βj · Year2020t · AfterWuhanShutdown_jWeekt + θt + dt + εct,
    - which produces a series of weekly estimates before, γi (i = 1, : : : , 4), and after, βj (j = 0, : : : , ≥ 12), the Wuhan shutdown.
    - The recovery is faster for cities further away from the epicenter. For most cities outside of Hubei province, e-commerce sales recover four to five weeks after Wuhan shutdown.
 
## 4.2. Impact of COVID-19 Intensity: Comparison Across Cities with Different Confirmed Cases Using 2020 Data
- A key observation of the COVID-19 outbreak in China is that not all of the 339 cities have experienced confirmed cases.
    - About 40% of cities have fewer than 10 total cases throughout the 2020 period.
    - Findings on the decrease and recovery of e-commerce sales can be further strengthened if a similar pattern can be observed by exploring the variation of e-commerce sales across cities in 2020.
    - Thus, the different number of confirmed cases across cities is leveraged to examine the impact of COVID-19 intensity using only the 2020 data.
- Several empirical models are estimated.
    - The main model takes the form similar to Equation (1).
        - The key difference is that it compares cities with more than 10 total cases and cities with 10 or fewer cases:
        - Log(Salesct) = α + β1 · TotalCases > 10c · DuringWuhanShutdownt + β2 · TotalCases > 10c · AfterWuhanReopent + Cityc + Datet + εct,
        - where *Cityc* and *Datet* are city-specific and date-specific fixed effects.
        -  *TotalCases > 10c* equals 1 for cities with total cases greater than 10 throughout the study period and 0 otherwise, comparing the 60% of the cities with higher intensity of COVID-19 with the 40% of the cities with very few cases.
        - The interaction effect β1 and β2 measure e-commerce sales changes for cities with many cases relative to cities with few cases.
    - The notion of “high” intensity can also be defined directly based on the number of daily confirmed cases.
        -  *DailyCasesct* denotes the number of confirmed new COVID-19 cases (in thousands) in city *c* at date *t*.
        - A dummy variable *DuringOutbreakct* equals 1 if city *c* has any confirmed cases and has not reached its peak cumulative number of cases by date *t*. Then, *AfterOutbreakct* indicates the period after city *c* has reached its peak cumulative number of cases.
- The results are presented in columns (1)–(3) of Table 2.
    - There is a similar pattern of decrease and recovery.
        - Compared with cities that had 10 or fewer cases, cities with more than 10 cases experienced a 12% decrease in e-commerce sales during the period of the Wuhan shutdown.
        - Such a decrease shrinks to about 6% within six weeks after the Wuhan reopening.
        - Column (2) estimates a negative effect of daily confirmed cases on e-commerce sales, while column (3) provides similar estimates as those in Column (1), showing that allowing the timing of outbreaks to be different across cities does not affect our results.
- A cleaner causal effect is established in two ways.
- City-level characteristics (by year-end of 2019) are collected from the National Bureau of Statistics of China, including administrative land area, gross domestic product per capita, average income, total population, population density per km2, number of employed, total retail sales, and number of hospitals.
    - Propensity score matching and coarsened exact matching coupled with the estimation of Equation (3) is conducted.
    - Cities with high intensity experience 15% and 16% decreases in e-commerce during the Wuhan shutdown. After Wuhan reopens, the negative effect decreases to 5%–6%. The qualitative insights are consistent.
- A robustness check is conducted by excluding the cross-city orders.
    - By focusing only on within-city orders, such influence can be eliminated.
    - The count of within-city orders is used to control the influence of price on overall sales.
    - COVID-19 intensity decreases the number of within-city orders by 24% during the Wuhan shutdown, and there is no significant effect after the Wuhan reopening, suggesting the influence of other cities (through cross-city deliveries) does not significantly affect the main qualitative insight.
- It is important to further explain the 6% decrease in e-commerce sales after the outbreak (Table 2), by breaking down *DuringWuhanShutdownt* and *AfterWuhanReopent* from Equation (3) into week-specific dummy variables.
- The weekly effect becomes insignificant two weeks after the Wuhan reopening.
 
## 4.3. Containment Effect: Policy Evaluation Using 2020 Data
- E-commerce sales respond to the pandemic (regarding overall hitting time and the number of confirmed cases) with a general drop, followed by a recovery.
- The corresponding containment policies are another important aspect of the pandemic.
- Leverage the policy variation across cities to identify the impact of each government containment measure on e-commerce sales.
- From a policy evaluation’s perspective, it is crucial to understand the economic impact of containment measures, designed to mitigate virus transmission.
    - If a particular containment policy has a large and negative effect on e-commerce sales, it would suggest that even the e-commerce channel cannot help to substitute for the dysfunction of the offline channels [Chen et al., 2021], and should be implemented with discretion.
- Since January 23, 2020, 80 out of 339 cities have enforced containment policies to reduce offline mobility, with three levels: checkpoints, partial shutdowns, and shutdowns [Fang et al., 2020].
    - A checkpoint is the least strict containment measure and was employed by 56 cities.
    - The partial shutdown was applied in seven cities, during which most public transportation was temporarily closed in addition to the checkpoint measures.
    - The shutdown is the strictest regulation and was applied in 17 cities (all in Hubei province), in which all public transportation and private vehicles are banned, all residential buildings are locked down, and residents are not allowed to leave the city.
- All 339 cities can be characterized into four groups: control (259 cities), T1 checkpoint (56 cities), T2 partial shutdown (7 cities), and T3 shutdown (17 cities).
- Cities without any containment policies are used as a comparison. The sales trend in Figure 3 shows visually parallel trends across groups.
- A fixed-effect DID model is specified:
    - Log(Salesct) = α + β1 · *Checkpointct* + β2 · *PartialShutdownct* + β3 · *Shutdownct* + β4 · *AfterCheckpointct* + β5 · *AfterPartialShutdownct* + β6 · *AfterShutdownct* + uc + θt + εct
    -  *Checkpointct* is a dummy variable, which equals 1 for cities in the T1 group during the period of checkpoint measures and 0 otherwise.
    -  *AfterCheckpointct* equals 1 for all cities in the T1 group after the end date of containment and 0 otherwise. The variables are defined similarly for partial shutdown and shutdown.
- City and date fixed effects are controlled using uc and θt.
- Comparing the cities (T1, T2, T3) under containment with the control cities cancels out the impact of the virus itself, controlling for city-level characteristics, teasing out a clean containment effect. The coefficients β1, … , β6 are of interest.
- The estimation results are presented in Table 3.
    - More strict containment measures have a significantly more negative impact on e-commerce sales. Checkpoints have the smallest impact on e-commerce sales, with a decrease of 5%. Partial shutdown and complete shutdown significantly decrease e-commerce sales by 11% and 56%, respectively.
    - All the negative containment effects shrink to approximately 5% or to a nonsignificant level after the measures are lifted.
- A complete shutdown measure can significantly decrease e-commerce sales (by 56%), and so should be made with discretion. Partial shutdown and checkpoints seem to have a relatively small impact.
- Across different containment measures, there is a similar recovery pattern once the measures are lifted.
- There is a general decrease of e-commerce sales (by about 11%–22%) after COVID-19 hit, followed by a recovery pattern (the decrease reduces to about 5%–8%).
- The decrease and recovery pattern has been observed from the overall impact of COVID-19 (based on a year-on-year comparison), the impact of COVID-19 intensity (based on a comparison across cities in 2020), and the impact of containment measures (based on a policy evaluation).
- This indicates a critical digital resilience aspect of e-commerce during a pandemic. Offline operations, as an underlying mechanism, may play an important role.
 
# 5. Mechanisms Underlying the Impact of COVID-19: “Logistics Capacity as an Operational Driver”
- With pandemic-related offline restrictions on workers and company operations, two possible operational factors exist: inventory and logistics.
- From an operational point of view, it is not clear whether and how inventory shortage and package delivery (logistics) contribute to the general decrease in e-commerce sales during the outbreak.
- Start with a glance at customers’ ordering processes. Logistics delivery is more likely to be the driving factor, and test whether delivery time could have been the bottleneck for e-commerce amid COVID-19.
 
## 5.1. Glance Through the Ordering Process
- A customer’s ordering process starts with viewing a number of items, followed by cart add-ons and placing the order.
- In Figure 4, weekly estimates based on Equation (2) for the overall impact of COVID-19 on the conversion rate through customers’ ordering processes: cart add-on/item-view ratio and order/cart add-on ratio are plotted.
- No significant changes are found in the cart add-on/item-view ratio over time. Since customers cannot add an item to the cart if it is out of stock, a significant decrease in the cart add-on/item-view ratio would indicate inventory-related issues such as stock-outs, but it's not likely the case.
- Panel (b) shows a significant decrease in the order/cart add-on ratio by approximately 30% immediately after the Wuhan shutdown, suggesting customers either chose not to or were not able to place orders for products in the carts.
- From an operational point of view, it signals that logistics delivery could have been the main hurdle, hypothesizing that COVID-19 may have significantly decreased offline logistics, causing decreases in online sales.
- Next, examine whether COVID-19 indeed had a significant and negative impact on Alibaba’s logistics operations.
 
## 5.2. Impact of COVID-19 on Logistics
- Then estimate each of specifications—Equations (1), (3), and (4)—for two dependent variables regarding logistics (*TotalOrder*, *AvgDeliveryTime*) in Table 4.
- The effect on the total number of logistics orders is examined, and it's found that the general decrease and recovery patterns are similar to what has been observed for sales.
- Compared with 2019, there is an overall 24% decrease in the number of orders during the Wuhan shutdown, followed by an 8% decrease after the Wuhan reopening.
- Results for the average package delivery time with the opposite signs are presented.
    - There is a significant increase in average package delivery time by 31% during the Wuhan shutdown, and it soon recovers to only a 3% increase.
    - There is a slight increase in delivery, by 1%, by comparing cities with many cases and cities with few cases.
- Partial shutdown and complete shutdown increase average package delivery time by 12% and 68%, respectively. After the containment measures are lifted, there is no significant effect.
- The results suggest that COVID-19 can have a significant and negative impact on logistics capacity. The sharp increase in package delivery time may fundamentally prevent customers from placing orders, eventually decreasing the overall e-commerce sales.
- The increase in delivery time could be the underlying key operational driver.
 
## 5.3. Testing the Mechanism
- To systematically test this key mechanism (that logistics capacity is the bottleneck of e-commerce operations amid COVID-19), a significant correlation between the average package delivery time and e-commerce sales must be empirically established.
- If it can be observed that whenever the average package delivery time increases in certain cities, e-commerce sales also decrease at the same location (and vice versa), it would suggest that the changes in delivery time very likely drive the changes in e-commerce sales. Test this by exploring the variation of these effects across time and cities, respectively.
- Explore the correlation between two effects: the estimated overall impact of COVID-19 on average package delivery time and the estimated impact of COVID-19 on e-commerce sales by exploring the dynamics of these two effects across time and cities.
- Weekly estimates (based on Equation (2)) for the two dependent variables are plotted.
    - The dynamics of the two effects evolve in the opposite direction during the Wuhan shutdown. When there is an increase in delivery time, there is a decrease in sales.
    - If package delivery is recovered to normal or near-normal operations, then e-commerce sales also recover to a normal level.
    - In this way, a strong correlation between the changes in logistics capacity and changes in sales over time is established.
- Such a correlation can also be demonstrated by drawing the connection between the two effects across cities, by estimating the overall impact of COVID-19 on e-commerce sales and average package delivery time for each of the 339 cities during Wuhan shutdown based on Equation (1).
- The estimates are plotted and show a correlated trend of these two effects across cities. Cities with an estimated increase in average package delivery time tend to have an estimated decrease in e-commerce sales.
- Regress the by-city estimates for e-commerce sales on the estimates for average package delivery time, and find that the coefficient is negative and significant.
    - A 1% increase in average package delivery time corresponds to a 0.99% decrease in sales (caused by COVID-19). R2 = 0.558 indicates that the variation of the estimates for delivery time can be used to explain almost 60% of the heterogeneous effect on e-commerce sales across cities.
- One alternative explanation for such a correlation of effects is that different cities may be exposed to the virus differently. To rule out this case, the total number of confirmed cases during the outbreak as a control variable is included.
    - A strong correlation still exists. Even after controlling for the total COVID-19 cases in each city, a 0.74% increase in average package delivery time corresponds to a 1% decrease in sales (affected by COVID-19).
- A strong correlation between the two effects across cities has been established.
- The empirical evidence is consistent with the proposed mechanism and suggests that logistics capacity is likely to be a critical operational driver underlying the overall impact of COVID-19 on e-commerce sales.
 
# 6. Conclusion and Implications
- The study provides a first set of empirical evidence on the impact of COVID-19 and government containment policies on e-commerce sales.
- Leverage a comprehensive data set from Alibaba, address two important research questions that are particularly urgent.
- The results bring two key messages: digital resilience and logistics capacity as the key operational driver, which also provides important insights into practice.
- First, findings on digital resilience bring positive news to practitioners and policymakers.
    - A subsample analysis with additional data in Section E (of the online supplement) is carried out to investigate how different buyer/seller segments react to the pandemic.
    - During the Wuhan shutdown, e-commerce sales for buyers and sellers generally decrease, except for a significant increase of 35% for small-to-medium individual sellers on the customer-to-customer platform (Taobao.com).
    - After the outbreak, there is a significant increase in sales by 11%–26% for less frequent buyers and small- to medium-size sellers, suggesting possible digital expansion in the long run.
    - Both anecdotal evidence [Badger and Parlapiano, 2020; Columbus, 2020] and results from recent literature [Hwang et al., 2020] show that customers who purchased online less often would adapt to the online channel shortly after the pandemic hit.
    - Less active customers have adapted to the online channel, which may result in a possibly permanent behavioral change [Netessine, 2021].
    - Recent literature [Bartik et al., 2020; Fairlie, 2020] has documented a significantly negative impact of COVID-19 on small businesses.
    - There is an expansion in sales from small- to medium-size sellers on both the customer-to-customer and business-to-customer (Tmall.com) platforms, which also echoes anecdotal evidence [Bary, 2020], suggesting these sellers can further grow under COVID-19 by embracing e-commerce.
- There is an ongoing policy debate centering around the economic and social costs of containment policies [Andersen et al., 2020; Chetty et al., 2020; Chronopoulos et al., 2020].
    - Estimates help identify the adequate measures that balance public health efforts to limit virus transmission and its economic costs on e-commerce [Adda, 2016; Fang et al., 2020; Jia et al., 2020].
    - A complete shutdown (closing public transportation and limiting factory capacity) could have a significant and negative impact even on the online channel, whereas a partial shutdown or checkpoint implementation may have minimal impacts.
    - Policymakers can incorporate the findings into consideration when evaluating and implementing containment measures.
- Findings on the key operational driver motivate more emphasis on logistics capacity.
    - The logistics capacity in the United States has also been strained due to limited numbers of workers [Ziobro, 2020a, b] and cannot meet online demand.
    - To facilitate faster worldwide recovery, retailers should further invest in logistics to avoid a bottleneck or competitive disadvantage during a pandemic, and pay more attention to the resilience of various uncertainties in demand.
    - Policymakers and platforms can further facilitate matching between customers and merchants within the same cities or regions from the platform’s perspective.
    - Such matching would reduce the friction in online shipping (or frustration from the inability to complete an order due to decreased logistics capacity), as well as the burden on offline logistics across regions.
    - One should strike a balance between matching and market thickness [Li and Netessine, 2020].
    - The platform should also put more investment in creating a centralized information-sharing and logistics infrastructure that can coordinate different carriers to cope with unexpected shocks due to shutdowns and lack of staffing in a pandemic.
- There are limitations of the study.
    - As COVID-19 has affected everywhere around the globe, it is challenging to construct a proper control group to identify its overall impact. The year-on-year comparison offers only a rough estimation of the general changes in e-commerce sales levels. The exact causal effect is unknown.
    - Analyses and discussions on the operational driver also do not allow us to disentangle the specific drivers for the decreased logistics capacity.
    - It is not recommended a direct generalization of findings in China to other countries.
    - It would be interesting for future research to examine a more systematic economic impact of COVID-19 at the global level.
- The work presents important evidence on e-commerce’s response and recovery from COVID-19.
    - Future research may look into specific ways for marketplace operations such as matching and pricing [Li et al., 2018; Li and Netessine, 2020] to further help retailers and merchants during COVID-19.
    - A further investigation into the behavior of sellers and buyers on the e-commerce platform, especially the long-term impact of the pandemic on e-commerce participants could be particularly interesting.
    - Further research may also explore how to build a more resilient and sustainable supply chain to tackle a global pandemic [Betcheva et al., 2020; Cohen and Lee, 2020; Dai et al., 2021].
 
# Executive summary of 1. Introduction
- The **COVID-19 pandemic** disrupted the **retail industry**.
- E-commerce offers **lower prices**, **more product variety**, and **greater convenience** than offline commerce.
- The study investigates the impact of **COVID-19** on **e-commerce sales** in China across three periods.
- The study seeks to answer what is the impact of **COVID-19** and **containment measures** on **e-commerce sales**, and what is the key **operational driver** underlying the impact.
 
# Executive summary of 2. Literature Review
- The study builds upon existing research on the **economic impact** of **COVID-19**, contrasting it with the literature on **offline consumption**.
- The study provides **evidence of e-commerce resilience**, identifies the impact of **containment policies**, and establishes **logistics capacity** as a key mechanism.
- Research in **operations management** and **economics** has been expanded by this study.
 
# Executive summary of 3. Data
- Data from **Alibaba** and **Cainiao Network** were used, including **e-commerce sales**, **logistics**, and **customer ordering processes**.
- Public data on **COVID-19 cases** and **containment measures** were appended.
- **Sales data** at the **city-day level** spans three years, matched by the **Chinese lunar calendar**.
- Variables included **total orders**, **delivery time**, and **conversion ratios**.
 
# Executive summary of 4. Impact of COVID-19 on E-Commerce Sales: “Digital Resilience”
- Three identification strategies were used to estimate the **overall impact** of **COVID-19**, the impact of **COVID-19 intensity**, and the impact of **containment measures**.
- Results show a **decrease and recovery pattern** in e-commerce sales.
- The study leverages **policy variation** across cities to identify the impact of each government **containment measure** on **e-commerce sales**.
- Analysis indicated the **digital resilience** of **e-commerce**.
 
# Executive summary of 5. Mechanisms Underlying the Impact of COVID-19: “Logistics Capacity as an Operational Driver”
- Explored the role of **inventory** and **logistics**.
- Customer **ordering processes** are examined.
- **Logistics delivery** is determined to be the driving factor.
- Data analysis shows that **average package delivery time** significantly affected **e-commerce sales**.
- Study provides strong evidence that **logistics capacity** is a key operational driver.
 
# Executive summary of 6. Conclusion and Implications
- The study provides **empirical evidence** on the impact of **COVID-19** and **containment policies** on **e-commerce sales**.
- Key findings highlight **digital resilience** and the importance of **logistics capacity**.
- Policymakers can use findings to balance **public health** and **economic considerations**.
- Retailers should **invest in logistics** and **improve resilience** to handle future disruptions.
- This work motivates more emphasis on **logistics capacity** during a pandemic.

</details>


# Anton Ivanov
- Assistant Professor of Business Administration and Deloitte Scholar
### education
- Ph.D., Management Science and Systems, University at Buffalo, 2018
- Candidate of Science, Economics, International Banking Institute, 2011
- Bachelor of Science, Public Relations, State University of Telecommunications, 2005
### teaching
- **Social Media Strategy (BADM 351)** foundational concepts of Social Media and their applications for generating value for their customers, the society, and the business itself. 
- **Business Intelligence (BADM 453)** Provides a conceptual and practical overview of analytical tools, techniques, and practices used to support data-driven decision making in organizations. Topics include data visualization, data mining, machine learning techniques and business intelligence programming languages.

```
title: Informational value of visual nudges during crises: Improving public health outcomes through social media engagement amid COVID-19
authors: Anton Ivanov, Zhasmina Tacheva, Abdullatif Alzaidan, Sebastian Souyris, Albert C. England III
journal: Production and Operations Management
published: 2023
```

# Executive Summary
- This study examines the use of **visual nudges** on social media by large-scale organizations (specifically universities) to promote safe behaviors and improve public health outcomes during the COVID-19 pandemic. **Visual nudges, in this context, refer to images shared by universities displaying mask-wearing, which subtly encourage individuals to adopt this behavior without mandates or significant economic incentives.**
- We find that universities sharing more images with mask-related information on Instagram (**IG**) experience a decrease in COVID-19 positivity rates by up to 25% on average.
- We provide suggestive evidence of the "boundary condition" of the visual nudge effect, given the fragmentary evidence behind Facebook (**FB**) and Twitter (**TW**) effects.
- We reveal the dynamic and curvilinear effect of visual nudges on positivity over time, with the most significant impact observed when communicated 3 to 5 weeks in advance.

- Key **theoretical** / **conceptual framework** discussions:
    - **Nudge Theory**: Positive reinforcement and indirect suggestions are used to influence behavior. Visual images displaying mask-wearing on social media act as nudges, promoting adherence to safety measures. **These nudges work by making the desired behavior (mask-wearing) more salient and socially acceptable within the university community.**
    - **Dual Process Theory**: Explains how individuals make decisions using two cognitive systems: System 1 (automatic, intuitive) and System 2 (reflective, deliberate). Nudges aim to influence System 1 to promote behaviors aligned with long-term goals. **Visual nudges are designed to tap into System 1, prompting quick, intuitive decisions in favor of mask-wearing.**

- Key **findings** / arguments including :
    - **IG Mask Effect**: A higher proportion of mask-wearing images shared by an institution on IG is associated with lower COVID-19 positivity rates (significant decrease of 13%-25%). **This highlights the power of visual cues on social media to shape behavior and improve public health outcomes.**
    - **Temporal Dynamics**: The informational value of visual nudging is most prominent when communicated 3 to 5 weeks ahead of time, indicating a delayed effect between exposure to images and outcomes. **This suggests that it takes time for individuals to process and internalize the message conveyed by visual nudges.**
    - **Boundary Condition**: Fragmentary evidence of **FB** and **TW** effects suggests that the effectiveness of visual nudges depends on the platform characteristics and stakeholder audience. **This implies that the choice of social media platform is crucial for maximizing the impact of visual nudges.**
    - **Counterfactual Analysis**: Increasing the intensity of mask-related images shared by universities leads to a predicted decrease in new COVID-19 cases.

#nudgetheory #visualnudges #covid19 #socialmediaengagement #publichealth #epidemiccontrol #healthcommunication #informationalvalue #dualprocesstheory #instagram #facebook #twitter #paneldata #econometricmodeling #epidemiologicalmodeling

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The COVID-19 pandemic caused widespread closures and economic consequences [Mukherjee et al., 2021].
- Reopening educational institutions safely requires measures like testing, mask-wearing, and social distancing [Abaluck et al., 2022; Dayaratna et al., 2020; Mukherjee et al., 2021].
- Engaging the public to adopt behaviors that curtail the virus's spread is vital due to the uncertainty surrounding COVID-19 [Camargo & Grant, 2015].
- Social media is essential for public communication and crisis management [Cui et al., 2018; Gour et al., 2022; Yan & Pedraza-Martinez, 2019; McKee, 2013].
- This study examines the relationship between visual nudges (mask-related information) and COVID-19 positivity rates.
- Building on nudge theory [Thaler & Sunstein, 2009], social media posts can act as nudges that influence behavior positively.
- While the role of nudges has been studied in digital behavior [Charry & Tessitore, 2021; Purohit & Holzer, 2021], privacy and credibility of information [Bhuiyan et al., 2021; Nekmat, 2020; Wang et al., 2013], and disaster communication [Mirbabaie et al. 2021], the use of *visual nudges* has not been evaluated in the context of social media.
- This study assesses whether sharing images displaying mask-wearing on social media is related to reductions in COVID-19 positivity rates.
- The contribution of our study is threefold: clarifying the relationship between institutional actors’ engagement on social media and health outcomes; elucidating the mechanism underlying the effect of visual nudges from the temporal perspective; laying out a potential approach for noninvasively and cost-effectively shaping public health attitudes and behavior through real-time social media interventions.

# 2. Background
- **Nudging** involves steering people in a direction that promotes their welfare while preserving freedom of choice [Thaler and Sunstein, 2009].
- A **nudge** alters people's behavior in a predictable way without forbidding options or significantly changing economic incentives [Thaler and Sunstein, 2009].
- Images containing masks on social media are considered nudges that add to the choice architecture of institutional stakeholders.
- Institutions make strategic choices to adjust their content-sharing approach by adding images (intervention) that convey the prominent and essential role of mask-wearing.
- Such nudges can influence behavior by correcting misapprehensions about social norms associated with mask-wearing hesitation.
- University campuses include diverse groups, making their response to the pandemic a representative example of large-scale organizations.
- This study seeks to understand how COVID-19-related visual nudges exemplified by a university’s social media messaging strategy are associated with the university’s COVID-19 positivity rate.
- The effect of visual nudging can have direct implications for managing large-scale public health crises at the organizational level and devising effective social media messaging strategies.
- Social media surveillance offers quicker detection and response compared to traditional methods of epidemiology and surveillance [St Louis & Zorlu, 2012].
- This study focuses on Instagram (**IG**), a leading social networking service focused on visual content, because of its emphasis on visual content.
- IG is the fourth most popular social network with nearly 1.5 billion monthly active users.
- IG is an ideal platform for the study of visual nudging because humans process visual information faster than text [Serences & Yantis, 2006].
- Many academic institutions rely on Facebook (**FB**) and Twitter (**TW**) to share textual posts complemented by photos and videos.
- **FB** is generally used by academic institutions to increase student enrollment, build an online community, and maintain their reputation [Srikoo & Chung, 2021].
- **TW** has been used during and after major disasters due to the speed of information diffusion on the platform [Gong & Lane, 2020].
- The choice of a particular online platform may depend on the characteristics and affordances of the platform, stakeholders, and the university itself [Luttrell et al., 2017].
- Geographic characteristics also appear to influence the choice and level of engagement with online channels [Srikoo & Chung, 2021].
- It is not feasible to definitively say how many stakeholders follow a university’s social media account.
- This study attempts a multifaceted best-practices approach to demonstrate the strong correspondence between universities’ followers and these institutions’ campus stakeholders.

# 3. Literature Review
- Theoretical and empirical investigations of the use of nudges on social media platforms have increased [Lee et al., 2018], particularly since the onset of the COVID-19 pandemic [Roozenbeek et al., 2021].
- This research rests on principles from behavioral psychology, economics, and informatics [Krawiec et al., 2021; Mirbabaie et al., 2021; Charry & Tessitore, 2021].
- There are two streams of underlying research:
    -  Engages issues of information privacy and accuracy.
    - Focuses on modifying offline behaviors using online nudge tools and techniques.
- The first stream relates to fighting misinformation and promoting healthy information ecosystems [Pennycook et al., 2020; Chou et al., 2021; Ng et al., 2023].
- Studies in this stream are centered on understanding the factors that make online users susceptible to misinformation [Nekmat, 2020] and engaging these dimensions to promote greater vigilance and information literacy [Bhuiyan et al., 2021; Wang et al., 2013].
- The second stream focuses on applying online nudges to offline behaviors such as promoting energy-saving behavior [Jung et al., 2021], boosting educational attainment [Tang, 2022], encouraging patient acceptance of AI-augmented healthcare delivery systems [Dai & Tayur, 2022], curbing the spread of COVID-19 [Sasaki et al., 2020], and increasing compliance with public health measures [Moya, 2020].
- Demonstrating a causal mechanism in the latter case is more challenging, which is likely the reason for the primarily correlational nature of the studies.
- Studies across both streams employ textual nudges [Sasaki et al., 2020] or platform affordances [Purohit et al., 2020], but the use of visual nudges has not been evaluated in the context of social media nudges.
- This omission constitutes an important gap in the scientific understanding of digital nudges since the rise of unstructured user-generated content suggests that Internet users are exposed to a significant amount of non-textual information.
- Examining the mechanism of visual nudge processing is even more pertinent due to the psychological impact of information overload, when simple visual cues can be perceived as easier to decode and interpret.

# 4. Theoretical Development
- We examine the theoretical foundation of nudges using a lens of **dual process theory** [Kahneman, 2011; Thaler and Sunstein, 2009].
- The dual-process model distinguishes between two cognitive systems: the automatic (System 1) and the reflective (System 2) [Kahneman, 2011; Van Gestel et al., 2020].
    - System 1: Intuitive, uncontrolled, effortless, fast, associative, and unconscious thinking.
    - System 2: Controlled, effortful, slow, deductive, and self-aware.
- People generally act intuitively by default, as System 1 requires little effort compared to System 2.
- A **default** is an option that is preselected, such that, in the absence of an active decision, the decision-maker will stick with the preselected option [Van Gestel et al., 2020].
- A normative choice may be adopted unconsciously as a social default effect [Huh et al., 2014].
- In the context of mask-related online images shared by institutional actors, a nudge framework is beneficial to analyzing their effects in the context of individual decision-making [Rainford & Tinkler, 2011].

- *P1*: The higher the proportion of images displaying mask-wearing shared by an institution, the lower the COVID-19 positivity rates.
    - *Rationale*: Mask-related images act as visual nudges, influencing individuals' perceptions of social norms and prompting safer behaviors. More exposure to such images reinforces the importance of mask-wearing and increases compliance, leading to lower positivity rates.

# 5. Data and Variables
- We collected a unique panel dataset from multiple public and proprietary sources including COVID-19 testing data, IG, TW, and FB images from official university accounts, university COVID-19 policies, university characteristics, and variety of local area (county) characteristics.
- We collected all data for a sample of N = 117 universities in the United States.
- Our sample of universities is limited to the universities that meet the following selection criteria: (1) conducts proactive asymptomatic and/or surveillance testing for COVID-19, (2) maintains an active COVID-19 tracking dashboard with regular updates (at least once every 14 days), (3) reports total numbers of (a) tests conducted and (b) positive cases since the beginning of the university’s reporting period, and (4) maintains an active public IG or TW account.

## 5.1. Dependent variable
- The focal dependent variable is the **COVID-19 weekly positivity rate**, defined as ( Number of weekly cases
Number of weekly tests ) × 100.
- The co-authors manually collected weekly data across the entire sample.
- The study is based on testing data spanning 22 weeks from September 14, 2020, to February 7, 2021.

## 5.2. Independent variables
- The independent variable represents visual nudges used by institutions to convey safety-related information to stakeholders.
- Such images portray students, faculty, representatives of top management, mascots, or even animals or statues wearing some form of face covering.
-  We collected images posted by the universities via their IG and TW accounts using open-source packages.
- We manually collected a corpus of image-based content posted by the universities on their official FB pages.
- For image classification, we relied on manual classification of all of the collected images.
- A human classifier labeled a given image as 1 (or 0 otherwise) if they could clearly observe an object wearing some form of face covering.
- For empirical analysis, we aggregated processed image data at the university level by week, resulting in a value that represents the proportion of mask images shared by the university.
- We used a set of IG and TW account-level control variables, such as the number of followers, followings, and an indicator if the account was formally verified.

## 5.3. Control variables
- To ensure correct identification of the focal effect and reduce the impact of omitted variable bias, we include an elaborate set of control variables.

### 5.3.1 COVID-19 testing policies data
- We conducted a comprehensive phone survey to obtain the following testing policy characteristics that were implemented by universities in our sample during the study period of 22 weeks: (1) testing of students residing on-campus, (2) testing of students residing off-campus, (3) testing of the entire university population, (4) testing of selected groups of the university population, (5) testing of random participants of the university population regardless of the exhibited symptoms (known as surveillance testing), (6) testing of the university population who exhibit symptoms (known as symptomatic testing), and (7) testing of the university population who do not exhibit symptoms (known as asymptomatic testing).
- We manually collected information associated with universities’ policies toward mask-wearing and social distancing.

### 5.3.2 University characteristics
- We obtained data associated with the size of the university student population and type of ownership from College Scorecard.
- We collected data on the number of social media platforms used by a given university as means of engagement.

### 5.3.3 Local mask-wearing prevalence
- We obtained data posted by The New York Times (NYT) on the estimates of mask usage by county populations in the United States.
- We used the measure of the proportion of NYT survey respondents that replied “always” to the question “How often do you wear a mask in public when you expect to be within six feet of another person?”

### 5.3.4 Local mobility and social distancing
- We used longitudinal proprietary data provided by Cuebiq to capture county-level internal mobility and social distancing.
- Cuebiq’s Mobility Index (CMI) quantifies the average daily distance travelled by devices associated with a given geography.
- Cuebiq’s Contact Index (CCI) provides a measurement of the number of people stopping in the same place at the same time.

### 5.3.5 Local population political affiliation
- We obtained data on the local area’s political affiliation from the 2020 US general election presidential results by county.
- We used the measure of the percentage of votes cast for the Democratic Party to serve as a proxy for the political views dominant in a given area.

### 5.3.6 Local mass media
- We obtained 2020 data associated with the number of active local newspapers.

### 5.3.7 Population characteristics
- We collected corresponding data from County Health Rankings and Roadmaps.

### 5.3.8 Weather
- We collected daily minimum and maximum temperatures using the Meteostat Python library.
- We collected county-level data on the corresponding climate zone where each university is located.

# 6. Visual Nudges and COVID-19 Positivity
- We investigate the relationship between visual nudges and COVID-19 positivity rates.
- Given the potential endogeneity concerns, we rely on the following three-step identification strategy:
    - We build a baseline regression model that accounts for alternative explanations by controlling for an extensive set of observed time-variant and static confounders at different levels.
    - To address self-selection bias originating from universities’ heterogeneous choices behind their idiosyncratic approaches to social media engagement strategies, we use coarsened exact matching (CEM).
    - To address bias originating from omission of unobservable COVID-19 true infectivity, we adapt an epidemiologic susceptible-infectious-removed (SIR) model to estimate it from the data.
- In the baseline specification, we estimate the following regression model:
    - log(yi,t+t′ +1) = a + log (yi,t+t′ −1 + 1) + IG Maski,t−𝜄→t𝛽
+ TW Maski,t−𝜄→t𝜇 + FB Maski,t−𝜄→t𝜌 + Controls j,t−1𝛿 +Controls j,t𝜃 +Controls j𝜂 + Controls i𝜋 + 𝜎s + 𝛾t + 𝜖it
- where log(yi,t+t′ + 1) denotes the future COVID-19 positivity rate, calculated as ( Number of weekly cases
Number of weekly tests ) × 100, for university i, where each combination of t′ = 1, … , 7.
- Maski,t−𝜄→t represents university i′s cumulative proportion of shared mask images (for IG, TW, and FB) where 𝜄 = 1, … , 7.
- The use of the cumulative explanatory variable, along with future values of the outcome, is necessary to comprehensively examine the mechanism of how the nudge effect unfolds over time given the delay between initial exposure to visual nudges and corresponding changes in the future positivity rate.

## 6.1 Main results
-  Signaling mask-related content via IG is significantly associated with reduced COVID-19 positivity rates.
    - That is, universities that share more mask-related visual content through IG observe, on average, a significant decrease in their positivity rates of 13%–25%.
    - Correspondingly, on average, for a 10% increase in the proportion of shared IG-masked images, there is an associated 1.3%–2.5% decrease in positivity.
- There is fragmentary evidence that universities that share more mask-related visual content through FB also observe a significant decrease in their positivity rates ranging from 14% to 23%, on average.
- The observed effects are not consistent across all of the models tested and are marginally significant.
- In case of the TW effect, the impact is not statistically significant (p > 0.1). That is, the proportion of masked images shared by academic institutions via TW is not associated with COVID-19 positivity.
- The obscured finding associated with FB and TW effect is rooted in the (i) irregularity of posting visual content including masks, (ii) platform differences in terms of the modality of communication, and (iii) compositional differences of the stakeholder audiences that follow the corresponding universities’ social media accounts.
- In the case of the statistically significant IG impact, the results of our lagged models indicate the temporal dynamics of the nudging effect.
- The strength of the association as well as the magnitude of its effect increase with longer time periods between initial exposure to visual nudges and future positivity change.
- This effect is most notably prominent between 3 and 5 weeks of exposure.
- The magnitude of association between the cumulative mask effect and positivity rate starts to dampen after Week 5 and becomes insignificant in about 6 to 7 weeks, on average.
- A potential reason for this diminished marginal gain could stem from the information saturation, a condition when the informational value of the visual nudge peaks and starts dampening as the visual’s impact on cognitive perception begins to wear off.
- Additionally, we can see a correlation between the accumulation of the mask effect and reduced positivity.
- This observation suggests that stakeholders’ attention to the topic could pass from sight after some time.
- We use CEM (Iacus et al., 2012) to account for potential bias associated with any differences in universities’ individual approaches to social media engagement.
- Estimation results provide suggestive evidence of the generally negative effect of the proportion of shared mask images, SATTIG = −0.134, p < 0.05.
- Overall, this result is qualitatively similar to our main results presented in Table 2.

## 6.2 Epidemiological model
- A critical factor that remains unobserved in our specification is the true COVID-19 infectivity on the university campuses.
- Omission of this latent variable in our modeling approach can lead to biased estimates and misleading inferences.
- To ensure correct identification of the previously identified statistically significant IG effect, we need a model specification that considers the disease dynamics originating at different levels of the population and allows estimation of true infectivity from the data.
- To this end, we adapt the widely accepted compartmental epidemiological SIR model [Gnanvi et al., 2021; Kermack & McKendrick, 1927].
- The set of equations that represent the infection dynamics is stated in System (2):
    - Sit+1 = Sit − Δit
I it+1 = I it + Δit − 𝛾I it
Rit+1 = Rit + 𝛾I it
- Using the data available for each university, we benchmark four possible models to test Proposition 1.

## 6.2.1 Temporal analysis of estimated coefficients
- In our models that account for the mask effect (models M2 and M4), we use an additive approach to estimate the base infectivity.
- When IG_Mask it approaches 1, the base infectivity increases, while when IG_Mask it approaches 0, the base infectivity decreases.
- Test results reject the null hypothesis of no breaks sup W𝜏 = 82.45, p < 0.001 and suggest an estimated break on November 29, 2020, a week corresponding to the Thanksgiving holidays.
- This result confirms our expectations about the potential reason behind the reduced effect of mask images driven by the fact that at least a portion of the university population left campus and the remaining part of the semester was held in on-line mode only.
- To conclude, using the SIR model, we were able to alleviate the omitted variable bias by taking into account the COVID-19 infectivity originating at different levels of the population.
- Our comprehensive results provide evidence of the relative consistency in the magnitude, direction, and temporal dynamics of the IG mask effect between epidemiologic and econometric models.

# 7. Discussion
- This work shows how innovative data analytics research can help promote public health.
- We present a novel mechanism for applying nudge theory in decentralized, informal communication channels directly by community leaders such as individual organizations, instead of the top-down approach practiced by government entities.
- Our findings contribute to prior literature by elucidating the informational value of visual nudges to promote safe behaviors in the context of mass health crises such as the global COVID-19 pandemic.
- Our study goes a step further by demonstrating the added value of using nudge techniques not just at the official government level but also in a diffused fashion by decentralized economic and social actors such as individual organizations.
- The power distance between the public and central agencies may be too great to sustain consistent compliance, but when these vital messages are disseminated through more proximal channels such as one’s university, and in a more evocative way, through visual nudges, there is substantial informational value as evidenced by the significance of our results.
- We contribute to the nudge theory by introducing the concept of temporal dynamics in the application of visual nudges.
- Signals received ahead of time, from 2 to 5 weeks as our results suggest, have a considerable effect on behavioral outcomes at time t.
- However, the effect starts to dampen considerably if signaled more than 6 weeks ahead of time.
- This finding highlights the challenge of “timeliness of information spread during a disease outbreak” to coordinate a speedy intervention [Gour et al., 2022] and further demonstrates the value of combing standard epidemiological models with heterogenous data-driven modeling approaches [Evgeniou et al., 2023].
- Our findings have significant implications for practitioners.
-  We conduct a counterfactual analysis using an epidemiological model (M4) to forecast the number of new cases per 1000 university population under three counterfactuals of mask images.
- We can see that for both scenarios, the predicted number of new cases is lower when universities use a higher intensity of signaling images containing masks.
- The direction of the effect and its magnitude are generally consistent with those observed in the econometric and epidemiologic models.
- Furthermore, we find evidence of the temporal heterogeneity; that is, the mask effect is considerable during the active phase of the semester but declines during holidays and breaks.
- These findings can be of interest to policymakers and public health officials already using forms of visual nudges in their communication strategies by presenting a viable approach for maximizing the effect of their current and future campaigns.
- We also find that the effectiveness of visual nudges unfolds in a cumulative fashion, so practitioners may benefit from the intense sharing of content associated with mask-wearing over a prolonged period of 3 to 5 weeks.
- Additionally, our findings provide insights for those actors in the information environment that have not yet adapted social media engagement in their crisis management practices.
- The results of our analysis revealed that the effect of visual nudges is not uniform across different social media platforms.
- The “boundary condition” of the visual nudge effect could arise from the following conditions:
    - There are considerable irregularity in posting visual content including that containing masks on different platforms.
    - When examining the visual nudge effect, one should account for the platform differences in terms of the modality of communication.
    - It is crucial to consider the compositional differences of the stakeholder audiences that follow the corresponding universities’ social media accounts.
- There are other contexts at the macro- and micro-levels where visual nudges can be used for public health applications (e.g., public COVID-19 vaccination attitudes).
- Unlike high-profile social influence campaigns public figures may engage in, the type of visual nudge explored in this paper represents a particular member of the family of classic nudges known as a soft nudge [Thaler & Sunstein, 2009].
-  Soft visual nudges seek to alter choices without constraining the decision-maker’s opportunity sets [Ehrig et al., 2015].
- These characteristics make soft visual nudges effective without posing an ethical threat to the communities and individuals exposed to them and are therefore safe and noninvasive.

# 8. Conclusion
- Using a theoretically informed framework rooted in nudge theory in combination with novel deep learning techniques, we developed a robust econometric model seeking to illuminate the role of organizational social media strategy in mitigating the spread of COVID-19.
- We found that using social media posts as a form of visual nudging to promote safe behaviors has a marked effect on COVID-19 positivity rates, even after controlling for a variety of organizational characteristics and disease dynamics at different levels.
-  We further show that the effect of nudges may not be immediately detectable, as it may take time to be processed and internalized.
- Specifically, our results suggest that the impact of visual nudges peaks between 4 to 6 weeks after initial exposure.
- Additionally, we demonstrated the marginal utility of nudging strategies depending on the level of virus spread across organizations, with the most heavily impacted organizations benefiting the most from the use of visual nudges.
- Our model can be fruitfully adopted not just by medical practitioners but also by large-scale organizations more broadly in ways that creatively use a channel of communication already in place to overcome the challenges posed by COVID-19.

---

# Executive summary of 1. Introduction
- The introduction highlights the problem of safely reopening educational institutions during the COVID-19 pandemic and the importance of engaging the public to adopt safe behaviors.
- It introduces the concept of visual nudges on social media as a tool to influence behavior and improve public health outcomes.
- The introduction emphasizes the gap in the literature regarding the use of visual nudges in the context of social media and outlines the study's objectives and contributions.

# Executive summary of 2. Background
- The background section defines nudging based on Thaler and Sunstein's work, emphasizing the preservation of freedom of choice while steering people towards beneficial behaviors.
- It frames images containing masks on social media as visual nudges and discusses the potential of such nudges to influence behavior by correcting misapprehensions about social norms.
- The section highlights the relevance of university campuses as a representative example of large-scale organizations and underscores the importance of understanding the relationship between visual nudges and COVID-19 positivity rates.

# Executive summary of 3. Literature Review
- The literature review discusses the increasing interest in the use of nudges on social media platforms, particularly in the context of the COVID-19 pandemic.
- It delineates two streams of underlying research: one focused on information privacy and accuracy and another focused on modifying offline behaviors using online nudge tools.
- The review highlights the gap in the literature regarding the use of visual nudges in social media and emphasizes the importance of studying their effects due to the rise of unstructured user-generated content.

# Executive summary of 4. Theoretical Development
- This section grounds the study in dual process theory, distinguishing between System 1 (automatic) and System 2 (reflective) cognitive processes.
- It explains how nudges aim to influence System 1 to promote behaviors aligned with long-term goals.
- It introduces Proposition 1, stating that a higher proportion of mask-wearing images shared by an institution will lead to lower COVID-19 positivity rates.

# Executive summary of 5. Data and Variables
- The data and variables section describes the unique panel dataset collected from multiple public and proprietary sources, including COVID-19 testing data, social media images, university policies, and local area characteristics.
- It outlines the criteria for selecting the sample of 117 universities and defines the dependent and independent variables.
- The section details the process of manual image classification to identify mask-related images and explains the control variables used to address potential confounding effects.

# Executive summary of 6. Visual Nudges and COVID-19 Positivity
- This section presents the empirical approach used to investigate the relationship between visual nudges and COVID-19 positivity rates, including the baseline regression model, coarsened exact matching (CEM), and the susceptible-infectious-removed (SIR) model.
- The main findings indicate that universities sharing more mask-related visual content on Instagram (IG) experience a significant decrease in COVID-19 positivity rates.
- The results also reveal the temporal dynamics of the nudging effect, with the most prominent impact observed when communicated 3 to 5 weeks in advance.

# Executive summary of 7. Discussion
- The discussion highlights the theoretical and practical contributions of the study, emphasizing the informational value of visual nudges in promoting safe behaviors and the concept of temporal dynamics in the application of visual nudges.
- It discusses the implications of the findings for policymakers and public health officials, providing insights on maximizing the effect of communication campaigns and initiating social media engagement in crisis management practices.
- The section also addresses the "boundary condition" of the visual nudge effect, explaining why the effect is not uniform across different social media platforms.

# Executive summary of 8. Conclusion
- The conclusion summarizes the study's findings, highlighting the marked effect of visual nudging on COVID-19 positivity rates, even after controlling for various organizational characteristics and disease dynamics.
- It emphasizes the delayed impact of nudges and the potential for large-scale organizations to adopt the model to overcome challenges posed by COVID-19.
- The conclusion acknowledges the limitations of the study and suggests areas for future research.

</details>

```
title: Exploring the Association Between the “Big Five” Personality Traits and Fatal Opioid Overdose: County-Level Empirical Analysis
authors: Zhasmina Tacheva, Anton Ivanov
journal: JMIR Ment Health
published: 2021
```

# Executive Summary

- This study examines the connection between community-level psychological traits, specifically the **Big Five personality traits** (**openness**, **conscientiousness**, **extraversion**, **agreeableness**, and **neuroticism**), and **fatal opioid overdose** rates at the county level across the United States.
- Addressing the limitations of traditional reactive approaches, we propose a proactive, preventive strategy that considers underlying psychological factors.
- **Novel Method**: We employ a unique approach leveraging **social media data** from **Twitter** for 2891 counties between 2014-2016.  This involves a novel **data mining technique** to derive average county-level Big Five personality scores from aggregated tweets. The core of this technique relies on a **lexicon-based approach** using the **Linguistic Inquiry and Word Count (LIWC) psycholinguistic dictionary** to analyze the text. As a robustness check, we also utilize **IBM Watson Personality Insights, which employs a global vectors approach for word representation (GloVe word embedding technique).** We then perform interval regression, using a control function to address **omitted variable bias** and account for potential endogeneity.
- Key **theoretical** / **conceptual framework** discussions
    - **Five-Factor Model (FFM)** of personality: This model posits that personality can be described by five broad dimensions:
        - **Openness**: Reflects intellectual curiosity, imagination, and a willingness to try new things.
        - **Conscientiousness**: Indicates diligence, organization, and a strong sense of responsibility.
        - **Extraversion**: Characterized by sociability, assertiveness, and positive emotionality.
        - **Agreeableness**: Encompasses traits such as trust, empathy, and a tendency to be cooperative.
        - **Neuroticism**: Reflects a tendency to experience negative emotions such as anxiety, sadness, and irritability.
        - The OCEAN traits and their relationship to substance use have been studied extensively in both the medical and psychological literature [4].
- Key **findings** / arguments including
    - **Extraversion** and **neuroticism** are significantly and positively associated with fatal opioid overdoses. (higher rates of each are associated with high overdose fatality rates)
    - A surprising finding shows that **conscientiousness** is also positively associated with fatal opioid overdoses (higher rates of conscientiousness is associated with high overdose fatality rates), which contradicts previous research.
    - **Openness** and **agreeableness** did not show significant associations with fatal opioid overdoses.
- **Practical Implications**: These findings can inform the allocation of resources like overdose-reversal medication and targeted intervention strategies based on the psychological profiles of communities.
 
#opioid_addiction #personality_traits #community_health #text_mining #opioid #addiction #psychological #five_factor_model #big_five #twitter #data_mining #econometrics #geo_personality #lexicon_based_approach #LIWC #GloVe_embedding


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction

- Opioid abuse and overdose deaths are at epidemic levels in the United States [1], outpacing car accident fatalities [2].
- Existing opioid overdose programs such as those providing Naloxone address the problem in a reactive fashion, this study proposes an approach that could allow health care providers and officials to act proactively in a preventive manner.
- We model fatal overdose by considering psychological and behavioral traits, using the **Five-Factor Model (FFM)** of the **Big Five personality traits** (OCEAN: openness, conscientiousness, extraversion, agreeableness, and neuroticism) as our theoretical foundation.
- The effect of the FFM dimensions on substance use has been demonstrated across different contexts, including but not limited to age and gender groups [5,6], nationalities [7,8], length and intensity of use [9,10], and types of substance [11,12].
- We focus on inferring the Big Five personality traits from self-expressive written artifacts such as social media posts [14,15] due to the moderate to high correlations between the linguistic features of such social media messaging and personality trait measurements established through conventional psychological test surveys [16].
- We use extensive unstructured data available from Twitter (analyzing nearly 19 million geo-tagged tweets) in combination with a literature-driven linguistic analysis approach [17] to derive unique personality profiles on US counties (known as geo-personality).
- Social media content has been shown to be useful for epidemiological surveillance for influenza [18] and HIV [19], as well as in the context of adverse behaviors such as suicide [20] and drug abuse [21].
- Unlike traditional methods of epidemiology and surveillance (which require significant time and resources to collect and analyze medical diagnostic information, thereby increasing the gap between emergency and response), social media surveillance offers quicker detection and response [22].
- We used an extensive set of control variables identified by prior literature and relied on a rigorous econometric specification. To alleviate the endogeneity concerns caused by the omitted variable bias, we used a control function approach.
- This study demonstrates the feasibility of assessing the Big Five personality traits from user-generated online content at scale in real time and extends the health informatics literature on the association between personality and opioid fatality, which has thus far only explored this relationship at the state level [24], to a more granular, county-level context.
- This study demonstrates the feasibility of assessing the Big Five personality traits from user-generated online content at scale in real time and extends the health informatics literature on the association between personality and opioid fatality, which has thus far only explored this relationship at the state level [24], to a more granular, county-level context.
- **Research Question**: "How can we use the Big Five personality traits in mitigating the opioid overdose crisis?"
    - This question is relevant because personality's role in opioid fatalities, especially at the community level, has been underexplored. Answering it can provide new strategies for prevention and resource allocation.
- We make a theoretical contribution by elucidating the relationship between personality and health-related outcomes and also has several practical implications for health care providers and administrators, as its findings can be applied in opioid overdose prevention and surveillance based on the local counties’ prevalent personality traits.

# 2. Literature Review

- Opioids interact with opioid receptors to produce pleasurable effects and relieve pain [25]. However, these beneficial effects are often outweighed by the risk of opioid drug dependency—a treacherous path toward addiction and possibly death.
- Factors contributing to fatal and nonfatal opioid overdose can be broadly categorized as:
    - Biological (age, gender, comorbidity [27]; history of substance use disorders [28]; or medication intake [29]).
    - Psychological (sexual identity [30], sexual behavior [31], and history of psychiatric problems [32]).
    - Socioeconomic (socioeconomic status [33], educational attainment [28], and history of criminal charges and detention [34]).
- Research into personality’s influence on fatal drug overdose is lacking.
- Psychology has assembled a compelling body of evidence in support of the link between personality and health behaviors and outcomes [35].
- In longitudinal studies, the best-known taxonomy of personality, the “Big Five” Factor model, has been found to be predictive of health care decision-making styles [36], physician visits and hospitalization probability [37], longevity [38], and obesity [39], among other things.
- A rich body of literature in both the medical and psychological domains has amassed ample evidence of the relationship between the five FFM traits and various aspects of substance use and dependence. (Table 1 is presented)
- Despite these compelling findings, there is, to the best of our knowledge, a dearth of research exploring personality’s role in fatal opioid overdose behavior specifically.
- Prior research has mainly used experimental or quasi-experimental cross-sectional or longitudinal cohort studies, this study uses an analytical approach that is justified due to the well-documented intrapersonal stability of Big Five traits [51] and the established feasibility of capturing population psychological characteristics through social media [52].

# 3. Hypotheses Development

- **Personality traits** are enduring styles of thinking, feeling, and acting that characterize an individual [54].
- The relative stability of these traits points to consistent and recurrent patterns of acting and reacting that both characterize individuals and differentiate them from others.
- We used the “Big Five” FFM of personality, considered the most robust categorization of personality traits to date [61]. Notably, the Big Five have demonstrated to be universally representative and to exhibit the same structure across different regions and cultures [62].
- The use of the FFM in the study of opioid overdose is particularly salient because of the long-standing stream of studies exploring its relationship with various substance use behaviors.
- Different personality traits in the FFM framework play a different role in substance-related behaviors. We further formulate a set of testable hypotheses informed by the extant literature.
- ***H1***: Openness will have a positive impact on fatal opioid overdose.
    - Openness is characterized by intellectual capacity, wide interests, and unconventional thought [63].
    - Meta-analyses have largely failed to find a significant impact of openness on substance abuse [4] and mental illness [64].
    - Since the overwhelming majority of FFM studies on substance use point to a positive role for openness, we hypothesize the above.
- ***H2***: Conscientiousness will have a negative impact on fatal opioid overdose.
    - Conscientiousness combines the traits of being diligent, thorough, and being governed by one’s conscience [65].
    - High scorers on this dimension are expected to shun intentional overdose due to imminent feelings of guilt and this trait’s strong underlying facets of responsibility, traditionalism, and self-control [67].
- ***H3a***: Extraversion will have a positive impact on fatal opioid overdose.
- ***H3b***: Extraversion will have a negative impact on fatal opioid overdose.
    - Extraversion is characterized by positive affectivity, adventurousness, energy, warmth, and gregariousness [65].
    - Its role in substance use behaviors remains unclear in the literature [9]. This lack of clarity is evident from the inconsistent empirical findings for this personality indicator—a phenomenon not observed for the other four traits.
    - In light of these conflicting findings and the well-documented lack of consistency in this indicator’s effect on substance use, we contend that when it comes to fatal intake of opioids, the role of extraversion is best captured in a set of competing hypotheses.
- ***H4***: Agreeableness will have a negative impact on fatal opioid overdose.
    - Agreeableness comprises traits such as trust, modesty, compliance, caring, and emotional support [65].
    - It is negatively associated with substance use [70], substance dependence severity and polydrug use [41], lifetime substance abuse or dependence [40], alcohol and drug dependence [44], marijuana use [11], cocaine and heroin use [43], first-time and subsequent illicit drug use [48], and substance use and addictive disorders [6,71].
- ***H5***: Neuroticism will have a positive impact on fatal opioid overdose.
    - Neuroticism (also referred to as emotional range) is reflected both in a person’s tendency to experience distress and in the cognitive and behavioral styles that stem from it. Individuals scoring high on this dimension tend to experience chronic negative effects and are prone to various psychiatric disorders [65].
    - Perhaps most telling of this trait’s potential role in opioid overdose is its documented positive effect on longitudinal pain and prescription opioid medication use [10].

# 4. Methods

## 4.1. Data Collection

- The first step in our data collection is related to the dependent variable: opioid overdose deaths.
    - Yearly (2014-2016) panel data were obtained through the WONDER (Wide-Ranging Online Data for Epidemiologic Research) online database [75] from the CDC.
- In the second step, we obtained unstructured text data for language analysis from Twitter and integrated it with the mortality data.
    - We used the publicly available snapshots of Twitter traffic known as “spritzer.”
    - We extracted only those tweets that were in English and included a geo-tag.
    - Having preprocessed 36 months of data, we were able to extract nearly 19 million tweets satisfying the aforementioned requirements.
    - Next, we excluded duplicate tweets that were posted by the same author and those that contained less than three words.
    - Further, for the purpose of our county-level analysis, we linked tweets to their origins in the respective counties in the United States.
    - Finally, to increase the validity and reliability of our personality mining approach, we created personality profiles of the individual counties (vs individual tweets at the user level) by aggregating (ie, concatenating) the resulting text data extracted from tweets at the FIPS code level by year.
- In the final step of data collection, we merged the opioid-related mortality data and Twitter data with an extensive set of county-level population characteristics provided by County Health Rankings and Roadmaps (CHRR).

## 4.2. Variable Operationalization

- **Dependent Variable**: The number of deaths associated with opioid drug overdose.
    - Data for counties with fewer than 10 deaths were suppressed; that is, the data were not available to the public under any circumstances due to the CDC’s privacy policy.
    - To model our dependent variable, we used an interval regression approach.
- **Independent Variables: Personality Traits Mining**
    - We operationalized our main predictor variables—OCEAN—by analyzing a vast unstructured body of tweets obtained from Twitter.
    - To extract information on the Big Five personality traits associated with individual US counties, we used tweets aggregated (concatenated) at the county level and merged into a single vector per county to infer the latent personality traits by means of linguistic analysis [24].
    - Specifically, after a preprocessing step including stop word removal, stemming, and lemmatization, the content of each vector was matched with the Linguistic Inquiry and Word Count (LIWC) psycholinguistic dictionary [85].
    - Once the LIWC linguistic dimensions for each vector were available, we followed the procedure in Adamopoulos et al [17] and matched them with their corresponding weighted coefficients developed by Yarkoni [14] by estimating the relationships between OCEAN dimensions obtained from traditional psychological test assessments and LIWC items from user-generated content by the same individuals.
    - The product of each LIWC item score and its corresponding weighted coefficient was used to calculate the dot product for each OCEAN trait for each county, which was then rendered as a percentile score to obtain a comparable indicator for each psychological trait across counties [17,86].
- **Control Variables**
    - To ensure correct identification of the focal effects, we included an extensive set of county-specific control variables associated with the health and well-being of the counties’ populations (Table 2).
    - Furthermore, based on the prior literature, we considered two essential correlates of the Big Five: alpha and beta “superordinate” (high-order) factors [61,87].
    - To extract data on a plethora of personality facets underlying the corresponding alpha and beta dimensions, we used the IBM Watson Personality Insights service, a tool used in prior studies [90].
    - To account for the reflective nature [89] of alpha and beta, we employed principal component analysis to reduce the dimensions of the discovered facets.

## 4.3. Econometric Model Specification

- Our estimation method is based on the nature of our dependent variable. Given the privacy constraints resulting in data suppression when the number of reported deaths is less than 10, we decided to treat the missing observations in the outcome as an interval censored between zero and nine. Therefore, for an interval type of outcome, we chose a linear regression model with panel-level random effects to test our hypotheses.
    - yit = Xitβ + Citβ + υi + εit     (1)
- To alleviate omitted variable bias concerns, given the nonlinear outcome distribution and the continuous nature of the endogenous Big Five, we used a control function method [93].
- We identified multiple language-related characteristics as candidates for instrumental variables.
- To correct for omitted variable bias in equation 1, we included the first-stage residuals (denoted Rit) obtained from equation 2.

# 5. Results

- The results of our analysis reveal several insights, including a counterintuitive finding.
- In Table 5, we present our estimates obtained across several models.
- Hypothesis 1 predicts a significantly positive impact of openness on fatal opioid overdose. However, the results of our main (conservative) analysis reveal an insignificant relationship (P=.32).
- In hypothesis 2, we hypothesized a negative effect of conscientiousness on the outcome. Surprisingly, however, the coefficient is significantly (and consistently) positive (βConscientiousness=.229, P<.001), contrary to both hypothesis 2 and prior literature demonstrating the negative effect of conscientiousness on multiple types of substance use disorders such as alcohol abuse and dependence [12,44,46], longitudinal substance use [9,40], and drug use [11,48]. Hypothesis 2 is therefore not supported.
- Hypotheses 3a and 3b are a set of competing (positive and negative) hypotheses, which account for the conflicting empirical evidence provided in the existing literature about the relationship between extraversion and substance use. Our model shows a positive statistically significant coefficient for extraversion (βExtraversion=.308, P<.001), thus supporting hypothesis 3a.
- Based on hypothesis 4, agreeableness will have a significantly negative effect on the outcome. However, our main (conservative) results previously presented demonstrate insignificant impact (P=.42).
- Finally, we observed a significantly positive impact of neuroticism on fatal opioid overdose (βNeuroticism=.248, P<.001).

# 6. Discussion

## 6.1. Theoretical and Practical Implications

- Our study is part of a nascent stream of research in health informatics that combines geospatial information, medical data, and unstructured user-generated content used to infer community characteristics.
- In the context of the relationship between personality and opioid mortality specifically, our study demonstrates the relevance and usefulness of examining personality at the community level, also referred to as geo-personality [24].
- Our model and findings address a gap in the literature, which has hitherto not considered the explanatory power of personality in opioid-related outcomes. Specifically, building on the theoretical foundations of the FFM, we provide a more nuanced understanding of how and to what extent openness, conscientiousness, extraversion, agreeableness, and neuroticism contribute to fatal overdose.
- In light of the record-high budget allocations for opioid addiction countermeasures, our findings also contribute to practice and can be used for the purpose of developing actionable intervention plans on the part of local municipalities and health providers alike to prompt assessments of at-risk individuals in real time (and prior to prescription) as opposed to implementing impersonal en masse Naloxone programs.
- This suggests the use of our Big Five population assessment mechanism to potentially predict rates of neonatal abstinence (withdrawal) and to plan for county resources for rehabilitation of individuals with opioid use disorder.

## 6.2. Implications for Health Informatics

- From a health informatics perspective, our research represents a novel approach in three ways.
    - First, we demonstrate the feasibility of intelligently mining unstructured (Twitter) data for epidemiologic discoveries, eliminating the potential ethical dangers of privacy and confidentiality breaches by aggregating personality scores at the communal (county) level—a research technique with proven value in the epidemiology literature [92].
    - Second, we show that language used by Twitter users can provide cues associated with the Big Five personality traits at the county level.
    - Finally, given the fact that major opioid-related statistics are reported by counties and states with a time lag, analysis of readily available Twitter data allows us to overcome this limitation and provide up-to-date estimates of opioid-related outcomes.

## 6.3. Conclusion and Limitations

- We studied the impact of personality traits at the county level on fatal opioid overdose, a nationwide crisis.
- We used publicly available multisource data and operationalized our focal predictors using a robust lexicon-based implementation well established in the information systems literature.
- Overall, our results obtained by means of Twitter mining are consistent with the prior literature, yet they suggest several surprising insights.
- The study is not without limitations including: missing values, data suppression on outcomes, the challenges of handling interval-censored count outcomes, the small fraction of Twitter users reporting locations, the variation in county characteristics, and the character limits of individual tweets.

---

# Executive summary of 1. Introduction

- Opioid abuse and overdose deaths are a major crisis, requiring proactive solutions.
- The study uses the Five-Factor Model (FFM) of personality to model fatal overdose by considering psychological and behavioral traits, drawing on data from Twitter to derive unique personality profiles on US counties (geo-personality).
- The study aims to demonstrate the feasibility of assessing the Big Five personality traits from user-generated online content at scale in real time.
- **Research Question**: "How can we use the Big Five personality traits in mitigating the opioid overdose crisis?"

# Executive summary of 2. Literature Review

- The literature review examines factors contributing to opioid overdose and the role of personality in health behaviors.
- Despite evidence linking personality to substance use, research exploring personality’s role in fatal opioid overdose is lacking.
- Existing research designs, while rigorous, often lack generalizability.
- This study addresses these gaps by analyzing unstructured social data at the population level.

# Executive summary of 3. Hypotheses Development

- The study develops hypotheses based on the Five-Factor Model (FFM) of personality, examining how each trait (openness, conscientiousness, extraversion, agreeableness, and neuroticism) relates to fatal opioid overdose.
- *H1*: Openness will have a positive impact on fatal opioid overdose.
- *H2*: Conscientiousness will have a negative impact on fatal opioid overdose.
- *H3a*: Extraversion will have a positive impact on fatal opioid overdose.
- *H3b*: Extraversion will have a negative impact on fatal opioid overdose.
- *H4*: Agreeableness will have a negative impact on fatal opioid overdose.
- *H5*: Neuroticism will have a positive impact on fatal opioid overdose.

# Executive summary of 4. Methods

- The study collects and merges data from multiple sources: opioid overdose deaths from the CDC, Twitter data for language analysis, and county-level population characteristics from CHRR.
- The dependent variable is the number of deaths associated with opioid drug overdose.
- Independent variables (OCEAN) are derived from Twitter data using linguistic analysis.
- Control variables include county-specific factors associated with health and well-being.
- An interval regression model with panel-level random effects is used, with a control function to address omitted variable bias.

# Executive summary of 5. Results

- The analysis reveals that extraversion and neuroticism are positively associated with fatal opioid overdose.
- Surprisingly, conscientiousness is also positively associated with fatal opioid overdose, contrary to expectations.
- Openness and agreeableness did not show significant associations with fatal opioid overdose.

# Executive summary of 6. Discussion

- The study contributes to health informatics by combining geospatial information, medical data, and user-generated content to infer community characteristics.
- Findings can be used to develop actionable intervention plans and prompt assessments of at-risk individuals in real time.
- The research demonstrates the feasibility of mining unstructured Twitter data for epidemiologic discoveries and providing up-to-date estimates of opioid-related outcomes.
- Limitations include missing data, data suppression, and the representativeness of Twitter users.

</details>

```
title: Impact of User-Generated Internet Content on Hospital Reputational Dynamics
authors: Anton Ivanov, Raj Sharman
journal: Journal of Management Information Systems
published: 2018
```
 

 # Executive Summary
 - This study investigates the impact of **user-generated content (UGC)** on **hospital reputational dynamics**, specifically focusing on cancer-treatment hospitals.
 - It adapts Rindova et al.’s [49] **two-dimensional model of reputation** (*perceived quality* and *prominence*) to the context of UGC and examines the impact of UGC antecedents on financial performance.
 - Data was **manually collected** from publicly available sources like US News & World Report, CMS, Facebook, Google, Twitter, and YouTube. **Persuasive feedback** was measured using **Facebook and Google star ratings**. **Structural Equation Modeling (SEM)** with a lagged design was used to analyze the data.
 - Key **theoretical** / **conceptual framework** discussions
     - Rindova et al.’s [49] **two-dimensional model of reputation**: Reputation is characterized by *perceived quality* (stakeholder evaluation on a specific attribute) and *prominence* (large-scale collective recognition).
     - **Types of signals from UGC**:
         - *Persuasive* (opinions influencing purchase decisions)
         - *Awareness* (actions prompting awareness)
         - *Content variance* (heterogeneity in user opinions)
 - Key **findings** / arguments including
     - *Quality signals* (Facebook and Google star ratings - **persuasive feedback**) significantly impact *patient-perceived quality ratings*.
     - *Awareness signals* (Twitter followers, YouTube views, Alexa ranking) and *content variance* (Facebook likes) significantly impact *prominence rating*.
     - *Perceived quality* significantly impacts *financial performance* (Medicare payments), while *prominence* does not.
     - *Facebook ratings* play a unique role as an antecedent to both *quality* and *prominence* dimensions of reputation.
 - The findings offer managerial implications for hospitals to improve their online presence and reputation through efficient use of UGC.
 

 #hospital_reputation #healthcare #online_prominence #quality_signals #content_variance #user_generated_content #hospital_utilization #financial_performance #online_engagement #two_dimensional_reputation #social_media #reputation_management
 
<details>
    
  <summary>Click to expand sections</summary>


 # 1. Introduction
 - Organizations pursue strong reputations to achieve sustainable competitive advantages and improve performance [49].
 - In healthcare, a strong reputation influences patients' hospital selection [22, 23, 46, 55, 57, 65], and hospitals use online mechanisms to engage stakeholders [51].
 - Social networking sites (e.g., Facebook), media sharing platforms (e.g., YouTube), microblogging sites (e.g., Twitter) are used by hospitals to engage with stakeholders [16].
 - Online engagement reduces information asymmetry and facilitates user-generated content (UGC) which gives cues to online users regarding providers’ quality and performance.
 - While online user engagement is commonplace, its implications in the context of UGC remain largely unaddressed.
 

 # 2. Background
 ## 2.1. Perspectives on Reputation
 - Research on organizational reputation in management, economics, sociology, and marketing defines reputation based on two school of thoughts [62].
 - *Economics perspective*: Scholars define reputation as observers’ expectations of an organizational attribute [6, 35, 40, 49].
 - *Institutional theory perspective*: Scholars define reputation as a global impression or the ways in which a collective perceives a firm, often due to information exchange and social influence [4, 36, 47, 49, 56].
 - This paper is based on the two-dimensional perspective in which reputation is characterized as an amalgamation of quality and prominence dimensions [49].
     - *Perceived quality*: The degree to which stakeholders evaluate an organization positively on a specific attribute [49].
     - *Prominence*: The degree to which an organization receives large-scale collective recognition [49].
 - We adapt this two-dimensional model to investigate the antecedents and consequences of reputational dimensions in the context of UGC.
 

 ## 2.2. User-Generated Antecedents of Reputation
 - The dimensions of quality and prominence are likely to have different antecedents.
     - *Perception of quality* is influenced by signals (information) about organizations’ capabilities to produce quality goods or services [42].
     - *Prominence* is influenced by the choices of stakeholders whose attention to an organization may be seen as a form of endorsement [41].
 - We study reputational dynamics in the context of UGC, focusing on feedback provided by online users in response to online engagement efforts initiated by hospitals.
 - The management information systems literature [10, 15, 32, 39, 58] emphasizes three types of signals from user-generated feedback:
     - *Persuasive*: Opinions that encourage or discourage consumers.
     - *Awareness*: Actions that prompt awareness of a product or service.
     - *Content variance*: Capturing heterogeneity in user opinions.
 

 ### 2.2.1 Persuasive Feedback and Quality Evaluations
 - Persuasive feedback is typically associated with online user ratings and reviews.
 - Prior studies have shown its associations with consumer purchase decisions and firm pricing strategies [33], product returns [52], and analyst expectations [2].
 - In healthcare, signals of quality in website ratings (e.g., Yelp) relate to traditional hospital performance measures [3].
 - Facebook star ratings and reviews are significantly related to readmission rates [14] and patient willingness to recommend a hospital [21].
 - *H1*: The higher the quality evaluations contained in persuasive feedback, the higher the hospital’s perceived quality.
 

 ### 2.2.2 Awareness Feedback and Exposure
 - Awareness feedback is associated with broad exposure and visibility [38].
 - Increased exposure and visibility might lead some organizations to gain disproportionate public attention and support [28].
 - Number of Twitter followers relates to researcher citation count [66] and journal impact factor [24].
 - Healthcare providers use Twitter to enhance visibility and reputation [7], and content contribution through YouTube is driven by the desire for exposure [59].
 - Awareness measures offer "ready-made" metrics of hospitals' standing, reducing stakeholders’ need to evaluate attributes and quality directly [48].
 - Stakeholders can use awareness signals as an overall evaluation of whether an organization is among the leaders in its industry.
 - *H2*: The higher the extent of awareness, the higher the hospital’s prominence.
 

 ### 2.2.3 Content Variance and Evaluative Tone
 - Content variance refers to the contextual polarity or evaluative tone of the content shared by an entity.
 - Quality content is important because content that embodies pleasing character traits attracts more stakeholders [34, 62].
 - The emergence of social media has transformed the conceptualization of content variance.
 - The Facebook “like” button expresses approval of the content and collects opinions of known groups [31], which can be used to infer details [25, 26].
 - In healthcare, likes have shown negative association with 30-day mortality rates and positive association with patient recommendations [60] and predictive value for health outcomes [13].
 - Through attribute priming, the cumulative effect of content variance shapes attitudes and opinions of online users [5].
 - The fact that someone “likes” the shared content creates a contextual effect, providing future viewers with cues about its interpretation [29].
 - *H3*: The higher the variance of the content shared by the hospital, the higher its prominence.
 

 ## 2.3. Consequences of Reputation
 - Quality and prominence both reflect common concerns of consumers by identifying providers of high-quality goods or services [28, 49].
 - Organizations perceived as high quality are more likely to be mentioned, re-patronized, and attract new customers [49].
 - *H4*: The higher the hospital’s perceived quality, the greater its prominence in the professional field.
 - Financial performance has been established as a major outcome of organizational reputation [12, 50].
 - Reputation reduces stakeholder uncertainty, inducing buyers to pay a price premium [49] or protecting firm value [18].
 - Better financial performance can be viewed “either as a return on reputation or as an incentive payment to induce quality maintenance” [54].
 - Stakeholders are likely to favor prominent organizations because prominence reflects the “majority vote” [28, 48].
 - *H5*: The higher the hospital’s perceived quality, the higher the payments associated with its services.
 - *H6*: The greater the hospital’s prominence, the higher the payments associated with its services.
 

 # 3. Method
 ## 3.1. Sample and Data
 - The sample of high-acuity cancer-treatment hospitals was drawn from the US News and World Report (USNWR) Best Hospitals listing [45].
 - Panel data came from multiple publicly available sources: USNWR, Centers for Medicare and Medicaid Services (CMS), Hospital Compare Database, Facebook, Google, Twitter, YouTube, Alexa, Hospital Safety Score, and Castle Connolly’s Top Doctors.
 - **Data was collected manually** in July 2014 and July 2016.
 - All data were merged to create a unique dataset that resulted in N = 495 hospitals.
 

 ## 3.2. Measurement: Outcomes
 - *Prominence*: USNWR Best Hospitals rating score, reflecting hospital performance in structure, process, and outcomes [9, 20].
 - *Perceived Quality*: HCAHPS summary star rating [43, 44].
 - *Financial Performance*: Price- and risk-adjusted data on Medicare payments obtained from the Hospital Compare Database, covering the 2014 to 2017 reporting period [8]. Approximated at the hospital level using aggregated public data from heart attack, heart failure, and pneumonia payments.
 

 ## 3.3. Measurement: Independent Variables
 - *Quality Signals* (**Persuasive Feedback**): Facebook and Google star ratings.
 - *Awareness*: Aggregate of:
     - Number of Twitter followers
     - Number of YouTube views
     - Alexa global rank
 - Principal components analysis (PCA) was used to reduce the dimensions of the awareness measures.
 - *Content Variance*: Aggregated number of Facebook likes for content shared by the hospital.
 

 ## 3.4. Measurement: Control Variables
 - The study controlled for:
     - Affiliation with high-status actors
     - Patient safety
     - Hospital size
     - Extent of hospital and user online engagement
 

 ## 3.5. Statistical Analysis
 - A path-model was estimated using structural equation modeling (SEM) to test the relationships.
 - A lagged effects modeling approach was used to estimate focal effects, with explanatory variables collected at time T–2 (2014) indicative of reputations at time T (2016).
 - Each variable was modeled as a single indicator and presumed to contain no measurement errors.
 - Exogenous variables were allowed to co-vary in the estimation of the model.
 - The Maximum Likelihood (ML) estimator was used as implemented in the Stata MP/15 -SEM- command [1].
 - Multicollinearity was tested using the variance inflation factor (VIF) and the condition number test.
 - Heteroskedasticity was tested using the Breusch-Pagan test, and robust standard errors (SE) were used for the heteroskedastic data.
 - The covariance residual matrix was examined to ensure the appropriateness of the model fit.
 

 # 4. Results
 - Both Google and Facebook star ratings (**persuasive feedback**) were statistically significant predictors of perceived quality (supporting H1).
 - An aggregate of Twitter, YouTube, and Alexa metrics was found to be a statistically significant positive predictor of prominence (supporting H2).
 - The number of Facebook likes significantly predicted prominence (supporting H3).
 - Facebook rating was a statistically significant predictor of prominence.
 - The path from quality to prominence was only marginally significant, partially supporting H4.
 - Perceived quality had a significant positive relationship with Medicare payments (supporting H5).
 - Prominence was not significantly related to Medicare payment (H6 was not supported).
 - *Perceived quality* had the largest significant effect on *payment*.
 - *Facebook rating* had the largest statistically significant effect on *quality* and on *prominence*.
 - *Google rating* had the second largest statistically significant effect on *quality*.
 - *Awareness signals* had a significant total effect on *prominence*, but not on *payment*.
 - *Content variance* had a significant total effect on *prominence*.
 - The findings suggest that cancer-treatment hospitals may benefit more from their overall level of quality than from their prominence in terms of Medicare payments.
 - Our findings are consistent with those obtained by [49], except that we found an insignificant effect of prominence and significant effect of quality on financial performance.
 - Kernel-based regularized least squares (KRLS) analysis did not undermine our inferences.
 

 # 5. Discussion and Conclusion
 - Online user feedback is of the utmost importance as it provides efficient yet inexpensive solutions for reputation management and social interaction with online stakeholders.
 - *Know Your Ratings and Raters*: Signals of quality can serve as free indicators of service quality and patient satisfaction.
     - Hospitals should take a cue from service recovery models and contact users who left negative reviews to resolve the issue.
     - Hospitals should explore multiple sources of patient-generated information and use machine-learning techniques to address false reviews.
     - Hospitals should recognize the differences in how Facebook and Google rating systems identify raters, attributing less weight to evaluations from questionable accounts.
 - *Increase Awareness and Visibility*: By sharing content through owned social media accounts and hospital websites, entities can establish lasting contact with their online audiences.
     - Hospitals should develop online engagement strategies aimed at becoming “follow-worthy” by composing distinctive content, using mindful dissemination strategies, using comments and hashtags, and affiliating with influential accounts.
     - Healthcare professionals should increase their engagement through video sharing.
     - Providers should generate their own content and collect feedback generated by online users.
 - *Create Favorable (“Like-able”) Content*: Content sharing through Facebook will contribute to increased prominence.
     - Hospitals should engage in content sharing by providing users with content that is adequate, competent, properly positioned, compelling, fulfilling, and convenient.
     - The shared content should be just and trustworthy, and ideally of “mixed” type, including text, image(s), video(s), and link(s).
     - Content should combine informative characteristics with brand personality-related content.
 - Limitations:
     - Results are only for cancer-treatment hospitals.
     - The Facebook data only examined positive evaluative tone.
     - There is a lack of agreement among national hospital rating systems.
     - There were missing values in our sample.
     - Financial performance was approximated with a total of Medicare payments.
     - There could still be endogeneity caused by omitted variable bias.
 
</details>


# Mei-Po Kwan
- Professor of Business Administration
### education
- Ph.D., Geography, University of California at Santa Barbara, 1994
- M.A., Urban Planning, University of California at Los Angeles, 1989
### research interest
- Geographic information systems and science (GIS); Environmental health; Human mobility; Sustainable cities; Information and communication technologies (ICT).
### teaching interest
- Geographic information systems and science (GIS); Business Applications of Geographic Information Science.

```
title: Geographies of Mobility
authors: Mei-Po Kwan, Tim Schwanen
journal: Annals of the American Association of Geographers
published: 2016
```

# Executive Summary

- This article introduces a special issue on geographies of mobility, reflecting on the rise of the **mobilities turn** and its relationship with established research traditions like **transportation geography**.
- We argue for a pluralistic approach, viewing different approaches as situated, partial, and generative modes of abstraction.
- We outline five lines of conversation to structure future research:
    - **Conceptualizations and analysis** of mobility.
    - **Inequality** in mobility.
    - **Politics** of mobility.
    - **Decentering and decolonization** of mobility studies.
    - **Qualifying abstraction** in mobility research.
- We also discuss three fruitful directions for future research:
    - **Health and well-being**: Examining the relationship between mobility and health, considering both objective and subjective dimensions.
    - **Further decentering and decolonization**: Shifting the focus away from the Global North and engaging with postcolonial and decolonial thinking.
    - **Combining big and small data**: Integrating traditional data with big data to overcome limitations and enrich analysis.
- We highlight that mobility is endemic to life, society, and space, and emphasize the generative qualities of research, where mobility is always more than what a single study can make understandable.
- **Mobility as a Core Geographic Concept**: We emphasize the elevation of mobility to a core geographic concept, alongside space, place, network, scale, and territory.
- **Mobility-Inequality Nexus**: We highlight the importance of studying the interplay between mobility and various dimensions of social inequality, including gender, race, migration status, and sexuality.
- **Politics and Power in Mobility**: We bring to the forefront the need to critically examine how mobility is implicated in the production and distribution of power, thereby shaping social relations and citizenship.
- **Mobility Research and the Global South**: We bring attention to the necessity of decentering mobility research from the Global North, engaging with diverse perspectives and contexts, and amplifying voices from marginalized regions.
- **Analytical Frontiers in Mobility Studies**: We call for innovative research methodologies that integrate big and small data, extend time-geographic analysis, and forge connections between geographic studies of mobility and the digital humanities.
- **Key Findings Related to Health and Well-being**:
    - Emphasize the importance of understanding the effect of people's mobility on their health and well-being, moving beyond traditional static, area-based contexts.
    - The discursive constitution of certain forms of mobility as healthy or unhealthy and the effects that such constitution has on mobility practices and experiences in different places deserve further scrutiny.
- **Key Findings Related to Decolonization and Decentering**:
    - Research on mobilities beyond the Global North is for the most part conducted by scholars born in or at least trained in the center.
    - This form of geographic scholarship on mobility would not be a hegemonic project seeking to provide somehow superior alternative knowledges but options.
- **Key Findings Related to Data Combination**:
    - Studies using big data sets also tend to overestimate people’s mobility and underestimate their daily travel distance.
    - None of the rich and nuanced data collected through traditional or qualitative methods are available in popular big data sets, these data can be used to complement or enrich the analysis performed with big data in human mobility research.

#mobility_turn #transportation_geography #modes_of_abstraction #inequality #politics #decentering #decolonization #health_and_well_being #big_data #small_data #engaged_pluralism

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction

- We start by acknowledging the rise of the **mobilities paradigm** in social sciences since Sheller and Urry's [2006] paper.
- The mobilities turn has influenced geography, complementing studies of transport, travel, tourism, and migration [Lorimer, 2007; Cresswell, 2011, 2012, 2014; Cresswell and Merriman, 2011; Adey et al., 2014; Merriman, 2015].
- The acceptance of mobility as endemic to life, society, and space is a key tenet.
- The mobilities turn has broadened the scope of mobilities considered by geographers.
- A major achievement has been the elevation of mobility to a core geographic concept alongside space, place, network, scale, and territory.
- Two articles in this special issue explicitly address how mobility relates to these other core concepts.

# 2. Conceptualizing Mobility

- The elevation of mobility has diversified understandings and definitions of the term.
- Mobility has always been a fuzzy term, with diverse meanings and referents over time.
- In the last four decades of the twentieth century, mobility was used to denote residential movements, daily trip-making, social mobility, and even energy and force [Wheeler, 1972; Breese, 1963; Kollmorgen, 1969; Sack, 1976].
- Before 1960 and from 2000 onward, meanings and referents are more diverse.
- In the first half-century of scholarship, mobility was used in relation to faunal life, plants, technology, armies, commodities, energy, and human individuals [Joerg, 1914; Gleason, 1922; Ginsburg, 1957; Frey, 1941; Whittlesey, 1945; Platt, 1927; Marschner, 1944; Murphey, 1954; Hoffman, 1957; Sauer, 1941; Platt, 1928; Dryer, 1915].
- Globalization has changed the intensity and geographic scale of movements.
- Mobility doesn't merely denote actual movement but also potential movement or a capacity to become mobile [Smith, 1943; Hall, 1955].
- Sauer [1941] equated mobility with energy and dynamism.
- Whittlesey offered a relational understanding of mobility [Whittlesey, 1945, 1956].
- We do not argue that nothing has changed in recent decades, but there are resemblances between recent and older thinking about mobility.
- The suggestion of a linear progression in understandings of mobility should be avoided.

## 2.1. Richer Conceptualizations of Mobility

- Conceptualizations of mobility and immobility have become richer over the past decade [Adey, 2006; Cresswell, 2006, 2010; Merriman, 2007; Hanson, 2010; Bissell and Fuller, 2011; McCann, 2011; Ziegler and Schwanen, 2011; Söderström et al., 2013; Adey et al., 2014].
- Cresswell's [2006, 2010] understanding of mobility as the entanglement of physical movement, social meanings, and embodied practice is influential.
- This highlights that mobility is more than a functional task and that attempts to reduce it to functionality amount to depoliticization.
- Cresswell’s conceptualization has been criticized [Frello, 2008; Enders, Manderscheid, and Mincke, 2016].
- They argue that the rules of discourse formation dictate what can appear as movement and who can claim understanding of movement, drawing on Foucault's [1972] archaeological method.
- Mobility is an ever-changing object of knowledge coconstituted by practices involving geographers and other agents.

## 2.2. Mobilities and Transportation

- Cresswell’s conceptualization can reinforce a particular representation of transportation geography that separates research on mobilities and transportation [Shaw and Hesse, 2010; Bissell, Adey, and Laurier, 2011; Shaw and Sidaway, 2011; Cidell and Prytherch, 2015; Schwanen, 2016].
- According to Cresswell [2010], transportation research has failed to illuminate the representations and meanings of mobility or how it is embodied and practiced.
- Implied here is an opposition rather than a contrast: transportation versus mobilities research.
- Transportation geography is not merely partial but severely limited.
- Defending transportation geography is not our aim, but Cresswell’s account is problematic.
- Transportation geography is internally heterogeneous and coevolving with the mobilities turn [Kwan, 2007; Schwanen and Kwan, 2008; Goetz, Vowles, and Tierney, 2009; Bissell, Adey, and Laurier, 2011; Shaw and Docherty, 2014; Cidell and Prytherch, 2015; Wilsmeier and Monios, 2015].
- Parts of the subdiscipline can generate new understandings of mobility.

## 2.3. Beyond Dualistic Opposition

- Mobility is bifurcated between an objective realm of movement and a further reality of secondary qualities [Whitehead, 1920; Stengers, 2011].
- Physicists seek to uncover laws dictating movement [González, Hidalgo, and Barabasi, 2008; Simini et al., 2012], while mobility scholars criticize transportation researchers for substituting movement for the rich meanings of mobility.
- This critique is a version of abstraction as a malign process [McCormack, 2012].
- But what if the lived and the abstract cannot be placed in dualistic opposition?
- What if movement, meaning, and practice are understood as truly entangled?
- This alternative imagining allows us to think differently about researching mobility and turn oppositions into contrasts.
- Transportation geographers who reduce mobility to movement and mobilities scholars who privilege meaning are creating different abstractions.
- In so doing, they allow mobility to be articulated and exist in new ways.
- The specific practices of all communities of geographers studying mobility are generative rather than reductive [Latour, 2005].
- All articulations should be plausible to peer groups and inscribed into traditions of research [Stengers, 2000, 2005].
- Mobility is always more than what a single study can make understandable.

## 2.4. Practices of Abstraction

- Different approaches to understanding mobility are the consequence of differences in modes of abstraction.
- Whitehead [1926] argued that practices of abstraction were necessary and inevitable.
- Thought, research, and articulations of mobility become impossible without selection and simplification; what matters is how abstraction is practiced [Stengers, 2011; Schwanen, 2015].
- Whether practices of abstraction are good is difficult to tell because there is no external yardstick.
- Any evaluation is relational and dependent on the purpose of analysis and dynamics in how academics understand the world.
- Evaluation is shaped by asymmetric power relations [Haraway, 1991; Mouffe, 1999; Longino, 2002].
- The complexities of evaluating modes of understanding mobility should not result in fragmenting pluralism [Barnes and Sheppard, 2010].
- Barnes and Sheppard set a high standard and sought to avoid superficial engagement with other modes of abstraction.
- Theirs is a call for engaged pluralism—a conversation across dividing lines that marginalizes no mode of abstraction.
- We realize that engaged pluralism is far from straightforward.
- This special issue seeks to contribute to the creation of a pathway toward the habituation of such engaged pluralism.
- It does so by offering a forum that brings together different ways of studying mobility, by identifying lines of connection, and by outlining avenues for future research.
- As the eighth in a series of annual special issues, this one is unique in scale, openness, and heterogeneity of contributors.
- We issued a broad call for papers in September 2013, seeking contributions from a broad spectrum of scholars.
- The response was overwhelming: We received 230 abstracts.
- The selection process sought diversity in theme, perspective, method, and regional focus.
- Exclusions remain inevitable.
- Research from non-Anglophone speakers remains underrepresented.
- Papers that only consider the mobility of artifacts or migration in isolation were not included.

# 3. Lines of Connection

- The strength of special issues is that they enable new insights to emerge.
- Emerging themes and lines of connection become visible.
- This special issue suggests at least five lines of connection across the heterogeneous ways in which geographers study mobility.
- These lines are interwoven in multiple ways.
- Articles deal with commuting [Bissell this issue; Hu and Wang this issue; Naybor, Pool, and Casas this issue; Parks this issue; Preston and McLafferty this issue; Zhong and Bian this issue] and with health and well-being [Baker et al. this issue; Naybor Poon, and Casas this issue; Ritterbush this issue; van Blerk this issue; Zhong and Bian this issue].
- The fivefold division offers a useful way of organizing the articles.

## 3.1. Conceptualizing and Analyzing Mobility

- Addressing theoretical, conceptual, analytical, and methodological issues is an important concern.
- Mobility has become a significant core geographic concept.
- Two articles address how geographic work might connect theorizations of space with the rich conceptualizations of mobility.
- Cidell and Lechtenberg (this issue) draw on the work of Kamil Skrbek to develop a framework for connecting the spatialities of transportation geography and mobility studies.
- They explore four kinds of spaces and suggest that these notions could offer new analytic tools.
- Miller and Ponto (this issue) examine the connection between mobility and the four sociospatialities identified by Jessop, Brenner, and Jones [2008]: territory, place, scale, and networks.
- Based on automobility, they argue that mobility is a social, cultural, and political achievement.
- Kwan (this issue) highlights changes in the geographic knowledge production process using big data.
- She shows that its use can introduce more uncertainty in geographical studies of mobility.
- Big data does not speak for itself.
- She calls into question the notion of data-driven geography and suggests it is more appropriate to refer to algorithm-driven geographies.
- Birenboim and Shoval (this issue) discuss the opportunities and limitations of smartphone data for geographic scholarship.
- The authors also point to various risks, including selectivity in sampling, geoprivacy and data confidentiality, and data collection techniques that enact the mobility.
- Together these two articles highlight significant methodological issues in human mobility research that uses big data.

## 3.2. Inequalities in Mobility

- Inequality and exclusion are classic concerns in transportation geography [Hanson and Kwan, 2008; Lucas, 2012; Schwanen et al., 2015; Weber and Kwan, 2015] and the mobilities literature [Uteng and Cresswell, 2008; Ohnmacht, Maksim, and Bergman, 2009; Söderström et al., 2013; Adey et al., 2014].
- Mobility generates and is an outcome of inequalities and exclusion.
- Feminist scholars have focused on home–work relations and strategies for overcoming space–time constraints [Hanson and Pratt, 1995; England, 1996; Kwan, 1999, 2000; Jarvis, 2005].
- Work on race and mobility has been influenced by Kain’s [1968] spatial mismatch hypothesis [McLafferty and Preston, 1992; Ihlanfeldt, 1994] but has since moved beyond this idea [Uteng, 2009; Farber et al., 2015].
- Both strands of work are represented in this special issue.
- Articles focus on commuting as a racial mobility project [Parks this issue], gender and racial differences in commuting [Preston and McLafferty this issue], and activity patterns of widowed women [Naybor, Poon, and Casas this issue].
- Emphasis on gender, race, and their intersections is complemented by an orientation on other social identities.
- There is a body of work on the mobility of children and young people [Kullman, 2010; Buliung, Selima, and Faulkner, 2012] to which this special issue adds [Aitken this issue; Cope and Lee this issue; Van Blerk this issue].
- Cope and Lee qualify arguments that young people are the driving force behind “peak car” [Goodwin and Van Dender, 2013].
- They show the importance of the car alongside smartphones in fulfilling young people’s mobility needs.
- Attention for migrants and refugees and LGBTC individuals is more recent [Bose, 2014; Nash and Gorman-Murray, 2014].
- Contributions by Maldonado, Licona and Hendricks, and Ritterbush demonstrate how migrant status and sexuality are coproduced with inequalities in mobility.
- Inequalities in mobility are linked to social identity and network capital [Urry, 2007] and motility [Kaufmann, 2002].
- Sheller (this issue) contributes through a study of how communities in Haiti seek to resist the uneven distribution of network capital.

## 3.3. Politics of Mobility

- The studies mentioned under the previous heading also fit under this one.
- Politics of mobility is defined as "the ways in mobilities are both productive of social relations and produced by them" [Cresswell, 2010, 21].
- The contributions in this special issue extend understanding of such politics.
- Eidse, Turner, and Oswin (this issue) draw on Cresswell’s six elements of a politics of mobility, combining this with Kerkvliet’s [2009] notion of everyday politics in their study of street vendors in Hanoi.
- Struggles over who belongs in streetscapes are at the heart of Stratford’s article, which combines Cresswell’s six elements with Lefebvre’s right to the city.
- Other articles extend the literature on mobilities and citizenship [Cresswell, 2006, 2013; Spinney, Aldred, and Brown, 2015].
- Aitken (this issue), Price and Breese (this issue), and Staeheli, Marshall, and Maynard (this issue), as well as Maldonado, Licona, and Hendricks (this issue), show how citizenship is produced at many different sites [Spinney, Aldred, and Brown, 2015, 326].
- Where Aitken (this issue) and Price and Breese (this issue) consider the relation between individual and nation-state, Staeheli, Marshall, and Maynard (this issue) focus on citizenship beyond the state.
- Rowen (this issue) analyzes the relationships between tourism and state-level geopolitics.
- Bissell’s (this issue) contribution takes the politics of mobility theme in yet other directions.
- His concern is that the focus on subject-centered analysis risks drawing attention away from the micropolitics associated with the churn of events during movements.
- His Deleuzian approach offers a complement to other macropolitical work on how gender, race, and migration status shape (im)mobility.

## 3.4. Decentering Mobility

- Of the twenty-six main articles, four concentrate on East Asia, three on Africa, one each on Latin America, the Caribbean, and Eastern Europe, and another draws on notions postulated by Kamil Skrbek.
- Half of the contributions have a clear link with settings outside North America, Western Europe, and Australia and New Zealand.
- This is clear evidence that geographic scholarship is undergoing a shift away from the Global North.
- This decentering of orientation is beneficial for multiple reasons.
- It opens up new questions and concerns across traditions of studying mobility.
- Both the analysis by Naybor, Poon, and Casas (this issue) and the mobilities articles by van Blerk and Ritterbush highlight the relationships between mobility and livelihoods.
- Naybor, Poon, and Casas show how the lifting of constraints on mobility empowers widowed women in rural Uganda.
- In contrast, van Blerk (this issue) and Ritterbush (this issue) each study sex workers, with the former working in Ethiopia and the latter in Bogota.
- Both show how livelihood and identity can trap sex workers in particular places but are also made possible by moves away from familiar places.
- Mobility and immobility become imbued with multiple meanings.
- A focus on mobility outside Global North settings can demonstrate the spatial contingency of understandings of mobility.
- This is shown in Porter’s (this issue) contribution on mobile phone usage in rural areas in Tanzania and Malawi.
- Her research suggests that the conclusion in Western studies that increased mobile phone use has not generated a major reduction in travel activity does not hold in parts of rural Africa.
- There, the friction of distance is larger, mobility is costly, and traffic accidents take many lives.
- Decentering away from Global North settings might facilitate the diffusion of postcolonial and decolonial thinking.
- Past research has engaged with postcolonial theory [Sheller, 2003; Roy, 2012], but it has been taken up less in transportation geography and mobilities scholarship.
- Postcolonial theory can strengthen mobility research, as demonstrated by Best’s (this issue) study of dollar cabs in Brooklyn, New York.
- Dollar cabs become ambiguous elements in contemporary New York that open new understandings of speed, time, and everyday life in transnational migrant communities.
- Best’s paper offers an interesting complement and contrast to other papers on mobility, race, and migrant status in this special issue.

## 3.5. Qualifying Abstractions of Mobility

- From an affirmative perspective on abstraction [Whitehead, 1926; McCormack, 2012; Schwanen, 2015], all articles selectively engage and simplify mobility.
- They qualify abstractions: They make mobility to exist in particular ways and also identify particular qualities of mobility that remain unarticulated in other research.
- The set of articles under this heading is fairly arbitrary but gathers contributions that innovatively bring out aspects of mobility.
- Some papers articulate specific facets of mobility by drawing on specific bodies of literature.
- Spinney (this issue) seeks to understand governmental interventions to encourage cycling through biopolitics and Harvey’s [2001] work on spatial fixes.
- Baker et al. (this issue) focus on the mobility of ideas as emerging from people and their relations with others, combining understandings from the new mobilities paradigm with developments in urban planning and anthropology.
- Hu and Wang (this issue) evaluate excess commuting, using a Monte Carlo simulation-based approach that takes into account land use patterns.
- Zhong and Bian’s (this issue) article offers an interesting contrast with that by Baker et al., although the former engages less with social theory than the analytical frameworks of network science.
- Like Baker et al., Zhong and Bian examine how influenza diffuses spatially through the movements of people.
- Although not explicitly interested in meaning and power, these processes are still shaping and implicitly considered in Zhong and Bian’s analysis.
- Xu et al. (this issue) show how big data collected with mobile phones can be used to extend and improve time-geographic analyses of human activity spaces.

# 4. Avenues for Further Research

- Bringing together different ways of examining mobility and creating new connections, the special issue points to themes that could stimulate further conversations.
- We identify three themes and developments.

## 4.1. Health and Well-Being

- Health and well-being is a theme that runs across a number of articles.
- It is central to geographic scholarship on mobilities because exposure to factors that influence health, access to health care, and spread of disease are connected to human movement [Gatrell, 2011; Kwan, 2012, 2013; Schwanen and Wang, 2014; Chen and Kwan, 2015].
- As people move around, they are under the influence of different places and come into contact with different persons.
- Particular forms of mobility might be more or less healthy [Gatrell, 2013; Schwanen, 2016] and can induce experiences of belongingness [Hanson, 2010; Nordbakke and Schwanen, 2014].
- Several areas seem especially fruitful for future research.
- Moving beyond the notion of static geographic context to take into account the effects of mobility will be important [Kwan, 2012, 2013].
- Future work should consider both the objective and the subjective dimensions of well-being [Nordbakke and Schwanen, 2014, 104].
- Future research can benefit from adopting interdisciplinary perspectives.
- The experiences of mobility and well-being seem drastically different for different social groups.
- It is important for future research to be attentive to the effects of social difference [Bissell this issue].
- The discursive constitution of certain forms of mobility as healthy or unhealthy and the effects that such constitution has on mobility practices and experiences deserve further scrutiny.

## 4.2. Further Decentering and Decolonization

- This special issue suggests a trend of decentering of geographic scholarship on mobility away from the Global North.
- For various reasons, this process needs to be taken much further.
- From a policy point of view, mobility poses one of the biggest challenges in regions outside the Western world.
- It is in emerging economies that both overall mobility levels and inequalities are growing most rapidly.
- Geographers should address such questions and interrogate their framing.
- Research on mobilities beyond the Global North is for the most part conducted by scholars born in or trained in the center.
- Conversations on the geographies of mobility would be greatly enriched if they became more "worlded" [McCann, Roy, and Ward, 2013; Sheppard, Leitner, and Maringanti, 2013; Sheppard et al., 2015].
- The result will be the coming into being of geographies of mobility that reconfigure familiar distributions of core and periphery.
- It would also enable the generation of mobility theories that are no longer formulated predominantly in the West.
- This form of geographic scholarship on mobility would not be a hegemonic project but options [Mignolo, 2011].
- It would engage in dialogues and seek to induce change in those other modes [Longino, 2002, 129].

## 4.3. Combining Big and Small Data

- Geographers and transportation researchers have studied human mobility for decades.
- Many past studies used detailed data collected through activity-travel diary surveys [Hanson and Hanson, 1981; Kwan, 1999].
- Geographers have also incorporated GPS data [Shoval and Isaacson, 2007; Shen, Kwan, and Chai, 2013; Shoval et al., 2014].
- The rapid increase in inexpensive data from big data sources has stimulated new developments [González, Hidalgo, and Barabasi, 2008].
- Although this research has yielded interesting findings, what can be observed from big data about actual human movement is limited [Kwan this issue].
- Studies using big data sets also tend to overestimate people’s mobility and underestimate their daily travel distance.
- An important area for future research is how traditional data, including qualitative data, can be used together with big data [Kitchin and Lauriault, 2015].
- Activity-travel diaries record the details of activities and trips [Hanson and Hanson, 1981; Kwan, 1999].
- Qualitative methods have also been used to capture people’s experiences [Kwan and Ding, 2008; Bell et al., 2015; Curtis et al., 2015].
- Because none of the rich data collected through traditional methods are available in big data sets, these data can be used to complement analysis.
- Future research can explore the intersection between mobility and the digital humanities.
- In this way, connections can be forged between different approaches to the study of mobility and engaged pluralism can become the norm.

---

# Executive summary of 1. Introduction
- We introduce the special issue on geographies of mobility, noting the influence of the **mobilities turn** since Sheller and Urry's [2006] seminal paper.
- The mobilities turn has encouraged the field to study transport, daily travel, tourism, migration, etc. [Lorimer, 2007; Cresswell, 2011, 2012, 2014; Cresswell and Merriman, 2011; Adey et al., 2014; Merriman, 2015].
- We highlight the widespread acceptance of mobility as endemic to life, society, and space.
- We note the expansion of mobilities considered worthy of academic attention.
- **Key Argument**: A major achievement has been the elevation of mobility to a core geographic concept, alongside space, place, network, scale, and territory.

# Executive summary of 2. Conceptualizing Mobility
- The elevation of mobility to a core concept has led to diverse understandings of the term.
- We find that mobility has always been a fuzzy term, with varied meanings over time.
- Prior to the mobilities turn, mobility was used to denote residential movements, daily trips, social mobility, energy and force [Wheeler, 1972; Breese, 1963; Kollmorgen, 1969; Sack, 1976].
- **Key Argument**: Mobility is not merely actual movement but also potential movement.
- We highlight the relational understanding of mobility: mobility as a capacity to move afforded by interactions between vessel and ocean [Whittlesey, 1945].

## Richer Conceptualizations of Mobility
- Conceptualizations of mobility and immobility have become richer [Adey, 2006; Cresswell, 2006, 2010; Merriman, 2007; Hanson, 2010; Bissell and Fuller, 2011; McCann, 2011; Ziegler and Schwanen, 2011; Söderström et al., 2013; Adey et al., 2014].
- **Key Argument**: Cresswell's [2006, 2010] influential understanding is that mobility is the fragile entanglement of physical movement, social meanings, and embodied practice.
- We note that Cresswell’s conceptualization has been criticized by [Frello, 2008; Enders, Manderscheid, and Mincke, 2016].
- It is argued that the rules of discourse formation dictate what can appear as movement and who can claim understanding of movement.

## Mobilities and Transportation
- We point out that Cresswell’s conceptualization reinforces a representation of transportation geography that separates research on mobilities and transportation [Shaw and Hesse, 2010; Bissell, Adey, and Laurier, 2011; Shaw and Sidaway, 2011; Cidell and Prytherch, 2015; Schwanen, 2016].
- **Key Argument**: Transportation geography is internally heterogeneous and coevolving with the mobilities turn [Kwan, 2007; Schwanen and Kwan, 2008; Goetz, Vowles, and Tierney, 2009; Bissell, Adey, and Laurier, 2011; Shaw and Docherty, 2014; Cidell and Prytherch, 2015; Wilsmeier and Monios, 2015].

## Beyond Dualistic Opposition
- We observe that mobility is bifurcated between an objective realm of movement and a further reality of secondary qualities [Whitehead, 1920; Stengers, 2011].
- We suggest that transportation geographers who reduce mobility to movement and mobilities scholars who privilege meaning are creating different abstractions.
- **Key Argument**: Specific practices of all communities of geographers studying mobility are generative rather than reductive [Latour, 2005].
- We highlight that mobility is always more than what a single study can make understandable.

## Practices of Abstraction
- We argue that different approaches to understanding mobility are the consequence of differences in modes of abstraction.
- **Key Argument**: Thought, research, and articulations of mobility become impossible without selection and simplification; what matters is how abstraction is practiced [Stengers, 2011; Schwanen, 2015].
- We call for engaged pluralism: a conversation across dividing lines that marginalizes no mode of abstraction.
- This special issue seeks to contribute to the creation of a pathway toward the habituation of such engaged pluralism.

# Executive summary of 3. Lines of Connection
- We find that the strength of special issues is that they enable new insights to emerge.
- The special issue suggests at least five lines of connection across the heterogeneous ways in which geographers study mobility.
- These lines are interwoven in multiple ways.

## Conceptualizing and Analyzing Mobility
- Addressing theoretical, conceptual, analytical, and methodological issues is an important concern.
- We emphasize that mobility has become a significant core geographic concept.
- Cidell and Lechtenberg draw on the work of Kamil Skrbek to develop a framework for connecting the spatialities of transportation geography and mobility studies.
- Miller and Ponto examine the connection between mobility and the four sociospatialities identified by Jessop, Brenner, and Jones [2008]: territory, place, scale, and networks.
- **Key Argument**: Kwan highlights changes in the geographic knowledge production process using big data.
- Big data does not speak for itself and its use can introduce more uncertainty in geographical studies of mobility.
- We call into question the notion of data-driven geography and suggest it is more appropriate to refer to algorithm-driven geographies.
- Birenboim and Shoval discuss the opportunities and limitations of smartphone data for geographic scholarship.

## Inequalities in Mobility
- Inequality and exclusion are classic concerns in transportation geography [Hanson and Kwan, 2008; Lucas, 2012; Schwanen et al., 2015; Weber and Kwan, 2015] and the mobilities literature [Uteng and Cresswell, 2008; Ohnmacht, Maksim, and Bergman, 2009; Söderström et al., 2013; Adey et al., 2014].
- Mobility generates and is an outcome of inequalities and exclusion.
- Articles focus on commuting as a racial mobility project [Parks this issue], gender and racial differences in commuting [Preston and McLafferty this issue], and activity patterns of widowed women [Naybor, Poon, and Casas this issue].
- **Key Argument**: Migrant status and sexuality are coproduced with inequalities in mobility [Maldonado, Licona and Hendricks, and Ritterbush].
- We note that inequalities in mobility are linked to social identity, network capital [Urry, 2007] and motility [Kaufmann, 2002].

## Politics of Mobility
- Studies mentioned under the previous heading also fit under this one.
- **Key Argument**: Politics of mobility is defined as "the ways in mobilities are both productive of social relations and produced by them" [Cresswell, 2010, 21].
- Eidse, Turner, and Oswin draw on Cresswell’s six elements of a politics of mobility, combining this with Kerkvliet’s [2009] notion of everyday politics in their study of street vendors in Hanoi.
- We argue that Bissell’s contribution takes the politics of mobility theme in yet other directions.

## Decentering Mobility
- We note that half of the contributions have a clear link with settings outside North America, Western Europe, and Australia and New Zealand.
- We argue that this is clear evidence that geographic scholarship is undergoing a shift away from the Global North.
- Both the analysis by Naybor, Poon, and Casas and the mobilities articles by van Blerk and Ritterbush highlight the relationships between mobility and livelihoods.
- **Key Finding**: Decentering away from Global North settings might facilitate the diffusion of postcolonial and decolonial thinking.
- Best’s study of dollar cabs in Brooklyn opens new understandings of speed, time, and everyday life in transnational migrant communities.

## Qualifying Abstractions of Mobility
- We highlight that all articles selectively engage and simplify mobility.
- **Key Argument**: They make mobility to exist in particular ways and also identify particular qualities of mobility that remain unarticulated in other research.
- Spinney seeks to understand governmental interventions to encourage cycling through biopolitics and Harvey’s [2001] work on spatial fixes.
- Baker et al. focus on the mobility of ideas as emerging from people and their relations with others.
- Xu et al. show how big data collected with mobile phones can be used to extend and improve time-geographic analyses of human activity spaces.

# Executive summary of 4. Avenues for Further Research

- Bringing together different ways of examining mobility and creating new connections, the special issue points to themes that could stimulate further conversations.
- We identify three themes and developments.

## Health and Well-Being

- We emphasize that health and well-being is a theme that runs across a number of articles.
- Exposure to factors that influence health, access to health care, and spread of disease are connected to human movement [Gatrell, 2011; Kwan, 2012, 2013; Schwanen and Wang, 2014; Chen and Kwan, 2015].
- **Key Argument**: Future work should consider both the objective and the subjective dimensions of well-being [Nordbakke and Schwanen, 2014, 104].
- Future research can benefit from adopting interdisciplinary perspectives.
- It is important for future research to be attentive to the effects of social difference [Bissell this issue].
- We propose that the discursive constitution of certain forms of mobility as healthy or unhealthy and the effects that such constitution has on mobility practices and experiences deserve further scrutiny.

## Further Decentering and Decolonization

- We argue that this special issue suggests a trend of decentering of geographic scholarship on mobility away from the Global North, and that needs to be pushed further.
- **Key Argument**: Research on mobilities beyond the Global North is for the most part conducted by scholars born in or trained in the center.
- We propose that conversations on the geographies of mobility would be greatly enriched if they became more "worlded" [McCann, Roy, and Ward, 2013; Sheppard, Leitner, and Maringanti, 2013; Sheppard et al., 2015].
- This form of geographic scholarship on mobility would not be a hegemonic project but options [Mignolo, 2011].
- It would engage in dialogues and seek to induce change in those other modes [Longino, 2002, 129].

## Combining Big and Small Data

- We observe that geographers and transportation researchers have studied human mobility for decades.
- **Key Argument**: An important area for future research is how traditional data, including qualitative data, can be used together with big data [Kitchin and Lauriault, 2015].
- Qualitative methods have also been used to capture people’s experiences [Kwan and Ding, 2008; Bell et al., 2015; Curtis et al., 2015].
- We propose that future research can explore the intersection between mobility and the digital humanities.
- In this way, connections can be forged between different approaches to the study of mobility and engaged pluralism can become the norm.

</details>

```
title: Predicting demand for 311 non-emergency municipal services: An adaptive space-time kernel approach
authors: Li Xu, Mei-Po Kwan, Sara McLafferty, Shaowen Wang
journal: Applied Geography
published: 2017
```
 
# Executive Summary
- This study introduces a **locally adaptive space-time kernel estimation model** to predict the demand for 311 non-emergency municipal services, addressing challenges in resource allocation and response time due to fluctuating demand.
- The model treats 311 requests as an **inhomogeneous Poisson process (IHPP)** and generates space-time predictions, which can improve **resource allocation**, **reduce response time**, and enable **long-term dynamic planning**.
- Key **theoretical** / **conceptual framework** discussions
    - **Inhomogeneous Poisson Process (IHPP)**: 311 requests are modeled as events occurring in space and time with varying intensity.
    - **Adaptive Space-Time Kernel Density Estimation (stKDE)**: A method that combines spatial kernels with temporal weights to capture the complex spatiotemporal dynamics of 311 requests. The spatial structure is identified by a bivariate spatial kernel, and the temporal dynamics are captured by weighting each kernel based on past observations.
- Key **findings** / arguments including
    - A **bivariate spatial kernel** identifies spatial structures, with each kernel weighted by past observations to capture temporal dynamics.
    - **Short-term serial dependency** and **weekly temporality** are modeled using temporal weights adaptive to local community areas.
    - By fitting to the **autocorrelation function of historical requests**, the parameter estimation is transformed into a low-dimensional optimization problem.
    - The model, validated with sanitation service requests in Chicago, demonstrates superior performance compared to industry practices and conventional spatial models with comparable computational cost.
    - The dependency parameters (short-term dependencies and weekly temporalities) appear to have an exclusive pattern, with locations with greater short-term dependency tending to have a lower weekly dependency, and vice versa, but weekly dependencies are generally greater than short-term dependencies.
    - The adaptive space-time KDE **outperforms** the conventional space-time KDE and other models in terms of **logarithmic loss (LL)**, **root mean square error (RMSE)**, and **root mean square Anscombe residuals (RMSAR)**, with interpolation over temporal weights boosting the predictive accuracy modestly.
 
#311 #spatial_point_process #modeling #space_time_kernel_estimation #inhomogeneous_poisson_process #chicago #sanitation_service #stkde #ma up

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Many cities in the United States and Canada use the 311 helpline for residents to request non-emergency municipal services [‘3-1-1’ 2016].
- The 311 system, which started as a police non-emergency number, has evolved into a standalone helpline for various public issues like graffiti, potholes, and sanitation [‘3-1-1’ 2016].
- There are over 70 cities and counties in the United States and 18 cities in Canada offering 311 services, with some European countries having similar helplines [‘3-1-1’ 2016].
- Cities have experienced a dramatic increase in the demand for these municipal services since their launch [‘3-1-1’ 2016].
- To handle the increasing demand, cities have adopted systems like Open311, allowing citizens to submit requests online or through smartphone apps [‘3-1-1’ 2016].
- The ability to predict demand is critical for both emergency and non-emergency municipal services [Goldberg, 2004].
- The demand for 311 services varies greatly by time and location, posing challenges for resource allocation [Goldberg, 2004].
- Call centers evaluate requests, schedule dispatch, and provide on-site service, but resources are often limited due to the volume and spatial variation of requests [Channouf et al., 2007].
- Effective scheduling and routing require reliable and accurate demand forecasts of future 311 requests in space and time [Avramidis et al., 2010].
- Analyses of 311 data are needed to inform planning, budget recommendations, citizen relationship management, neighborhood improvement, and understanding social disparities [International City/County Management Association, 2009; Schellong, 2008; O'Brien, 2016, 2015; O'Brien, Gordon, & Bladwin-Philippi, 2014; Clark, Brudney, & Jang 2013].
- There is a need to accurately predict the spatial and temporal variations of demand in the non-emergency sector, but a problem of data sparsity exists due to uneven distribution of requests.
- Current industry practice relies on simplistic local averaging of historical demand data, which is prone to inaccuracies and sensitive to zoning [Goldberg, 2004; Setzler, Saydam, & Park, 2009; Openshaw, 1984].

## 1.1. Limitations of Existing Methods
- Existing methods, like averaging historical data, do not account for complex spatial and temporal variations and are sensitive to spatial unit definitions [Openshaw, 1984].
- Early demand modeling studies used regression models with socioeconomic variables, which provided good predictions for large regions but lacked fine spatial and temporal granularity [Aldrich, Hisserich, & Lave, 1971; Cadigan & Bugarin, 1989; Kamenetzky, Shuman, & Wolfe, 1982].
- Some studies have applied time series models, but they often lack spatial variations [Trudeau, RousseauFerland, and Choquette, 1989; Channouf et al., 2007; Matteson, McLean, Woodard, & Henderson, 2011; Vile, Gillard, Harper, & Knight, 2012].
- Artificial neural network (ANN) models have been explored but have not shown statistically significant improvements over simpler methods at fine spatial granularity due to data sparsity [Setzler et al., 2009].
- Time-varying Gaussian mixture models have been developed, but they require expert knowledge and are computationally intensive [Zhou et al., 2014].
- Kernel density estimation (KDE) is a nonparametric method used in spatial analysis, but most studies either create independent spatial kernels for each time block or combine them with temporal kernels without considering interactions between space and time [Silverman, 1986; Kwan, 2000, 2004; Brunsdon, Corcoran, and Higgs, 2007].

## 1.2. Space-Time Interactions and Proposed Solution
- Space-time processes are often not completely separable, and interactions between spatial and temporal dimensions need consideration [Schoenberg, 2004].
- Self-exciting point processes have been applied to urban crimes, combining stochastic declustering and KDE to model space-time triggering functions [Mohler et al., 2011].
- Temporally weighted KDE methods weigh conditional spatial density functions more by recent events, but no study has applied this to improve 311 service request predictions [Bowers, Johnson, & Pease 2004; Johnson, Bowers, Birks, & Pease, 2009; Porter & Reich, 2012; Zhou and Matteson, 2015].
- I propose a locally adaptive space-time kernel estimation model to accurately predict 311 requests at high spatial and temporal resolution.
- The model is adapted from the stKDE approach developed for ambulance demand modeling, which identifies spatial structure through bivariate spatial kernels and weights each kernel by corresponding past observations to capture temporal dynamics [Zhou & Matteson, 2015].
- I compare the predictive and computational performance of our model to current industry standards and conventional models.
- To our best knowledge, this is the first model that generates 311 demand forecasts in space and time.
 
# 2. Study Area and Data
- Chicago started the first comprehensive 311 system in 1999, which has been recognized nationally [‘3-1-1’ 2016].
- The City of Chicago maintains a geocoded dataset of 311 service requests since 2010, including the top twelve reported categories of non-emergency public issues.
- The dataset contains information such as the date a service request was received, the date a service was completed, details about the nature of the requested service, and the street address and geographic coordinates of the reported issue.
- Sanitation service requests received by the Chicago 311 call center are used as a case study to validate the presented model, which can also be applied to other services.
- Sanitation service is chosen because it presents a clear weekly rhythm that can be modeled by temporal weights and has a relatively smaller number of requests, which helps validate the model's ability to address data sparsity.
- I geocoded six years' requests reported to the Chicago 311 call center from January 1, 2011 to December 31, 2016, aggregated them at one-day intervals, and used them to train our model and estimate parameters.
- A small proportion of requests regarding the same issue, in the same geographic area, and within a short time period of a previous request are considered as duplicates (4% of the total requests), thus are removed from the training data before entering the model.
- Sanitation service requests are aggregated in community areas to train the temporal weight functions.
- Community areas are administrative units officially recognized by the City of Chicago and are tied to Census data, facilitating the linkage of demographic and socioeconomic characteristics.

# 3. Method
- I model geocoded 311 service requests as an **inhomogeneous Poisson process (IHPP)** through a locally adaptive space-time kernel estimation model.
- The model weighs spatial kernels by corresponding temporal dependency-based functions within each community area to incorporate the complex spatiotemporal variations in 311 service requests.
- I also transform the parameter estimation procedure to a low-dimensional optimization problem to achieve a computation cost comparable to conventional methods and industry practice.

## 3.1. Adaptive Space-Time Kernel Density Estimation Model
- 311 service requests at each time point are a spatial point process since they are a countable set of events in a planar region.
- The Poisson point process is a fundamental and tractable class for modeling spatial point patterns.
    - A counting point process *X* on a space *S* ⊆ R² is a Poisson point process with intensity or rate λ if:
        1.  For any finite planar region *A* ⊆ *S*, *X(A)* has a Poisson distribution with mean λ*A*, where *A* is the area of *A*.
        2.  Given *X(A) = n*, the *n* events on *A* form an independent random sample from the uniform distribution on *A*. [Diggle, 2013; Møller & Waagepetersen, 2004].
- If the intensity function λ is a constant, *X* is said to be a homogeneous Poisson point process, which is the simplest possible stochastic mechanism for the generation of spatial point patterns [Diggle, 2013].
- A more natural model is the **inhomogeneous Poisson process (IHPP)**, which is obtained by replacing the constant intensity λ with a location-dependent intensity function λ(*s*).
- The mean of an IHPP at any finite planar region *A* is computed as ∫λ(*s*)d*s* over *A*.
- A spatial density function *f(s)* can thus be defined as λ(*s*) = λ *f(s)*, such that ∫*f(s*)d*s* = 1 over *A*.
- The spatial density function *f(s)* is extended to become a spatiotemporal density *f(s,t)* if it also depends on time.
- I model *f(s,t)* of 311 service requests to forecast the spatiotemporal demand.

- The conventional spatiotemporal KDE estimates the likelihood of an event occurring at location (x, y) at time t through the following formula:
 
- fˆ(x,y,t) = (1 / (n hs^2 ht)) * Σ ks((x - xi) / hs, (y - yi) / hs) * kt((t - ti) / ht), where the sum is over all i from 1 to n.
 
    - *x<sub>i</sub>*, *y<sub>i</sub>*, *t<sub>i</sub>* are observed event in the space-time domain
    - *k<sub>s</sub>* and *k<sub>t</sub>* are the kernel functions for spatial and temporal domains
    - *h<sub>s</sub>* and *h<sub>t</sub>* are the corresponding bandwidths
 
- The conventional space-time KDE models space and time as independent and undifferentiated dimensions.
- However, the influence of time over the studied process or observations can be very different from the spatial effect, and the temporal patterns of events may depend significantly on geographic location.
- I consider 311 service requests as an IHPP and models the density function through a locally adaptive space-time KDE.
- The model adopts the structure of the stKDE [Zhou & Matteson, 2015] that places a spatial kernel at each past observation and weights each kernel by the corresponding observation.
- I sum the effects from all observations in a space-time region S × T where S ⊆ R² is the spatial domain of the study area, and T is the time window in which the past 311 service requests contribute to the current prediction.
- Let (s, t) represent a 311 service request occurring at location s and time t, where s denotes a pair of coordinates (x, y).
- The demand density fˆ(s, t) is estimated as: fˆ(s, t) = (Σ Ks(s - si, H) * w(t - ti)) / (Σ w(t - ti)), where the sums are over all (si, ti) in S × T.
 
    - *Ks(s - si, H)* is a bivariate spatial kernel estimator
    - *w(t - ti)* is a locally adaptive temporal weight function.
 
- Unlike the same bandwidth used for x and y in the conventional KDE, H here is an unconstrained bandwidth matrix for the bivariate kernel estimator that allows the kernel to be obliquely or arbitrarily oriented.
- Non-negative integrable kernel functions such as uniform, Epanechnikov, and Gaussian are commonly used to ensure that the estimation results in a probability density function.
- For example, a bivariate kernel estimator taking the standard Gaussian form is:
 
    - *Ks(s - si, H) = exp(-0.5 * (s - si)<sup>T</sup> * H<sup>-1</sup> * (s - si)) / (2π |H|<sup>1/2</sup>)*
 
- To avoid the uncertainty introduced by arbitrarily choosing the number of spatial cells and cell size, I train the temporal weight functions to be locally adaptive to the observations within a community area.
- In the model, the temporal weight function is a combination of all possible community area-specific weights *w<sub>j</sub>(t - t<sub>i</sub>)*, multiplied by an indicator function *I(s<sub>i</sub> ∈ Community area j)*, i.e.,
 
    - *w(s<sub>i</sub>, t - t<sub>i</sub>) = Σ (w<sub>j</sub>(t - t<sub>i</sub>) * I(s<sub>i</sub> ∈ Community area j))* where the sum is over all j from 1 to N.
 
        - *N* is the total number of community areas in the study area.
 
    - *I(s<sub>i</sub> ∈ Community area j)* returns 1 if the location of a past event (s<sub>i</sub>, t<sub>i</sub>) in the time window is within community area j, otherwise returns 0.
- Instead of modeling time as just another dimension in the kernel density estimation, *w<sub>j</sub>(t - t<sub>i</sub>)* is constructed based on the dependencies in historical observations for each community area.
- The stKDE [Zhou & Matteson, 2015] models hourly, daily, and weekly temporalities.
- However, the exploratory analysis on the 311 service requests after aggregating observations over each day indicates strong weekly temporality and short-term serial dependency.
- Therefore, I model the community area-specific weight as a sum of the two terms:
 
    - wj(t - ti) = ρj1 * exp(-|t - ti|) + ρj2 * sin(π * (t - ti) / 7)
 
        - *ρ<sub>j1</sub>* is the short-term serial dependency
        - *ρ<sub>j2</sub>* represents the weekly temporality, for community area *j*.
 
- *ρ<sub>j1</sub>* corresponds to the stationary first-order autoregressive model, and *ρ<sub>j2</sub>* is similar to the commonly used periodic covariance function in Gaussian processes [Rasmussen and Williams 2006].
- Since they both range from 0 to 1, the temporal weight function is constrained to take values from 0 to 2, and is normalized in equation (2).
- Temporal weight captures the effects of a past event over the current prediction.
- Without weekly seasonality, it decays with time quickly and reaches zero in less than two weeks, while it has a regular weekly pattern without serial dependency.
- A temporal weight with both terms is a combination of the two.
- The longer a past event is away from the current prediction, the weight is discounted more from *ρ<sub>j1</sub>*.
- However, the total size of effect from the same event is not necessarily smaller, as the second term *ρ<sub>j2</sub>* fluctuates over a weekly basis.

## 3.2. Parameter Estimation
- Although the Epanechnikov kernel is often used for its optimal mean square errors and computational efficiency, our model employs the Gaussian kernel for its great tractability and convenient mathematical properties as the bivariate spatial kernel function.
- The selection of bandwidth is more crucial than the choice of the kernel function as it is a smoothing parameter that exerts a strong influence over the kernel estimation results and thus can affect the model performance substantially.
- Although Silverman's rule of thumb (1986) is an easy-to-compute Gaussian approximation for selecting a bandwidth for density that is close to normal, it works only for selecting diagonal bandwidth matrices and thus does not apply to selecting the unconstrained (non-diagonal) bandwidth matrix used in this study.
- Two data-driven methods are commonly used to select the practical optimal unconstrained bandwidth matrix, the cross-validation selectors and the plug-in selectors.
- I eventually use the plug-in method as cross-validation suffers from a high degree of sample variability and tend to smooth out important features of the data [Wand & Jones, 1994].
- The plug-in method constructs the bias of fˆ as a function of the unknown f and plugs a pilot estimate fˆ to estimate the bias [Loader, 1999].
- The optimal bandwidth is then selected to minimize the asymptotic mean integrated square error (AMISE).
- The unconstrained pilot bandwidth is derived based on Chacón and Duong (2010).
- The temporal weight functions require estimating N pairs of dependency parameters, i.e., one short-term serial dependency parameter and one weekly temporality parameter for each community area.
- Estimating each of the N pairs of dependency parameters requires training a large data set within the time window.
- The computation is very intensive if standard approaches, such as maximizing the joint likelihood of the training data, are used.
- In this paper, I adopt a faster alternative method to estimate the dependency parameters through fitting the temporal weights to the autocorrelation function [Zhou & Matteson, 2015].
- The temporal weights aim to capture the contribution of the past observations to the present demand, which matches the concept of autocorrelation in time series studies.
- Therefore, I estimate the dependency parameters through minimizing the residual sum of squares between the temporal weights and the autocorrelation function of the past observations within each community area.
- Let *l = t - t<sub>i</sub>* denote the time lag between a past observation and the present prediction, the estimation becomes an optimization problem:
 
- *min Σ (ACF(l) - ρ<sub>0</sub> * w<sub>j</sub>(l))<sup>2</sup>* where the sum is over all *l* in {0, 1, ..., L}
 
    - subject to: *ρ<sub>0</sub>, ρ<sub>1</sub>, ρ<sub>2</sub> ≥ 0*
    - *Σ w<sub>j</sub>(l) = L*
 
    - where: *w<sub>j</sub>(l) = ρ<sub>1</sub> * exp(-l) + ρ<sub>2</sub> * sin(π * l / 7)*
 
        - *ACF(l)* is the autocorrelation function at time lag *l*
        - *ρ<sub>0</sub>* is a scaling parameter added to adjust the magnitude of temporal weight *w<sub>j</sub>(l)* to match that of *ACF(l)*
 
- I consider observations in a time window of preceding four weeks, that is, the maximum time lag L equals to 28.
- To mitigate the data sparsity (low intensity) problem, I have aggregated the historical events in the same four-week window for all past available years (2011–2016) to train the temporal weight parameters.
- The model constrains the sum of *w<sub>j</sub>(l)* over all time lags to be *L*, aiming to normalize the size of temporal weights and make them comparable across different community areas.
- The optimization problem is much more efficient than standard ways of maximizing the joint likelihood of the training data as it avoids the computation of kernel density estimation over the large amount of observations.
- As shown in the objective function in Equation (6), the computationally intensive estimation becomes a three-dimensional optimization problem (*ρ<sub>0</sub>, ρ<sub>1</sub>, ρ<sub>2</sub>*) with a small number of observations (*L = 28*).
- Since the objective function and the constraint are both non-linear, I used an R package designed for implementations of non-linear optimization algorithms called ‘nloptr’ to solve this problem.
- I have run a few different optimization algorithms in this package until the parameters converged to some values for a sample data set, and compared how long it took for them to reach the same value.
- Eventually I applied the Improved Stochastic Ranking Evolution Strategy as the global optimization algorithm for the entire training data set, and then used the estimation results as input for a local optimization through Augmented Lagrangian Algorithm with L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm).
- Two ways of allocating the temporal dependency parameters estimated from the optimization were employed.
- One approach was to directly use the dependency parameters estimated in each community area to compute weights for any event locations within the four-week window.
- The dependency parameters are the same for any events in the same community area in this approach.
- The other approach places the estimated serial dependency and weekly seasonality values at the centroid of each corresponding community area and uses the kriging method to interpolate them independently over the study area based on the assumption that they vary over the space smoothly.
- For each past event, the dependency parameters were extracted from the interpolated surfaces based on its location to compute the temporal weight.
- Therefore, each past event potentially has a different size of temporal contribution to the prediction in this approach.
- Either way, the temporal weights are combined with the spatial kernel estimates to generate the predicted density surface.

# 4. Results and Discussion
- I estimated a pair of dependency parameters, *ρ<sub>j1</sub>* and *ρ<sub>j2</sub>*, and a scaling parameter, *ρ<sub>j0</sub>*, in the temporal weight function for each community area, using sanitation service requests received between January 1, 2011 and December 31, 2016.
- The estimated short-term dependencies and weekly temporalities appear to have an exclusive pattern:
    - Locations with greater short-term dependency tend to have a lower weekly dependency, and vice versa.
    - Weekly dependencies are generally greater than short-term dependencies.
- The dependencies are allocated in two ways to compute temporal weights.
- The temporal weights of sanitation service requests are adaptive to the local observations within the corresponding community areas.
- They become much smoother after interpolation where fewer extreme values are found.
- Once the temporal weights and the spatial kernel are determined, I combine them in Equation (2) to generate the predictive density surface.
- Each prediction requires the past observations in a space-time region S × T as input.
- Here, the spatial domain S is the City of Chicago, and the temporal window T is a four-week period prior to the date to be predicted.
- This space-time region slides over time as the prediction moves forward day by day.
- Most of the actual observation points are captured by high predicted densities.

## 4.1. Model Comparison
- I generated predicted density surfaces for all days in January 2017, tested them with the actual observations in the same month, and compared the performance of our model to the MEDIC method in industry practice, the conventional space-time KDE, and two spatial regression models.
- For the MEDIC method, I used the average of the same day of the preceding four weeks from 2011 to 2016 in a community area to estimate the number of requests.
- The estimated numbers were normalized by the total count in the test time period to achieve the predicted densities.
- The conventional space-time KDE was constructed based on Equation (1), which also used the observations from the preceding four-week window and the same study area.
- The spatial regression models used to predict the number of requests in each community area were a spatial lag model and a spatial Durbin model.
- The input variables include demographic and socioeconomic characteristics from Census, such as the proportion of African American population, median age, the proportion of population speaking English at home, the proportion of the population with Bachelor degree or higher, the proportion of unemployed persons, the proportion of households below the poverty line, the proportion of household having at least one car, and median income.
- Meteorological variables, i.e., precipitation and mean temperature, are also included as input for the spatial regression models.
- Three statistics were used to compare the performance of various approaches:
    - Logarithmic loss (LL)
        - Measures the average accuracy loss in a prediction and corresponds to the average log likelihood of the test data.
        - LL = - Σ log(fˆ(si, ti)) / Nt
    - Root mean square error (RMSE)
        - Commonly used for numerical predictions.
        - RMSE = (Σ (nj - nˆj)2 / N)1/2
    - Root mean square Anscombe residuals (RMSAR)
        - Specifically designed to measure predictive accuracy for data following discrete distributions.
        - RMSAR = (Σ (nj2/3 + (3/2) - nˆj2/3)2 / N)1/2

- The locally adaptive space-time KDE outperforms the conventional space-time KDE and other models in terms of all three metrics, with one minor exception.
- Interpolation over temporal weights boosts the predictive accuracy modestly.
- The spatial regression models that incorporated local demographic, socioeconomic, and meteorological information perform slightly worse than the KDE models, in part because they are spatial-only models without any temporal variation.
- Spatial Durbin model is superior to spatial lag model.
- The simplistic averaging method used by MEDIC has the lowest predictive accuracy in terms of all three metrics.

## 4.2. Limitations and Future Work
- Our model is based on point process, but the temporal weights are estimated at areal units, i.e., community areas.
- The choice of spatial divisions exerts a strong influence on temporal dependencies estimation, which is associated with the scale and zoning effects of the modifiable areal unit problem (MAUP) [Openshaw, 1984].
- Aggregation to larger spatial units, such as counties, yields similar dependency parameters that smooth out spatial variability in the observations, while data aggregated at smaller spatial units, such as census tracts, is too sparse to accurately estimate the dependency parameters.
- Community area is chosen in this study as I believe it achieves a good balance between providing enough observations to make accurate estimations and maintaining spatial heterogeneity.
- As an officially recognized unit of the City of Chicago, it is also substantively relevant to service management for the 311 call center and related government departments and agencies.
- To mitigate the zoning effect or sensitivity of the estimations to the choice of spatial divisions, I placed the estimated temporal weights at the centroid of each community area and interpolated their values over the study area.
- Using this method, any patterns tend to vary over the space smoothly rather than being discretized by arbitrarily delineated spatial unit boundaries.
- This approach thus helps to boost model performance by smoothing the discretized temporal weights in Table 2.
- Another concern is edge effects in the kernel estimation.
- Estimations close to the study area border tend to be biased because the events occur there could be impacted by unobserved events outside of the border.
- However, the 311 service requests in this study are recorded only for areas within the city of Chicago, and so I am not able to include events outside of the city's border.
- Using a smaller bandwidth can help mitigate edge effects, but I decided to use the optimal bandwidth selected by the plug-in method.
- In future work, I plan to incorporate ancillary variables in our model to further improve its performance, especially at locations where predictive accuracy is constantly low.
- I found that the number of 311 service requests in many categories is affected by meteorological variables such as temperature and precipitation.
- Another interesting future direction is to extend our model to a network-based approach, as some 311 service request categories are always aligned with the road network.
- In addition to the number of requests modeled in this paper, the time needed to complete a service is another proxy of demand.
- I will include it in our model for better planning purpose and policy recommendation.
- Finally, I would also like to validate our model at more locations, especially cities with significantly different urban structures and climate conditions.

# 5. Conclusion
- The ability to produce accurate forecasts of demand for non-emergency services is of paramount importance as the 311 service request system becomes more established.
- The increasing utilization of 311 municipal services and the great fluctuation with time and location in these requests have posed great challenges to 311 call center managers, including the resource acquisition problem, the resource deployment problem, and the issue of dealing with acute events.
- This paper addresses these challenges by developing and implementing an analytical method to generate reliable predictions of demand in time and space using sanitation service requests in Chicago as a case study.
- The predictions are made one day ahead based on past events but can also be made iteratively day by day to medium-term or long-term forecasts.
- Therefore, they can be analyzed and utilized by the call center to support short-term operational decisions, medium-term tactical decisions, or long-term strategic decisions.
- This work also has policy implications because it offers a methodological contribution to the literature on neighborhood evaluation, helps policymakers to identify problematic neighborhoods, and provides decision support for generating budget recommendations.
- This is the first paper on 311 demand modeling or forecasting.
- I presented a locally adaptive space-time KDE that multiplies a spatial kernel as a function of the past observations and the present prediction locations by a temporal weight function that indicates the contribution of the past observations.
- The temporal weight functions are inferred from dependencies in historical data and are dependent on the observations within the local community area.
- The community area-specific dependency parameters can be updated infrequently, but the actual weights rely on the observations in a time window of the preceding four weeks.
- They are thus constantly changing as the time window moves forward day by day.
- The presented model produces daily spatially explicit density surfaces and decomposes the complex temporal effects into a short-term serial dependency and a weekly temporality.
- Therefore, it is straightforward to interpret and use by even non-expert personnel in the 311 call center and relevant governmental departments.
- Our model proved more accurate than the MEDIC method, an industry standard, as well as the conventional space-time KDE and two types of spatial regression models.
- Rather than maximizing the joint likelihood of a large amount of observations, the presented method transforms the computationally intensive estimation procedure to a low-dimensional optimization problem through fitting to the autocorrelation function of historical data.
- Therefore, the presented model achieves a higher predictive accuracy than conventional models with a comparable computational cost.

---

# Executive summary of 1. Introduction
- Introduction highlights the increasing importance of 311 non-emergency services, the challenges in predicting demand due to spatial and temporal variations, and the limitations of existing methods.
- It emphasizes the need for accurate demand forecasts to improve resource allocation and response times.
- It sets the stage for the proposed locally adaptive space-time kernel estimation model.

# Executive summary of 2. Study Area and Data
- Section 2 introduces Chicago as the study area, which implemented the first comprehensive 311 system.
- It describes the geocoded dataset of 311 service requests maintained by the City of Chicago.
- It justifies the choice of sanitation service requests as a case study to validate the model.
- It explains how the data were preprocessed and aggregated in community areas for training the model.

# Executive summary of 3. Method
- Section 3 details the methodology used in the study, including modeling geocoded 311 service requests as an inhomogeneous Poisson process (IHPP) through a locally adaptive space-time kernel estimation model.
- It describes the conventional spatiotemporal KDE model and proposes the structure of the stKDE that places a spatial kernel at each past observation and weights each kernel by the corresponding observation.
- It explains the parameter estimation, including the choice of the Gaussian kernel, bandwidth selection using the plug-in method, and estimation of dependency parameters through fitting the temporal weights to the autocorrelation function.

# Executive summary of 4. Results and Discussion
- Section 4 presents the results of the study, including the estimated dependency parameters for each community area and the generated predictive density surfaces for sanitation service requests in Chicago.
- It compares the performance of our model to the MEDIC method in industry practice, the conventional space-time KDE, and two spatial regression models using logarithmic loss (LL), root mean square error (RMSE), and root mean square Anscombe residuals (RMSAR).
- It discusses the limitations of the model, such as the impact of the choice of spatial divisions and edge effects in the kernel estimation.

# Executive summary of 5. Conclusion
- Section 5 concludes by summarizing the key findings of the study and reiterating the importance of accurate demand forecasts for 311 non-emergency services.
- It highlights the contributions of the presented model, including its superior accuracy compared to conventional methods and its computational efficiency.
- It discusses the policy implications of the work and suggests directions for future research.

</details>


# H. Dharma Kwon
- Associate Professor of Business Administration and Robert and Karen May Faculty Fellow
### education
- Ph.D., Management, Operations Research, UCLA Anderson School of Management, University of California at Los Angeles, 2008
- B.S., Magna Cum Laude, Physics, Korea Adv. Inst. of Science & Tech KAIST, 1991
### research interest (chatgpt says...)
- Game Theory: free-rider problems; stochastic dynamic games
- Decision analysis and operations research: Bayesian sequential decisions; real options; stochastic dynamic games
- Technology Management: technology adoption; R&D management
### teaching
- Decision making under uncertainty

```
title: Investment in the Common Good: Free Rider Effect and the Stability of Mixed Strategy Equilibria
authors: Youngsoo Kim, H. Dharma Kwon
journal: Operations Research
published: 2024
```
# Executive Summary
- This paper explores the **free rider problem** in investments that benefit everyone, like a company's recycling program that lowers material costs across the industry.
    - We use a **stochastic game** model, picturing two players whose earnings depend on a state variable representing the overall health of the "common good."
    - Our focus is on how inequality (asymmetry) between these players affects the chances of a **mixed strategy equilibrium**.
        - A mixed strategy equilibrium (MSE) is where players randomly choose between different actions to keep their opponents guessing.
- Players have **multiple opportunities to invest**, unlike simpler models that assume just one chance.
    - This setup allows us to see how repeated investments (CSR programs, advertising campaign ...) impact the free rider problem.
    - Each investment involves both a fixed cost (setting up the program) and a variable cost (ongoing expenses).
- **Key finding:** Repeated investment opportunities can actually *sustain* a mixed strategy equilibrium, even when players are unequal, if the level of inequality isn't too extreme.
    - The ability of one player to aggressively invest in the *future* (with probability *q* > 0) creates an incentive for the *other* player to also invest now, maintaining the equilibrium.
- The study introduces a new type of **impulse control game** reflecting these repeated investment decisions.
    - A key element is a **verification theorem**, which we use to prove that our proposed mixed strategy equilibrium is truly stable (a **subgame perfect equilibrium (SPE)**).
- **Specific Findings:**
    - *Mixed Strategy Equilibrium:* A mixed strategy equilibrium (TSSPE) can exist even if the players are unequal, as long as one player's costs aren't drastically higher than the other's.
        - In this equilibrium, one player waits for the "common good" to decline to a certain point, then invests with a specific probability, while the other player invests randomly within a certain range.
    - *Efficiency:* An equilibrium where *only one player always invests* (a pure strategy equilibrium) leads to a better outcome for everyone compared to the mixed strategy equilibrium.
        - This suggests that the mixed strategy equilibrium, while stable, isn't the most efficient way to manage the common good.
    - *Policy Implications:* Policymakers might want to deliberately create inequality between players (e.g., through subsidies) to *discourage* the mixed strategy equilibrium and encourage the more efficient pure strategy outcome.

#free_rider_problem #mixed_strategy_equilibrium #stochastic_game #investment_in_common_good #impulse_control_game #game_theory #asymmetry #subgame_perfect_equilibrium #CSR #advertising

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The paper examines the impact of asymmetry on the free rider effect in a stochastic game of investment in the common good.
    - The **free rider problem** arises when one party's investment benefits others (e.g., corporate social responsibility (CSR) programs, commodity advertising) [Gunther, 2015; Lee and Fairchild, 1988].
    - This delay in investment can diminish the level of the common good.
- This study is motivated by the finding that even a small degree of asymmetry can destabilize mixed strategy equilibria (MSEs) in stochastic environments [Georgiadis et al., 2022].
    - This contradicts the common view that MSEs are characteristic of concession games, like the war of attrition.
    - **Research Questions**:
        - Is there a mixed strategy equilibrium in the model that we study?
        - If so, under what conditions do they exist?
        - Lastly, what is the managerial implication to the policy makers?
- We analyze a model of two players with payoffs influenced by a stochastic stock of the common good.
    - Players have infinite opportunities to invest, incurring a fixed up-front cost and a variable cost.
    - The state variable represents the stock of goodwill (in commodity advertising) [Nerlove and Arrow, 1962; Lon and Zervos, 2011] or industry-wide profitability (in CSR) [Gunther, 2015; Serafeim, 2017].
- The paper contributes both managerially and methodologically.
    - We highlight the impact of repeated investment opportunities, showing that a mixed strategy equilibrium can exist under moderate asymmetry, in contrast to one-shot games.
    - This provides insights for policymakers; a sufficiently high degree of asymmetry can mitigate the free rider effect.
    - It presents an equilibrium solution to a novel class of impulse control games with a free rider effect.
    - A verification theorem is formulated for this class of games, and a mixed strategy subgame perfect equilibrium (SPE) is constructed.
- The paper is organized as follows:
    - Section 2 reviews related literature.
    - Section 3 examines the single-investment game, reproducing Georgiadis et al.’s [2022] result.
    - Section 4 presents the central results, examining the game with infinite investments and formulating a verification theorem.
    - Section 5 concludes the paper.

# 2. Related Literature
- This paper contributes to the literature on games of concession and impulse control games.
- **War of Attrition**:
    - We consider the literature on the war of attrition and its mixed strategy equilibrium [Maynard Smith, 1974; Hendricks et al., 1988; Murto, 2004; Steg, 2015].
    - Empirically, mixed strategy equilibrium has been also found and examined [Wang, 2009; Takahashi, 2015].
    - However, Georgiadis et al. [2022] showed that even slight asymmetry destabilizes these equilibria in stochastic settings.
    - Kwon [2022] also shows a similar absence of inefficient equilibrium in the game of contribution to the common good under asymmetry.
    - This paper recovers the mixed strategy equilibrium with repeated investment opportunities, contrasting with these findings.
- **Commodity Advertising Decisions**:
    - The control game model is suited for commodity advertising, known to suffer from the free rider problem [Lee and Fairchild, 1988; Kinnucan and Myrland, 2003].
    - Advertising can be modeled as an investment in the stock of goodwill [Nerlove and Arrow, 1962], which has been modeled under stochastic environment as stochastic control [Sethi, 1977; Jack et al., 2008; Lon and Zervos, 2011; Kwon and Zhang, 2015].
    - Advertising has also been modeled as an impulse control problem due to substantial up-front investment [Reddy et al., 2016].
- **CSR Investment Decisions**:
    - The model applies to CSR investment decisions, viewing CSR as a private provision of public goods [Bagnoli and Watts, 2003; Kotchen, 2006; Morgan and Tumlinson, 2019].
    - This study focuses on the free rider effect from the mixed strategy equilibria, identifying conditions under which the provision of public goods is hastened or delayed.
- **Stochastic Impulse Control**:
    - The theory of stochastic impulse control has long been applied to various operations research problems [Constantinides and Richard, 1978; Ormeci et al., 2008; Cadenillas et al., 2010; Mitchell et al., 2014; Bensoussan and Chevalier-Roignant, 2019].
    - The impulse control framework has also been applied to game-theoretic models [Stettner, 1982; Cosso, 2013; El Asri and Mazid, 2018; Dutta and Rustichini, 1995; Guo and Xu, 2019; Basei et al., 2020; Ferrari and Koch, 2019; Zabaljauregui, 2020; Campi and De Santis, 2020; Aïd et al., 2020].
    - This paper examines a novel class of impulse control games possessing a free rider effect, yielding a mixed strategy equilibrium.

# 3. Single-Investment Game
- The single-investment game is examined as a benchmark model.
    - Although unrealistic for CSR or advertising, it simplifies analysis, reducing to a stopping game (Dynkin game).
    - The main goal of this section is to show that this simplification may not be so innocuous after all.
- The central goal of this section is twofold.
    - First, we illustrate mixed strategy equilibria of a symmetric game in Section 3.3.
    - Second, in Section 3.4, we turn our attention to an asymmetric game and search for a mixed strategy equilibrium that shares the same simple structures as those illustrated in Section 3.3.

## 3.1. The Model and the Equilibrium Concept
- A model of two players is introduced, with payoffs influenced by the common good, that allow for only one opportunity of investment.
- State Variable:
    - The level of the common good is modeled as a stochastic process.
    - The uncontrolled state variable *X* is modeled as a regular diffusion process:
        - *dX<sub>t</sub> = μ(X<sub>t</sub>)dt + σ(X<sub>t</sub>)dW<sub>t</sub>*
        - where μ(·) < 0 and σ(·) > 0 to model a stochastically declining state.
    - The r-excessive characteristic differential operator for the diffusion process *X*:
        - *A := (1/2)σ<sup>2</sup>(x) (d<sup>2</sup>/dx<sup>2</sup>) + μ(x) (d/dx) - r*
        - where *φ(·)* and *ψ(·)* denote decreasing and increasing solutions to *Af(x) = 0*, respectively.
- Strategy and Payoff:
    - Each player's strategy is *ν<sub>i</sub> = (τ<sup>(i)</sup>, ζ<sup>(i)</sup><sub>τ<sup>(i)</sup></sub>)*, where *τ<sup>(i)</sup>* is the stopping time of investment and *ζ<sup>(i)</sup><sub>τ<sup>(i)</sup></sub> ≥ 0* is the boost.
- Transformation into a Stopping Game in a Subgame Perfect Equilibrium
    - We define the equilibrium solution concept that will be used throughout this paper.
    - **Definition 1**: A strategy profile *ν* is an *subgame perfect equilibrium* if it is a Nash equilibrium in any subgame.

## 3.2. Benchmark: Single-Player Problem
- In Section 3.3, it will be shown that the payoff associated with a mixed strategy MPE is identical to the optimal payoff function from the single-player problem.
- We assume that player *j* never invests and solve for player *i*'s best response and payoff.
    - This effectively reduces to maximizing:
        - *E<sub>x</sub> [∫<sub>0</sub><sup>τ</sup> π(X<sub>t</sub>)e<sup>-rt</sup> dt + e<sup>-rτ</sup>g<sub>i</sub>(X<sub>τ</sub>)]*
    - with respect to stopping time *τ*.
- **Proposition 2**:
    - The optimal time of investment for player *i* is when the state variable falls below a certain threshold, and the optimal boost is a function of how far below the threshold the state variable falls.

## 3.3. Mixed Strategy SPE of Symmetric Games
- We return to the two-player game with identical players (*c = c<sub>1</sub> = c<sub>2</sub>*) and construct a mixed strategy MPE and a two-stage subgame perfect equilibrium (TSSPE).

### 3.3.1. Formulation of Mixed Strategies
- The strategy space is expanded to encompass probabilistic mixtures of stopping times.
    - An additional random variable for each player is realized in the beginning of the game but unknown to the opponent.
    - **Mixed strategy formulation**:
        - *τ̂<sup>(i)</sup> = inf{t ≥ 0 : M<sup>(i)</sup><sub>t</sub> ≤ l̂<sup>(i)</sup>}*
        - where *M<sup>(i)</sup><sub>t</sub>* is the survival probability, and *l̂<sup>(i)</sup>* is a randomizer uniformly distributed over [0, 1].

### 3.3.2. Characteristics of Mixed Strategy MPE
- **Definition 2**: An SPE *ν = (ν<sub>1</sub>, ν<sub>2</sub>)* is an MPE if each player's strategy *ν<sub>i</sub>* satisfies a Markov property, meaning the investment decision depends only on the current value of the state variable, not the entire history.

### 3.3.3. Mixed Strategy MPE
- We now construct an MPE strategy profile *ν<sup>M</sup> = (ν<sup>M</sup><sub>1</sub>, ν<sup>M</sup><sub>2</sub>)*.
    - Each player invests at a rate that depends on the current state of the "common good."

### 3.3.4. Two-Stage SPE
- A TSSPE *ν<sup>S</sup>* is constructed.
    - *Stage 1*: Each player has a probability of investing upon the "common good" reaching a certain level.
    - *Stage 2*: Players employ the mixed strategy MPE profile from Section 3.3.3.

## 3.4. Absence of Two-Stage SPE in Asymmetric Games
- We establish the central result of this section: an asymmetric game has no TSSPE that can be represented by survival probability of the form given in (17) and (18).
    - **Theorem 1**: A two-stage equilibrium as described above cannot exist in a single-investment game where the players have different costs.
    - This is because differing costs mean different investment thresholds, making a common mixed strategy region impossible.

# 4. Impulse Control Game
- The game is now turned to a game with an infinite number of investment opportunities.
- The goal of this section is to prove that a moderately asymmetric game possesses a mixed strategy TSSPE.
    - A verification theorem (Theorem 2) is derived and used to construct a mixed strategy SPE (Theorem 3).

## 4.1. The Model
- The model introduced in Section 3.1 is extended to allow for an infinite number of investment opportunities.

## 4.2. Verification Theorem
- We construct a specific strategy profile and payoff functions and prove that they constitute a mixed strategy equilibrium if they satisfy a set of conditions.

### 4.2.1. Strategy Profile
- In the proposed strategy profile *ν<sup>*</sup>*, each period is divided into two stages.
    - The players use mixed strategies in two stages.

### 4.2.2. Payoff Functions
- We construct functions *F<sub>i</sub>(x) = {F<sub>i,t</sub>(x)}<sub>t≥0</sub>* for each *i ∈ {1, 2}*.
    - Players' expected payoffs must satisfy certain inequalities to make the proposed equilibrium a stable one.

### 4.2.3. Survival Probability
- Per-period survival probabilities *M<sup>(i)</sup><sub>m,t</sub>* are constructed based on the strategy profile *ν<sup>*</sup>*.
- **Theorem 2**: Under Assumptions 1 and 2, if *F<sub>1</sub>(·)* and *F<sub>2</sub>(·)* that satisfy the conditions exist, then *ν<sup>*</sup>* is an SPE.

## 4.3. Existence of Mixed Strategy TSSPE
- We show that a mixed strategy TSSPE exists in our model under certain conditions.
- **Theorem 3**: A mixed strategy TSSPE exists if the difference in players costs isn't too great.

## 4.4. Example
- We illustrate an example of the mixed strategy TSSPE.
    - "Common good" follows a geometric Brownian motion.
- **Proposition 6**: The example satisfies earlier assumptions for specific cost ranges.

## 4.5. Comparison with Pure Strategy Equilibrium
- In this subsection, we demonstrate that there exists a pure strategy equilibrium, which is more efficient than the mixed strategy SPE *ν<sup>*</sup>*.
    - A situation where only one player invests in the "common good" is better for everyone.
    - **Proposition 7**: The pure strategy equilibrium is more efficient than the mixed strategy equilibrium.

# 5. Conclusions
- Repeated opportunities of concession stabilize the mixed strategy equilibrium, despite asymmetry.
- Policymakers can try to shift the situation away from a mixed strategy equilibrium by creating imbalances, perhaps through offering different subsidies.

</details>


```
title: Impact of Bayesian Learning and Externalities on Strategic Investment
authors: H. Dharma Kwon, Wenxin Xu, Anupam Agrawal, Suresh Muthulingam
journal: Management Science
published: 2016
```
 
# Executive Summary
- This paper investigates the **interplay between *Bayesian learning* and externalities** in **competitive investment decisions** with **uncertain returns**. *Bayesian learning* allows firms to update their beliefs about market profitability based on observed outcomes.
- We examine a **duopoly game-theoretic investment model** where:
    - A firm learns about investment profitability by observing the first mover's performance (*Bayesian learning*). This learning process is modeled using a Brownian motion and the resulting posterior probability update.
    - *Externalities* exist between the investments of the two firms (positive or negative). These *externalities* affect the profit streams of both firms once both are invested in the market.
- We find a region of a **war of attrition** where this interplay leads to counterintuitive effects. The *war of attrition* arises because each firm wants to be the follower, gaining the benefit of learning from the leader's experience.
- **Key Findings**:
    - An increased rate of learning can *hasten* the first investment, contrary to conventional war of attrition logic. This is because with faster learning, the firms require less time to collect sufficient information to make an informed investment decision.
    - The effect of the **learning rate** on the **follower's investment timing** exhibits a *single-crossing property*:
        - For *low* prior beliefs (p) about market profitability, a higher learning rate *hastens* investment. The follower benefits more from quicker access to information.
        - For *high* prior beliefs, a higher learning rate *delays* investment due to the increased value of waiting for more information. The follower is more inclined to wait and refine their assessment of market profitability.
    - The **leader's payoff** also displays a *single-crossing property* with respect to the *learning rate*, driven by the interplay of externalities and learning.
        - With *positive externalities*, an earlier follower investment improves the leader's payoff. This is due to the time value of money and the positive impact of the follower's investment on the leader's profits.
        - With *negative externalities*, an earlier follower investment diminishes the leader's payoff. The leader benefits from the follower delaying entry, allowing the leader to capture more profits.
    - The **time to the first investment** also exhibits a *single-crossing property*. This variable represents the expected time until either firm makes the initial investment.
        - With *positive externalities*, if the leader's payoff *increases* with the learning rate, the time to first investment *decreases*. Faster learning and the resulting positive effect on leader's payoff encourages investment.
        - The opposite holds if the leader's payoff *decreases* with the learning rate. The lower payoff discourages immediate action.
- **Theoretical contribution**:
    - We demonstrate how *Bayesian learning* about uncertain profitability influences investment behavior in a competitive setting with externalities. The model incorporates the dynamics of belief updating through the posterior probability *Pt*.
    - We challenge the intuition derived from standard war of attrition models, showing that increased learning can accelerate investment under certain conditions. We introduce a new element that is not present in most of the traditional war of attrition literature that the players can influence their decision to enter based on how much they can learn.
 
#bayesian_learning #externalities #strategic_investment #game_theory #duopoly #war_of_attrition #single_crossing_property #competitive_advantage


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Investment decisions in uncertain markets or with unproven technologies involve significant uncertainty [Carruth et al., 2000; Dixit, 1992].
- Returns on investments depend on timing and investment decisions made by other firms (i.e., externalities).
    - **Positive externalities**: a firm's returns improve with an increase in the number of firms in the market.
    - **Negative externalities**: a firm's returns diminish with an increase in the number of firms.
- Existing literature examines learning effects and externalities separately.
- We examine a **duopoly game of investment with uncertain profitability** where **learning effects and externalities coexist**.
    - When one firm enters as a leader, the follower observes the leader's performance and learns about the market's profitability.
    - If both firms are in the market, their profits exhibit externalities.
- We study how:
    - The follower's time to invest changes with the rate of learning.
    - The leader's payoff changes with the rate of learning.
    - The time to the first investment changes with the rate of learning.
- We find a ubiquitous **single-crossing property** of the derivatives of the payoff and the time to investment with respect to the rate of learning, driven by the interplay between externalities and learning.
- Under certain conditions, the model reduces to a war of attrition, where the leader's payoff is less than the follower's payoff.
    - In a conventional war of attrition, increased benefits for the follower delay both players' decisions [Hendricks et al., 1988].
    - However, we find that an increased rate of learning has a more nuanced effect.
- Our main findings are:
    - An increased rate of learning has two opposing effects on the follower's time of investment:
        - It can hasten investment because the follower acquires more meaningful information within a shorter time.
        - It can delay investment because of the increased value of waiting.
        - We find that the derivative of the follower's time to investment with respect to the rate of learning exhibits a single-crossing property.
    - The interplay of externalities with learning drives the single-crossing property of the comparative statics of the leader's payoff.
        - Under positive externalities, earlier investment of the follower improves the leader's payoff.
        - Under negative externalities, the comparative statics are reversed.
    - The comparative statics of the time to the first investment also has a single-crossing property.
        - In the case of positive externalities, if the leader's payoff increases (decreases) in the learning rate, then the time to the first investment tends to decrease (increase) in the learning rate.
- We provide examples of investment problems with both positive and negative externalities from real life.
    - **Positive externalities**: Introduction of organic cotton garments (economies of scale).
    - **Negative externalities**: Nucor's adoption of thin slab casting technology (competition driving down profits).
 
# 2. Related Literature
- Our work builds on:
    - Bayesian decision models in investment under uncertainty [Jensen, 1982; McCardle, 1985; Ulu and Smith, 2009; Ryan and Lippman, 2003; Kwon and Lippman, 2011].
        -  These papers examine a single decision maker’s problems, whereas this paper investigates investment decisions in a duopoly under uncertainty.
    - Externalities and complementarities in investment games [Dybvig and Spatt, 1983; Katz and Shapiro, 1986; Nielsen, 2002; Femminis and Martini, 2011; Mamer and McCardle, 1987; Weeds, 2002].
    - Learning effects in investment games [Kapur, 1995; Hoppe, 2000].
    - The war of attrition [Smith, 1974].
- Décamps and Mariotti [2004] incorporated both externalities and Bayesian learning.
    -  The focus is on the interplay of the information externality and private information on costs, whereas we focus on the interplay of externality and learning.
    - Their model assumes that the leader's payoff is independent of the follower's learning rate; in contrast, in our model, the leader's payoff depends on the follower's time of investment, which drives our main results.
- Thijssen et al. [2006] also incorporated both externalities and Bayesian learning.
    - Their model assumes that the leader's investment immediately reveals the true profitability to the follower.
- Choi [1997] studied the interplay between informational externalities and payoff interdependency through network externalities, focused on herd behavior.
- Frisell [2003] developed a market entry model in which payoff externalities and informational externalities coexist, focusing on private signal about market demand.
 
# 3. The Game of Externality and Bayesian Learning
- Consider two firms (i = 1, 2) with a one-time option to make an irreversible investment to enter a new market with unknown demand.
- Investments have mutually positive or negative externalities.
- If one firm enters first (leader), the other (follower) observes its performance and learns about the market demand.
- We define the strategy space, payoff function, and objective of each firm.
    -  *Ti* = firm *i*'s time of investment.
    - (*T1*, *T2*) = strategy profile.
    - *Vi*(p, *T1*, *T2*) = payoff for firm *i* given (*T1*, *T2*) and prior probability *p* (initial belief that market demand is high).
- We assume firm 1 is the leader and firm 2 is the follower (*T1* ≤ *T2*).
    -  *τ* = *T2* - *T1* (elapsed time between leader and follower investments).
    - *X* = leader's cumulative profit before follower invests.
    - *r* > 0 = discount rate for both firms.
- Model *X* as a Brownian motion: *dXt* = *μ* *dt* + *σ* *dWt*, *t* ∈ [*T1*, *T2*], where
    - *σ* > 0 = noise level of the leader's income stream.
    - *μ* = time-averaged profit per unit time (either *h* for high demand or *l* for low demand).
    - *Wt* = Wiener process.
- Each firm maximizes its expected cumulative discounted profit:
    - *V1*(p,*T1*, *T2*) = e^(-rT1) Ep[-k + ∫(*τ*,0) e^(-rt) *dXt* + e^(-r*τ*) *ÛL*]
    - *V2*(p,*T1*, *T2*) = e^(-rT1) Ep[e^(-r*τ*) (*ÛF* - k)]
    - *k* = up-front cost of investment for each firm.
    - *ÛL* and *ÛF* = leader's and follower's expected cumulative discounted incomes after the follower invests, conditional on *μ*.
- Consider *τ* > 0 (i.e., *T2* > *T1*).  Use *I* ∈ {L, F} to denote role (Leader, Follower).
- Random variable *ÛI* is given as follows:
    - *ÛI* ≡ E [∫(∞,0) *μ*(1 + *A_I*)e^(-rt) dt + ∫(∞,0) e^(-rt) *σ_I* *dW_I(t)* | *μ*] = *μ*/r (1 + *A_I*) if *T1* < *T2*
    - *A_I* represents the degree of externality (positive if > 0, negative if < 0).
    - Mixed signs of the externalities such as *A_L* > 0 > *A_F* or *A_F* > 0 > *A_L* are also possible.
- For simultaneous investment (*T1* = *T2*), assume each player has a 50% chance of being the leader or follower. The degree of externality is *A_S* = (*A_L* + *A_F*)/2.
    - *ÛL* = *ÛF* = *ÛS* ≡ E [∫(∞,0) *μ*(1 + *A_S*)e^(-rt) dt + ∫(∞,0) e^(-rt) *σ_S* *d(W_L(t) + W_F(t))/2* | *μ*] = *μ*/r (1 + *A_S*) if *T1* = *T2*
- Assumption 1:  *A_I* ∈ (-1, ∞), *A_L* ≥ *A_F*, 0 < *l*/r < *k* < *h*/r, and 0 < (1 + *A_I*) *l*/r < *k* < (1 + *A_I*) *h*/r for each *I*.
    - Investment without learning would be profitable when *μ* = *h* and unprofitable when *μ* = *l*.
- Construct the Bayesian updating process for the posterior probability of *μ* = *h* for time *t* ∈ [*T1*, *T2*].
    - *Pt* = posterior probability of *μ* = *h* at time *t*, conditional on prior probability *p*.
    - *Pt* = P(*μ* = *h* ∩ *Xt*) / (P(*μ* = *h* ∩ *Xt*) + P(*μ* = *l* ∩ *Xt*)) = [1 + ((1-p)/p) exp {-(*h*-*l*)/(*σ*^2) [*Xt* - ((*h*+*l*)/2)*t*]}]^(-1)
    - *Pt* is the unique strong solution to: *dPt* = *Pt*(1 - *Pt*) ((*h* - *l*)/ *σ*) *d(W_hat(t))*, where *W_hat(t)* is an observable Wiener process.
- *Pt* is defined only within the interval [*T1*, *T2*].
- Define *m*(p) ≡ Ep[*μ*] = *h*p + *l*(1 - p).
- We can express payoff functions in terms of *Pt*.
- Before the leader invests, neither firm receives any information, so the probability of {*μ* = *h*} coincides with *p* for all *t* ∈ [0, *T1*].
- If *T1* = ∞ and *T2* = ∞, then *Vi*(p, ∞, ∞) = 0 for both *i* = 1, 2.
 
### 3.1. Payoff Functions in Terms of *Pt*
-  *Vi*(p,*T1*, *T2*) in terms of the process *P* when *τ* = *T2* - *T1* is a stopping time:
    - For *τ* > 0 (i.e., *T2* > *T1*):
        - *V1*(p, *T1*, *T2*) = e^(-rT1) {(*m*(p)/r) - k + (*A_L*/r) Ep[e^(-r*τ*) *m*(P*τ*)]}
        - *V2*(p, *T1*, *T2*) = e^(-rT1) Ep{e^(-r*τ*)[(1 + *A_F*) *m*(P*τ*)/r - k]}
    - For simultaneous investment (*τ* = 0; *T2* = *T1*):
        - *V1*(p, *T1*, *T2*) = *V2*(p, *T1*, *T2*) = e^(-rT1)[(1 + *A_S*) *m*(p)/r - k]
 
### 3.2. Three-Stage Game
- Our model is a three-stage game:
    - Stage 1: *t* < *T1* (before the first investment): Firms wait.  No profit stream.  Probability of {*μ* = *h*} remains constant (*p*).
    - Stage 2: *t* ∈ [*T1*, *T2*) (if *T2* > *T1*):
        - Leader (firm 1) earns cumulative profit stream *X*.
        - Follower (firm 2) updates posterior process *P* based on *X*.  Processes *X* and *P* terminate when the follower invests at *T2*.
    - Stage 3: *t* ≥ *T2*: Neither firm updates the probability of {*μ* = *h*}. Both firms earn final profit streams in perpetuity.
 
# 4. Classification of Equilibria
- We consider both positive (*A_I* > 0) and negative (-1 < *A_I* < 0) externalities with *A_L* ≥ *A_F*.
- We obtain pure strategy and mixed strategy subgame perfect equilibria.
 
## 4.1. Pure Strategy Subgame Perfect Equilibria
- We obtain the follower's optimal policy and its associated payoff using backward induction.
- Suppose firm 1 is the leader (*T2* ≥ *T1*).
- Once the leader invests at *T1*, firm 2 (follower) maximizes its payoff *V2*(p, 0, *τ*) with respect to *τ* = *T2* - *T1*.
- Let *VF*(p) ≡ sup(*τ*≥0) *V2*(p, 0, *τ*) denote the optimal payoff for the follower for *T1* = 0 and *T2* ≥ 0.
- Proposition 1:
    - (i) At time *T1*, the follower's optimal payoff is *VF*(p) = max{*AF*(p), *AS*(p)*}, where
        - *AF*(p) = { (n(p)/n(aF))[(1+AF)m(aF)/r − k], p < aF;  1/r (1+AF)m(p) − k, otherwise }.
        - *AS*(p) = 1/r (1+AS)m(p) − k.
        - *aF* and *n*(x) are defined in Appendix A. The follower’s optimal policy is to invest immediately at *T1* if *AS*(p) ≥ *AF*(p) and to wait and invest as soon as Pt hits the upper threshold *aF* if *AS*(p) ≤ *AF*(p).
    - (ii) There exists *aS* ≤ *aF* such that *AS*(p) > *AF*(p) if and only if p > *aS*.
- Proposition 1 establishes that the follower's optimal strategy is to invest at *T2* = *T1* + *τ*^*, where
    - *τ*^* = { inf{t > 0: Pt ≥ *aF*}, if *AF*(p) ≥ *AS*(p); 0, if *AF*(p) < *AS*(p) }.
- *AF*(p) = optimal value function for the follower under the constraint *T2* > *T1*.
- *AS*(p) = value function for *T1* = *T2* (simultaneous investment).
- When *AF*(p) ≥ *AS*(p), the follower's optimal policy is to invest as soon as *Pt* hits the optimal upper threshold *aF*.
- Intuitively, the optimal threshold *aF* of the follower's investment can be obtained as follows:
    - Define *τa* = inf{t > 0: Pt ≥ *a*}, hitting time for some threshold *a*.
    - By stopping theory, Ep[exp(-r*τa*)] = *n*(p)/*n*(a*).
    - Thus, *V2*(p, 0, *τa*) = ((1+AF)*m*(a*)/r - k) *n*(p)/*n*(a*).  The optimal threshold *aF* is obtained from the necessary first-order condition d*V2*(p, 0, *τa*)/d*a* = 0.
- We obtain the leader's (firm 1's) best response *T1* conditional on the follower's optimal stopping time *τ*^*:
    - *VL*(p) ≡ sup(*T1*) *V1*(p, *T1*, *T1* + *τ*^*).
- Proposition 2:
    - Given the follower's time of investment *τ*^*, the leader's optimal payoff is *VL*(p) = max{*AL*(p), 0*}, where
        - *AL*(p) = { m(p)/r − k + AL m(aF)n(p)/(r n(aF)), p < *aS*; 1/r (1+AS)m(p) − k, otherwise }.
        - The leader's best response is to invest at *T1* = 0 if *p* ≥ *aL* and at *T1* = ∞ if *p* < *aL*, where *aL* ∈ (0, *aS*) is defined by *aL* = inf{p: *AL*(p) > 0}.
- *AL*(p) = leader's payoff from immediate investment when the follower is expected to invest at time *τ*^*.
- The leader immediately invests if and only if its net payoff from investment exceeds zero; otherwise, the leader never invests.
- Next, we obtain the strategies in the pure strategy subgame perfect Nash equilibria.
- Proposition 3:
    - (i) If *p* ∈ (0, *aL*), neither player invests in equilibrium.
    - (ii) If *p* ∈ [*aL*, *aS*), there are two pure strategy subgame perfect equilibria, each with a leader and a follower. The leader invests at *τL* = 0, and the follower invests at stopping time *τF* = inf{t > 0: Pt ≥ *aF*} > 0.
    - (iii) If *p* ∈ [*aS*, 1), there exists a symmetric pure strategy Nash equilibrium in which players invest immediately at the same time at *t* = 0.
- Under the pure strategy equilibria in Proposition 3(ii), a leader and a follower exist.
- Lemma 1: There exists *ac* ∈ (*aL*, *aS*) at which *VL*(p) > *VF*(p) for *p* ∈ (*ac*, *aS*) and *VF*(p) > *VL*(p) for *p* ∈ (*aL*, *ac*), with the understanding that (*ac*, *aS*) is empty whenever *ac* = *aS*.
- We call the interval (*aL*, *ac*) a war of attrition (WA) region, the interval (*ac*, *aS*) a preemption (PE) region, and the interval [*aS*, 1) a simultaneous move (SM) region.
 
## 4.2. Mixed Strategy Subgame Perfect Equilibrium in the War of Attrition Region
- We obtain mixed strategy equilibria in the WA region (*aL*, *ac*) by employing the results of Hendricks et al. [1988] pertaining to a war of attrition in continuous time.
- In the WA region, a mixed strategy profile is characterized by:
    - Each firm's stopping time for investment as a follower in stage 2 if the other firm invests first.
    - Each firm *i*'s probability distribution of the random time *T_hat(i)* of investment for stage 1.
- *G_i*(p, .) : R+ -> [0, 1]: Firm *i*'s cumulative probability distribution function for time *T_hat(i)* given prior probability *p*.
- Given a strategy profile (*G1*(p), *G2*(p)), the payoff for firm *i* is:
    - Vi(p, G1(p), G2(p)) = E[1(T_hat(i) < T_hat(j)) e^(-r T_hat(i)) VL(p) + 1(T_hat(i) > T_hat(j)) e^(-r T_hat(j)) VF(p) + 1(T_hat(i) = T_hat(j)) e^(-r T_hat(i)) AS(p) | (G1(p), G2(p))]
    - = ∫(0, ∞) {e^(-rt) [1 − G_j(p, t)] VL(p) + [lim(u↑t) ∫(0, u) VF(p) e^(-rs) dG_j(p, s)] + e^(-rt) AS(p) q_j(p, t)} dG_i(p, t)
- Proposition 4:
    - (i) For *p* ∈ (*aL*, *ac*), a strategy profile (*G1*(p), *G2*(p)) with q1(p, 0) < 1 and q2(p, 0) < 1 is a subgame perfect mixed strategy equilibrium if and only if:
        - (a) (q1(p, 0), q2(p, 0)) ∈ [0, 1]^2 and q1(p, 0)q2(p, 0) = 0.
        - (b) For both *i* = 1 and 2, *Gi*(p, t) = 1 − (1 − q_i(p, 0)) exp(−t/τ_bar_M(p)), where τ_bar_M(p) = (VF(p) − VL(p))/(r VL(p)).
    - (ii) Under the subgame perfect mixed strategy equilibrium, the payoff for firm *i* is *Vi*(p, *G1*(p), *G2*(p)) = *q_j*(p, 0)*VF*(p) + (1 - *q_j*(p, 0))*VL*(p). The expected time to the first investment is E[min(T_hat(1), T_hat(2))] = (1 − q1(p, 0) − q2(p, 0)) *τ_bar_M*(p)/2.
- At least one of *q1*(p, 0) and *q2*(p, 0) must be zero (one firm taking the leader's role with probability at t = 0).
- If we focus on a completely symmetric equilibrium, we can set *q1*(p, 0) = *q2*(p, 0) = 0.
- *V_M*(p) = mixed strategy equilibrium payoff for *q1*(p, 0) = *q2*(p, 0) = 0, which coincides with *VL*(p). Each firm's investment time *T_hat(i)* is exponentially distributed with rate 1/*τ_bar_M*(p). The expected time of the first investment is E[min(T_hat(1), T_hat(2))] = *τ_bar_M*(p)/2.
- We briefly comment on the possibility of a mixed strategy equilibrium in the PE region.
 
# 5. Impact of Learning
- Explore the impact of learning on equilibrium strategies.
- We show that there exists an interplay between learning and externalities due to strategic interactions between the firms.
 
## 5.1. Benchmark Model
- In our benchmark model, externalities do not exist while Bayesian learning does.
- We show that the expected time to the first investment monotonically increases with the rate of learning.
- If there is no externality between the two investments (AL = AF = 0), the leader's payoff is independent of the follower's action: VL(p) = m(p)/r − k, which does not depend on the rate of learning or the follower's strategy.
- The follower's optimal payoff is VF(p) = Ep[e^(-r τF)(m(p)/r − k)] = (m(aF)/r − k) *n*(p)/ *n*(aF).
- The expected time to the first investment is given by τ_bar_M(p) = (VF(p) − VL(p))/(r VL(p)).
- VF(p) increases with the rate of learning, consistent with the intuition that a higher rate of learning improves the follower's profit.
- It follows that τ_bar_M(p) increases with the rate of learning.
 
## 5.2. The Follower's Payoff and Strategy
- We study the comparative statics of VF(p), aF, and Ep[τF|τF < ∞] with respect to the rate of learning for nonzero AL and AF.
- Because of the improved value of waiting and learning before investment, the follower delays its investment as the learning rate increases.
- This result is consistent with the conventional result that the signal-to-noise ratio (h − l)/σ increases the value of waiting as well as the upper threshold of investment [Bergemann and Valimaki, 2000].
- aF increases with the rate of learning.
- We obtain the form of Ep[τF|τF < ∞]:
    - Ep[τF|τF < ∞] = log(aF/(1 − aF) * (1 − p)/p) * σ^2/(h − l)^2
- For notational convenience, we define a0 ≡ lim(rate of learning→0) aF = (kr − l(1 + AF))/((1 + AF)(h − l)).
- We characterize the regions in which Ep[τF|τF < ∞] increases or decreases with the rate of learning.
- Theorem 1:
    - For fixed rate of learning, there exists a threshold in (a0, aF) such that Ep[τF|τF < ∞] decreases with the rate of learning for p < threshold and increases with the rate of learning for p > threshold. For fixed p ∈ (a0, aF), there exists a threshold in the rate of learning such that Ep[τF|τF < ∞] increases with the rate of learning for rate of learning < threshold and decreases with the rate of learning for rate of learning > threshold. For fixed p < a0, Ep[τF|τF < ∞] decreases with the rate of learning.
- An increase in the rate of learning has two countervailing effects on the follower's time to investment: hasten investment vs. delay investment.
- When p is sufficiently close to aF, the comparative statics of Ep[τF|τF < ∞] is strongly influenced by the comparative statics of aF.
- If p is sufficiently far away from aF, the comparative statics of aF has little effect on Ep[τF|τF < ∞].
- The single-crossing property in p also explains the single-crossing property in the rate of learning because aF strictly increases with rate of learning.
 
## 5.3. The Impact of Learning on the Mixed Strategy Equilibrium
- We investigate the impact of learning on the equilibrium payoff VM and τ_bar_M.
### 5.3.1. Comparative Statics of the Mixed Strategy Equilibrium Payoff
- Now we obtain the comparative statics of VM and aL with respect to the rate of learning under positive and negative externalities.
- Theorem 2:
    - (i) Suppose AL > 0, AF > 0. For p > a0, there exists threshold in the rate of learning such that VM decreases with rate of learning for rate of learning < threshold and increases with rate of learning for rate of learning > threshold. Furthermore, for a fixed rate of learning, there exists threshold ∈ (aL, ac) such that VM increases with rate of learning if aL < p < threshold and decreases with rate of learning if threshold < p < ac. If aL < p < a0, then VM increases with rate of learning for sufficiently large or sufficiently small values of rate of learning.
    - (ii) Suppose AL < 0, AF < 0. For p > a0, there exists threshold in the rate of learning such that VM increases with rate of learning for rate of learning < threshold and decreases with rate of learning for rate of learning > threshold. Furthermore, for a fixed rate of learning, there exists threshold ∈ (aL, ac) such that VM decreases with rate of learning if aL < p < threshold and increases with rate of learning if threshold < p < ac. If aL < p < a0, then VM decreases with rate of learning for sufficiently large or sufficiently small values of rate of learning.
- The single-crossing property of dEp[τF|τF < ∞]/d(rate of learning) provides an intuitive explanation for the single-crossing property of dVM/d(rate of learning).
 
### 5.3.2. Comparative Statics of τ_bar_M(p)
- Now we examine the impact of learning on τ_bar_M and focus on the comparative statics of τ_bar_M for large values of the rate of learning.
- Theorem 3:
    - (i) Suppose AL > 0, AF > 0. For sufficiently high values of p in the interval (aL, ac), τ_bar_M increases with the rate of learning. Furthermore, whenever rate of learning > threshold, there exists q ∈ (aL, ac) such that τ_bar_M decreases with the rate of learning for p ∈ (aL, q) and increases with the rate of learning for p ∈ (q, ac).
    - (ii) Suppose AL < 0, AF < 0. For each fixed value of p, there exists threshold > 0 such that τ_bar_M increases with the rate of learning whenever rate of learning > threshold.
- The comparative statics of τ_bar_M also has a single-crossing property.
- The combined effect of positive externality and an increase in rate of learning may improve the leader’s payoff and hence hasten the firms’ investments.
 
### 5.3.3. Discussion on the Interplay of Externality and Learning
- Overall, the impact of learning on the equilibrium payoffs and the time to the first investment is nontrivial.
- In the WA region, firms will tend to delay their investments with an increased rate of learning because learning tends to benefit the follower.
- Our results show that an increased rate of learning may improve the leader’s payoff and hence hasten the firms’ investments.
- This finding is driven by the following two conditions: (a) The value of the leader’s investment decreases (increases) with the follower’s time to investment under positive (negative) externalities. (b) The follower’s time to investment depends on the learning rate.
 
## 5.4. Case of Second-Mover Advantage
- Next we consider an interesting case when AF > 0 > AL, which represents situations with second-mover advantage.
- First, note that Theorem 1 always holds irrespective of the sign of AF.
- Next, we establish the following:
    - Proposition 5: If AF > 0 > AL, then (aL, 1) is the WA region.
- VM and τ_bar_M have no dependence on the rate of learning for p > aF.
- It is straightforward to prove that the statements of Theorem 2(ii) exactly apply for the case AF > 0 > AL.
- Last, we obtain the following comparative statics of τ_bar_M:
    - Theorem 4: Suppose AF > 0 > AL. For sufficiently high values of p in the interval (aL, aF), τ_bar_M decreases with the rate of learning. Furthermore, there exist threshold > 0 and a function p such that τ_bar_M increases with rate of learning whenever p < p(rate of learning).
- The main effect of the second-mover advantage with different signs of AF and AL is that there is no PE region.
 
# 6. The Impact of Externality
- We briefly discuss the impact of externality.
- We consider the case where AL = AF = A and illustrate examples of numerical comparative statics with respect to A.
- An increase in the externality means an increase in the value of A.
- Theorem: With positive externalities, VM always increases with A.
- With negative externalities, the time to the first investment does not necessarily decrease with A.
- The comparative statics results indicate that a higher degree of positive externality encourages firms to invest earlier. In contrast, the impact of negative externality is more nuanced.
 
# 7. Some Related Models
- In this section, we briefly discuss two related models and check whether our main results hold.
## 7.1. Nonzero Cost of Learning
- We consider the case in which it is costly for the follower to collect information and learn the true market demand in stage 1.
- The game with a nonzero follower's cost of collecting information can be conveniently transformed into another game with a lower investment cost.
- All the results of the previous sections continue to hold for this model.
 
## 7.2. Learning from a Public Signal
- In some cases, the signal of the market demand is exogenous and public.
- We consider a model with learning from a public signal.
- In this model of a purely public signal, a war of attrition never happens.
 
# 8. Conclusions
- Investments in new unproven projects in competitive situations are fraught with uncertainty.
- Returns from such investments are governed by positive or negative externalities from investments made by competing firms.
- Firms often have the opportunity to learn the potential value of investing in similar projects by observing the performance of their competitors’ investments.
- We investigate the impact of learning and externalities on equilibrium investment strategies.
- We find that due to the strategic interactions, externalities and learning opportunities have counterintuitive effects on investment strategies and on the time to the first investment.
- Overall, our results suggest that firms facing entry into an unproven market need to consider the strategic effects arising from the interplay between externalities and learning.
 
# Executive summary of 1. Introduction
- This section introduces the problem of strategic investment under uncertainty, highlighting the roles of learning and externalities.
- It emphasizes the lack of prior research examining the combined effect of these two factors.
- The research questions center on how the follower's investment timing, the leader's payoff, and the time to first investment are affected by the rate of learning in a duopoly setting.
- The core finding of a single-crossing property related to the learning rate is foreshadowed.
 
# Executive summary of 2. Related Literature
- This section positions the paper within the broader literature on investment decisions, focusing on Bayesian learning, externalities, and war of attrition models.
- It emphasizes how the current work builds upon and differentiates itself from existing research.
- Prior studies either address learning or externalities, but not the interplay between them.
- Décamps and Mariotti [2004] and Thijssen et al. [2006] are mentioned as related papers, with specific differences highlighted to justify the novelty of this study.
 
# Executive summary of 3. The Game of Externality and Bayesian Learning
- This section details the model setup, including the duopoly, uncertainty about market demand, the Brownian motion representation of profit, and the Bayesian updating process.
- Key assumptions, including the definitions of positive and negative externalities, are stated.
- The three-stage game is described: waiting, learning, and perpetual profit streams.
- The equations for firms' payoffs under different scenarios (leader, follower, simultaneous investment) are presented.
 
# Executive summary of 4. Classification of Equilibria
- This section focuses on classifying the equilibria under different conditions.
- Pure strategy equilibria are derived, with Proposition 3 outlining the conditions under which firms do not invest, a leader-follower structure emerges, or simultaneous investment occurs.
- The concept of a war of attrition region and preemption region is introduced.
- A mixed strategy subgame perfect equilibrium is characterized, based on Hendricks et al. [1988].
 
# Executive summary of 5. Impact of Learning
- This section presents the core findings related to the effect of the learning rate.
- A benchmark model without externalities is used to establish a baseline intuition (increased learning delays investment).
- Theorem 1 demonstrates the single-crossing property of the follower's investment timing with respect to the learning rate.
- Theorems 2 and 3 describe the impact of learning on the mixed strategy equilibrium payoff and the expected time to the first investment, also highlighting single-crossing properties.
- The section concludes with a discussion of the interplay between externality and learning and an extension to the case of second-mover advantage (Theorem 4).
 
# Executive summary of 6. The Impact of Externality
- This section briefly discusses the impact of the externality parameter (A), assuming that AL = AF.
- The focus is on illustrating the comparative statics of VM and τ_bar_M with respect to A.
- It is shown that the impact of a positive externality is straightforward (higher externality encourages investment), whereas the impact of a negative externality is more nuanced.
 
# Executive summary of 7. Some Related Models
- This section briefly considers two related models: one with a nonzero cost of learning for the follower and another where firms learn from a public signal.
- It is shown that a nonzero cost of learning does not qualitatively change the main results.
- However, in the case of learning from a purely public signal, a war of attrition does not occur, so this model falls outside the scope of the paper.
 
# Executive summary of 8. Conclusions
- This section summarizes the key findings, emphasizing that strategic effects arising from the interplay of externalities and learning need to be considered in competitive investment decisions.
- The main message is that firms facing entry into an unproven market need to incorporate the effect of externalities when modeling a competitive investment problem with learning opportunities as a war of attrition.

</details>

```
title: Retention of capable new employees under uncertainty: Impact of strategic interactions
authors: H. Dharma Kwon, Onesun Steve Yoo
journal: IISE Transactions
published: 2017
```

# Executive Summary

- The study investigates the strategic interaction between a firm and a newly hired employee with initially unknown capability, focusing on the impact of **Bayesian learning** on employee retention.
- **Methodology**: A **stochastic dynamic game model** is employed to analyze equilibrium termination strategies, considering:
    - The firm's option to terminate underperforming employees.
    - The employee's option to leave for better financial opportunities.
- **Key Theoretical Framework**:
    - **Bayesian Learning:** Both the firm and employee observe performance and update their beliefs about the employee's capability over time.
    - **Stopping Time Game:** The firm and employee strategically decide when to terminate the employment relationship, maximizing their own payoffs.
    - **Markov Perfect Equilibrium (MPE):** The study identifies stable strategies where neither player has an incentive to deviate, given the other player's strategy.
- **Core Finding**:
    - *Paradoxical Impact of Learning Rate*: In a region of *sufficiently high* learning rates, *slowing down the learning process* can surprisingly *increase the equilibrium payoff for both* the firm and the employee.
        - This contrasts sharply with cooperative settings, where faster learning invariably benefits both parties.
    - **Rationale**: Slower learning prolongs the employment relationship, allowing:
        - The firm to retain potentially capable employees longer while minimizing the risk of paying unproductive ones.
        - The employee to remain employed longer while maintaining their outside option.
- **Conditional Effects of Learning Rate**:
    - The study also identifies specific scenarios where increasing the learning rate benefits either the firm or the employee *depending on*:
        - The prevailing learning rate.
        - The shared beliefs regarding the employee's capability.
- **Managerial Implications**:
    - The findings suggest active *non-financial strategies* for firms to improve the retention of capable new employees by deliberately controlling the learning rate:
        - Task Assignment: Assigning tasks that reveal employee capability quickly or slowly.
- **Theoretical Contributions**:
    - Presents an instance where *slower information acquisition* improves payoffs for *all players*, even under symmetric information conditions.
        - This contrasts with traditional real options models where increased uncertainty typically enhances value.
- **Robustness Checks**: The findings are shown to be robust to:
    - **Posterior-dependent wages**: Scenarios where employee compensation is tied to the firm's belief about their capability.
    - **On-the-job learning**: Situations where employees can transition from low to high capability over time.
- **Significance**: The research contributes to workforce management literature by:
    - Considering the *strategic behavior of employees*, which is often neglected in traditional models that assume unilateral firm decisions.
- **Numerical Results**:
    - The study includes extensive numerical analyses, which confirm the applicability of the findings (Proposition 4) across a range of learning rate values (σ).
    - This provides insights into comparative statics for intermediate learning rate scenarios.

#bayesian_learning #employee_retention #timing_game #stochastic_dynamic_game #markov_perfect_equilibrium #noncooperative_game #learning_rate #information_asymmetry

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- This article explores the challenges faced by newly hired or promoted employees due to lack of experience and uncertainty about their productivity [Promotion into a new role, Hiring into a different environment, Starting a new career].
- The central question is: Which player benefits from a higher rate of learning about the employee's ability?
- We address this question by modeling the noncooperative interaction between the firm and the employee as a stopping time game, obtaining equilibrium strategies, and analyzing the impact of the learning rate.
- The employee is either capable (high type) or not (low type), and both parties share a common prior belief about the employee's type.
- They update their beliefs in a Bayesian fashion as they observe the employee's contribution to the firm's profit stream.
- At any time, either party can terminate the employment: the firm can dismiss a low-capability employee, or a high-capability employee can quit for better compensation.
- The unique Pareto-dominant Markov perfect equilibrium involves the firm terminating the employee if the posterior belief falls below a lower threshold, and the employee leaving if it exceeds an upper threshold.
- We analyze the effect of the learning rate on equilibrium strategies and payoffs, comparing it to a cooperative Nash bargaining benchmark.
- In contrast to the Nash bargaining solution where higher learning always benefits both parties, the noncooperative setting shows a more nuanced effect: in the region of sufficiently high learning rates, faster learning can decrease payoffs for both.
- This suggests that in a noncooperative setting, slower learning can simultaneously benefit both parties by prolonging the employment relationship and minimizing downside risks.
- Results indicate that firms can increase retention of capable employees through active nonfinancial strategies, i.e., control the rate of learning by assigning tasks that reveal capability quickly or slowly.
- From a game theory perspective, this article contributes to the literature by presenting an example where slower information acquisition may improve payoffs for both players, even with symmetric information [Kreps, 1988].
- The Bayesian framework is based on Shiryaev [1967], and has been applied in various decision-theoretic settings [Ryan and Lippman, 2003; Decamps et al., 2005; Kwon, 2010; Bolton and Harris, 1999; Bergemann and Valimaki, 2000].
- This work differs by applying the framework to a game between two non-rival players in an employment relationship.
- Our key finding contrasts with real options models where value generally increases with uncertainty [Dixit, 1992], due to the noncooperative interaction between firm and employee.
- We complement this line of research by focusing on firm-level dynamics and how the speed of the resolution of uncertainty impacts the retention of newly hired/promoted employees within a firm.

## 1.1. Literature Review Context
- This article contributes to labor economics literature by examining the impact of uncertainty regarding employee type [Johnson, 1978; Viscusi, 1980; Jovanovic, 1979; Freeman, 1977; Papageorgiou, 2014].
- This article also relates to the operational problem of workforce management, where traditional models often assume that firms unilaterally decide on retention/termination [Holt et al., 1960; Bitran et al., 1981; Pinker and Shumsky, 2000; Gans and Zhou, 2002; Bassamboo et al., 2006; Arlotto et al., 2014].
- Unlike those models, we consider the employee's option of voluntarily quitting when making a termination decision.
- Recent research has considered the strategic behavior of workers in service systems [Gurvich et al., 2015], and this work complements these recent lines of research.

# 2. Model
- The cumulative profit contributed by the employee is modeled as a stochastic process **X** = {Xt: t ⩾ 0}, represented by Brownian motion with drift μ (employee's capability) and constant volatility σ (degree to which luck contributes to the profit stream).
- The drift μ is either *h* (high capability) or *l* (low capability), where h > l, and is initially unknown to both parties.
- The prior probability (belief) that the employee is capable is *p*.
- Bayes' rule is used to update the posterior belief Pt based on the observable process Xt.
- The speed of Bayesian updating is proportional to ((h − ℓ)/σ)Pt(1 − Pt), with ((h − ℓ)/σ) interpreted as the **rate of learning** about the employee’s capabilities.
    - *Low (high) σ corresponds to a faster (slower) learning rate*.
- Both firm and employee are risk-neutral with discount factor α < 1.
- The employee earns a fixed wage s and a proportion λ ⩾ 0 of the profit Xt.
- If the employment relationship is terminated, the firm obtains an outside option u, and the employee obtains an outside option W, dependent on the true capability (wh > wℓ).
- The payoffs for the players are the expected values of cumulative discounted profits.
- τf and τe are the discretionary times of separation for the firm and the employee, respectively, and τS = τf∧τe is the time of separation.
- The expected payoffs for the firm and the employee are defined in Equations (1) and (2) in the original text.
- The problem is shown to be equivalent to one with zero sharing of the profit, simplifying analysis.
- The focus is on the regime where the firm wants to dismiss a low-capability employee and retain a high-capability employee ((h − s)/α > u > (ℓ − s)/α), and the employee will quit only if of high capability (wh > s/α > wℓ).

# 3. Benchmark model: Cooperative setting
- As a benchmark, the Nash bargaining solution between the firm and the employee is investigated [Papageorgiou, 2014].
- The cooperative separation decision is made along with a salary structure that depends on the posterior belief [Papageorgiou, 2014].
- This section analyzes the cooperative separation decision of a similar Nash bargaining solution and examine the comparative statics of the payoffs with respect to the learning rate.

## 3.1. Cooperative termination time
- In the cooperative model, the firm and the employee agree on the termination time τ* that maximizes their combined payoff, and they divide the payoff based on the Nash bargaining solution [Papageorgiou, 2014].
- The expression for the total payoff is defined by Equation (3) in the original text, where τ is the agreed stopping time for separation.
- In the Nash bargaining solution, the disagreement points for the firm and the employee are their outside options, u and Ep[W] = pwh + (1 − p)wℓ, respectively.
- *Lemma 1* states that if wh − wℓ − h/α + ℓ/α < 0, there is a lower threshold θd such that is the optimal stopping time that maximizes Vc(p; τ).
- Similarly, if wh − wℓ − h/α + ℓ/α > 0, there is an upper threshold θu such that is the optimal stopping time that maximizes Vc(p; τ).
- Lemma 1 states that in the cooperative model, it is optimal for the firm and employee to agree upfront on the employment separation terms.
- If wh − wℓ − h/α + ℓ/α < 0, the employer’s incentive is accentuated, and the optimal cooperative decision is to terminate when the employee is believed to be of low quality.
- On the other hand, if wh − wℓ − h/α + ℓ/α > 0, the employee’s incentive is accentuated, and the optimal policy is to terminate the employment when the employee is believed to be of high quality.

## 3.2. Effect of learning rate
- The cooperative Nash bargaining solution shows that the problem basically amounts to a conventional optimal stopping problem and that the value function is always nondecreasing with the speed of learning 1/σ.
- Proposition 1 (Benchmark) states that under the cooperative setting, both of the value functions Ve(p) and Vf(p) are always non-increasing with σ.
- Faster learning is better for the centralized (cooperative) decision maker, as the faster gain of information can be exploited to make a better decision earlier.
- *Proposition 2* indicates the form of the payment transfer between the two parties.

# 4. Noncooperative model of a separation game
- The equilibrium of the noncooperative game-theoretic model is investigated to characterize the structure of the best responses of the firm and the employee.
- This section also shows the existence of Markov Perfect Equilibrium (MPE) strategies and payoffs.
- Then it examines how these are impacted by the rate of learning and illustrates how this differs from the benchmark cooperative model.

## 4.1. MPE
- In the noncooperative model, the employment separation time τS is the smaller of the firm’s and the employee’s stopping times τf and τe: τS = min {τf, τe}.
- The firm’s objective is therefore to find a τf that maximizes Equation (Equation1) given the employee’s strategy τe, and the employee’s objective is to find a τe that maximizes Equation (Equation2) given the firm’s strategy τf.
- The focus is restricted to stationary Markov strategies because (i) the posterior process Pt is a Markov process and (ii) neither gf( · ) nor ge( · ) depends on calendar time [Oksendal, 2003].
- *Lemma 2* states that if there exists a best response C*f to a given strategy Ce, then C*f = (θf, 1] for some θf that depends on Ce. Similarly, if there exists a best response C*e to a given strategy Cf, then C*e = [0, θe) for some θe that depends on Cf.
- The firm does not want to terminate a high-capability-type employee, so it waits until the posterior Pt is sufficiently low.
- Similarly, the employee will want to quit the job only if the employee is sufficiently optimistic about his or her capability; hence, the employee waits until the posterior Pt is high enough.
- *Proposition 3* states that a player’s threat increases with the opponent’s threat.
    - (i) The employee’s best response θe is non-increasing with θf.
    - (ii) The firm’s best response θf is non-increasing with θe.
- An equilibrium strategy profile (θ*f, θe*) is one where θ*f and θe* are the best responses to each other.
- An MPE can include an equilibrium strategy profile (θ*f, θe*) that leads to immediate termination of employment—i.e., θ*f ⩾ θe*; this is a degenerate profile.
- *Assumption 1* provides a sufficient condition for the existence of a non-degenerate MPE (i.e., θ*f < θe*).
- Proposition 4 states that Under Assumption 1, there exists an MPE with a strategy profile (θ*f, θe*) that satisfies θ*f < θe*. Furthermore,
    - (i) a unique Pareto-dominant MPE is characterized by the highest ratio θ*e/θf* among all MPEs;
    - (ii) for sufficiently small σ, there exists a unique non-degenerate MPE.

## 4.2. Effects of the learning rate
- The investigation focuses on how the rate of learning affects MPE strategies and payoffs by examining the comparative statics of θ*f, θe* and the MPE payoffs V*f(p) ≡ Vf(p; θ*f, θe*) and V*e(p) ≡ Ve(p; θ*f, θe*) with respect to σ.
- To derive analytical results, the comparative statics for limiting values of the volatility σ (i.e., σ → 0 and σ → ∞) are examined.
- *Proposition 5* Given Assumption 1, the following statements hold:
    - (i) In the limit σ → 0, we have θ*e↑θe0 < 1 and θ*f↓θf0 > 0. Moreover, for sufficiently small values of σ, the MPE payoffs V*f(p) and V*e(p) increase with σ, ∀p ∈ (θ0f, θe0).
    - (ii) In the limit σ → ∞, we have θ*e↓θe∞ ≡ (s/α − wℓ)/(wh − wℓ) and θ*f↑θf∞ ≡ (uα + s − ℓ)/(h − ℓ).
- Part (i) of Proposition 5 examines the region of fast learning rates (or small σ).
- In such settings, both the firm and the employee learn quickly about the latter’s capability.
- Remarkably, in this region of high learning rates, the expected payoffs for both the firm and the employee increase with σ, i.e., lowering the learning rate benefits both parties as long as the prior probability p is within an intermediate regime.
- This result, which is in stark contrast with that for the cooperative benchmark model, is a direct consequence of the noncooperative interaction between the firm and the employee.
- Part (ii) of Proposition 5 examines the region of slow learning rates (or large σ).
- Under this condition, both the firm and the employee will learn very slowly about the employee’s capability.
- For higher values of p, V*f(p) increases with σ, whereas V*e(p) decreases with σ.
- For lower values of p, V*f(p) decreases with σ, whereas V*e(p) increases with σ.
- The comparative statics of the payoffs for the firm and the employee have a rich structure with respect to the learning rate, and they exhibit stark differences between the cooperative model and the noncooperative model.
- The comparative statics results are summarized in Table 1 in the original text.
- To better understand intermediate regions of σ, extensive numerical studies are conducted.
- In conclusion, the numerical analysis reveals that Proposition 4 applies to nontrivial regions of σ, and it provides important insights regarding the comparative statics for intermediate regions of σ.

# 5. Extensions and robustness of the main results
- In this section, the robustness of key results is checked against the posterior dependence of the wage and the potential improvement of the employee’s capability through on-the-job learning.

## 5.1. Posterior dependent wage
- In this subsection, the assumption that the firm pays the employee a fixed salary is relaxed, and the firm is allowed to pay the employee more if the employee is believed to be capable.
- Suppose that the wage is given by s(p) = s0 + s1p for some s1 > 0.
- The main results of this article continue to apply to this case as long as h − s1 − ℓ > 0 and wh − s1/α − wℓ > 0.
- A wage that increases with Pt does not affect the main results, provided that s1 is not unreasonably high.
- Results are robust to profit-sharing compensation plans as given by Equations (Equation1) and (Equation2).
- More complicated nonlinear posterior-dependent wages can be incorporated in the model in a similar manner, although general analytical expressions may not be available.
- V*e(p) and V*f(p) assuming a logarithmic wage with respect to σ and with respect to p are plotted in Figure 2 in the original text.
- Comparison reveals that the main results (Proposition 4) of this article continue to hold, even in the case of a nonlinear wage schedule.

## 5.2. Impact of on-the-job learning
- In this subsection, the assumption that a low-quality employee remains low quality throughout the duration of the employment relationship is relaxed, and a low-quality employee is allowed to become a high-quality one through on-the-job learning.
- We assume that the transformation takes place at a random exponential time with an arrival rate of η > 0.
- The posterior probability that a given employee is of high quality can be expressed as where p = P0 is the initial posterior.
- V*e(p) and V*f(p) with arrival rate η = 0.5 with respect to σ and with respect to p are plotted in Figure 3 in the original text.
- Comparison demonstrates that the main results of this article (Proposition 4) continue to hold once again.

# 6. Discussion
- This section discusses the implications of our research.
- *First*, from a theoretical perspective, the discovery of an atypical example of a game in which a faster learning rate (more information per unit time) can decrease the payoff for both players.
- *Second*, results point to an active nonfinancial strategy that firms can employ to increase their retention of capable employees.
- Firms can control the learning rate σ by assigning their new employees to tasks in which their capabilities will be revealed more quickly or more slowly.
- If the task is relatively volatile—i.e., the value of σ is high—then firms should next examine the prevailing values of p.

# 7. Conclusions
- This article examined the setting in which an employee is hired into a new role and both the firm and the employee must learn about the employee’s capability.
- The analysis provides counterintuitive and practical insights into how firms can manage noncooperative interactions and retain capable employees.

---

# Executive summary of 1. Introduction
- The introduction sets the stage for examining the dynamics between firms and newly hired employees, marked by uncertainty about the employee's capability and the strategic decisions both parties make.
- **Key question:** Does faster learning always benefit both the firm and the employee?
- The study introduces a **stopping time game** framework to model the noncooperative interactions, highlighting the tension between the firm's desire to retain capable employees and the employee's option to pursue better opportunities.
- *Counter-intuitive finding*: Faster learning rates do not always lead to better outcomes for both parties.
- It emphasizes the practical implications for firms in terms of managing the learning environment to improve employee retention.

# Executive summary of 2. Model
- The model section formalizes the interactions between the firm and the employee using a **Brownian motion** to represent the employee's contribution to profit.
- **Bayesian updating** to model how both parties learn about the employee's capabilities over time.
- The learning rate, determined by the **signal-to-noise ratio**, is a central parameter.
- The model incorporates outside options for both parties and defines their payoffs as expected discounted profits.
- Simplifications are made to analyze the equilibrium, focusing on scenarios where the firm aims to retain capable employees and the employee seeks better opportunities elsewhere.

# Executive summary of 3. Benchmark model: Cooperative setting
- This section introduces a cooperative **Nash bargaining** model as a benchmark to contrast with the noncooperative setting.
- In the cooperative setting, the firm and employee agree on a termination time that maximizes their combined payoff.
- *Lemma 1* outlines the optimal termination policy based on whether the profit stream or the employee's outside option dominates.
- *Proposition 1* highlights that in a cooperative scenario, *faster learning is always beneficial for both parties*, as it enables better decision-making.

# Executive summary of 4. Noncooperative model of a separation game
- This section delves into the **noncooperative game-theoretic model**, where the firm and employee make independent decisions about termination.
- It characterizes the best responses of the firm and the employee and demonstrates the existence of **Markov Perfect Equilibrium (MPE)** strategies.
- *Proposition 3* states that *a player's threat of termination increases with the opponent's threat*, highlighting the strategic interplay.
- Proposition 4 establishes the existence of an MPE under certain conditions, where the firm's and employee's thresholds for termination are distinct.
- Proposition 5 reveals that in the region of high learning rates, the payoffs for both parties *increase with the volatility*, which is a contrasting effect to the cooperative benchmark.
- This slower learning benefits both parties by prolonging the employment relationship and minimizing the downside risks.
- Numerical studies are conducted to better understand the effects of the learning rate for intermediate values of volatility.

# Executive summary of 5. Extensions and robustness of the main results
- The model's robustness is tested by considering **posterior-dependent wages** and **on-the-job learning**.
- The analysis shows that the main results continue to hold even with linear and nonlinear wage contracts.
- Allowing for on-the-job learning, where low-quality employees can transform into high-quality ones, does not change the results.

# Executive summary of 6. Discussion
- The discussion section emphasizes the key theoretical and practical implications of the research.
- The research contributes an atypical example of a game where *faster learning rates can decrease payoffs for both players*.
- This indicates that firms can actively influence employee retention by carefully managing the rate at which an employee’s capability is revealed.
- Depending on whether the task is volatile or not, the firm can encourage the employee to take on more or less risk to benefit both the firm and the employee.

# Executive summary of 7. Conclusions
- This article concludes by summarizing the setting of the study, where both firm and employee must learn about the employee's capability.
- The analysis provides counterintuitive and practical insights into how firms can manage noncooperative interactions and retain capable employees.

</details>

# Mili Mehrotra
- Associate Professor of Business Administration 
- Academic Director of Supply Chain Management Major 
- Academic Director of Illinois Supply Chain Management Program
### education
- M.S., Supply Chain Management, University of Texas at Dallas, 2010
- Ph.D., Operations Management, University of Texas at Dallas, 2010
- M.S., Mathematics, Banaras Hindu University, 2002
- B.S., Mathematics, Banaras Hindu University, 2000
### research interest (chatgpt says...)
- Healthcare Operations and Incentive Design
- Resource Allocation and Contract Design
- Supply Chain Disruptions and Logistics
### teaching
- Supply chain optimization and simulation modeling

```
title: An Analysis of Incentive Schemes for Participant Retention in Clinical Studies
authors: Xueze Song, Mili Mehrotra, Tharanga Rajapakshe
journal: Manufacturing & Service Operations Management
published: 2023
```

# Executive Summary

- This study explores how to best keep participants engaged in **clinical trials**, a major headache for medical research. We look at the economic benefits of using **monetary rewards** combined with **practical support** to keep dropout rates low.
- We build a **mathematical model** to figure out the best way to balance **payments** to participants and the amount of **effort** the clinic puts in, taking into account what we know (or don't know) about the people taking part.
- We develop **easy-to-use computer programs** that find the ideal solution when we have lots of information. These programs work for personalized payment plans, as well as common reward systems like: **Fixed Payment (FP)** (everyone gets the same) and **Logistics Reimbursement (RE)** (covering travel costs).
- We also create methods for situations where we *don't* know everything about the participants, focusing on the **FP** and **RE** systems.
- By running lots of computer simulations, we learn how the different reward systems compare.
    - With complete information, a **personalized payment approach** cuts retention costs by about 46% compared to the standardized **RE** and **FP** systems.
    - If the clinic *lacks* information on participant characteristics, the **RE** system becomes a better choice than **FP** in a broader range of situations. It makes sense because participant logistics are known.
    - Knowing more about the participants is *much more* helpful when using the **FP** system than when using the **RE** system.
- **Key Takeaways**
    - **Effort Matters:** When participants are similar in how much inconvenience bothers them, using a **fixed payment** usually means the clinic has to put in *more effort* overall than if it could tailor payments to each person.
    - **RE Can Be Risky:** The **logistics reimbursement** system can sometimes be a *really bad* option compared to personalized plans. A **fixed payment**, on the other hand, has limits to how poorly it can perform.
    - **Fixed Payment Benefits:** If participants are all in the same boat when it comes to travel costs, the clinic will save money by using a **fixed payment** over just reimbursing expenses.
- **Specifics from the Computer Study**
    - **Low Health Improvement:** If the participants aren't expecting a big health boost from the trial, **reimbursing logistics** is better if the clinic can provide practical support cheaply. **Fixed Payment** is better if that support is expensive.
    - **High Health Improvement:** As participants expect *more* health benefits, the tables turn. Now **Fixed Payment** is usually the better bet, *especially* if offering practical support doesn't break the bank.
    - **High Retention Needed:** The **logistics reimbursement** gets more attractive when the clinic *really* needs to keep almost everyone enrolled.
    - Ditching the **FP** or **RE** systems for a *personalized* approach, on average, reduced costs by 45% and 48% respectively. That's a significant advantage!
    - If a clinic adopts the **RE** scheme, obtaining participant-specific information has less cost reduction benefit (6%) than under the **FP** scheme (13%).

#participant_retention #clinical_studies #incentive_schemes #fixed_payment #logistics_reimbursement #information_asymmetry #integer_programming #optimization #effort #healthcare_operations

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Clinical studies, which are research investigations involving human subjects to evaluate medical interventions, procedures, or behavioral changes [ClinicalTrials.gov 2020], are a significant market with continuous growth [Grand View Research 2022].
- However, clinical studies face challenges like globalization, regulatory barriers, and participant recruitment and retention [Patlak et al. 2008, Institute of Medicine 2010]. We focus on retaining enough participants until the completion of the study.
- Participant retention is a major issue, as approximately 15%-40% of participants drop out [Nuttall 2012, Atlant Clinical 2020].
- Dropouts can lead to insufficient data, attrition bias, and significant expenditures [Dumville et al. 2006, Lopienski 2015, Moore et al. 2018].
- Several factors affect attrition, including lower expected benefits, inconvenience at study sites, and travel/logistics expenses [National Research Council 2010].
- We observe the use of two mechanisms in practice to deal with these factors: monetary incentives (Fixed Payment Scheme, Logistics Reimbursement Scheme) and effort to reduce inconvenience [National Research Council 2010, Booker et al. 2011, Lopienski 2015].
- Incentives are generally acceptable, but excessive incentives can be viewed as undue influence [HHS.gov 2019]. The FDA recognizes reimbursement for travel as not undue influence [U.S. FDA 2018].
- In practice, the selection of incentive schemes and payments is often *ad hoc*. Medical ethics literature recommends further research to identify appropriate payments [Bentley and Thacker 2004, Grady et al. 2005, Ripley 2006, Resnik 2015, Parkinson et al. 2019].
- There is a lack of economic analysis of effort and monetary payments. The relative effectiveness of retention mechanisms is largely unknown [Booker et al. 2011].
- Therefore, we analyze the economic impact of combining monetary payments and effort to achieve a target retention rate while minimizing retention cost.

## 1.1. Research Questions
- Given a target retention rate and an incentive scheme, what is an **optimal combination** of **incentive payments** and **effort** to **minimize the retention cost**?
- How do the two commonly observed incentive schemes (**fixed payment** and **logistics reimbursement**) perform relative to the **optimal incentive scheme** and to each other?
- We consider the interaction between a clinic and participants with a two-stage decision-making model.
    - In the first stage, the clinic decides its effort level and the incentive payments to the participants, aiming to minimize its cost of retaining a target number of participants.
    - In the second stage, each participant decides whether to complete or leave the study based on their utility, which consists of expected improvement in health, monetary payment, logistics cost, and inconvenience cost.
- Using this framework, we conduct theoretical and numerical analyses to:
    - Identify optimal incentive payments and effort level for a clinical study under **full information**.
    - Understand the performance of commonly observed incentive schemes compared with the full information benchmark and relative to each other.
    - Study the **value of information** for the clinic when it does not observe participants’ characteristics.

# 2. Literature Review
- Our work contributes to advancing clinical studies from an operations-management perspective.
- Recent research in operations management focuses on statistical testing [Goh et al. 2018, Bertsimas et al. 2019, Bertsimas and Sturt 2020], scheduling of participants' visits [Colvin and Maravelias 2010], and clinical trial supply chain management [Fleischhacker and Zhao 2011, Fleischhacker et al. 2015].
- Some papers study participant-recruitment issues:
    - Kouvelis et al. [2017] analyze opening testing sites and recruiting patients for each site.
    - Tian et al. [2022] incorporate belief updates on drug quality and patient dropouts.
- Our work differs by explicitly considering participants' decisions to complete the study and analyzing the efficacy of different mechanisms (incentive payments and effort) in improving participant retention.
- Our work is also connected to literature on adoption and design of economic mechanisms and incentive schemes to promote voluntary actions.
- Giuffrida and Gravelle [1998] consider financial incentives to encourage patients' compliance with medical advice.
- Several research papers focus on designing incentive schemes to motivate participation in voluntary wellness programs:
    - Charness and Gneezy [2009] study the impact of monetary incentives on gym attendance.
    - Acland and Levy [2015] show that subjects receiving incentives overpredict future attendance.
    - Carrera et al. [2020] consider the impact of constant, front-loaded, sporadic incentive payments over time.
    - Calzolari and Nardotto [2017] study whether sending reminders improves gym attendance.
- The above studies examine the impact of these two mechanisms (monetary incentives or effort) in isolation, they do not consider the cost or optimize the payment or effort. We identify an optimal combination of incentive payment and effort level to achieve the target retention rate, while minimizing the associated cost.

# 3. The Model Setting
- We consider a clinic conducting a Phase II/Phase III clinical study to test the efficacy of a treatment.
- We model a single-shot participant-retention problem where the clinic has a target retention rate *δ* ∈ (0, 1] and an enrollment number *N* of participants. The clinic must retain at least *δN* participants for successful completion.
- To achieve its goal of retaining *δN* participants, the clinic can offer monetary incentives (*P*, *P* ≥ 0 per participant) to participants who complete the study and exert effort *e*, *e* ∈ [0, 1] to reduce inconvenience experienced by the participants during the study.
- A participant’s characteristics are her **health condition**, **logistics value**, and **inconvenience cost**.
- The interaction between the clinic and participants is modeled using a two-stage sequential decision-making framework:
    - Stage 1: Given *N*, *δ*, and an incentive scheme, the clinic determines the incentive payment and the amount of effort to exert, aiming to minimize the total cost of retaining *δN* participants (retention cost).
    - Stage 2: The participants decide whether to complete or leave the study before it ends, depending on the clinic’s decisions and their characteristics.

## 3.1. A Participant’s Decision: To Complete or to Leave
- We capture the **utility** (i.e., expected benefit) of the participants from completing the study after recruitment.
- Several factors impact a participant’s utility:
    - **Improvement in health condition**: The magnitude of the improvement depends on the expected effectiveness of the treatment and participant *i*'s initial health condition.  We consider that the improvement in health condition depends on the effectiveness of the treatment and on the participant's initial health condition. The higher the effectiveness of the treatment, the higher the improvement in health condition.  We use a factor to translate the health condition improvement to monetary benefit.
    - **Logistics cost**: The logistics cost includes total distance traveled and overnight stays during the study.  The cost depends on the logistics value for participant *i* and the unit logistics cost.
    - **Inconvenience cost**:  We consider the monetary value of inconvenience for participant *i*. The inconvenience cost decreases with the effort *e* exerted by the clinic.
    - **Monetary incentive**: Participant *i* receives a monetary payment upon completion.
- Given the incentive payment and effort level of the clinic, each participant weighs the benefit of health improvement and monetary incentive against the burden of logistics and inconvenience costs. If the net benefit (utility) is positive, the participant completes the study.

## 3.2. The Clinic’s Cost Components
- The clinic’s cost consists of:
    - Total incentive payments to participants who complete the study.
    - The effort cost, which increases with effort. The increase in effort cost means that exerting some basic efforts cost less, whereas efforts to improve data-collection methods and staff availability cost more.

# 4. Analysis under Full Information
- We obtain the optimal decisions (incentive payment for each participant and the amount of effort to exert) for the clinic’s cost-minimization problem under full information.
- The clinic knows participants’ characteristics when designing the incentive payment.
- We refer to the optimal solution under the full information case as the full information benchmark, which serves as a benchmark to analyze the performance of two incentive schemes (Logistics Reimbursement and Fixed Payment) and understand the impact of information asymmetry on the participants’ retention.

## 4.1. Full Information Benchmark
- We first focus on the clinic’s problem where the clinic makes customized payments to each participant.  That is, the clinic offers personalized incentives based on their specific characteristics.
- The clinic decides participant-specific incentive payment based on each participant’s utility function.
- The objective of the clinic is to minimize the total cost to retain participants by deciding how much payment for each participant, and how much effort to exert. The total cost include the payments and the effort. There is also constraint to ensure the clinic retains enough (at least certain number) participants.
- To derive the optimal payment, we first partition the range of effort level into several intervals, then within each intervals, we rank participants in descending order of their utility in the absence of any incentive payment. After that, we only need to find the optimal effort level within each intervals to find the effort and incentive payments that would result in the minimum cost for the clinic.
- The optimal payment to participants can be determined as the payment to ensure that all the targeted participants are just willing to participate. By solving that payment, we get the optimal effort and incentive payment for each participants.

- We need to make an assumption for full information benchmark that in the absence of any incentive payment and any effort, the number of participants who complete the study is less than the target number of participants to retain.

- The optimal effort level and the optimal incentive payment to participant can be obtained by the above approach. In this approach, we also solve an algorithm (ALGORITHM OPT-R) that obtains the optimal solution in polynomial time.

- In the full information benchmark, the clinic offers personalized incentive payment to each participant. This flexibility allows the clinic to target each participant independently and retain participants, while minimizing its cost. However, this solution can be challenging to implement in practice. Because this solution requires the administrative staff to communicate with participants during invitation regarding their personalized prospective payment, make participant-specific disbursement, and keep a record of and verify the expenditure in the study. Further, it may not be easy to explain a participant-specific scheme to participants.

## 4.2. Analysis of the Commonly Observed Incentive Schemes
- Next, we study the incentive schemes that are easier to implement in practice: the Logistics Reimbursement (RE) scheme and the Fixed Payment (FP) scheme.

### 4.2.1. Logistics Reimbursement (RE) Scheme
- Under the RE scheme, the clinic reimburses the logistics cost incurred by participant upon completion of the study, that is, incentive payment is equal to logistics cost.
- This incentive payment does not depend on the effort level. Consequently, the clinic now decides the effort level alone. The clinic chooses the effort level to reduce the total cost and retain enough participants.
- The utility function of participant reduces, that is, the benefit from participating for participants depend on the the treatment, inconvenience cost, the effort from the clinic.
- With RE scheme, if the clinic exerts more effort, it will retain more participants. The participation increase when the effort increases.

- The optimal effort level in RE scheme is the minimum effort level such that there are enough participants who complete the study. That is, we need to find the minimum value such that a participant get enough benefit from participating in the study. We also solve an algorithm (ALGORITHM OPT-RE) that finds the optimal solution under the RE scheme in polynomial time.

- Although the RE scheme also offers a customized payment to each participant, implementing such an incentive scheme is relatively easy. The clinic can announce the payment by informing the participants that all logistics costs incurred will be reimbursed. Also, the administration staff can reimburse the cost based on the receipts that participants provide.

### 4.2.2. Fixed Payment (FP) Scheme
- Under the FP scheme, the clinic offers the same amount of payment to each participant who completes the study. The clinic’s decisions now reduce to identify optimal values of fixed payment and effort level.  That is, every participant receive the same amount of payment when they complete. The clinic needs to decide what is the same amount and what level of effort to exert to reduce the total cost.

- We also design an algorithm (ALGORITHM OPT-FP) that finds the optimal solution under the FP scheme in polynomial time.
-  Under the FP scheme, the structure of the optimal solution is similar to that under the full information benchmark. However, it differs from the full information benchmark in that the optimal payment each participant receives now depends on the characteristics and decisions of other participants. Further, compared with the full information benchmark, the participants generally receive a positive utility when completing the study under the FP scheme, as the clinic is unable to offer participant-specific payments.

- Under the FP scheme, the clinic announces a fixed payment to all the participants, and, therefore, it is easy to implement in practice.

### 4.2.3. A Theoretical Comparison of the Incentive Schemes
- We next compare the clinic’s decisions and retention costs across different schemes.
- When participants have the same inconvenience cost parameter, the optimal effort level under the FP scheme is always higher than that under the optimal effort level for the full information benchmark.
That is, if the the annoyance for participating the study are same, the clinic need to exert more effort to attract participants.

-  When participants have the same logistics value, the FP scheme always achieves the target retention rate at a lower cost than the RE scheme.
That is, if the logistics burden are same for each participants, the clinic can lower cost by applying FP scheme than RE scheme.

# 5. Computational Study
- Our objective is to understand how the characteristics of the participant population, the clinic, and the clinical study affect the clinic’s optimal decisions and the relative performance of the RE and FP schemes.

## 5.1. The Test Bed
- We use publicly available data to calibrate the parameters in the experiments.

### 5.1.1. Expected Health Improvement
- We consider three different relationships between the health condition and expected health improvement: (1) Expected health improvement is decreasing in the health condition; (2) expected health improvement is increasing in the health condition; and (3) expected health improvement does not change with the health condition.
- To study the impact of participant’s health condition, we vary the distribution of participant’s health condition by choosing different value for mean and variance.

### 5.1.2. Logistics Cost
- We focus on the travel cost and assume that the study involves several visits.
- We change the average total travel distance.
- We use a number to indicate the cost for per mile.

### 5.1.3. Inconvenience Cost
- We consider two kinds of participants. One kind of participant feels less annoyance, and the other kind of participant feels more annoyance.
- We study what is the relationship between these kinds of participants.

### 5.1.4. Clinic’s Effort Cost
- We use a quadratic functional form to represent how much the clinic need to pay.

### 5.1.5. Retention Rate
- We consider different value for the target retention rates.

## 5.2. Results and Managerial Insights: Impact of Participant Characteristic
- The retention cost under each scheme increases with average participant health condition. If the average health level is high, the treatment will become less effective. To retain enough participant, the clinic should exert more effort.
- The clinic always exerts more effort under the FP scheme than that under the full information benchmark. The clinic need to exert more effort under FP scheme because it offers the same amount for every participant.
- The RE scheme outperforms the FP scheme at moderate levels of average participant health condition, whereas the FP scheme outperforms the RE scheme at low and high values of average participant health condition.

## 5.3. Results and Managerial Insights: Impact of Clinic Characteristic
- When the effort cost parameter increases, the retention cost increases for all schemes. The effort is a burden.
- The optimal retention cost for the FP scheme is lower than that under the RE scheme for low and high values of the effort cost parameter.

## 5.4. Results and Managerial Insights: Impact of Study Characteristics
- When the retention rate increases, the clinic increases the incentive payments and effort to retain more participants. The retention cost increases for all schemes. That is, the more the clinic want to attract participants, the clinic should exert more effort.
- As the retention rate increases, the FP scheme loses its competitiveness against the RE scheme, on average.
- The impact of the increase in the retention rate is moderated by expected treatment effectiveness. The FP scheme performs better than the RE scheme for higher values of expected treatment effectiveness. That is, if the treatment are already effective, the clinic need not exert to much effort.

## 5.5. A Summary of the Relative Performance of the RE and FP Schemes
- When the expected health improvement is high, the FP scheme always outperforms the RE scheme.  That is, to use FP scheme is enough when the treatment is enough effective.
- When the expected health improvement is medium, the performance of the RE scheme relative to the FP scheme improves with effort cost parameter.
- When the expected health improvement is low, the FP scheme performs better than the RE scheme for large values of the effort cost parameter.

## 5.6. Performance Gaps for the FP and the RE Schemes
- We find that the FP scheme performs better than the RE scheme, on average.
- There is a significant gap between the retention cost of scheme and that of the full information benchmark—on average, the full information benchmark provides higher performance than both RE scheme and FP scheme.
- We consider a hybrid scheme that combines RE and FP scheme and find that the hybrid scheme can achieve higher performance.

# 6. Analysis under Information Asymmetry
- We analyze the clinic’s cost-minimization problem to retain participants under information asymmetry.
- The clinic does not know the participants type but has belief on what distribution that participant follows.

## 6.1. The RE Scheme
- In the RE scheme, a clinic will exert how much effort to lower the retention cost and attract participants.
- That is, we need to find what is the minimum effort such that a participant can derive enough benefit from participating.
- The more the clinic want to exert, the more the clinic should pay.

## 6.2. The FP Scheme
-  The clinic will decide fixed payment to participants and exert how much effort to attract participants.
- The clinic will try to find the values for each of them and lower the retention cost.

## 6.3. Computational Analysis
- Our goal is to find value of information, find relative performance of the two schemes under information asymmetry, and find out how information asymmetry influence clinic's decision.

### 6.3.1. Value of Information
- The value of information is higher for FP scheme because information is more relevant.

### 6.3.2. Relative Performance of the RE and FP schemes
- The FP scheme becomes less favorable than the RE scheme when the clinic cannot observe the participants’ characteristics.

### 6.3.3. Impact of Information Asymmetry on the Clinic’s Choice of a Scheme
- However, when the clinic prefers the FP scheme, it significantly benefits from acquiring more information and optimizing its incentive payments and effort.

# 7. Conclusions and Discussions
- We solve the clinic’s problem under full information and identify optimal incentive payments and optimal effort.
- We also analyze two commonly observed schemes—namely, the RE scheme and the FP scheme.
- We extend our analysis under information asymmetry.
- Our paper provides insights into the relative effectiveness of the RE and FP schemes.
- The RE scheme performs better when the expected improvement and the clinic’s effort cost parameter are low or when expected improvement is moderate and the clinic’s effort cost parameter is high. Otherwise, the FP scheme performs better than the RE scheme. The performance of the RE scheme improves with the retention rate.
- The FP scheme outperforms the RE scheme when the participants are homogeneous in their logistics values.
- Our work also highlights the importance of offering a participant-specific incentive payment.
- The computational study suggests that by offering an optimal participant-specific incentive payment, the clinic can reduce its cost, that is, the flexible pay scheme are better.
- Our analysis shows that the RE scheme becomes more favorable than the FP scheme under information asymmetry. Further, the value of acquiring information is higher under the FP scheme.

---

# Executive summary of 1. Introduction
- This section introduces the challenge of **participant retention** in **clinical studies**, highlighting its economic and scientific implications.
- It discusses the factors affecting participant attrition, such as **expected benefits**, **inconvenience**, and **logistics expenses**, and introduces two common mechanisms used in practice: **monetary incentives** (fixed payment and logistics reimbursement) and **effort** to reduce inconvenience.
- The section also raises the ethical considerations surrounding incentive payments and emphasizes the lack of economic analysis of these mechanisms in the existing literature.
- The research questions are defined: determining the optimal combination of incentive payments and effort to minimize retention costs, and evaluating the relative performance of fixed payment and logistics reimbursement schemes.

# Executive summary of 2. Literature Review
- This section places the current research within the context of existing literature on **clinical studies**, **operations management**, and **incentive schemes**.
- It differentiates the current study from previous works by explicitly considering participants' decisions to complete the study and analyzing the efficacy of different mechanisms in improving participant retention.
- It also highlights the novelty of the current study in identifying an optimal combination of incentive payments and effort levels to achieve the target retention rate while minimizing the associated cost.

# Executive summary of 3. The Model Setting
- This section describes the **two-stage decision-making framework** used to model the interaction between a **clinic** and **participants** in a **clinical study**.
- It details the factors that influence a participant's decision to complete or leave the study, including **improvement in health condition**, **logistics cost**, **inconvenience cost**, and **monetary incentive**.
- It also defines the clinic's cost components, which consist of total incentive payments and the effort cost.

# Executive summary of 4. Analysis under Full Information
- This section focuses on obtaining the **optimal decisions** (incentive payment for each participant and the amount of effort to exert) for the clinic's **cost-minimization problem** under **full information**.
- It introduces the **full information benchmark** and discusses the implementation challenges associated with it.
- The section also analyzes the clinic's decisions under two commonly observed incentive schemes: the **logistics reimbursement (RE) scheme** and the **fixed payment (FP) scheme**.
- Finally, it provides a theoretical comparison of the incentive schemes and derives bounds on the optimal retention costs under the RE and FP schemes.

# Executive summary of 5. Computational Study
- The aim of this section is to understand how the **characteristics** of the **participant population**, the **clinic**, and the **clinical study** affect the clinic's optimal decisions and the relative performance of the RE and FP schemes.
- It describes the test bed used for the computational study, including the parameters considered and the ranges of values explored.
- The section presents the results and managerial insights obtained from the computational study, focusing on the impact of participant characteristics, clinic characteristics, and study characteristics on the clinic's retention cost and optimal decisions.

# Executive summary of 6. Analysis under Information Asymmetry
- This section analyzes the clinic's cost-minimization problem under information asymmetry, where the clinic does not observe participants' characteristics.
- It discusses the RE and FP schemes under information asymmetry and presents the results of a computational analysis conducted to obtain the value of information for the clinic, understand the relative performance of the RE and FP schemes, and analyze the impact of information asymmetry on the clinic's choice of a scheme.

# Executive summary of 7. Conclusions and Discussions
- This section summarizes the key findings and insights of the paper, highlighting the relative effectiveness of the RE and FP schemes and the importance of offering a participant-specific incentive payment.
- It also discusses the limitations of the study and suggests directions for future research, such as building a continuous or discrete time dynamic model to analyze incentive schemes for studies where the clinic can change its effort with every batch or in every time period.

</details>

```
title:   Value of Combining Patient and Provider Incentives in Humanitarian Health Care Service Programs
authors:   Mili Mehrotra, Karthik V. Natarajan
journal:   Production and Operations Management
published:   2020
```

#   Executive Summary

-   This research examines how humanitarian organizations can best design incentives to improve healthcare programs in developing countries.
-   The study emphasizes that combining incentives for healthcare providers (supply-side) and patients (demand-side) leads to significantly better results than focusing on just one or the other.
-   **Theoretical Framework**:
    -   The research uses a **three-stage model** to understand how the organization, providers, and patients interact.
    -   It also uses **Donabedian's model** to look at healthcare quality, focusing on the resources available (**structural quality**) and how care is delivered (**process quality**).
-   **Key Findings**:
    -   Combining incentives can boost program success dramatically, sometimes up to 20 times more than using only provider or patient incentives.
    -   Even when using only provider or patient incentives, it's essential to match the incentive to the kind of healthcare being provided.
        -   For programs focused on things like equipment and staffing (structural quality), provider incentives work better.
        -   For programs focused on how patients are treated and involved (process quality), patient incentives are more effective.
    -   To get the most out of funding, it's crucial to have the right incentives in place for both providers and patients.
    -   Switching to a combination of provider and patient incentives can lead to better care for more people, even if the organization has less money.
-   **Specific Findings**:
    -   Giving more money to patients always leads to more patients seeking care when only patient incentives are used.
    -   Giving more money to providers also generally leads to better care when only provider incentives are used.
    -   However, when using both provider and patient incentives, simply giving more money to both isn't always the best strategy. Sometimes, giving more to patients and less to providers works better.
    -   When only provider incentives are used, the amount of money given to providers should be lower when the program focuses more on structural quality.
    -   The relationship between how much a program focuses on process quality and how much money providers get is complicated. It depends on how much better resources help providers see more patients.
    -   The advantage of using combined incentives over only provider or patient incentives gets bigger as the program's budget increases.
    -   Having providers who can handle a larger number of patients makes additional funding more valuable.
    -   When programs focus on structural quality, provider incentives become less effective as providers' capacity increases. Patient incentives become more effective as providers' capacity increases, regardless of the program's focus.

#incentive\_design #healthcare\_services #humanitarian\_operations #supply\_side\_incentives #demand\_side\_incentives #structural\_quality #process\_quality #developing\_countries #program\_performance #budget\_constraints

<details>
    
  <summary>Click to expand sections</summary>

#   1. Introduction

-   Over the last twenty years, there's been a huge increase in attention to health problems in developing countries, with a lot more money being spent to improve health \[Chang et al., 2019].
-   While this extra money has helped improve some health measures, the progress has not been consistent, and many countries haven't reached the health goals set by the United Nations \[The World Bank, 2015].
-   Often, the biggest problems are on the **supply side**, meaning there aren't enough good quality healthcare services, facilities, or trained healthcare workers.
-   Historically, efforts to improve health programs in developing countries have concentrated on **supply-side interventions** \[Ensor and Cooper, 2004].
-   However, problems on the **demand side**, like people not knowing about services or being unable to afford the time to get care, are also very important.
-   If these demand-side problems aren't addressed, efforts to improve healthcare can have limited success.
    -   For example, in India, improving health centers led to more births in hospitals for villages close to the centers, but not as much for villages far away \[Amudhan et al., 2013].
-   But just focusing on the demand side isn't enough either.
-   If you get more people to want healthcare but the system doesn't have the resources to provide it, it doesn't lead to better health.
    -   For example, in Sierra Leone, when they announced free healthcare for pregnant and breastfeeding women, the system couldn't handle the increased demand \[Fofanah, 2010].
-   To improve health, you need to both increase the availability and quality of healthcare services and encourage people to use them.
-   However, organizations that provide healthcare in developing countries often have limited budgets.
-   This means they have to decide how to divide their limited funds between improving services and providing incentives for people to use those services.
-   This study looks at a situation where an organization with a limited budget is trying to provide a particular healthcare service.
-   The organization wants to maximize how many people are getting good quality care, even with limited money.
-   **Quality** in healthcare can be defined in different ways. One important way is the framework developed by Donabedian (2005), which has three main parts:
    -   **Structure**: This refers to the physical resources and organization of care, like equipment, supplies, staff, and management. These resources create the environment for good care but don't guarantee it.
    -   **Process**: This is about how healthcare is delivered, including how providers interact with patients, whether care is appropriate and complete, whether guidelines are followed, and whether patients are given information and involved in decisions.
    -   **Outcome**: This is the end result of healthcare, like recovery rates or death rates. Outcomes are influenced by many things besides medical care and can be difficult to measure \[Donabedian, 2005; Boyce, 1996; Hayford and Maeda, 2017].
-   Most healthcare programs, and this study, focus on **structure** and **process** \[Gertler and Vermeersch, 2012; USAID, 2016b,c].
-   In many developing countries, healthcare is provided by private organizations.
-   Increasingly, organizations are using **performance-based contracts** to pay these providers, instead of just paying them for their time \[The AIDSTAR-2011, 2011; World Bank Group, 2015].
-   This study assumes that the organization evaluates providers based on structure and process quality and pays them accordingly.
-   The study focuses on **linear performance-based payment systems**, where providers are paid a set amount for each patient, adjusted for the quality of care they provide.
    -   These systems are common in healthcare programs \[Gertler and Vermeersch, 2012; USAID, 2016a,b,c].
    -   The study shows that this type of payment system works almost as well as a system where the organization directly controls the provider.
-   On the demand side, the study looks at using money to encourage patients to seek care.
    -   Giving people money or vouchers to cover costs is a common practice \[Grainger et al., 2014; Jehan et al., 2012; Lagarde et al., 2007].
-   The study uses a **three-stage model** to understand how the organization, providers, and patients interact.
-   A main goal is to understand how the type of healthcare service (whether it focuses more on structure or process) affects the best way to provide incentives.
-   The research tries to answer these questions:
    1.  How do systems that only incentivize providers or patients compare to each other for different types of healthcare? How do incentives change based on how much money is available, the type of service, and the specific situation of the program?
    2.  If you have a limited budget, what's the best way to divide incentives between providers and patients to get the best results? How do these incentives change depending on the budget, service type, and program situation?
    3.  When is it much better to use incentives for both providers and patients, instead of just one? When is it good enough to only incentivize either providers or patients?

##   1.1. Subsection Title

-   The research shows that there are both similarities and differences between using incentives for only providers or patients, and using incentives for both.
-   When you only incentivize patients, giving patients more money always leads to more patients seeking care.
-   Similarly, when you only incentivize providers, giving providers more money generally leads to better care.
-   However, when you use incentives for both, simply giving more money to both isn't always the best strategy. Sometimes, giving more to patients and less to providers works better.
-   If you only incentivize providers, you should give them less money when the program focuses more on structural quality.
-   The impact of focusing on process quality on how much money you give providers is more complicated. It depends on how much better resources help providers be able to see more patients.
-   These findings show that organizations need to be careful and understand the specific situation of their program before trying to apply incentive strategies from other programs.
-   The research also shows that using the right combination of incentives for both providers and patients can significantly improve how well a program works (sometimes up to 20 times better).
-   Using the right incentives is also essential to get the full benefit from raising more money. Simply raising more money won't help if you don't have incentives for patients to seek care and for providers to give good care.
-   Finally, the research shows that switching from only provider or patient incentives to incentives for both can allow organizations to provide better care to more people, even if they have less funding.
-   This is important because many healthcare programs in developing countries are facing increasing demand for services at the same time that they are receiving the same amount or less money from aid \[Leach-Kemon et al. 2012].

#   2. Literature Review

-   This research adds to the existing body of work on managing socially responsible and humanitarian operations \[Alizamir et al., 2019; Chen et al., 2019; Gallien et al., 2017; Mu et al., 2016; Natarajan and Swaminathan, 2014; Privett and Erhun, 2011].
-   Previous studies have looked at different issues, but the most relevant to this research are those on how to allocate resources and how to design incentives.
-   Jonasson et al. (2017) studied how to best distribute resources for diagnosing HIV in infants.
-   Arora et al. (2017) analyzed how non-profit organizations should divide funding between advising clients and providing services.
-   Yang et al. (2013) developed a system to decide which children should receive specific types of food to treat malnutrition.
-   Paul et al. (2019) examined the challenges of contracts and incentives in blood bank operations.
-   Berenguer et al. (2017) and Taylor and Xiao (2014) analyzed how subsidies affect the use of socially beneficial products, like mosquito nets.
-   Dai et al. (2016) studied contracts between manufacturers and retailers to improve the timely delivery of vaccines.
-   The studies mentioned above mainly focus on how to allocate resources and design incentives from the perspective of the organization providing the service.
-   This research is different because it looks at how to use funding to provide incentives to both providers and patients in a humanitarian healthcare setting.
-   Natarajan and Swaminathan (2019) also looked at allocating resources to influence both the supply of and demand for healthcare.
-   However, they studied programs that provide health products, while this research focuses on healthcare services.
-   Also, in their work, the organization manages the procurement of the health product, while demand is encouraged through contracts with local workers. In this research, demand is encouraged directly through money given to patients.
-   Finally, this research also considers performance-based incentives for providers, which is not a factor in their work.

#   3. Model

-   The study considers a humanitarian organization with a limited budget that is managing a healthcare program.
-   The organization wants to determine the best way to provide incentives to patients and healthcare providers to maximize the impact of the program, given its limited funding.
-   The interaction between the organization, the provider, and patients is modeled using a three-stage process:
    -   First, the organization decides how much money to give to patients and the healthcare provider.
    -   Second, the provider decides how quickly to provide service and how good the quality of care will be, considering the incentives.
    -   Third, patients decide whether or not to seek care, based on the incentives and the provider's decisions.

##   3.1. Patient's Decision: To Seek or To Not Seek Care?

-   In developing countries, many things can prevent people from seeking healthcare, even if it's free.
-   These include not knowing about the services, the cost of transportation, and the value of the time it takes to get care.
-   The model includes:
    -   Vb: How much a patient values getting treatment (this is related to how much they know about it).
    -   Ct: The cost of getting to the health center.
    -   Cw: The expected cost of the time spent waiting and receiving treatment.
-   The model uses a common approach to understand how the provider's speed and the number of patients affect waiting times.
-   Giving patients money (P) can help offset the costs and inconvenience of getting care.
-   A patient will seek care if the value of treatment and the money they receive is greater than the cost of transportation and the value of their time.
-   The model assumes that the healthcare service is provided for free.
-   The number of patients seeking care can also be affected by the quality of care.
    -   Better structural or process quality could increase how much patients value treatment (Vb) and lead to more patients.
    -   Structural quality improvements can also indirectly lead to more patients by increasing how many patients the provider can see.
-   The model includes how structural quality affects the provider's capacity.
-   However, to keep things simple, the main model doesn't directly include how structural and process quality affect how much patients value treatment (Vb).
-   A separate analysis looks at this, and the main findings of the study still hold true.

##   3.2. The Provider's Optimal Response

-   Healthcare providers want to make the most profit.
-   They are evaluated on structural quality (Rs) and process quality (Rp).
    -   Rs = 0 means the provider isn't investing in resources beyond the bare minimum.
    -   Rp = 0 means the care provided meets the minimum standards, but the provider isn't doing anything to make the patient's experience better.
-   The organization assigns importance to structural and process quality (ws and wp).
-   The provider's overall quality score is R = wsRs + wpRp.
-   The provider is paid a set amount per patient (p), multiplied by their quality score (R).
-   The provider can control how much they get paid by influencing the quality of service.
-   The provider can improve structural quality (Rs) by investing money (I) in things like facilities and equipment.
    -   Having more resources can allow the provider to see more patients.
    -   These investments have diminishing returns, meaning each additional dollar spent improves quality less and less.
-   The provider can also influence process quality (Rp) by changing how much time they spend with each patient.
    -   Seeing patients more quickly can reduce waiting times and allow the provider to see more patients.
    -   However, in healthcare, seeing patients more quickly can also lead to lower quality of care.
    -   Providers have to balance seeing more patients with providing good quality care.
    -   Their choices affect patient waiting times and the number of patients seeking care.
-   The provider's service rate is: λ = a + cRs - αRp.
    -   a: The provider's basic capacity (how many patients they can see).
    -   α: How much service speed is affected by process quality.
-   Structural quality investments have two benefits:
    -   Increased capacity (cRs) can be used to see more patients at the same level of process quality.
    -   It can be used to provide better process quality without seeing fewer patients.
-   The provider first decides on structural quality, then adjusts service time to influence process quality.
-   Table 1 summarizes the notation used in the model.
-   The provider's goal is to maximize profit.
-   The provider's best choices for structural and process quality are determined using a step-by-step approach (Lemma 1).
-   Lemma 1 shows how incentives affect the provider's quality choices and the number of patients seeking care.
-   Proposition 1 shows that structural quality, process quality, and the number of patients increase when the provider is given more money (p).
-   Proposition 2 shows that structural quality, process quality, and the number of patients increase when patients are given more money (P).

##   3.3. The Humanitarian Organization's Incentive Design Problem

-   The organization's goal is to maximize how many people are getting good quality care.
-   The organization's problem is to give money to providers and patients in a way that maximizes this goal, without spending more money than they have.
-   Giving more money to providers (p) and patients (P) increases both the quality of care and the number of patients seeking care.
-   However, the limited budget creates a trade-off: more money for providers means less for patients, and vice versa.
-   The organization must carefully balance these incentives to have the greatest impact.
-   The study analyzes this trade-off and presents results on the best incentives in section 5.
-   Systems that only incentivize providers or patients are analyzed as a comparison.
-   The payment system is justified:
    -   Paying providers a set amount per patient adjusted for quality is common and relatively simple.
    -   This system works almost as well as a system where the organization directly controls the provider.
    -   Using a single quality score (R) is reasonable.

#   4. Analysis of the Current Practice: One-Sided Incentive Schemes

-   Many healthcare programs in developing countries use systems that only incentivize either providers or patients:
    -   Only providers: These are called pure supply-side incentive schemes (P = 0).
    -   Only patients: These are called pure demand-side incentive schemes (p = 0).
-   Examples:
    -   Supply-side: Rwanda, Burundi, Nigeria, Senegal, Mozambique \[Gertler and Vermeersch, 2012; Rudasingwa et al., 2015; USAID, 2016a,b,c].
    -   Demand-side: Brazil, India, Bangladesh, Pakistan \[Carvalho et al., 2014; Jehan et al., 2012; Lagarde et al., 2007].
-   The study compares how well these schemes perform.

##   4.1. Pure Demand-Side Incentive Schemes

-   In these schemes, providers are not paid based on quality or the number of patients (p = 0).
-   Providers have no incentive to invest in structural quality (R\*s = 0).
-   Providers don't have a financial reason to choose one level of process quality over another.
-   It is assumed that the provider will choose the level of process quality that is best for the patients.
-   The organization's goal is to give patients as much money as possible, until they run out of budget.
-   Corollary 1 shows that in these schemes, the amount of money given to patients (P\*), the number of patients seeking care (μ\*e), and the quality of care (R\*) all increase when the organization has more money (B).
-   Proposition 3 shows that provider quality choices and the amount of money given to patients do not change based on whether the program focuses on structural or process quality.
-   This means providers don't consider the specific needs of different types of healthcare.
-   Structural quality is always zero, which is a problem for programs that focus on structural quality.
-   Proposition 4 shows how patient and provider characteristics affect the system:
    -   Provider characteristics (α): Giving providers the ability to see more patients (higher α) leads to less money given to patients (P\*), but more patients seeking care (μ\*e) and higher quality of care (R\*).
    -   Patient characteristics (Vb, Ct, cw): If patients value treatment more (higher Vb), less money needs to be given to patients (P\*), but more patients seek care (μ\*e) and quality of care is higher (R\*). The opposite is true for the cost of getting to the health center (Ct) and the value of patients' time (cw).
-   In schemes that only incentivize patients, the amount of money given to patients isn't adjusted for the specific type of healthcare service being provided.

##   4.2. Pure Supply-Side Incentive Schemes

-   In these schemes, the organization only gives money to the healthcare provider (p = 0).
-   The provider's best choices for quality are determined using Lemma 1.
-   The organization's goal is to give the provider more money, at least up to a certain point.
-   Proposition 5 describes the amount of money given to providers (p\*).
-   Corollary 2 shows how incentives and program success change with the budget (B):
    -   If B is below a certain level, giving providers more money increases incentives and program success.
    -   If B is above that level, giving providers more money doesn't improve program success.
-   Giving providers more money leads to better quality of care and more patients seeking care, but only up to a point.
-   After that point, the only way to improve things is to give money to patients.
-   Schemes that only incentivize providers don't do this, so they stop improving.
-   Proposition 6 shows the impact of what the program focuses on (ws and wp):
    -   Structural quality increases when the program focuses more on either structural or process quality.
    -   Process quality always increases when the program focuses more on process quality.
    -   The organization gives less money to providers when the program focuses more on either structural or process quality.
-   Provider investments in structural quality increase when the program focuses more on either type of quality.
    -   This is because structural quality improvements can allow providers to see more patients, which also allows them to improve process quality.
-   Whether structural quality investments increase or decrease when the program focuses more on process quality depends on how much those investments help providers see more patients.
    -   If they don't help much, they cost more than they're worth, and providers will reduce them.
    -   If they help a lot, they're worth it, and providers will increase them.
-   The organization adjusts how much money they give providers based on this.
-   This shows that organizations need to consider the specific situation of their program.
-   In schemes that only incentivize providers, provider quality choices change based on what the program focuses on, unlike schemes that only incentivize patients.
-   Proposition 7 shows how patient and provider characteristics affect the system:
    -   Provider characteristics (α): Giving providers the ability to see more patients decreases the money given to providers, but increases the number of patients seeking care and quality of care.
    -   Patient characteristics (Vb, Ct, cw): If patients value treatment more, less money needs to be given to providers, but more patients seek care and quality of care is higher. The opposite is true for the cost of getting to the health center and the value of patients' time.
-   In summary, schemes that only incentivize providers:
    -   Increase provider incentives and program success with the budget, but only up to a point.
    -   Require organizations to carefully adjust provider incentives based on the type of service.
    -   Should give providers less money when structural quality is more important.
    -   May give providers less money when process quality is more important, but this depends on how much structural quality improvements help providers see more patients.

##   4.3. Performance Comparison of the One-Sided Incentive Schemes

-   In schemes that only incentivize patients, providers don't invest in structural quality.
-   In schemes that only incentivize providers, providers do invest in structural quality, and structural quality increases when the program focuses more on it.
-   For programs where structural quality is important, schemes that only incentivize providers are likely to work better than schemes that only incentivize patients.
-   The result is less clear for programs emphasizing process quality.
-   Proposition 8 shows that when incentives are chosen optimally, schemes that only incentivize patients work better than schemes that only incentivize providers in programs that focus more on process quality.
-   Proposition 8 shows the importance of matching the incentive scheme to the type of healthcare service.
-   In practice, this is often not done.
    -   For example, schemes that only incentivize providers have been used for prenatal care (which focuses on process quality), but schemes that only incentivize patients might work better.
-   Even if organizations continue to use schemes that only incentivize either providers or patients, they can improve program success by better matching the scheme to the type of healthcare service.

#   5. Two-Sided Incentive Design: Analysis and Comparison with One-Sided Incentives

-   This section analyzes schemes that incentivize both providers and patients and compares them to schemes that only incentivize one or the other.

##   5.1. Two-Sided Incentives: Insights

-   Proposition 9 describes the best incentives in schemes with limited budgets.
-   Corollary 3 shows how incentives and program success change with the budget (B):
    -   If B is below a certain level, giving more money to providers and patients increases incentives and program success.
    -   If B is above that level, giving more money to patients increases patient incentives, but decreases provider incentives. However, program success still increases.
-   Proposition 9 and Corollary 3 show the complex decisions involved in designing incentives in programs with limited budgets.
-   For budgets below a certain level, the best way to divide the money is to make sure that giving a little more to either providers or patients has the same effect.
-   After that level, the best strategy is to give more money to patients and less to providers.
-   Proposition 10 shows how incentives and program success change based on what the program focuses on.
    -   Structural quality increases when the program focuses more on either structural or process quality.
    -   Process quality always increases when the program focuses more on process quality.
    -   If the budget is above a certain level, increasing how much the program focuses on either type of quality leads to less money given to providers and more money given to patients.
-   Whether structural quality investments increase when the program focuses more on process quality depends on how much those investments help providers see more patients.
    -   They increase if they help a lot and decrease if they don't help much.
-   This again shows the importance of considering the specific situation of the program.
-   Propositions 9 and 10 show both similarities and differences between schemes that incentivize both providers and patients and schemes that only incentivize one or the other.
-   Proposition 11 shows the impact of patient and provider characteristics:
    -   If the budget is above a certain level, giving providers the ability to see more patients decreases both types of incentives, while increasing program success.
    -   If the budget is above a certain level, if patients value treatment more, decreasing both types of incentives, while increasing program success. The opposite is true for the cost of getting to the health center and the value of patients' time.

##   5.2. Comparison of the Two-Sided and One-Sided Incentive Schemes

-   This section compares incentives and program success across the different incentive schemes.

###   5.2.1. Comparison of Incentives Offered.

-   The amount of money given to patients always increases with the budget, regardless of whether incentives are given to both providers and patients or only patients.
-   However, the amount of money given to providers behaves differently.
    -   In schemes that only incentivize providers, it increases with the budget up to a point, then stays the same.
    -   In schemes that incentivize both, it decreases with the budget after a certain point, while patient incentives increase.
-   This strategy of giving more money to patients and less to providers maximizes the use of additional funding.
-   Schemes that only incentivize providers cannot use this strategy.
-   This shows that the effect of funding on incentives when considered alone may differ from the effect when multiple incentives are offered.
-   The effect of what the program focuses on on provider incentives is similar in schemes that incentivize both and schemes that only incentivize providers.
-   However, the effect on patient incentives is different.
    -   In schemes that only incentivize patients, it does not change.
    -   In schemes that incentivize both, it does change, allowing for more precise adjustment.
-   The effects of patient and provider characteristics on incentives are similar in both types of schemes.
-   However, even with similar directions of effects, the actual amounts of money given can be very different.

###   5.2.2. Performance Comparison of the Two-Sided and One-Sided Incentive Schemes.

-   The performance gap is calculated as the difference in success between schemes that incentivize both and schemes that only incentivize one or the other.
-   Proposition 12 shows that the performance gap between schemes that incentivize both and schemes that only incentivize providers increases as the budget increases, after a certain point.
-   Proposition 13 shows that:
    -   For programs that only focus on process quality, there's no difference in success between schemes that incentivize both and schemes that only incentivize patients.
    -   For programs that only focus on structural quality, the performance gap between schemes that incentivize both and schemes that only incentivize patients increases as the budget increases.
-   The key findings are:
    -   In some situations, giving money only to patients is the best approach.
    -   However, giving money only to providers is never the best approach.
    -   The difference in success between schemes that incentivize both and schemes that only incentivize one or the other increases with the budget (except for programs that only focus on process quality).
-   Numerical results confirm that the performance gap increases with the budget.
-   Schemes that incentivize both are more effective at using additional funding.
-   Switching from one-sided incentive schemes to two-sided incentive schemes can improve program success even with a smaller budget.

#   6. Computational Study

##   6.1. The Test Bed

-   The numerical study varies several parameters, using public data when possible.
-   The values used for the type of program and the organization's budget are:
    -   **Process and structural quality weights (wp; ws):**
        -   Structural quality: (0.1, 0.9) and (0.3, 0.7)
        -   Process quality: (0.9, 0.1) and (0.7, 0.3)
        -   Equal emphasis: (0.5, 0.5)
    -   **Program budget B:** 10, 20, 30, 40, 50, 75, 100, 125, 150, and 200 (per hour)
-   Other parameters varied include provider capacity, sensitivity of capacity to structural quality investments, patients' willingness to wait, opportunity cost of waiting, and structural investment scaling factor.
-   In total, 67,500 different scenarios are considered.

##   6.2. Impact of Program Type and Provider's Base Capacity Under the Two-Sided Incentive Scheme

-   Quality-adjusted coverage increases with provider capacity.
-   The benefit of additional funding is greater when the provider has a higher capacity.
-   Incentives decrease with provider capacity.
-   Incentives are more sensitive to provider capacity in process-focused programs.
-   Provider characteristics should be considered when designing incentives.

##   6.3. Impact of Program Type and Provider's Base Capacity On the Performance Gap Between Two-Sided and One-Sided Incentive Schemes

-   Systems that only incentivize providers work better in structure-focused programs, while systems that only incentivize patients work better in process-focused programs.
-   The negative impact of using one-sided incentive schemes increases with provider capacity for supply-side incentives and decreases with capacity for demand-side incentives.
-   Switching to a more effective one-sided incentive scheme improves performance, but systems that incentivize both are still more effective.

#   7. Model Extension and Justification for the Aggregate-Quality Based Payment Scheme

-   This section considers a more general version of the model and justifies the use of a linear, aggregate-quality based payment scheme.

##   7.1. Model Extension

-   The original model assumes patients' decisions only depend on waiting time.
-   This extension considers the direct impact of process and structural quality on patients' decisions.
-   The perceived value of treatment is modeled as increasing with both types of quality.
-   This makes the model analytically intractable, so a computational analysis is used.
-   The key findings from the original model still hold true, demonstrating their robustness.

##   7.2. Justification for the Aggregate-Quality Based Payment Scheme

-   A centralized model, where the organization and provider are the same entity, is used as a benchmark.

###   7.2.1. Centralized Model.

-   In a centralized model, the organization has full control over the provider's quality decisions.
-   The centralized model's performance is compared to the aggregate-quality based payment scheme.
-   The aggregate-quality based payment scheme performs well, with a performance gap of less than 2% in most scenarios.
-   This justifies the use of this scheme and suggests that switching to a different scheme is unlikely to improve performance.

###   7.2.2. Offering Separate Payments Based on Rs and Rp.

-   Instead of a single payment, separate payments could be offered for structural and process quality.
-   This makes the model analytically intractable, so a numerical study is used.
-   Offering separate payments does not significantly improve performance compared to the aggregate-quality based payment scheme.

#   8. Conclusion

-   This study analyzes how organizations with limited budgets should design incentives for patients and providers.
-   Combining patient and provider incentives is more effective than using one-sided incentive schemes.
-   Switching from one-sided to two-sided incentives improves performance.
-   It's always beneficial to increase patient incentives when more funds are available.
-   However, increasing both patient and provider incentives simultaneously may not always be optimal.
-   The provider's capacity influences the benefits of fundraising efforts.
-   Fundraising efforts are ineffective without proper incentives.
-   Relaxing the limited liability assumption and considering information asymmetry are potential areas for future research.


</details>

# Ujjal Mukherjee
- Associate Professor of Business Administration
### education
- M.S., Statistics, University of Minnesota, 2015
- Ph.D., Business Administration, University of Minnesota, 2015
- MBA, Xavier Institute, 2002
- BEME, Jadavpur University, 1997
### research interest
- Predicting failures of innovations-in-use; Technology adoption and mediation in healthcare.
### teaching
- Supply Chain Management, Business Analytics, Healthcare Operations, and Healthcare Analytics

```
title: Explaining Heterogeneity in Environmental Management Practice Adoption across Firms
authors: Rick Hardcopf, Rachna Shah, Ujjal Mukherjee
journal: Production and Operations Management
published: 2019
```

# Executive Summary
- This study investigates the sources of variation in **Environmental Management Practice (EMP) adoption** among firms, addressing the research gap of why firms differ in the number of EMPs they adopt.
- Using a 12-year panel data of 880 firms across 258 industries and Hierarchical Linear Modeling (HLM), we find that:
    -  **Temporality** (variation over time) accounts for 40% of the variance.
    -  **Firm-specific** choices and characteristics account for 26% of the variance.
    -  **Industry membership** accounts for 34% of the variance.
- Furthermore, the explanatory power of these sources depends on the *type* of EMP (strategic, operational, or tactical).
- Specific **firm characteristics** (size, profitability, labor intensity, prior ISO 9000/QMS adoption) and **industry attributes** (environmental risk/regulation, competition, uncertainty—munificence, dynamism, complexity) significantly influence EMP adoption.
- The **theoretical contribution** lies in identifying and quantifying the relative importance of temporality, firm-specific factors, and industry membership as sources of variation in EMP adoption.
-  This provides insights for motivating increased EMP adoption at the firm level, such as directing initiatives at individual firms or implementing industry-wide policies.
- **Findings** related to specific **firm characteristics** and **industry attributes** include:
    - Five of six firm characteristics (firm size, firm profitability, labor intensity, and prior experience adopting ISO 9000 or a quality management system) support EMP adoption.
    - One industry attribute (regulation) support EMP adoption.
    - Industry competition and industry uncertainty dimensions (munificence, dynamism, and complexity) hinder it.
- This study used panel data from 2002 to 2013 and applied HLM to account for the nested data structure (EMPs within firms within industries), providing generalizable results.
 
#environmental_management #environmental_management_practices #emp_adoption #hierarchical_linear_modeling #hlm #sustainability #panel_data #firm_characteristics #industry_attributes #strategic_emps #operational_emps #tactical_emps
 
<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Big environmental disasters have made companies more responsible for their impact on the environment.
- Companies are increasingly adopting **Environmental Management Practices (EMPs)**. This shows they care about the environment and can also improve their operations and financial performance [Anton et al., 2004; Melnyk et al., 2003; King and Lenox, 2002].
- We know a lot about **why** companies adopt EMPs. But we don't know much about **why** some companies adopt **more** EMPs than others, even within the same industry.
- This study wants to find out what causes these differences in EMP adoption.
    - This is important because different causes might need different solutions.
- Of course, **time** plays a role. As society becomes more aware of environmental issues, companies feel more pressure to adopt EMPs.
- But besides time, it's not clear **what else** influences how many EMPs a company adopts.
- Other research suggests that companies adopt EMPs because of:
    - Regulations [Bansal and Roth, 2000; Sarkis et al., 2010]
    - Competition [Hofer et al., 2012]
    - Pressure from other stakeholders [Buysse and Verbeke, 2003].
- A company's internal characteristics (like how much money it has) and its specific needs also matter [Buysse and Verbeke, 2003; Delmas and Toffel, 2004].
- Even if companies face the same pressures, their unique characteristics might lead them to adopt EMPs differently [Delmas and Toffel, 2008; Sharma, 2000].
- The literature suggests that there is variation in EMP adoption:
    - between industries (due to different pressures)
    - within industries (due to unique company traits).
- But nobody has looked at these different sources of variation together.
- So, this study focuses on **time**, **company traits**, and **industry** as three main reasons for differences in EMP adoption. We want to see **how much** each of these matters.
- Most previous studies focused on companies in a single industry or in similar industries [Christmann, 2004; Sarkis et al., 2010].
- We also look at **specific** company traits (like size, profitability, etc.) and **industry factors** (like environmental risk and competition) that might influence EMP adoption.
- We don't have specific predictions (hypotheses), but we discuss how we expect each of these things to affect EMP adoption.
- This study is mainly about **describing** what we see and **explaining** it using what we already know from previous research.
- We use data from 2002 to 2013 on 880 companies in 258 different industries. This data represents a wide range of businesses in the US.
- Because the data is grouped (EMPs within companies within industries), we use a method called **hierarchical linear modeling (HLM)**.
-  We do the analysis in two steps:
    - First, we figure out how much of the variation in EMP adoption is due to time, company traits, and industry.
    - Second, we look at how specific company traits and industry factors influence EMP adoption.
- Other researchers have used a similar approach to understand differences in company profitability [McGahan and Porter, 2002; Misangyi et al., 2006].
- The results show that:
    - 40% of the variation in EMP adoption is due to **time**.
    - 26% is due to **company traits**.
    - 34% is due to **industry**.
- We also look at **different types of EMPs** (strategic, operational, and tactical) [Hofer et al., 2012; Montabon et al., 2007; Sroufe et al., 2002].
    - Strategic EMPs are about how a company presents itself to the outside world.
    - Operational EMPs are about how the company changes its internal operations.
    - Tactical EMPs are somewhere in between.
- The results change depending on the type of EMP we're looking at.
- Finally, we find that:
    - Certain **company traits** (size, profitability, labor intensity, and prior experience with ISO 9000 or QMS) and one **industry factor** (regulation) encourage EMP adoption.
    - **Industry competition** and **uncertainty** make it less likely for companies to adopt EMPs.
- This study makes several contributions.
- First, it's the first to show that time, company traits, and industry are distinct sources of variation in EMP adoption.
- It's also the first to measure how important each of these is.
- This is helpful for figuring out **how to encourage** more companies to adopt EMPs.
    - If company traits are most important, then we should target individual companies with incentives.
    - If industry factors are most important, then we need industry-wide policies.
- Looking at different types of EMPs gives us even more information.
-  We find that industry is most important for strategic EMPs, while company traits are most important for operational EMPs.
- This means that industry regulations might be best for encouraging strategic EMPs, while incentives for individual companies might be better for operational EMPs.
- Second, by looking at company traits and industry factors together, we can see how each one affects EMP adoption.
- Some of these things have a consistent effect, while others change over time.
- For example, bigger companies tend to adopt more EMPs. But as companies grow, they adopt even more.
- Knowing which company traits support EMP adoption is helpful for companies that want to be proactive.
- Knowing which industry factors matter helps companies understand how their external environment affects their decisions.
- The company traits and industry factors we identified could also be used in future research.
- Finally, this study is more generalizable than previous ones because we looked at a wider range of EMPs, companies, and industries over a longer period.
- We included 50 different EMPs, covering almost everything related to environmental management.
 
# 2. Literature Review
- There's a lot of research on EMPs, but there's no clear agreement on **what an EMP actually is**.
- EMPs are "the activities firms undertake to reduce the impact of their operations, or supply chain, on the natural environment."
- We organize the existing research by:
    - What it focuses on (performance or reasons for adoption)
    - What theories it uses (institutional, stakeholder, or organizational).
- Most studies look at **how EMP adoption affects performance**.
    - These studies generally find that EMP adoption leads to better environmental, operational, and financial performance [Anton et al., 2004; Zhu and Sarkis, 2004; King and Lenox, 2002].
- However, EMPs can be different, so researchers have created categories to understand these differences.
- There's less agreement about **what causes** companies to adopt EMPs.
    - Some studies look at internal factors (like company size and profitability), while others look at external factors (like regulations and competition).
    - But only company size and regulation have been consistently shown to matter.
- Also, many studies look at different factors in different industries, making it hard to compare results.
- Many studies try to explain **why companies adopt EMPs** using:
    - Institutional theory (companies adopt EMPs to fit in)
    - Stakeholder theory (companies adopt EMPs to please stakeholders)
    - Organizational theory (companies adopt EMPs based on their resources and capabilities).
- **Institutional theory** says that companies adopt practices to gain legitimacy [DiMaggio and Powell, 1983].
- In the context of EMPs, this means that companies adopt EMPs to respond to pressure from stakeholders [Bansal and Roth, 2000; Sarkis et al., 2010].
- This suggests that companies in the same industry should adopt similar EMPs [Jennings and Zandbergen, 1995].
- However, some argue that how a company interprets these pressures depends on its own characteristics [Delmas and Toffel, 2008].
- Organizational theories suggest that companies have different resources and needs [Bansal and Roth, 2000; Sarkis et al., 2010].
- This means that even if companies face the same pressures, they might respond differently, leading to differences in EMP adoption even within the same industry.
- In short, the literature suggests that there are several reasons **why** companies might differ in their EMP adoption: **time, company traits, and industry**.
- But there's no clear understanding of **how much** each of these matters, or **which specific** company traits and industry factors are most important.
 
## 2.1. Identifying Firm Characteristics and Industry Attributes
- Company traits and industry factors influence managers' decisions about EMP adoption.
- We chose factors that:
    - (a) were relevant to our research
    - (b) haven't been studied much before, or have produced mixed results
    - (c) would be helpful for stakeholders
    - (d) could be measured using available data.
- We reviewed the EMP and innovation literature.
- This led to a list of ten company traits and five industry factors.
- With the exception of company size, no other single variable was looked at by more than a few studies
- Several company traits were excluded because we couldn't measure them with the available data or because they were too similar to other measures.
- The final list included company size, profitability, capital slack, labor intensity, ISO 9000 adoption, and QMS adoption.
- All of the industry factors were included.
 
## 2.2. Firm Characteristics
- **Company Size**: Larger companies tend to face more public scrutiny and have more resources to invest in EMPs [Bowen, 2000; Brammer and Millington, 2006]. We expect a positive relationship between company size and EMP adoption.
- **Profitability**: Adopting EMPs can be expensive [Hart and Ahuja, 1996; Tate et al., 2010] and the financial returns are uncertain [King and Lenox, 2001, 2002]. More profitable companies are better able to afford these investments. We expect a positive relationship between profitability and EMP adoption.
- **Capital Slack**: This refers to resources a company has beyond what it needs to operate [Damanpour, 1991]. These resources can be used to implement innovations like EMPs [Bourgeois, 1981; Jeyaraj et al. 2006]. We expect a positive relationship between capital slack and EMP adoption.
- **Labor Intensity**: Implementing EMPs requires skilled labor. Companies with more available labor should be more likely to adopt EMPs.
- **ISO 9000 Adoption**: Having experience with other management standards like ISO 9000 can make it easier to adopt EMPs [Albuquerque et al., 2007]. We expect a positive relationship between prior ISO 9000 adoption and EMP adoption.
- **Quality Management System (QMS) Adoption**: QMS (like Lean or Six Sigma) and environmental management have similar goals (reducing waste) and methods. We expect a positive relationship between prior QMS adoption and EMP adoption.
 
## 2.3. Industry Attributes
- **Environmental Risk/Regulation**: Industries that pose a greater risk to the environment are more likely to be regulated. Regulatory pressure encourages EMP adoption [Bansal and Roth, 2000]. We expect a positive relationship between environmental risk and EMP adoption.
- **Competition**: The relationship between competition and EMP adoption is complex. It's not clear whether more or less competition encourages EMP adoption.
- **Environmental Uncertainty**: This refers to the instability and unpredictability of the industry [Dess and Beard, 1984]. It has three dimensions:
    - **Munificence**: How much resources are available in the industry. More resources should make it easier to adopt EMPs. We expect a positive relationship between industry munificence and EMP adoption.
    - **Dynamism**: How quickly the industry is changing. It's not clear whether more dynamism encourages or discourages EMP adoption.
    - **Complexity**: How many different companies and types of companies are in the industry. More complexity might make it harder to understand what EMPs are needed. We expect a negative relationship between industry complexity and EMP adoption.
 
# 3. Data and Research Method
## 3.1. Data and Sample
- We used data from several sources: Compustat, ASSET4, and Worldscope.
    - ASSET4 provided data on EMP adoption, ISO 9000, and QMS.
    - Worldscope provided data on company size, profitability, and resources.
    - Compustat provided data on industry factors.
    - First for Sustainability provided data on environmental risk.
- The data covers 2002-2013. We only included US companies to avoid differences due to different countries' environmental policies. We focused on public companies because their financial information is readily available.
- Industries were defined using 4-digit SIC codes.
- The final sample included 880 companies from 258 industries.
- We removed about 10% of the data due to missing information.
 
## 3.2. Research Method
- We used **hierarchical linear modeling (HLM)**.
- Our data is grouped: EMPs within companies within industries. HLM is designed for this kind of data.
- HLM lets us understand how much of the variation in EMP adoption is due to each level (time, company, and industry).
- Other methods (VCA and ANOVA) assume that observations are independent, which is not true in our data.
 
## 3.3. Variable Measurement
### 3.3.1. Dependent Variable
- **EMP adoption** is measured as the number of EMPs a company has divided by the total number of EMPs available in its industry.
- We included 50 different EMPs in our measure.
- This measure captures the wide variety of environmental activities that companies undertake.
- It also allows us to compare companies in different industries.
 
### 3.3.2. Unique Sources of Heterogeneity
- **Temporality**: Variation in EMP adoption due to time.
    - We looked at both year-to-year changes and within-year changes.
- **Firm**: Variation due to company-specific choices.
- **Industry**: Variation due to industry factors.
 
### 3.3.3. Firm-Level Sources of Heterogeneity
- We standardized the values of continuous variables (like size and profitability) within each industry.
- **Firm Size**: Measured as the natural log of net sales.
- **Profitability**: Measured as the natural log of return on assets.
- **Capital Slack**: Measured as the current ratio (total assets / total liabilities).
- **Labor Intensity**: Measured as total employees / total assets.
- **ISO 9000 Adoption**: Measured as a yes/no variable.
- **Quality Management System (QMS) Adoption**: Measured as a yes/no variable.
 
### 3.3.4. Industry-Level Sources of Heterogeneity
- **Environmental Risk**: Measured using industry assessments from First for Sustainability.
- **Competition**: Measured using the Herfindahl-Hirschman Index (HHI).
- **Munificence**: Measured as the slope of industry net revenue over time.
- **Dynamism**: Measured as the standard error of the regression slope of industry net revenue over time.
- **Complexity**: Measured by regressing market shares of firms in the current year on their market shares in the initial year.
 
## 3.4. Descriptive Statistics
- EMP adoption has increased over time, and there is significant variation in EMP adoption across companies.
- Company size, profitability, labor intensity, ISO 9000, and QMS are positively correlated with EMP adoption.
- Capital slack is not correlated with EMP adoption.
- Environmental risk, competition, and munificence are positively correlated, and complexity is negatively correlated, with EMP adoption.
 
# 4. Data Analysis and Results
- We did two main analyses:
    - Multi-level analysis (how much variation is due to time, company, and industry)
    - Firm characteristic and industry attribute analysis (how specific factors influence EMP adoption).
 
## 4.1. Multi-Level Analysis
- We followed the approach of Misangyi et al. [2006].
    - First, we divided the variation in EMP adoption into time, company, and industry.
    - Second, we divided the variation due to time into year-to-year changes ("time") and within-year changes ("year").
- Results show that:
    - 40% of the variation is due to **temporality**.
    - 26% is due to **company**.
    - 34% is due to **industry**.
- Breaking down temporality, we find that:
    - 6% is due to **year**.
    - 34% is due to **time**.
- We did several checks to make sure these results are reliable.
 
# Executive summary of 1. Introduction
- This section introduces the problem of **heterogeneity** in Environmental Management Practice (EMP) adoption across firms.
- While previous research focuses on **why** firms adopt EMPs, this study addresses the gap in understanding **why** firms differ in the number of EMPs they adopt.
- We highlight the increasing importance of EMPs due to environmental events and regulations.
- We introduce the study's approach: identifying and quantifying the relative importance of temporality, firm-specific characteristics, and industry membership as sources of variation in EMP adoption.
 
# Executive summary of 2. Literature Review
- The section provides an overview of the existing literature on EMPs, noting the lack of a universally accepted definition of EMP.
- We define EMPs as activities firms undertake to reduce their impact on the environment.
- The literature review is organized by thematic focus (performance consequences and antecedents of EMP adoption) and theoretical lens (institutional, stakeholder, and organizational theories).
- The review identifies two key gaps in the literature: the absence of a systematic assessment of the relative influence of different sources of heterogeneity in EMP adoption and the lack of a cohesive set of firm characteristics and industry attributes that explain this heterogeneity.
 
# Executive summary of 2.1. Identifying Firm Characteristics and Industry Attributes
- This subsection describes the process of selecting firm characteristics and industry attributes for analysis.
- The selection criteria included theoretical relevance, limited prior research, usefulness for stakeholders, and availability of secondary data.
- The final list includes firm size, profitability, capital slack, labor intensity, ISO 9000 adoption, QMS adoption, environmental risk, competition, munificence, dynamism, and complexity.
 
# Executive summary of 2.2. Firm Characteristics
- This subsection discusses the selected firm characteristics and their expected relationships with EMP adoption.
- We expect a positive relationship between EMP adoption and firm size, profitability, capital slack, labor intensity, ISO 9000 adoption, and QMS adoption.
 
# Executive summary of 2.3. Industry Attributes
- This subsection discusses the selected industry attributes and their expected relationships with EMP adoption.
- We expect a positive relationship between EMP adoption and environmental risk and munificence.
- The expected relationship between EMP adoption and competition, dynamism, and complexity is less clear.
 
# Executive summary of 3. Data and Research Method
- This section describes the data sources, sample, and research methods used in the study.
- The data includes information on US firms from Compustat, ASSET4, and Worldscope.
- The sample consists of 880 firms from 258 industries over the period 2002-2013.
- The study uses hierarchical linear modeling (HLM) to account for the nested data structure and to assess the relative importance of different levels of analysis.
 
# Executive summary of 3.3. Variable Measurement
- This subsection describes how the key variables in the study are measured.
- EMP adoption is measured as the ratio of the number of EMPs a firm has to the total number available in its industry.
- We also explain how we measure temporality, firm, and industry as sources of heterogeneity.
- Finally, we describe how we operationalize firm-level and industry-level sources of heterogeneity.
 
# Executive summary of 4. Data Analysis and Results
- This section presents the results of the multi-level analysis and the firm characteristic and industry attribute analysis.
- The multi-level analysis quantifies the contribution of temporality, firm-specific characteristics and choices, and industry membership to explaining heterogeneity in EMP adoption.
 
# Executive summary of 4.1. Multi-Level Analysis
- This subsection describes the multi-level analysis, which partitions observed variation in EMP adoption into temporal, firm, and industry components.
- The results show that temporality, firm characteristics and choices, and industry membership account for 40%, 25.7%, and 34.3%, respectively, of the aggregate variance in firm-level EMP adoption.
 

</details>

```
title: Hiding in the Herd: The Product Recall Clustering Phenomenon
authors: Ujjal K. Mukherjee, George P. Ball, Kaitlin D. Wowak, Karthik V. Natarajan, Jason W. Miller
journal: MANUFACTURING & SERVICE OPERATIONS MANAGEMENT
published: 2021
```

# Executive Summary
- We investigate the **product recall clustering phenomenon**, where recalls occur in close temporal proximity, with a leading recall exciting subsequent recalls.
- We examine **how the stock market penalizes firms** based on their recall's position within the cluster.
- **Theoretical framework**:
    - Dynamic game-theoretic model: Based on **attribution theory**, with results related to stock market penalties for firms issuing leading/following recalls, forming a dynamic game, establishing insights regarding firm recall strategies (e.g., optimality of clustering following recalls after a competitor initiates a leading recall).
- **Methodology**:
    - We analyze **3,117 auto recalls across 48 years** using a **Hawkes process model** to examine self-excitation of events, categorize recalls as leading/following.
    - **Event study**: Examine how the stock market effects of a recall vary depending on its position within a cluster.
- **Key findings**:
    - **73% of recalls occur in clusters**, forming after a 16-day gap.
    - Clusters last 34 days and have 7.6 following recalls after the leading recall.
    - **Leading recalls** are associated with up to **67% larger stock market penalty** than following recalls.
    - Stock market benefit of a following recall weakens as time since the leading recall increases.
    - Stock market penalty for a leading recall grows as time since the last cluster increases.
- **Practical implications**:
    - Regulators should consider recall clustering and its stock market effects.
    - We propose a **cost-neutral policy** for the National Highway Traffic and Safety Administration (NHTSA) to limit recall clustering.

#recall_clustering #herding_theory #nash_stopping_time_games #hawkes_process_model #event_study #attribution_theory #stock_market_penalty


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Automobile recalls are increasing, often with delays.
- We explore *recall clustering* - recalls in close temporal proximity, where a leading recall excites following recalls [Scharfstein and Stein, 1990].
- Previous research treats recalls as independent across firms [Haunschild and Rhee, 2004; Thirumalai and Sinha, 2011; Shah et al., 2017].
    - Although underlying causes exist, such as inadequate learning, risky innovation, or overworking employees, recalls by one firm are assumed independent of competitors.
- Anecdotal evidence suggests auto firms deviate from this assumption, with "recall tidal waves" where firms follow suit [Stephens, 2014].
    - E.g., Ford recall followed by recalls from Honda and Chrysler.
- *RQ1:* **Do firms cluster recalls?**
    - This may be due to managers consciously or unconsciously delaying recalls to cluster them behind a competitor's recall to "hide in the herd" [Scharfstein and Stein, 1990].
    - Herding theory explains temporal clustering of accounting phenomena [Tse and Tucker, 2010], but few studies have demonstrated it in an operational setting.
- *RQ2:* **Does the stock market penalty attributed to a recall depend upon its position within a cluster?**
    - We explore whether the stock market penalizes firms differently based on a recall's position in a cluster.
    - If leading recalls have a larger market penalty, it may explain recall delays.
    - E.g., Chrysler, GM, and Honda recalls in October 2015 had varying stock market reactions.
    - Chrysler, the first to recall, had the largest penalty.
    - Attribution theory suggests more blame is attributed for uniquely negative outcomes [Kelley and Michela, 1980].
- To answer these questions, we develop hypotheses based on a dynamic game-theoretic model and empirical data on auto recalls over 48 years.
    - Model uses attribution theory to develop results related to stock market penalties.
    - We examine support for hypotheses related to stock market penalties and recall clustering using empirical data on all auto recalls announced in the United States over a 48-year period.
- We leverage a **self-excited Hawkes point process model** to categorize auto recalls into leading, following, or neither.
    - We find that 73% of recalls exist in a cluster.
    - We use an **event study** to explore the stock market impact of a recall announcement.

## 1.1. Contributions
- We contribute to *herding theory* [Scharfstein and Stein, 1990] in the context of *product recall timing* [Hora et al., 2011; Ball et al., 2018; Wowak et al., 2020] by finding evidence of recall clusters.
- We contribute to the literature on how the *impact of a recall on a firm's financial performance varies depending on contextual factors* [Rhee and Haunschild, 2006; Chen et al., 2009].
    - We extend *attribution theory* [Kelley and Michela, 1980] to an operational setting by finding that stock market consequences vary depending on when recalls are announced relative to competitors.
- We propose a **cost-neutral policy recommendation** for the NHTSA to improve the speed and transparency of recall decision-making and limit recall clustering.

# 2. The Recall Process
- We interviewed three senior executives at a major U.S. automaker to understand the recall decision-making process.
- Auto manufacturers establish product quality analysis teams to monitor quality signals from internal and external data sources.
    - Examples: internal product failure analyses, defect reports, and competitor recall information.
- When a recall may be warranted, a recall decision-making meeting is convened with senior management.
- Two important aspects of this process are:
    - (1) Firms maintain a pool of product quality issues under consideration.
    - (2) Firms scan their environment for new quality issues.
- Firms are constantly trying to determine if product quality issues are "recall-worthy" or should remain in the "wait-and-see" category.

# 3. Theoretical Model
- Consider two firms selling similar products with quality problems.
- In each period, firms can:
    - (1) Leave the product on the market and incur a continuous market cost (product failure, warranty claims, lost sales, regulatory fines) [Liu et al., 2016; Berman, 1999; Hirsch, 2014].
    - (2) Recall the product and face a stock market penalty [Davidson and Worrell, 1992].
- Firms may not be able to estimate the potential costs with certainty ex ante.
- The continuous market cost stochastically increases over time.

## 3.1. Stock Market Penalty
- We model the stock market return for firm *i* without recalls as *dS<sub>i</sub><sup>t</sup>/S<sub>i</sub><sup>t</sup> = μ<sub>i</sub>dt + σ<sub>i</sub>dB(t)*.
    - *S<sub>i</sub><sup>t</sup>* is stock price, *μ<sub>i</sub>* and *σ<sub>i</sub>* are drift and volatility, and *B(t)* is a Brownian motion.
- After issuing a leading recall, firm *l* experiences a negative shock to its stock price (Equation 1).
    - Term *φ<sub>l</sub>(τ<sub>l</sub>) < 0* captures the negative shock, where *τ<sub>l</sub>* is the recall time of the leader.
    - Attribution theory suggests the market may place a higher attribution of blame on the firm issuing the leading recall as firm *l* waits longer to issue a leading recall.
    - The term *γ<sub>l</sub>(t - τ<sub>l</sub>) > 0* represents the correction to the market returns over time.
- The follower (*f*) also experiences a negative shock *φ<sub>f</sub>(τ<sub>l</sub>, τ<sub>f</sub>) < 0* (Equation 2).
    - The market attributes less blame, resulting in a lower penalty relative to the leader.
    - The benefit of a reduced penalty attenuates over time.
    - There exists a constant *K* such that *|φ<sub>f</sub>(τ<sub>l</sub>, τ<sub>f</sub>)| < |φ<sub>l</sub>(τ<sub>l</sub>)|* for all *τ<sub>f</sub> ≤ τ<sub>l</sub> + K*.
    - *φ<sub>f</sub>(τ<sub>l</sub>, τ<sub>f</sub>)* increases with *τ<sub>l</sub>* for any given value of *τ<sub>f</sub> - τ<sub>l</sub>*.

### 3.1.1. Lemmas
- *Lemma 1 (Within-Cluster Effects):*
    - *(a)* The cumulative abnormal returns (CAR) experienced by the leader is larger in magnitude than the CAR experienced by the follower.
    - *(b)* The CAR experienced by the follower increases in magnitude as the time difference between the leading and following recalls increases.
- *Lemma 2 (Between-Cluster Effects):* The CAR experienced by the leader increases in magnitude with *τ<sub>l</sub>*.

## 3.2. Optimal Recall Strategy
- The optimal recall strategy for each firm is determined by the dynamic programming problem in Equation (3).
- *V<sub>i</sub><sup>t</sup>(R<sub>j</sub>=0, e<sub>j</sub>=0)* represents the cost-to-go for firm *i*, given that firm *j* has not yet recalled.
    - *P<sub>i</sub><sup>t</sup>(l)* is the stock market penalty for a leading recall.
    - Second term is the cost if firm *i* does not recall, including continuous market cost and future cost-to-go.
    - *r<sub>j</sub><sup>t</sup>* is the probability that firm *j* will issue a recall.
- The decision of whether to issue a following recall is determined by: *V<sub>2</sub><sup>t</sup>(R<sub>1</sub>=1, e<sub>1</sub>=t-τ<sub>1</sub>) = min{P<sub>2</sub><sup>t</sup>(f), C<sub>2</sub><sup>t</sup> + EV<sub>2</sub><sup>t+1</sup>(R<sub>1</sub>=1, e<sub>1</sub>=t+1-τ<sub>1</sub>)}*.

### 3.2.1. Propositions
- *Proposition 1:* Suppose that firm 1 announces a leading recall at time *τ<sub>1</sub>*. Then,
    - *(a)* In every time period *t* such that *τ<sub>1</sub> < t < T*, it is better for firm 2 to issue a following recall than to keep the product on the market.
    - *(b)* Given that firm 1 issued a leading recall in period *τ<sub>1</sub>*, the optimal response for firm 2 is to issue a following recall right away in the next time period (i.e., *τ<sub>1</sub> + 1*).
- *Proposition 2:* The equilibrium recall strategy in every period is threshold-type.
    - In any given period *t*, there exist thresholds *TC<sub>i</sub><sup>t</sup>, i = 1, 2* such that it is optimal for firm *i* to issue a recall if the continuous market cost *C<sub>i</sub><sup>t</sup>* is greater than *TC<sub>i</sub><sup>t</sup>* and keep the product on the market otherwise.

# 4. Hypotheses
- We develop testable hypotheses regarding recall clusters and the stock market penalty.

## 4.1. Recall Clusters
- Herding behavior occurs when firms mimic each other's actions [Devenow and Welch, 1996].
    - Firms may extract information from a competitor's recall announcement.
- Because the auto industry is highly concentrated, firms closely observe their rivals' actions [Desir, 2012] and could herd recalls for related or unrelated issues.
- GM may modify their view on a potential brake problem based on a recall by Toyota.
- *H1:* **Product recalls occur in clusters such that a leading recall by one firm excites following recalls by other firms.**

## 4.2. Stock Market Effect of Recall Announcements
- Little is known about how the timing of a recall affects shareholder value.
- Examining this may shed light on a consequence of, and motivation for, herding behavior.

### 4.2.1. Within-Cluster Stock Market Effects
- We build on *attribution theory* [Kelley and Michela, 1980].
    - When a firm's behavior is unique, external observers attribute blame to something specific about the entity [Weiner, 1985].
    - When behavior is similar to others, blame is attributed to factors beyond the entity's control [Bettman and Weitz, 1983].
- *H2a:* **The market penalty for leading recalls will be larger than the penalty for following recalls.**
- *H2b:* **The market penalty for a following recall will increase as the time since the leading recall increases.**

### 4.2.2. Between-Cluster Stock Market Effects
- We also analyze how blame attribution leads to differences in stock market effects between clusters.
- We focus on factors explaining differences in the stock market penalty associated with leading recalls.
- *H2c:* **The market penalty for a leading recall will increase as the time between clusters increases.**

# 5. Data Analysis
## 5.1. Research Setting
- We select the auto industry because:
    - (1) Most recalls are voluntarily issued.
    - (2) Firms announce a large number of recalls.
    - (3) Data are recorded by the NHTSA dating back to 1966.

## 5.2. Data Collection
- We collected data for auto recalls recorded by the NHTSA spanning 48 years (1966–2013).
- Recall data were combined with COMPUSTAT and Center for Research in Security Prices databases.
- We also collected auto model variety, volume, and age measures.
- The final sample consists of 3,117 recalls across six publicly traded auto firms.

## 5.3. Research Method: The Hawkes Process Model
- We selected the Hawkes process model because:
    - (1) Recall clustering contradicts the assumption of Poisson process models.
    - (2) The model provides leading and following recall designations.
    - (3) The recall designations are inputs to the event study.

### 5.3.1. Model Description
- The Hawkes process model is a self-excited point process model (Equation 4).
- The recall rate *λ<sub>m,t</sub>* consists of two terms:
    - The first term *exp(X'<sub>m,t</sub>β)* represents the background recall rate.
    - The second term represents the excitation of the recall rate due to prior recalls.
- If the excitation parameter *θ* is positive and statistically significant, it supports recall clustering.
- The model also includes excitation decay, *e<sup>-ω*(t-t')</sup>*, where *ω* is the decay parameter.

### 5.3.2. Model Implementation
- The Hawkes process model uses a two-step expectation-maximization (EM) algorithm.
    - The E step classifies recalls as leading, following, or neither.
    - The M step computes the log-likelihood value.
- The two steps are executed iteratively until convergence.

## 5.4. Background Recall Rate Covariates
- We include covariates used in past research to predict recall count or timing.
    - *Ln_Model Variety:* Natural log of the number of factory-installed options.
    - *Ln_Model Volume:* Natural log of the number of vehicles produced.
    - *Ln_Model Age:* Natural log of the number of days between model launch and recall.
    - *Ln_Model Age<sup>2</sup>:* To control for nonlinear effects of model age.
    - *Year-Quarter:* To control for time fixed effects.
    - *Firm:* Firm fixed effects.

## 5.5. Estimation Results
- The excitation parameter is positive and significant (*θ* = 0.24, p < 0.001), supporting *H1*.
- The decay parameter estimate is positive and significant (*ω* = 0.07, p < 0.001).
- 2,273 of the 3,117 recalls (73%) occur in clusters.
- This equates to 266 clusters with one leading recall and 7.6 following recalls.
- The average time between clusters is 16 days, and clusters last 34 days on average.
- 31% of Toyota's recalls act as leading recalls, a significantly larger proportion.
- Figure 1 displays the average marginal effect on the Poisson rate of a following recall.

## 5.6. Clustering Robustness Checks
- Our first robustness check is a placebo test.
    - We created a fictitious seventh firm and re-estimated the Hawkes process model.
    - The excitation parameter was not significant.
- The second robustness check examines differences in discretion between leading and following recalls.
    - Following recalls should be more discretionary than leading recalls.
    - Leading recalls are associated with 296,274 units recalled, while following recalls are associated with 133,682 units.
- The third robustness check is an alternate modeling approach to explore clustering.
    - A logistic regression analysis identifies if past recalls stimulate a future recall.
    - The numbers presented in Table A1 illustrate that a recall within the last 1–14 days is a significant predictor of the likelihood of an impending recall, lending further support to Hypothesis 1 and recall clustering.
- The fourth robustness check examines if the following and leading recalls within a cluster are based on the same underlying recalled component.
    - we can conclude that more than 50% of the time, the leading recall and the following recalls within a cluster are based on unrelated components, indicating that clustering is not solely an artifact of related problems across competitor firms.

# 6. Stock Market Analysis
- We now turn to our event study model to examine Hypotheses 2a–2c.

## 6.1. Estimation of Expected Returns
- To estimate the expected stock market return in the absence of a recall, we use a market model (Equation 5).
    - We use the observed daily market returns over a 30-day period prior to the event window.
- We use a GARCH model to estimate the error term (Equations 6 and 7).

## 6.2. Examining Significance of Abnormal Returns
- The second step is to test the significance of abnormal returns (Equation 8).
    - We aggregate daily abnormal returns over multiple days to create the CAR measure (Equation 9).
- We use the adjusted Patell t-test for clustered events to examine if leading and following recalls are individually associated with abnormal returns.
- Leading recall CARs are significantly more negative than following recall CARs for four event windows.

## 6.3. Inclusion of Covariates in Explaining Abnormal Returns
- The third step is to incorporate appropriate covariates:
    - *Ln_Revenue*, *Ln_Recall Size*, *Ln_Past Recalls*, *Ln_Market Share*, *Ln_Market Cap*, *Component*.
- Our main variable of interest is *Leading Recall*.
- We utilize a generalized estimating equation (GEE) model to accommodate clustered recalls and correct for serial correlation (Equation 10).
- The coefficient for *Leading Recall* is negative and significant across several time windows (Table 5), supporting H2a.

### 6.3.1. Robustness test on H2b and H2c
- To examine support for Hypothesis 2b, we replicate the event study results from Table 5 using only following recalls and examine how the time elapsed since the announcement of the leading recall influences the stock market penalty experienced by following recalls.
- To test Hypothesis 2c, we replicate the above event study only for leading recalls and explore how the time elapsed since the end of the previous cluster impacts the stock market penalty experienced by the leading recall of the next cluster.

## 6.4. Robustness Checks for Stock Market Penalty Results
- Our first stock market robustness check takes into account recalls that may have occurred in the pre-event window that is used to calculate estimated returns in the event study discussed above.
- Next, we examine if the number of units recalled may be an alternative explanation for the observed differences in the stock market penalties experienced by leading and following recalls.
- The third robustness check examines if the market corrects the penalty it applies to the leading recall as the number of following recalls increases.
- The fourth robustness check investigates if component similarity between the leading and following recalls might explain our event study results.

# 7. Discussion, Contributions, and Implications
- We find robust evidence that firms cluster recalls, with 73% of auto recalls appearing in clusters.
- These clustering findings contribute to the recall and herding literature streams.
    - Our study lays important groundwork in this burgeoning area of research and highlights the need for future work to account for competitor recall endogeneity, in particular competitor recall timing, when studying recalls.
    - Additionally, we contribute to literature on herding theory by demon-strating that firm reputation may play a key role in creating a herd (Hirshleifer and Hong Teoh 2003).
- We also find that leading recalls suffer as high as a 67% larger stock market penalty than following recalls.
    - This stock market penalty difference dissipates over time within a cluster.
- These stock market findings contribute to the recall and attribution literature streams.

## 7.1. Implications
- *We narrow the above findings and contributions to one parsimonious practical implication. Because our study demonstrates that auto firms consciously or unconsciously time their recalls to minimize stock price penalties, any implication of substance should be directed toward increasing recall decision-making transparency. Making auto recall decisions transparent should make them more timely, reducing the prevalence of clustering, which creates unnecessary delays in removing harmful products from the market.*
- *To this end, we examined how the NHTSA manages recalls compared with another major U.S. regulatory agency that oversees recalls, the Food and Drug Administration (FDA). Through Freedom of Information Act requests, we obtained FDA recall data for drug and medical device recalls from 2003–2012. That data include a mandatory, firm-provided, defect awareness date, along with a recall initiation date. This allows the FDA to measure the actual time to recall, making it harder for firms to delay a recall decision. The NHTSA does not require firms to provide defect awareness dates. However, requiring auto firms to re-port the date that they first became aware of a defect may discourage them from hiding in the herd and prompt them to make more timely and transparent recall deci-sions. We have proposed this solution to the NHTSA.*

# 8. Limitations and Future Research
- As with all secondary data studies, there could be unobserved variables and endogeneity that may bias our results.
- We encourage future researchers to explore recall clustering in industries that are less concentrated and have more diverse products.

---

# Executive summary of 1. Introduction
- The introduction establishes the context of the study, focusing on the increasing rate of automobile recalls and the unexplained delays in their announcement.
- It introduces the concept of *recall clustering*, defining it as a collection of recalls within close temporal proximity, where a leading recall excites subsequent recalls.
- The introduction contrasts this phenomenon with previous research that treats recalls as independent events.
- It presents two research questions: *(1) Do firms cluster recalls?* and *(2) Does the stock market penalty attributed to a recall depend upon its position within a cluster?*
- The authors outline their methodology, which includes developing hypotheses based on a dynamic game-theoretic model and analyzing empirical data on auto recalls over 48 years.
- It highlights the contributions of the research to herding theory and attribution theory and proposes a practical policy recommendation for the NHTSA.

# Executive summary of 2. The Recall Process
- Section 2 provides insights into the recall decision-making process in the auto industry, based on interviews with senior executives at a major U.S. automaker.
- It describes the role of product quality analysis teams in monitoring quality signals from various data sources and alerting senior management when recalls need to be considered.
- The recall decision-making process involves convening meetings with senior management from various functions to make an official recall recommendation.
- Two key aspects of the process are: (1) firms maintain a pool of product quality issues under consideration, and (2) firms scan their environment for new quality issues.
- The section emphasizes that firms are constantly evaluating whether product quality issues are "recall-worthy" or should remain in the "wait-and-see" category.

# Executive summary of 3. Theoretical Model
- Section 3 presents a dynamic game-theoretic model to analyze the recall strategies of two firms selling similar products with quality problems.
- Firms face a trade-off between leaving the product on the market and incurring a continuous market cost or recalling the product and facing a stock market penalty.
- The model incorporates attribution theory to capture the stock market penalties faced by firms issuing leading and following recalls.
- Lemmas 1 and 2 establish qualitative insights regarding the stock market penalties experienced by the leader and the follower.
- The section also derives propositions regarding the optimal timing for a firm to issue a following recall and the equilibrium recall strategies for the two firms.

# Executive summary of 4. Hypotheses
- This section develops testable hypotheses based on insights from the theoretical model and relevant literature.
- *H1* proposes that product recalls occur in clusters, with a leading recall exciting following recalls.
- *H2a* suggests that the market penalty for leading recalls will be larger than the penalty for following recalls.
- *H2b* posits that the market penalty for a following recall will increase as the time since the leading recall increases.
- *H2c* proposes that the market penalty for a leading recall will increase as the time between clusters increases.
- The hypotheses are grounded in herding theory and attribution theory, explaining how firms may strategically time their recall announcements to minimize stock market penalties.

# Executive summary of 5. Data Analysis
- This section describes the data collection and research methods used to test the hypotheses.
- The study focuses on the auto industry, analyzing 3,117 recalls across six publicly traded auto firms over a 48-year period (1966–2013).
- The Hawkes process model is selected to examine support for clustering, as it accounts for dependence among events and provides leading and following recall designations.
- The model implementation involves a two-step expectation-maximization (EM) algorithm to classify recalls and compute the log-likelihood value.
- Estimation results show that the excitation parameter is positive and significant, supporting *H1*, and the decay parameter is also positive and significant.
- The section also discusses clustering robustness checks to validate the findings.

# Executive summary of 6. Stock Market Analysis
- Section 6 details the event study conducted to examine the stock market impact of recall announcements.
- The estimation of expected returns involves using a market model and a GARCH model to account for dependence among recall events.
- Abnormal returns are calculated as the difference between observed and estimated returns, and cumulative abnormal returns (CAR) are used to measure the significance of these returns.
- The adjusted Patell t-test is employed to examine if leading and following recalls are individually associated with abnormal returns.
- The section includes covariates in the analysis to explain abnormal returns and utilizes a generalized estimating equation (GEE) model to accommodate clustered recalls.
- The results provide support for *H2a*, *H2b*, and *H2c*, indicating that the stock market penalty varies depending on the recall's position within a cluster.
- Further robustness checks are performed to ensure the validity of the stock market penalty results.

# Executive summary of 7. Discussion, Contributions, and Implications
- This section summarizes the key findings of the study, highlighting the robust evidence that firms cluster recalls and that the stock market penalty attributed to a recall varies depending on its position within a cluster.
- The findings contribute to the recall and herding literature streams, demonstrating that firms may strategically time their recall announcements to minimize stock market penalties.
- The study extends attribution theory to a recall setting, showing that the market reaction to a recall varies depending on the recall's uniqueness in comparison with competitors.
- The practical implication of the study is the need for regulators to increase recall decision-making transparency, which can be achieved by requiring auto firms to report the date they first became aware of a defect.

# Executive summary of 8. Limitations and Future Research
- Section 8 acknowledges the limitations of the research, such as the potential for unobserved variables and endogeneity.
- It suggests future research directions, including exploring recall clustering in industries that are less concentrated and have more diverse products.
- The section concludes by emphasizing the potential for future research to build upon the findings of this study and further explore the dynamics of recall clustering and its implications.

</details>

```
title: Does Organizational Forgetting Affect Quality Knowledge Gained Through Spillover?—Evidence from the Automotive Industry
authors: Anupam Agrawal, Ujjal Mukherjee, Suresh Muthulingam
journal: Production and Operations Management
published: 2020
```
 
# Executive Summary
- This study examines how **organizational forgetting** (the **loss of organizational knowledge** over time) affects the **improvements in quality** that come from **knowledge spillover** in the **automotive industry**.
- We analyze data from **191 suppliers** who provide similar products to two separate parts of a large automotive company: one that makes **cars** and another that makes **commercial vehicles**.
- We find that the **sharing of quality knowledge** is significant at **93** of the **191 shared suppliers**.
- Knowledge sharing is more likely when the components supplied to the different buyers have more **similar products** and **manufacturing processes**. It's less likely when suppliers use **separate facilities** to meet the needs of the different buyers.
- *H1*: **Improvements in quality that come from knowledge sharing decrease faster than the improvements that come from a company working directly with its suppliers to improve quality.**
    - We find that forgetting reduces the quality improvements gained from knowledge sharing by **19.09% each year**. This is a bigger loss than the **10.93%** annual reduction in quality improvements that comes from directly working with suppliers.
- The effect of forgetting depends on where the quality knowledge is kept:
    - *H2*: **Improvements in quality that come from knowledge sharing decrease the least when the knowledge is part of the company's technology, followed by when it's part of their routines. The greatest decrease is seen when the knowledge is held by the company's employees.**
    - We find that the quality knowledge that is **part of a company's routines decreases faster** (**29.14%**) than the quality knowledge that is **part of their technology** (**11.79%**) or held by **their employees** (**18.80%**).
- The effect of forgetting on knowledge sharing depends on where the quality improvement efforts are focused:
    - *H3*: **Improvements in quality that come from knowledge sharing decrease the least when the knowledge relates to the final output activities, followed by the in-process activities. The greatest decrease happens when the knowledge relates to the supplier's input activities.**
    - Quality improvements from efforts in the **output activities** (**19.29%**) decrease less than the improvements from efforts in the **in-process activities** (**25.19%**).

#knowledge_management #spillover #learning #forgetting #automotive #quality_knowledge #shared_suppliers #product_commonality #process_commonality #multiple_facilities #routines #technology #organizational_members #output_activities #in_process_activities #input_activities


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Knowledge is very important for a company's growth, ability to innovate, and efforts to improve how it operates [Argote, 2013].
- Companies are increasingly relying on their suppliers for manufacturing, especially in the automotive industry where companies work with suppliers to fix product quality problems [Handfield et al., 2006].
- The knowledge that suppliers gain about quality can be shared with other companies that use the same suppliers [Muthulingam and Agrawal, 2016].
- There are different opinions about knowledge sharing:
    - Some companies worry about it and try to prevent competitors from getting the shared knowledge [Spekman and Gibbons, 2008].
    - Others, like Toyota and John Deere, don't worry about it because they don't think it gives competitors a lasting advantage [Dyer and Hatch, 2006].
- We don't know when each of these approaches is best.
- This study looks at whether knowledge sharing leads to lasting quality improvements or if the knowledge fades over time. We also find out what operational factors affect knowledge sharing and how it fades.
- We use data about quality problems in the automotive industry to give ideas about how to manage quality in automotive supply chains.
- It's hard to study whether **organizational forgetting** (the loss of knowledge over time) affects the quality improvements that come from knowledge sharing.
- To study this, we need to show that knowledge sharing happens and improves quality, and then see if the quality gets worse over time. We also need data on quality levels, improvement projects, and supplier operations.
- We solve these problems using data from two separate businesses within a large automotive company: one that makes **Cars** and another that makes **Commercial vehicles**. These businesses are run separately and get parts from **191 shared suppliers**.
- We got data on the quality of parts coming in from these suppliers from April 2006 to December 2009, and information on **2121 quality improvement projects** that the Car business did with these suppliers starting in 2006.
- This data was used before by Muthulingam and Agrawal [2016] to find out when quality knowledge is shared.
- Our study focuses on how forgetting affects the quality improvements from knowledge sharing. We try to understand when these improvements are temporary and when they last.
- Our analysis shows that knowledge is shared from the Car business to the Commercial vehicle business at **93 of the 191 shared suppliers**.
- Knowledge sharing is more likely when the parts and manufacturing processes are more similar. It's less likely when suppliers use different factories for the different buyers.
- At the 93 suppliers where knowledge sharing is significant, we find that forgetting has a big effect. For the Commercial vehicle business, the quality improvements from knowledge sharing decrease by **19.09% each year**. For the Car business, they only decrease by **10.93% each year**. So, forgetting has a bigger effect on shared knowledge.
- Two things affect how forgetting impacts quality improvements from knowledge sharing:
    - **Where the quality knowledge is kept**: in **technology**, **routines**, or **employees**. The loss of knowledge is greatest when it's in **routines** (**29.14%**), and less when it's in **technology** (**11.79%**) or **employees** (**18.80%**). This is different from what other studies have found, which say that knowledge in technology and routines is less likely to be lost than knowledge held by employees [Argote, 2013; Argote and Darr, 2000].
    - **Where the improvement projects are done**: in **input**, **in-process**, or **output activities**. The improvements from shared knowledge in the **output activities** decrease less (**19.29%**) than the improvements from efforts in the **in-process activities** (**25.19%**). No knowledge sharing happens when the efforts are in the input activities.
- This study adds to what we know about knowledge sharing by looking at how forgetting affects the quality knowledge that comes from it.
- We also show that shared knowledge in routines fades faster than knowledge held by employees. Also, shared knowledge in in-process activities fades faster than that in output activities. We add to the existing research by showing how operational factors can affect how forgetting impacts shared quality knowledge.
- Our results are helpful for managers because they give ideas on how companies can work with suppliers and reduce the risks of quality knowledge sharing.

# 2. Hypotheses
- To improve quality, companies need to analyze problems, find the root causes, and put solutions in place [Deming, 1986; Ishikawa, 1985]. This helps suppliers gain quality knowledge.
- Once suppliers have this knowledge, they can share it with other companies that use them [D’Aspremont and Jacquemin, 1988; Jaffe et al., 1993; Knott et al., 2009; Kang et al., 2009].
- Quality knowledge that is shared can fade over time [Lapre et al. 2000; Li and Rajagopalan 1998; Agrawal and Muthulingam 2015].
- We think that shared quality knowledge will fade faster because of three things:
    - It might not fit with the company's existing knowledge [de Holan and Phillips, 2004]. This means they need to "unlearn" old ways, which makes it hard to take in new knowledge [Bettis and Prahalad, 1995].
    - The company might not be able to take in the knowledge [Cohen and Levinthal, 1990]. They might not be able to recognize, take in, and use valuable information from outside the company. They need to make an effort and invest to get the benefits of the shared knowledge [Zahra and George, 2002].
    - The company might not have the right processes to keep the shared knowledge [Argote et al., 1990; Darr et al., 1995; Epple et al., 1996]. For example, the Commercial vehicle business might not have ways to keep the process changes that the Car business started.
- *H1*: **Improvements in quality that come from knowledge sharing will decrease faster than the improvements that come from a company working directly with its suppliers to improve quality.**
- Organizational knowledge can be explicit (easy to put into words) or tacit (hard to put into words) [Argote, 2013].
- This affects where the quality knowledge is kept (technology, routines, or employees), which affects how much it fades.
- Explicit knowledge can be kept in a company's technology [Nonaka and Von Krogh, 2009; Zack, 1999]. This lets the company use the shared knowledge again and again without much change.
- *H2*: **Improvements in quality that come from knowledge sharing will decrease the least when the knowledge is part of the company's technology, followed by when it's part of their routines. The greatest decrease will be seen when the knowledge is held by the company's employees.**
- **Routines** are standard ways of doing things, but they have two parts [Zollo and Winter 2002; Feldman and Pentland 2003]:
    - **Ostensive aspect**: the standard and repeated part.
    - **Performative aspect**: the flexible and adaptable part.
- Shared knowledge in routines, especially the performative part, is likely to be lost because people make changes to it.
- Employees can be good at keeping tacit quality knowledge [Argote, 2013]. But this knowledge is likely to fade because employees leave, move to other jobs, or their skills fade [David and Brachet, 2011; Narayanan et al., 2009; Weldon and Bellinger, 1997].
- We think of manufacturing units as things that take in inputs, run processes, and give outputs [Juran, 1988].
- In output activities, quality can be improved by finding and removing bad parts and by improving transportation [Juran, 1988]. Because these activities are at the boundary between the buyer and supplier, other buyers can see the changes.
- In in-process activities, quality is improved by making the manufacturing processes better [Juran, 1988]. Other buyers might not work closely with their suppliers' production and might not take in the quality knowledge well because it's important to be close to the source of knowledge [Sorenson et al., 2006].
- In input activities, quality is improved by getting better materials (improving the quality of sub-suppliers) [Juran, 1988].
- *H3*: **Improvements in quality that come from knowledge sharing will decrease the least when the knowledge is about output activities, followed by in-process activities. The greatest decrease will be for knowledge about the supplier's input activities.**

# 3. Data and Measures
## 3.1. Data Used for the Analysis
- The analysis uses data on the quality of suppliers from the **Car** and **Commercial vehicle** businesses.
- Both businesses check the quality of suppliers using inspections of incoming parts, evaluations during the process, and final product testing.
- In 2006, the Car business created a **Supplier Improvement Unit (SIU)** of engineers who worked with 295 suppliers to improve the quality of parts coming in.
- The SIU engineers and suppliers worked together to improve quality. Suppliers kept all the benefits, and the prices of parts weren't affected by the quality improvements.
- Of the 295 suppliers, 191 also supplied parts to the Commercial vehicle business.
- From 2006 to 2009, 16 SIU engineers worked with these 191 shared suppliers to do 2121 quality improvement projects. During this time, the Commercial vehicle business didn't do any quality improvement projects with these suppliers.
- During this time, the average quality of these suppliers improved by 83.19% for the Car business and 11.97% for the Commercial vehicle business.
- We collected data on all 2121 quality improvement projects and the monthly quality of each supplier for the Car and Commercial vehicle businesses.
- We also did 32 interviews with managers from the Car and Commercial vehicle businesses and 29 interviews with people at some of the 191 shared suppliers. We spent over 17 weeks at the facilities.
## 3.2. Measures Used for the Analysis
### 3.2.1. Dependent Variable
- **Defect Rate**:
    - For the Car business, the defect rate is the number of bad parts per million received from supplier i in period t.
    - For the Commercial vehicle business, the defect rate is calculated in the same way.
### 3.2.2. Variable to Evaluate the Overall Impact of Spillover
- **Lagged Quality Improvement Experience**: This is the number of quality improvement projects that the Car business did with supplier i up to period t.
### 3.2.3. Variables to Evaluate the Impact of Where Quality Knowledge Gets Embedded
- The SIU engineers and managers decided whether the quality improvement projects focused on technology, routines, or employees.
    - 1027 projects were classified as "Technology," 821 as "Routines," and 273 as "Operator."
- **Lagged Cumulative Technology**: This is the number of "Technology" projects done with supplier i up to period t.
- "Lagged Cumulative Routines" and "Lagged Cumulative Operator" are calculated in the same way.
### 3.2.4. Variables to Evaluate the Impact of Initiative Location on Spillover
- The SIU engineers and managers also decided whether the quality improvement projects were done in the input, in-process, and output activities of suppliers.
    - 484 projects focused on input, 1161 on in-process, and 476 on output activities.
- **Lagged Cumulative Input**: This is the number of quality improvement projects related to input activities done with supplier i up to period t.
- "Lagged Cumulative In-Process" and "Lagged Cumulative Output" are calculated in the same way.
### 3.2.5. Controls
- **Lagged Production Experience**: This is the number of units supplied by supplier i up to period t.
- "Lagged Production Experience" for the Commercial vehicle business is calculated in the same way.
- **Complexity of Supplier Operations**: measured on a scale of 0 (low complexity) to 1 (high complexity).
- **Supplier Random Effects**.
- **SIU Engineer Controls**: these control for the specific SIU engineer from the Car business who worked with each supplier.
- **Product Mix Controls**: these show whether a specific product was made in a month.
- **Year Fixed Effects**: these control for things that changed over time, including changes in technology.

# 4. Methodology
- The analysis was done in three steps:
    - Find out how much quality knowledge is shared between all suppliers.
    - Find the suppliers where a lot of knowledge is shared and study what affects knowledge sharing.
    - Estimate how forgetting affects the knowledge that is shared and the knowledge that is gained directly for the suppliers where a lot of knowledge is shared.
- STATA (version 14) was used for all the analyses.
## 4.1. Identifying Shared Suppliers Where Quality Knowledge Spills Over
### 4.1.1. Assessment of the Development and Spillover of Quality Knowledge Across all Shared Suppliers
- A supplier's quality for the Car business depends on their quality improvement experience and production experience.
- We find out how much quality knowledge is gained by removing the quality improvement experience from the calculation.
- The difference between the calculations with and without quality improvement experience shows how much quality knowledge was gained from that experience for the Car business.
- We use two calculations for the Commercial vehicle business:
    - One that includes the quality knowledge gained by the Car business.
    - One that doesn't include that knowledge.
- The difference between these calculations shows how much quality knowledge is shared with the Commercial vehicle business.
### 4.1.2. Identification of Shared Suppliers with Substantial Spillover: Latent Class Analysis
- We think that there are hidden differences between the suppliers where a lot of knowledge is shared and those where very little is shared. We call these two groups S (substantial) and N (negligible).
- We use a method called latent class analysis to estimate how knowledge sharing is spread out within these two groups.
- The latent class analysis starts by estimating the average and spread of knowledge sharing and the sizes of each group (S and N). We also check that our data only supports having two groups.
- This method identifies **93 suppliers** in the group where a lot of knowledge is shared (S) and **98 suppliers** in the group where very little knowledge is shared (N).
- The analysis shows that for the group with a lot of knowledge sharing, the amount of sharing is 0.00538, which is greater than 0. For the group with very little knowledge sharing, the amount of sharing is 0.00062, which is almost 0.
- Other calculations show that it's best to divide the suppliers into just two groups.
### 4.1.3. Post hoc Identification of the Determinants of Spillover at the Shared Suppliers
- We use a method called logistic regression to see how different things affect whether a supplier is likely to be in the group with a lot of knowledge sharing or the group with very little knowledge sharing.
- We find that when the products and manufacturing processes are more similar, knowledge sharing is more likely. But when the suppliers use more factories, knowledge sharing is less likely.
## 4.2. Exploring How Organizational Forgetting Affects the Retention of Quality Knowledge
### 4.2.1. Models to Evaluate the Overall Impact of Organizational Forgetting
- We want to see if quality knowledge is only shared with the group where a lot of knowledge is shared.
- We change the calculations to include things called kQ and kPc, which show how much of the quality improvement experience and production experience from the past is still available in the future.
- Because the calculations are now more complicated, we use a special method that involves doing a lot of calculations and using a technique called bootstrapping to estimate the different parts of the calculations.
### 4.2.2. Models to Evaluate the Impact of Where Quality Knowledge Gets Embedded
- We change the calculations to divide the quality improvement experience into technology, routines, and employees, which show where the quality knowledge is kept.
### 4.2.3. Models to Evaluate the Impact of Where Quality Improvement Efforts are Undertaken
- We change the calculations to divide the quality improvement experience into input, in-process, and output activities, which show where the quality improvement efforts are focused.

# 5. Results and Robustness Checks
## 5.1. Results
- The calculations show that when the Car business does quality improvement projects, the quality of the suppliers improves.
- For the Commercial vehicle business, quality only improves for the group of suppliers where a lot of knowledge is shared.
- We find that the quality improvements from the Car business decrease by 10.93% each year. For the Commercial vehicle business, the quality improvements decrease by 19.09% each year. So, the shared knowledge fades faster than the knowledge that is gained directly.
- We confirm that the suppliers in the group where a lot of knowledge is shared gain quality knowledge from the Car business and that knowledge is only shared with the Commercial vehicle business for projects done in the in-process and output activities.
- For the Car business, the gains from technology, routines, and employee-related projects all decrease significantly over time. The losses are different for each of these areas.
- For the Commercial vehicle business, the gains from knowledge kept in technology, routines, and employees all decrease significantly over time. The losses are different for each of these areas. The shared knowledge fades fastest for knowledge kept in routines.
- The estimates indicate that the gains from induced learning in Car depreciate by 14.11%, 17.3% and 2.49% per year in the high spillover group for input, in-process, and output-related initiatives, respectively.
- The estimates imply that the gains from induced learning in Commercial for the substantial-spillover group depreciate by 25.19%, and 19.29% per year for knowledge embedded in in-process and output-related activities, respectively.

## 5.2. Robustness Checks
- We used a different method to divide the suppliers into the groups with a lot of knowledge sharing and very little knowledge sharing. The results were almost the same.
- We used a different way to measure how the quality knowledge decreases over time. The results were still similar.
- We used different statistical tests to make sure that our results are correct. The tests showed that our results are reliable.
- We looked at the effects of different things all in one calculation. The results were similar to what we found before.
- Our analysis uses methods that assume the quality improvement projects are not related to each other. To make sure this isn't a problem, we used a different method that allows the projects to be related. The results were almost the same.

# 6. Discussions and Conclusion
- The results show that shared quality knowledge doesn't give other buyers a lasting advantage.
- Shared knowledge in routines fades faster than knowledge held by employees.
- Quality knowledge about output activities fades less than knowledge about in-process activities.
## 6.1. Managerial Implications
- The faster decrease in shared knowledge can be caused by two things:
    - The type of knowledge that is shared: If the shared knowledge is mainly about "how" to fix quality problems but not "why" they happen, suppliers might not be able to keep the quality solutions working in changing environments.
    - The amount of knowledge that is shared: Not all of the knowledge is shared, which means the company that receives the knowledge gets less of a quality improvement. This makes them less likely to invest in keeping the shared knowledge.
- Three potential explanations can explain why spilled-over knowledge embedded in routines depreciates faster than such knowledge embedded in organizational members:
    - How routines are established: When knowledge is taken in passively, it doesn't get mixed well with the existing knowledge. This makes it hard to create standard activities related to routines.
    - Routines include ostensive aspects that ensure repetitive actions and performative aspects that allow agents to improvise, and allow agents to improvise, which may introduce variations that erode the quality gains over time.
    - Skills in quality initiatives that focus on organizational members can be transferred to the manufacturing of other components without modification and, hence, they are less likely to depreciate.

---

# Executive summary of 1. Introduction
- The introduction establishes the research gap: while knowledge spillover in supply chains is recognized, its long-term impact and the conditions affecting it are not well understood.
- It highlights the conflicting views on knowledge spillover, with some firms concerned about it and others unconcerned.
- We introduce our research questions: Does knowledge spillover provide sustained quality improvements, or does it depreciate over time? What operational factors affect spillover and its depreciation?
- We use longitudinal data from the automotive industry, specifically from two independent businesses within AMC (Car and Commercial), to address the research questions.

# Executive summary of 2. Hypotheses
- This section develops three hypotheses related to the depreciation of quality knowledge gained through spillover.
- *H1* proposes that quality gains from spillover depreciate faster than those from direct quality improvement initiatives.
    - Rationale: knowledge from spillover may be inconsistent with existing knowledge, lack absorptive capacity, and have inadequate processes for consolidation.
- *H2* suggests that the depreciation rate depends on where the knowledge is embedded: technology, routines, or organizational members. We predicts lowest depreciation for technology and highest for organizational members.
    - Rationale: Technology provides stable, codified knowledge, while routines offer some stability but are subject to improvisation, and knowledge in organizational members is vulnerable to turnover and skill decay.
- *H3* posits that the depreciation rate also depends on where the quality improvement initiatives are implemented: input, in-process, or output activities. We predicts lowest depreciation for output activities and highest for input activities.
    - Rationale: Output activities are more visible, facilitating better understanding and retention of knowledge, while input activities are more removed and may lack investment support.

# Executive summary of 3. Data and Measures
- This section outlines the data and measures used to test the hypotheses.
- The data includes supplier-level quality performance from Car and Commercial, focusing on 191 shared suppliers.
- We describe the data on quality improvement initiatives, defect rates, production experience, and supplier complexity.
- The key measures include:
    - **Defect rate** as the dependent variable.
    - **Lagged quality improvement experience** to evaluate the overall impact of spillover.
    - **Categorization of initiatives** into technology, routines, and operator, and input, in-process, and output activities to assess the impact of where knowledge gets embedded and where initiatives are undertaken.
    - **Control variables** such as lagged production experience, supplier complexity, SIU engineer controls, product mix controls, and year fixed effects.

# Executive summary of 4. Methodology
- The empirical analysis involves three steps:
    - Estimating the spillover of quality knowledge across all suppliers using panel data regression.
    - Identifying suppliers with substantial spillover using latent class analysis and examining the determinants of spillover using logistic regression.
    - Estimating the impact of forgetting on spilled-over and directly developed knowledge using a modified panel data regression model with a non-parametric bootstrap technique.
- We use latent class analysis to categorize suppliers into substantial and negligible spillover groups.
- We estimate model parameters using a two-step expectation maximization algorithm.
- We then use a logistic regression to understand differences between suppliers that exhibit substantial and negligible spillover.
- Finally, we analyze how organizational forgetting affects the retention of quality knowledge by modifying panel data regression models and incorporating parameters to capture the proportion of knowledge retained over time.

# Executive summary of 5. Results and Robustness Checks
- We find evidence of quality knowledge spillover from Car to Commercial at the 93 suppliers identified as the substantial-spillover group.
- The forgetting parameter is significantly lower for Commercial than for Car, supporting *H1*: Spillover knowledge depreciates faster.
-  *H2* is partially supported: spilled-over knowledge in Commercial depreciates fastest for knowledge embedded in routines, followed by knowledge embedded in operators. The least depreciation is observed in spilled-over knowledge embedded in technology.
- *H3* is partially supported: Spilled-over knowledge depreciates slower for knowledge in the output activities of a shared supplier than for knowledge in the in-process activities.
- We perform several robustness checks, including using K-means clustering, an alternative specification for measuring forgetting, and generalized estimating equations, which confirm the validity of our results.

# Executive summary of 6. Discussions and Conclusion
- This section summarizes the main findings and discusses managerial implications and future research directions.
- The key finding is that quality knowledge gained from spillover depreciates faster than that gained from direct investments.
- The depreciation rate depends on where the knowledge is embedded (routines depreciate faster) and where the initiatives are implemented (output activities depreciate slower).
- Managerial implications:
    - Faster depreciation of spilled-over knowledge is due to lack of "know-why" and lower gains in quality improvement.
    - The content of spilled-over knowledge is essentially similar to the directly created knowledge at the suppliers.
- Further research: exploring the performance of spillover mechanisms in other industries and detailed cost information to understand if there is a relation between investments in quality improvement and the depreciation of the spilled-over quality knowledge

</details>

# Mark Peecher
- Executive Associate Dean of Faculty 
- Research and Deloitte Professor of Accountancy
### education
- Ph.D., Accountancy, Dissertation: Consequentiality, Justification and Auditors' Decision Processes: A Theoretical Framework and Two Experiments, University of Illinois at Urbana-Champaign, 1994
- M.A.S., Accountancy, University of Illinois at Urbana-Champaign, 1989
- B.S., Highest Honors, Accountancy, University of Illinois at Urbana-Champaign, 1988
### research and teaching interest
- auditing

```
title: How Trial Preparation Factors Influence Audit Litigation Outcomes: Insights from Audit Litigators
authors: Eldar Maksymov, Mark E. Peecher, Jeffrey Pickerd, Yuepin (Daniel) Zhou
journal: The Accounting Review
published: 2024
```

# Executive Summary
- We investigate the **trial preparation factors** that influence **audit litigation risk (ALR)** by interviewing 39 audit litigators.
- Guided by the **elaboration likelihood model (ELM)**, we develop a framework to explain how these factors influence jurors' **elaboration levels**, which impacts **litigation outcomes**.
- Litigators strategically maneuver factors related to **trial venues, jury pools, and case arguments** to influence juror elaboration.
- **Plaintiff litigators (PLs)** prefer **lower elaboration** so that jurors rely on nontechnical information (e.g., financial losses).
- **Auditor defendant litigators (DLs)** prefer **higher elaboration** so that jurors thoughtfully consider technical information (e.g., auditing standards).
- Key factors influencing jurors' **capacity** for elaboration:
    - **Trial venue level (federal vs. state courts):** Federal courts tend to have judges who are more knowledgeable about audit litigation and technical aspects of accounting and auditing, thereby creating an environment where jurors are more likely to be *able* to engage in higher elaboration. State courts, conversely, may have judges with less expertise, hindering jurors' ability to process complex information.
    - **Juror business sophistication:** Jurors with higher levels of education and business experience are better *equipped* to understand complex auditing and accounting concepts, increasing their capacity for higher elaboration. Less sophisticated jurors may struggle to grasp technical details, leading to reliance on simpler, nontechnical cues.
    - **Argument complexity:** When case arguments are presented in a simplified, easily understandable manner, jurors are more *capable* of processing the information thoroughly. Highly complex arguments, laden with technical jargon and intricate details, can overwhelm jurors and reduce their capacity to engage in higher elaboration.
- Key factors influencing jurors' **motivation** for elaboration:
    - **Juror business orientation:** Jurors with a background or interest in business are more *motivated* to engage with the complexities of an audit case. This personal relevance increases their willingness to carefully consider technical details.
    - **Juror hometown bias:** Jurors with a strong affinity for a local plaintiff company may be less *inclined* to scrutinize the auditor's actions critically, even if the technical evidence supports the auditor's defense. This bias reduces their motivation for thorough elaboration.
    - **Argument emotionality:** Arguments that evoke strong emotions can *distract* jurors from carefully considering the technical details of the case. Emotional appeals can decrease motivation for higher elaboration, leading jurors to rely on feelings and gut reactions rather than rational analysis.
- Our findings indicate that auditors often underestimate ALR by not considering trial preparation factors, leading to potential litigation losses.
- We suggest that auditors can systematically incorporate these factors into ALR assessments by considering the characteristics of clients, venues, and potential jury pools.
- We propose strategies for auditors to better manage ALR, including broadening ALR factor monitoring, increasing audit quality, incorporating risk premia into fees, improving client communication, simplifying audit documentation, and educating the public about auditor responsibilities.
- Our study explains the mixed findings between archival and experimental methods by showing that archival research uses more federal court data, which encourages higher elaboration.

#trial_preparation #audit_litigation #elaboration_likelihood_model #juror_bias #audit_risk #litigation_outcomes #auditing_standards


<details>
    
  <summary>Click to expand sections</summary>


# 1. Introduction
- Recent research suggests that auditors often **underestimate audit litigation risk (ALR)** because they do not adequately understand factors that can heighten ALR, even on high-quality audits [Maksymov et al., 2020; Frank et al., 2021].
- Audit litigation attorneys believe that auditors do not understand and do not incorporate in their ALR assessment the factors that attorneys consider in trial preparation [Maksymov et al., 2020].
- It is critical that auditors improve their understanding of ALR factors to ensure their firms’ and the broader audit profession’s ability to sustainably strengthen capital markets [Deloitte, 2010; Harris, 2014; Peterson, 2017; Munter, 2021].
- We address three research questions:
    - What factors do audit litigators consider in preparing for trial?
    - How do they expect these factors to affect litigation outcomes?
    - How and to what advantage do they attempt to leverage these factors?
- We conduct semistructured interviews with 39 audit litigators, trial consultants, and expert witnesses, leveraging the **elaboration likelihood model (ELM)** [Petty and Cacioppo, 1986].
    - **ELM** posits that persuasion depends on the depth and extent of people’s thinking about available information.
    - Elaboration level is determined by **motivation and capacity** [Brinol and Petty, 2018].
- We outline a framework predicting how trial preparation factors influence jurors’ capacity and motivation to engage in higher elaboration and their expected effects on audit litigation outcomes.
- Litigators maneuver factors related to trial venue, potential jurors, and case arguments.
- Factors such as trial venue level, juror business sophistication, and argument complexity influence jurors' **capacity** to engage in higher elaboration.
- Factors such as juror business orientation, juror hometown bias, and argument emotionality influence jurors' **motivation** to engage in higher elaboration.
- **Plaintiff litigators (PLs)** prefer **lower elaboration**, whereas **auditor defendant litigators (DLs)** prefer **higher elaboration**.
- Auditors can systematically incorporate assessments of trial preparation ALR factors that shape jurors’ use of higher or lower elaboration [Maksymov et al., 2020; Frank et al., 2021].

## 1.1. Key Contributions
- We extend audit litigation research by clarifying how expert litigators update their expectations of case resolution by maneuvering factors related to trial venues, potential jurors, and case arguments.
- We develop a theoretical framework using the **ELM** to guide future researchers and auditors on how expert audit litigators expect key factors to influence litigation outcomes.
- We find that higher quality auditing is unlikely to mitigate the effects of some trial preparation factors that elevate ALR.
- We provide a plausible explanation of past mixed findings between archival and experimental methods.
    - Archival studies often rely on federal court data.
    - Experimental research may better reflect state court settings.

# 2. Theoretical Foundation
- Auditors assess **engagement risk**, which includes **audit litigation risk (ALR)** [DeFond, Lennox, and Zhang, 2016].
- High ALR can prevent low-quality audits [King and Schwartz, 1999], but even high-quality audits can lead to costly litigation [Palmrose, 1991, 1997; Reffett, 2010].
- Accurately assessing ALR allows firms to supply appropriate audit team know-how, set adequate audit fees, and manage client acceptance properly [Gramling et al., 1998; Shu, 2000; Blay, 2005; Kaplan and Williams, 2013; Anantharaman et al., 2016; Peterson, 2017].
- Factors considered in ALR assessment include:
    - Client's propensity for litigation
    - Frequency of auditor changes
    - Heightened user reliance on financial statements
- Auditors often overlook trial preparation factors used by expert litigators [Peterson, 2017; Maksymov et al., 2020; Frank et al., 2021].
- Litigators negotiate settlements based on their expectations of trial outcomes [Palmrose, 1991; Maksymov et al., 2020; Pickerd and Piercey, 2021].
- We aim to show how audit litigators manage these factors in trial preparation and how auditors can incorporate them into ALR assessment.

## 2.1. The Elaboration Likelihood Model Applied to Audit Litigation
- The legal literature insufficiently explores what factors litigators consider in trial preparation and how these vary depending on the side represented [Devine et al., 2001; Devine and Caughlin, 2014].
- Unique aspects of audit litigation make it distinct from other areas of law [Alexander, 1991; Eisenberg and Lanvers, 2009; Sandefur, 2015; Poppe and Rachlinski, 2016; Robbennholt and Hans, 2016].
- The technical nature of audit litigation introduces complexities unfamiliar to laypersons (e.g., jurors) but essential for case resolution [Sandefur, 2015; Poppe and Rachlinski, 2016; Maksymov and Nelson, 2017; Gimbar and Mercer, 2021; Goodson et al., 2023].
- We apply **ELM** [Petty and Cacioppo, 1986] as our theoretical lens.
    - Relevant detailed information and simple cues align closely with the technical and nontechnical aspects of a case [Alexander, 1991; Palmrose, 1997].
- When people engage in relatively **high-effort thinking (higher elaboration)**, they carefully consider the relevance of an argument's details.
- When people engage in relatively **low-effort thinking (lower elaboration)**, they are less likely to scrutinize details and make their judgment based on simple cues [Petty and Cacioppo, 1986].
- Persuaders (e.g., litigators) can influence the likelihood that people use higher elaboration by changing two determinants:
    - **Capacity:** Ability to think and process information [Petty et al., 1997]
    - **Motivation:** Willingness to think and process information [Petty and Cacioppo, 1979; Cacioppo and Petty, 1982]
- People must have both sufficient motivation and sufficient capacity [Brinol and Petty, 2012b; Griffith et al., 2018].
- We examine the roles of technical and nontechnical information in audit litigators' trial preparation.
- *RQ1:* How and to what advantage, if at all, do audit litigators try to manage characteristics of trial venues and potential jurors to affect audit litigation outcomes?
- *RQ2:* How and to what advantage, if at all, do audit litigators try to manage the characteristics of case arguments to affect audit litigation outcomes?

# 3. Method
- We use semistructured interviews of experts in audit litigation [Cohen et al., 2002; Power and Gendron, 2015; Westermann et al., 2015; Malsch and Salterio, 2016; Hayne, 2022; Maksymov et al., 2020].
- We interviewed 39 experts: attorneys, trial consultants, and expert witnesses.
- Participants had an average of 31 years of litigation experience, and interviews lasted about 39 minutes.
- Our interviews used open-ended questions to guide rather than constrain responses [Yin, 2013; Hayne, 2022].
- We carefully reviewed the relevant literature, our theoretical framework, and insights from Maksymov et al. [2020] when developing the questionnaire.
- Throughout the interview process, we made changes to the script, incorporating insights gained from prior interviews [Hirst and Koonce, 1996; Cohen et al., 2002; Westermann et al., 2015].
- We provided interviewees the primary questions from the script beforehand.
- We informed them that the interviews were confidential and that they could review a draft of our paper [Gendron et al., 2004; Power and Gendron, 2015; Hayne, 2022].
- We audio recorded each interview after obtaining the interviewee’s permission.
- We stopped the interview process after interviewees repeatedly provided the same observations, indicating saturation [Malsch and Salterio, 2016].
- We transcribed the audio recordings and identified themes suggested by the data and our theoretical framework.
- Two coauthors coded the data into the final agreed-upon themes.
- We performed a member-checking procedure and deviant case analyses, noting no elements that alter our interpretations [Malsch and Salterio, 2016].

# 4. Insights from Litigator Interviews
- *RQ1:* How and to what advantage do audit litigators try to manage characteristics of trial venues and potential jurors?
    - Litigators on each side differentially manage characteristics of trial venues and potential jurors.
    - **DLs (PLs)** try to increase the likelihood that jurors process information at **higher (lower) elaboration level** so that jurors are more likely to rely on **technical (nontechnical) aspects of the case**.
- Audit litigators try to manage the following characteristics:
    - Venue level
    - Jury pool’s business orientation
    - Jury pool’s sophistication
    - Jury pool’s hometown bias
- Venue level and jurors’ business sophistication mainly affect jurors’ **capacity** to engage in higher elaboration.
- Business orientation and hometown bias mainly affect jurors’ **motivation** to engage in higher elaboration.

## 4.1. Venue Level: Federal versus State
- Most litigators agree that **PLs typically have some choice in selecting a venue level**, noting that generally **federal courts are more favorable to auditors than the state courts.**
- Federal judges tend to be more sophisticated about audit litigation, having higher capacity to process technical information.
- PLs believe that filing claims in a federal court is less favorable to them because federal judges are much more thorough in processing each case [PA17].
- Cases in the federal court system tend to be less frivolous than cases in the state courts.
- These results indicate that federal courts give some advantage to DLs due to their tendency to have judges who have higher capacity to engage in high elaboration.

## 4.2. Jury Pool's Sophistication
- Litigators try to manage venue jurisdictions, such as different states and even different courts within a state.
- PLs in audit litigation are typically not limited to one venue and often have options of where to file because they are the first mover in litigation [DC1].
- However, auditors can also manage the jurisdiction after the case is filed [C3].
- Overall, our data suggest that venues with more sophisticated jury pools are more likely to have jurors who will use a higher elaboration level.
- Most litigators share a belief that DLs need to drive home that an audit is not a guarantee and that the real responsibility rests with management [DA3].
- Generally, most litigators note that they have to educate jurors about auditors’ specific responsibilities [C2].
- A sophisticated jury pool is particularly important to the defense because DLs focus on technical aspects of a case.
- Many PLs acknowledge that jury sophistication is critical for DLs, which is why DLs sometimes elect to avoid the possibility of a jury trial altogether when an arbitration option is available [PA15].
- DLs try to assess the sophistication level of the jury pool in a potential venue [DA2].
- Litigators’ consideration of sophistication extends not only to a venue’s jury pool but also to a venue’s judge [DA1].
- Thus, most litigators believe sophisticated jurors are more capable of being educated about auditing and accounting concepts.

## 4.3. Jury Pool's Business Orientation
- Litigators focus on the potential jury pool's business orientation.
- Venues with more business-oriented jury pools are more likely to have jurors who will use a higher elaboration level because they are more motivated to incorporate technical aspects of cases in their decisions.
- Business-oriented jury pool favors auditors [DC3].
- Jurors with relevant business experience are more likely to be able to relate to an auditor who does not know all of the information [DC3].
- Personal relevance of the information is a key determinator of one’s motivation to engage in higher elaboration [Petty and Cacioppo, 1979].
- High need for cognition is favored by DLs [C3; Cacioppo and Petty, 1982].
- Litigators keep in mind that the type of person that reads every word of a contract before signing it tends to be a defense juror, while people that just skim a contract are more likely to be plaintiff jurors [DC1].
- When a venue has a jury pool that is not business oriented, those jurors will be less motivated to process technical aspects of audit cases.
- These jurors are more likely to be influenced by nontechnical aspects, such as emotion or moral arguments.
- PLs typically prefer jurors who are more likely to be influenced by emotion [PA2].
- DLs prefer jurors who are more capable and motivated to process technical case information, whereas PLs prefer jurors who are more likely to focus on emotional aspects of a case [PA11].
- Overall, these results indicate that business-oriented jurors are more motivated to process technical aspects of cases and are more favorable to auditors.

## 4.4. Jury Pool's Hometown Bias
- Typically, litigators on both sides consider their side's home venue to be favorable to them [DA5; PA1].
- Hometown venue likely provides more benefits to plaintiffs than to auditors because jurors with pre-existing hometown bias are less motivated to process and incorporate in their judgments technical aspects of cases.
- Jurors with hometown bias are less motivated to process technical case aspects and are less favorable to auditors [DC6].

## 4.5. Characteristics of Case Arguments
- *RQ2:* How and to what advantage do audit litigators try to manage the characteristics of case arguments?
- Litigators on each side differentially manage characteristics of the case arguments.
- DLs (PLs) try to increase the likelihood that jurors process information at higher (lower) elaboration level to focus more on technical (nontechnical) aspects of the case.
- Audit litigators try to manage **complexity and emotionality** of arguments because they expect them to affect jurors’ **capacity and motivation**, respectively, to process information.
- Higher complexity and emotionality are likely to lead jurors to engage in lower elaboration and base their decisions more on nontechnical aspects of a case.

### 4.5.1. Complexity
- Complexity of technical aspects of a case is one of the features of audit litigation that makes audit litigation particularly difficult for jurors [Maksymov et al., 2020].
- Litigators on both sides strive to reduce the complexity of their arguments [A1].
- Greater complexity of a case requires greater capacity to thoroughly process the case.
- DLs are more dependent on technical aspects of the case.
- PLs can leverage jurors’ default low elaboration mode [PA8].
- This dynamic places greater pressure on DLs to educate the jury on the technical aspects of a case.
- A PL explained this dynamic as follows: “[As a PL,] you have to tell a story, and your guy has to be the good guy at the end of the day..." [PA8].
- Most litigators on both sides emphasize the substantial challenge DLs face in overcoming jurors’ natural tendency to use lower elaboration [DC5; PA15].
- It is likely to PLs’ advantage to leverage jurors’ default low elaboration by painting a high-level picture in their arguments while minimizing time on technical aspects of the case [PA4].
- DLs primarily focus on technical aspects of the case, such as audit guidance [DA2].
- To overcome jurors’ natural tendency to engage in lower elaboration, DLs spend significant effort to educate the jury [DA4].
- PL succinctly described this asymmetry between PLs and DLs in preparing arguments [PA4].
- PLs prefer jurors to use lower level of elaboration and focus on the big picture because they believe this will be more favorable to them [PA9].
- Most DLs stressed the significance of jurors being able to fully comprehend technical audit and accounting data.

### 4.5.2. Emotionality
- The extent to which an audit failure could lead to emotionality in case arguments reinforces jurors’ natural tendency to use lower elaboration.
- Greater emotionality reduces jurors’ motivation to engage in higher elaboration.
- This result is consistent with the culpable control literature in psychology [Alicke, 2000; Alicke and Rose, 2012].
- The greater the emotionality that PLs can infuse into their arguments, the higher the expected settlement is likely to be [PA11].
- PLs tend to infuse emotionality using nontechnical case aspects [PA6].
- PLs enhance emotionality of their arguments by dramatizing them in front of the jury, whereas DLs tend to stay more composed [DA1].
- Overall, our results show that PLs try to use emotionality to focus jurors on nontechnical aspects of cases, so jurors process arguments at their default lower elaboration [A3].

## 4.6. Reasons for the Difference in Litigators' Management of Juror Foci
- PLs want jurors to engage in **lower elaboration**, whereas DLs want jurors to engage in **higher elaboration**.
- Differences arise due to two pervasive features of jurors in audit litigation:
    - Misperceptions about what audits do enlarges auditor’s responsibility and are difficult to dispel during trials.
    - Jurors’ default use of lower elaboration due to a lack of capacity and/or motivation to process technical auditing and accounting aspects of cases.

### 4.6.1. Jurors' Misperceptions about Auditor Responsibility
- Jurors tend to have misconceptions unfavorable to auditors [Maksymov et al., 2020].
- Jurors typically expect auditors to identify any errors in the accounting records [PA6].
- Jurors tend to see auditing and accounting as fields with clear right and wrong answers, rather than as the judgment-based disciplines that they are [DC1].
- A key feature that causes PL versus DL differences in audit litigation is jurors’ misperceptions about the scope of financial statement audits that enlarge auditors’ responsibility, favoring PLs.

### 4.6.2. Jurors' Default Use of Lower Elaboration
- Jurors typically do not have the capacity or motivation to process technical aspects of cases at a higher elaboration level [DC6].
- Lay jurors typically lack experience working with auditors, making them less able to relate to auditors [PA1].
- The auditing profession has greater standardization compared with other professions [Madsen, 2011].
- Auditing standards tend to be highly technical and difficult for lay people to understand [DA3].
- Another key feature that causes PL versus DL approach differences in audit litigation is jurors’ default lack of capacity and motivation to process technical aspects of audit litigation cases.

### 4.6.3. Effect of Jurors' Misperceptions and Default Use of Lower Elaboration
- Jurors’ misperceptions and default use of low elaboration put DLs at a disadvantage compared with PLs.
- DLs bear the burden of overcoming jurors’ misperceptions and tendency to use lower elaboration by educating them about technical aspects [PA15].
- PLs on the other hand can succeed by focusing on the high-level nontechnical aspects of the case.
- PLs do not need to focus on technical aspects of the case to the same extent that DLs have to.
- PLs can be effective by simply keeping their arguments at a high level and leveraging jurors’ default misconceptions of auditor responsibility [A2].
- Two trial procedures enhance PLs’ position to benefit more than DLs from jurors’ misconceptions and tendency to use low elaboration.
    - First, the law requires PLs to identify a victim thereby naturally appealing to jurors’ lower elaboration (e.g., emotion).
    - Second, the law requires PLs to present their arguments before DLs have a chance to present their arguments, thereby receiving a first mover advantage.

# 5. Discussion and Conclusions
- We formulated a theoretical framework that outlines the roles of trial preparation factors in audit litigation resolution.
- Our framework illustrates the pivotal elements litigators manage during trial preparation, shaping their expectations for trial outcomes and consequent settlement terms.
    - Element 1: Factors independent of trial preparation (insurance policy limits, etc.)
    - Element 2: Factors litigators manage during trial prep (venue characteristics, juror characteristics, etc.)
    - Element 3: Juror elaboration level (capacity and motivation)
- **DLs aim to manage factors encouraging jurors to process technical case information at a higher elaboration level, whereas PLs seek to foster lower elaboration levels.**
- Higher elaboration tends to favor auditors.
- Future researchers can apply this model as a foundation for studying the complex dynamics of audit litigation.

## 5.1. Implications for Audit Research
- By understanding the elements shaping trial preparation and outcomes, scholars can delve deeper into each factor, examining their individual and collective impact on dispute resolutions.
- Researchers could scrutinize how characteristics of trial venues, potential jurors, and arguments may jointly affect elaboration levels and subsequent verdicts.
- Moreover, scholars can extend this model, incorporating additional elements or characteristics.
- Our study illuminates crucial nontechnical factors that can impact ALR.
- Archival research can investigate the association between proxies for these trial characteristics and future audit litigation outcomes.
- Experimental researchers may find the variables we identify to be valuable for juror judgment studies.
- An essential future inquiry is to scrutinize the role of elaboration level in juror judgment.
- Moreover, our findings inform the design of experimental materials, suggesting the need for careful consideration of jury characteristics, juror education, case material format, and comprehension checks in research design choices.

## 5.2. Implications for Assessing ALR
- Audit firms can use our results in assessing ALR, as they highlight factors influencing litigators’ expectations and settlements.
- Currently overlooked by the firms, these factors could significantly impact ALR assessments.
- Audit practitioners can also leverage these findings to enhance ALR assessment strategies.
- Initial resource investment into developing firm-wide risk measuring methods may be substantial but pales in comparison to typical audit litigation expenses.
- Our discussions with litigators suggest the firms’ national offices should conduct these assessments, aided by trial consultants who have databases of U.S. jury pools.
- For example, firms can use a categorical scale to assess factors’ impact on ALR.
- This approach would allow auditors to assess ALR more precisely.

## 5.3. Implications for Managing ALR Exposure
- Enhanced precision in ALR estimates will allow auditors to better manage their ALR exposure.
- For optimal exposure management, auditors should foster close relationships with their legal teams.
- Small firms, in particular, often delay seeking legal advice on potential disputes, increasing their ALR exposure [Frank et al., 2021].
- Our results and prior research also suggest some specific ways for how auditors can proactively reduce unnecessary ALR exposure.
    - First, incorporating ALR assessments into client acceptance practices coupled with risk-based fee premia to offset ALR is a logical approach.
    - Second, our research implies that exceeding audit standards to provide superior audit quality is necessary in certain situations to lessen ALR exposure.
    - Third, auditors can limit ALR exposure by effectively managing client expectations to reduce breach of contract lawsuits.
    - Fourth, auditors can limit ALR by simplifying audit workpapers and documentation.
    - Fifth, auditors, regulators, and institutions like the Center for Audit Quality (CAQ) can help enhance public understanding of auditing.
    - Last, standard-setting bodies like the PCAOB and the AICPA can do their part in better protecting investors and decreasing auditors’ unnecessary exposure to ALR by simplifying language in auditing standards.

---
# Executive summary of 1. Introduction
- We address the gap in auditor’s understanding of trial preparation factors that can elevate audit litigation risk, independent of audit quality.
- We address three research questions: (1) What factors do audit litigators consider in preparing for trial? (2) How do they expect these factors to affect litigation outcomes? (3) How and to what advantage do they attempt to leverage these factors?
- We conduct interviews with audit litigators leveraging the elaboration likelihood model (ELM), which suggests that persuasion occurs differently depending on the depth of thinking about available information. The depth of thinking, or elaboration, depends on one’s capacity and motivation to process the information.
- We then outline a framework predicting how various trial preparation factors influence jurors’ capacity and motivation to engage in higher elaboration and their expected effects on audit litigation outcomes.

# Executive summary of 2. Theoretical Foundation
- Auditors assess audit litigation risk (ALR) as part of engagement risk. While high ALR can prevent low-quality audits, even high-quality audits may face costly litigation. Accurately assessing ALR helps firms allocate appropriate resources, set fees, and manage client relationships.
- Audit guidance stresses ALR assessment but does not prescribe methods. Firms consider factors like client litigation history, auditor changes, and user reliance on financial statements. Recent insights suggest auditors often overlook trial preparation factors used by expert litigators.
- We apply the elaboration likelihood model (ELM) to examine how litigators evaluate and influence trial preparation factors. ELM posits that persuasion depends on cognitive effort, ranging from low to high elaboration. Litigators can influence elaboration by changing jurors' capacity and motivation to process information.

# Executive summary of 3. Method
- We used semi-structured interviews with 39 experts in audit litigation: attorneys, trial consultants, and expert witnesses.
- The interviews, lasting about 39 minutes each, used open-ended questions to allow for detailed responses.
- The script was developed based on literature, the theoretical framework, and prior research.
- We followed a rigorous process of transcription, theme identification, coding, and member-checking to ensure the reliability and validity of the findings.

# Executive summary of 4. Insights from Litigator Interviews
- In preparing audit cases, litigators on each side differentially manage characteristics of the trial venues and potential jurors—DLs (PLs) try to increase the likelihood that jurors process information at higher (lower) elaboration level so that the jurors are more likely to rely on technical (nontechnical) aspects of the case in making their judgments.
- Audit litigators try to manage venue level, jury pool’s business orientation, jury pool’s sophistication, and jury pool’s hometown bias.
- Venue level and jurors’ business sophistication mainly affect jurors’ capacity to engage in higher elaboration, whereas business orientation and hometown bias mainly affect jurors’ motivation to engage in higher elaboration.
- Litigators have different strategies in eliciting potential jurors’ focus—PLs want jurors to engage in lower elaboration so that jurors focus more on nontechnical aspects of the case, whereas DLs want jurors to engage in higher elaboration so that jurors focus more on technical aspects.

# Executive summary of 5. Discussion and Conclusions
- We develop a theoretical framework showing the pivotal elements litigators manage during trial preparation, shaping their expectations for trial outcomes and settlement terms.
- The model incorporates three influential elements on settlement terms: factors independent of trial preparation, factors litigators aim to manage during trial prep, and elaboration level of jurors.
- The framework shows that litigators manage factors to elicit higher (auditor defense) or lower (plaintiffs) juror elaboration.
- We conclude that by understanding the elements shaping trial preparation and outcomes, scholars can delve deeper into each factor and examine their individual and collective impact on dispute resolutions. Our findings have implications for future audit research and assessing and managing ALR.

</details>

```
title: Do Stronger Wise-Thinking Dispositions Facilitate Auditors’ Objective Evaluation of Evidence When Assessing and Addressing Fraud Risk?
authors: BILLY E. BREWSTER, ALEX J. JOHANNS, MARK E. PEECHER, IRA SOLOMON
journal: Contemporary Accounting Research
published: 2021
```
 
# Executive Summary
- **Audit standards** require auditors to objectively evaluate evidence when assessing fraud risk.
- Prior research suggests auditors often fail to **objectively evaluate evidence**, leading to calls for heightened **professional skepticism**.
- This study introduces **wise-thinking dispositions (WTDs)**, defining them as the tendency to engage in balanced belief revision through open and reflective thinking about evidence.
    - Key factors contributing to WTDs, and examples of questions used to measure them, include:
        - A high degree of **reflective thinking** (e.g., agreement with statements like "I always try to look at all sides of a problem" or disagreement with "I sometimes find it difficult to see things from another person’s point of view").
        - A tendency towards **evidence-based beliefs and doubts** (e.g., agreement with "One should always revise beliefs in response to new information or evidence" or disagreement with "One should disregard evidence that conflicts with one’s established beliefs.").
        - Consideration of **counterfactual thinking** (measured indirectly through items assessing openness to different perspectives and beliefs, e.g., disagreement with "My beliefs would not have been different if I had been raised by a different set of parents.").
    - **WTDs are distinct from intellect, knowledge, and abilities**: While intellect, knowledge, and abilities can be compared to a car's engine, **WTDs are like the car's driver**, guiding how these resources are applied. Unlike knowledge, which improves rapidly with instruction, WTDs are relatively stable traits that are not easily influenced by situational factors, requiring longer-term instructional approaches for development.
    - **Stronger WTDs** are hypothesized to improve auditors’ propensity to form different beliefs and take different actions given an objective evaluation of different states of evidence.
- The study investigates whether **auditors vary in WTD strength**.
- **Two experiments** were conducted to examine whether auditors with stronger WTDs objectively evaluate and respond to audit evidence when assessing and addressing fraud risk.
- **Experiment 1:** Auditors with stronger WTDs assessed fraud risk higher (lower) when evidence was more (less) indicative of fraud (**+22.21%**, p < 0.001 for more vs. less indicative evidence), regardless of skepticism prompts. Conversely, those with weaker WTDs did not show a significant difference in risk assessment based on evidence type.
- **Experiment 2:** Auditors with stronger WTDs were significantly more likely to take actions that persuasively address heightened fraud risk when evidence was more indicative of fraud (a **93.33%** vs. **63.16%** rate of appropriate follow-up, p = 0.020), even without skepticism prompts. In contrast, those with weaker WTDs were not significantly more likely to take appropriate action.
- **Confirmatory bi-factor analyses** validate the WTD construct, showing one higher-order general factor and three subfactors (reflective thinking, evidence-based beliefs & doubts, and counterfactual thinking).
- WTDs are shown to readily diverge from **Hurtt Trait Skepticism (HTS)** scores, indicating they measure different constructs:
    - While HTS reflects a general tendency to question and doubt, it **doesn't necessarily translate into objectively evaluating evidence**; auditors with high HTS may consistently assess higher fraud risk regardless of the evidence, unlike high-WTD auditors who adjust their assessment based on the evidence's strength.
    - A factor analysis showed that most factors loaded exclusively on HTS or WTD items, and interfactor correlations were generally weak, showing little overlap between the two constructs.
- Findings suggest that quality control standards and audit working paper review processes should consider that auditors may not objectively evaluate evidence unless they have stronger WTDs.
 
#wtd #wise_thinking_disposition #fraud_risk #professional_skepticism #auditor_judgment #evidence_evaluation #thinking_dispositions #earnings_management #experimental_design #objectivity #hurtt_trait_skepticism


<details>
    
  <summary>Click to expand sections</summary>

 
# 1. Introduction
- Audit standards emphasize the need for **objective evaluation of evidence** when assessing and addressing fraud risk [AICPA AU-C 200, 2014a; 240, 2014b; 330, 2014c; 500, 2014d; PCAOB AS 1015, 2010a; 1101, 2010b; 1105, 2010c; 2110, 2010d; 2401, 2010f].
    - Failure to do so increases investors' loss exposure and audit firms' legal risks [Knechel 2013; Rapoport 2018; Kowsmann et al. 2020; Rothenberg 2020].
- **Experimental studies** suggest that auditors often fail to objectively evaluate evidence [Nelson and Tan 2005; Nelson 2009; Knechel et al. 2013; Bauer et al. 2020].
    - This has led to calls for auditors to enhance their **professional skepticism** [Nagarajan 2020; Nicodemus 2020].
    - There have been appeals for research into auditor characteristics that influence objective evidence evaluation [Brown et al. 1999; Kachelmeier et al. 2014; Nolder and Kadous 2018].
- We introduce the construct of **wise-thinking dispositions (WTDs)**.
    - We theorize that auditors with stronger WTDs objectively evaluate evidence of fraud.
    - **Thinking dispositions** are defined as behavioral capacities and inclinations toward certain cognitive patterns [Perkins et al. 1993] or higher-level cognitive styles [Stanovich 2001].
    - These guide the use of intellect, knowledge, and abilities [Fuller and Kaplan 2004; West and Sa 1999; Sternberg and Grigorenko 1997; Perkins et al. 2000; Sternberg 2001].
    - **Stronger WTDs** are dispositions to engage in wise thinking, which involves balanced belief revision by thinking openly and reflectively about evidence [Meacham 1983; Kitchener and Brenner 1990; Ardelt 2003; Staudinger and Glück 2011a].
    - The **WTD construct** is developed from literatures on thinking dispositions, wisdom, and epistemic rationality [Meacham 1983; Stanovich and West 1997, 1998; West and Sa 1999; Ardelt 2003, 2004; Birren and Svensson 2005; Stanovich and West 2007, 2008; Staudinger and Glück 2011a; Glück et al. 2013].
    - A substantive contribution is introducing and measuring the WTD construct.
 
- We address the question of how much audit professionals vary in WTD strength.
    - If auditors with weaker WTDs consistently fail to objectively evaluate evidence, firms may prevent their entry or promotion.
    - However, auditors are not always rewarded for objective fraud evidence evaluation [Peecher et al. 2013; Hobson et al. 2017].
    - They may be rewarded for aligning with management goals [Kadous et al. 2003; Koch and Salterio 2017; Bhaskar et al. 2019].
    - Thus, we provide empirical evidence on variation in auditors' WTD strength.
- We provide theory-based evidence from two experiments that auditors with relatively stronger WTDs objectively evaluate and respond to audit evidence when assessing and addressing fraud risk.
    - Different audit contexts are used for enhanced confidence in the reproducibility of the predicted judgment benefits of stronger WTDs [Hail et al. 2020].
    - The dependent variables in the two experiments are different in a key respect.
    - Experiment 1 participants assess fraud risk after management makes a last-minute reduction to warranty expense that allows earnings to narrowly beat analysts’ forecasts.
    - Experiment 2 participants also assess fraud risk, but in a goodwill impairment setting.
    - Moreover, Experiment 2 participants also specify planned actions to address fraud risk.
- Experiment 1 includes a skepticism prompt, while Experiment 2 excludes it.
    - This allows us to investigate whether skepticism prompts attenuate the judgment benefits of stronger WTDs.
    - The question of whether stronger WTDs help unprompted auditors’ objectivity remains unaddressed.
    - Thus, reporting the results of a second experiment reexamining whether stronger WTDs enable auditors to objectively evaluate evidence given the absence of any skepticism prompt strengthens our theory testing [Asay et al. 2021].
- Findings in both experiments indicate that the strength of auditors’ WTDs varies considerably and provide support for our prediction.
    - In Experiment 1, only auditors with stronger WTDs assess fraud risk to be significantly higher (lower) when Evidence is more (less) indicative of fraud, and they do so regardless of skepticism Prompt condition.
    - In Experiment 2, only auditors with stronger WTDs are significantly more likely to take actions that more persuasively address the heightened risk of fraud when the underlying Evidence is more indicative rather than less indicative of fraud.
- Finally, we examine WTDs within a confirmatory bi-factor model as well as use a third experiment to examine the divergence of WTDs from Hurtt Trait Skepticism (HTS) scores.
    - Results confirm that WTDs are comprised of one higher-order general factor as well as three subfactors and that WTDs readily diverge from HTS.
- We conclude by discussing implications and opportunities for future research.
 
# 2. Background and Hypotheses
- Audit standards prescribe that auditors should objectively evaluate evidence when assessing and addressing the risk of material misstatement due to error or fraud [AICPA AU-C 240, 2014b; PCAOB AS 1105, 2010c; 2110, 2010d; 2301, 2010e; Bell et al. 2005; Peecher et al. 2007].
    - Objective evaluation requires a "balanced assessment of all relevant circumstances" [IIA 2021].
    - It is paramount for the proper exercise of professional skepticism [PCAOB AS 1101.08-0.11, 2010b; 1105, 2010c].
    - Auditors should increase (decrease) the assessed risk of fraud when evidence is objectively more (less) indicative of material misstatement due to fraud [Nolder and Kadous 2018].
    - They should also objectively alter their audit plans to address higher or lower fraud risk.
- Numerous studies find that, on average, auditors often fail to adapt their plans [Brown et al. 1999; Waller and Zimbelman 2003; Nelson and Tan 2005; Nelson 2009; Hammersley et al. 2010; Kochetova-Kozloski et al. 2011; Trotman and Wright 2012; Trompeter et al. 2013; Knechel et al. 2013; Hammersley et al. 2011; Kachelmeier et al. 2014; Griffith et al. 2015].
    - Zimbelman [1997] finds audit seniors and managers fail to alter assessed misstatement risk based on the presence versus absence of incentives for management to materially misstate assets.
    - Brown et al. [1999] shows audit seniors diagnosing the cause of an unexpected increase in income fail to prioritize evidence having greater diagnostic value.
    - Hoffman and Zimbelman [2009] shows that, unless prompted to think strategically, auditors fail to appropriately revise planned audit procedures to address seeded fraud cues.
    - Trotman and Wright [2012] finds auditors fail to respond to external performance indicators that objectively heighten misstatement risk despite seemingly positive internal performance indicators under management’s control.
    - Kachelmeier et al. [2014] finds that participants in an auditor role fail to distinguish between evidence that is more versus less indicative of intentional misstatement.
    - Overall, auditors appear on average to have difficulty objectively evaluating evidence.
 
### 2.1. The cult of the average, thinking dispositions, and WTDs
- Social psychologists warn against focusing solely on "the cult of the average" when studying human behavior [Achor 2010].
- The audit literature has yet to identify auditor-specific constructs that result in "above average" objectivity in evaluating evidence.
- We introduce **WTDs** to differentiate auditors in terms of their propensity for objectively evaluating audit evidence.
    - **Thinking dispositions** are behavioral capacities and inclinations to engage in particular patterns of cognitive behavior [Perkins et al. 1993] or higher-level cognitive styles [Stanovich 2001].
    - They capture variation in peoples’ goal management, epistemic values, and epistemic self-regulation [Stanovich et al. 2018].
    - They guide the application of decision makers’ intellect, knowledge, and abilities [West and Sa 1999; Sternberg and Grigorenko 1997; Perkins et al. 2000; Sternberg 2001], which helps calibrate how strongly they believe something to be true in light of available evidence [Stanovich et al. 2018].
    - A useful analogy is to compare individuals’ intellect, knowledge, and abilities to the engines of cars while thinking dispositions are the drivers of cars [Stanovich 2009].
- Measures of intelligence display only moderate to weak correlations with some thinking dispositions and near-zero correlations with others [Stanovich 2009].
- Thinking dispositions differ from knowledge in that they are stable traits that help explain intellectual performance and are not easily influenced by situational factors [Perkins et al. 2000].
- Thinking dispositions can improve over the longer term with instructional approaches, specifically an enculturation model [Tishman et al. 1993; Perkins et al. 2000].
- The disposition on which we focus concerns individuals’ disposition to engage in wise thinking.
    - **Wise thinking** is rooted in the construct of wisdom in both sociology [Ardelt 2003, 2004; Ardelt and Oh 2015] and psychology [Sternberg 2003; Staudinger and Glück 2011a].
    - Reflective thinking, which balances knowing and doubting in light of uncertainty, is the paramount taproot of wisdom [Ardelt 2003; Stanovich and West 2008; Hall 2010; Staudinger and Glück 2011b].
    - We define stronger WTDs as dispositions by which individuals naturally engage in wise thinking, which is the balanced revision of beliefs and doubts about target phenomena by thinking openly and reflectively about available evidence [Birren and Fisher 1990; Meacham 1983; Kitchener and Brenner 1990; Ardelt 2003; Staudinger and Glück 2011a].
    - Stronger WTDs likely improve auditors’ propensity to form different beliefs and take different actions given an objective evaluation of different states of evidence [Baron 1993, 2008; Meacham 1990; Tishman et al. 1993; Ardelt 2003; Stanovich 2009].
 
### 2.2. Hypotheses
- *H1:* **Auditors’ fraud risk assessments and planned actions** to address fraud risk will reflect a higher (lower) risk of fraud when evidence is more (less) indicative of fraud, but only when they have stronger WTDs.
    - Implicit in this hypothesis is the assumption that sufficient variation exists in WTD strength across auditors.
    - While prior studies demonstrate that adults vary in wisdom and epistemic rationality, no study examines these two phenomena in professionals charged with a responsibility to be objective.
- *RQ1:* To what extent do auditors vary in the strength of their WTDs?
    - This question is asked because it is not known whether adults who self-select into the audit profession may do so in part because they have, and enjoy applying, relatively strong WTDs. It also could be that professions have developed screening processes to weed out individuals with weaker WTDs.
- We investigate whether the presence and the nature of external professional skepticism prompts constitute boundary conditions for our hypothesis.
    - Audit standards implicitly assume that being mindful of the duty to exercise professional skepticism throughout the audit will help ensure the objective, good-faith evaluation of audit evidence [PCAOB AS 1015.07-0.09, 2010a].
    - Skepticism prompts remind auditors of particular attitudes to take when evaluating evidence [Peecher 1996; Brown et al. 1999; Turner 2001; Peytcheva 2014; Grenier 2017].
- We investigate whether skepticism prompts moderate our hypothesis by including two skepticism prompts in Experiment 1 and excluding skepticism prompts in Experiment 2.
    - Until the late 1990s, audit standards emphasized that professional skepticism requires a neutral attitude toward evidence (e.g., SAS No. 53).
    - Today’s audit standards, by contrast, have evolved toward an attitude of presumptive doubt [Bell et al. 2005; Nelson 2009; Cohen et al. 2017].
    - To capture this evolution, we employ both a less doubtful prompt (neutral) that reminds auditors to be skeptical if signs emerge that management lacks integrity and a more doubtful prompt (doubtful) that reminds them to be skeptical even if no signs emerge that management lacks integrity.
- *RQ2.1:* Does a doubtful prompt impair the propensity for auditors with stronger WTDs to reach judgments and decisions that reflect a higher (lower) risk of fraud when the underlying audit evidence is more (less) indicative of fraud?
- *RQ2.2:* Does a neutral prompt enhance the propensity for auditors with weaker WTDs to reach judgments and decisions that reflect a higher (lower) risk of fraud when the underlying audit evidence is more (less) indicative of fraud?
- *RQ2.3:* Does the propensity for auditors with stronger WTDs to respond objectively to evidence that is more indicative versus less indicative of fraud (Hypothesis 1) hold in both the presence and the absence of skepticism prompts?
 
# 3. Experiment 1
- Participants are 87 auditors from a Big 4 firm with an average audit experience of 44.5 months.
    - Auditors at this level commonly evaluate management’s nonrecurring journal entries.
    - Participants reported the number of engagements in which they had reviewed nonrecurring journal entries, and the mean (standard deviation) of their response is 10.1 (19.7), so we conclude that our participants have the requisite task experience.
- In Part I, participants first read a randomly assigned skepticism prompt and then they read about a fictitious audit client (AUP) which manufactures and sells auto parts.
    - They receive summary financial performance, a description of a prior favorable relationship with management, and a mock Forbes magazine interview by the CFO emphasizing integrity and a commitment to high-quality financial reporting.
    - Participants next evaluate a nonrecurring journal entry that reduces warranty expense.
    - Earnings fell short of analysts’ forecasts before this entry but beat forecasts afterwards.
    - The CFO explains that lower warranty expense reflects higher product quality and lower future product returns.
    - The Evidence manipulation either corroborates or undermines management’s explanation.
    - Participants then assess the likelihood that management is trying to commit fraud, engage in within-GAAP earnings management, or use high-quality accounting.
- After Part I, participants place materials in an envelope and begin Part II, providing demographics and completing scales for WTD and Tacit Knowledge.
 
### 3.1. Experimental Design
- We use a 222 (3) mixed design.
    - The (3) within-subjects part of the design entails the assessed likelihoods of Attempted Fraud, Within-GAAP Earnings Management, and High-Quality Attempt.
    - The 222 entails between-subjects manipulations of Evidence (more indicative vs. less indicative of fraud) and skepticism Prompt (doubtful vs. neutral) as well as a median-split measure of participants’ WTDs (stronger vs. weaker).
- The approach we use is as follows: in the less indicative Evidence condition, two key performance indicators (KPIs), lower product returns and less rework, corroborate management’s explanation for reducing warranty liability.
- In the more indicative Evidence condition two additional KPIs cast doubt on management’s higher product quality story: assembly-worker overtime and production-line utilization rates increased dramatically in the fourth quarter relative to the prior year, likely leading to higher worker fatigue and reduced product quality, thereby increasing future product returns.
- To implement the skepticism Prompt, we use a Federal Bureau of Investigation (FBI) double-agent vignette in both levels.
    - The doubtful level emphasizes being skeptical even if no signs appear to exist that management lacks integrity.
    - The less doubtful neutral level emphasizes being skeptical if red-flag signs emerge that management lacks integrity.
    - The doubtful vignette describes an FBI double agent who provided no outward cues of treason.
    - The neutral vignette describes the same double agent but reveals red flags of treason—that is, a lot of cash inexplicably found in his home.
 
### 3.2. Measurement of Auditors' WTDs
- To measure auditors’ WTDs, we construct a survey with 22 statements from everyday life.
- The survey adapts statements from Ardelt’s (2003) Three-Dimensional Wisdom Scale (i.e., 3D-WS) and from Stanovich and West’s (2007) Actively Open-Minded Thinking (AOMT) scale.
    - The three dimensions of 3D-WS are reflective, cognitive, and affective, with the reflective dimension being paramount (Ardelt 2003).
    - Five of the WTD survey statements adapted from 3D-WS relate to its reflective dimension, and two WTD survey statements are adapted from the cognitive dimension.
    - Our remaining 15 WTD survey statements are reflective and cognitive items from the AOMT.
    - A disposition to use active open-minded thinking is foundational for wise thinking, as prior literature notes [Stanovich 2001; Stanovich and West 2007, 226; Stanovich and West 2008, 130; Stanovich 2009].
- Participants express agreement, neutrality, or disagreement with each statement, using two categories of agreement and disagreement (e.g., strongly agree, agree, neutral, disagree, or strongly disagree).
- For each statement, the wisest response is strongly agree or strongly disagree.
- We score WTDs using the sum of the squared deviations: we record a deviation of 0, 1, 2, 3, or 4 and then square these deviations—that is, 0, 1, 4, 9, or 16, going from most-wise to least-wise responses.
- We then sum the squared deviations across the 22 statements with lower (higher) values indicative of stronger (weaker) WTD.
- Using the sum of squared deviations is a common way to score forms of practical intelligence.
- The sum of squared deviations helpfully increases the systematic variation in a measured construct relative to using the sum of linear deviations.
 
### 3.3. Dependent Measures
- We aimed to capture the degree of participants’ concern about potential fraud.
- We asked participants to assess the likelihood that management is attempting to engage in each of the following:
    - Fraud or Near Fraud—An intentional attempt to distort business reality that clearly violates GAAP or that really “pushes the envelope” of GAAP.
    - Earnings Management—An intentional attempt to “window dress” business reality that is acceptable under GAAP.
    - High-Quality Attempt—A good-faith attempt by management to use accounting that fairly and neutrally reflects business reality. This attempt could be successful, resulting in High Quality Accounting, or unsuccessful, resulting in Erroneous Accounting.
- We include the near fraud category to capture instances in which auditors are concerned about potential fraud but reluctant to believe management is committing out-and-out fraud [Hobson et al. 2017].
- A factor analysis returns three factors, but with the first factor being the strongest.
- Given this factor structure, we use two dependent measures to test our hypothesis.
    - One dependent measure is Attempted Fraud—the average of Fraud and Near Fraud—as the dependent variable.
    - The second dependent measure is a difference score between Attempted Fraud and the average of the two non-misstatement categories of Within-GAAP Earnings Management and High-Quality Attempt.
 
### 3.4. Check on prompt manipulation
- Right after reading the Prompt manipulation, participants assess whether the main point of the FBI double-agent narrative is that auditors should be skeptical when cues emerge that signal management lacks integrity.
- Participants’ evaluations fall below the midpoint in the doubtful condition but above the midpoint in the neutral condition, consistent with a successful Prompt manipulation.
 
### 3.5. RQ 1: Does the strength of auditors’ WTDs vary?
- We find considerable variation in the strength of auditors’ WTDs, ranging from 31 to 135.
- We find no evidence that greater general or task experience significantly correlates with stronger WTDs.
- Consistent with random assignment, WTD strength does not differ across the Prompt manipulation, the Evidence manipulation, or their interaction.
- Last, we use a median split to classify auditors as having stronger or weaker WTDs to enable use of planned contrasts.
- Means (standard deviations) for auditors with stronger WTDs are 51.95 (9.66) and for auditors with weaker WTDs are 83.32 (13.93).
 
### 3.6. Hypothesis 1 test
- Operationally, Hypothesis 1 predicts an Evidence effect, but only for stronger WTD auditors which would manifest as a two-way EvidenceWTD interaction.
- That is, we predict relatively higher likelihood assessments of fraud given Evidence that is more indicative as compared to less indicative of fraud, but only for stronger WTD auditors.
- Using the language of Guggenmos et al. [2018, 232], the hypothesis predicts a “Pac-Man” shaped interaction.
- We infer objective evaluation of evidence if auditors assess significantly higher likelihoods of fraud when Evidence is more indicative of fraud than when it less indicative of fraud.
- We infer an absence of objective evaluation of evidence if assessed likelihoods of fraud do not differ significantly across Evidence levels or if they differ significantly, but in the direction opposite to what the Evidence warrants.
- We test our hypothesis by using visual fits, planned contrasts, semi-omnibus tests, and contrast variance residuals, q2 [Guggenmos et al. 2018].
- The predicted “Pac-Man” shapes clearly manifest for both dependent variables.
- The contrast weights are [0, +1, 0, 1] corresponding to the labels [A, B, C, D] in Figure 1.
- The planned contrast is highly significant using Attempted Fraud and using Fraud vs. Not Misstated.
- As hypothesized, auditors assess a significantly higher (lower) risk of material misstatement due to fraud when Evidence is more (less) indicative of fraud, but only if they have a stronger WTD.
- In addition, the semi-omnibus F-test for the residual between-cell variance is not significant for Attempted Fraud or for Fraud vs. Not Misstated.
- Finally, there is evidence that our independent factors influenced auditors judgments in ways other than hypothesized in that the contrast variance residual is nontrivial both for Attempt Fraud and for Fraud vs. Not Misstated.
- Overall, these results support Hypothesis 1: auditors assess a significantly higher (lower) risk of material misstatement due to fraud when Evidence is more (less) indicative of fraud, but only if they have stronger WTDs.
 
### 3.7. Investigation of RQ2
- RQ2 has two parts addressed by Experiment 1, with RQ2.1 focusing on stronger WTD participants and RQ2.2 focusing on weaker WTD participants.
- *RQ2.1*: Does a doubtful prompt impair the propensity for auditors with relatively stronger WTDs to make judgments and decisions that reflect a higher (lower) risk of fraud when the underlying audit evidence is more (less) indicative of fraud?
- We examine the assessed likelihood of fraud (Attempted Fraud), and the differences between the assessed likelihood of Attempted Fraud and the average of the assessed likelihoods of Within-GAAP Earnings Management and High-Quality Attempt (Fraud vs. Not Misstated).
- In both two-way ANOVAs, Evidence is highly significant, and neither Prompt nor EvidencePrompt is close to being significant.
- Furthermore, in both two-way ANOVAs, the Evidence main effect accounts for at least 97.9% of the total between-cell variance while the Prompt main effect and the PromptEvidence interaction account for a total of 2% or less of the total between-cells variance.
- We examine all the simple main effects associated with this ANOVA, reporting p-values for Fisher’s least significant difference (LSD) test and for Tukey’s honestly significant difference (HSD) test.
- The simple effect of Evidence is statistically significant for both the neutral and the doubtful levels of skepticism Prompt for Fisher’s LSD and at least moderately statistically significant using the conservative Tukey’s HSD test.
- These results indicate that, regardless of skepticism Prompt level, stronger WTD auditors express greater (lesser) concern about fraud risk when Evidence is more (less) indicative of fraud.
 
### 3.8. Investigation of RQ2 (cont.)
- *RQ2.2:* Does a neutral prompt enhance the propensity for auditors with relatively weaker WTDs to reach judgments and decisions that reflect a higher (lower) risk of fraud when the underlying audit evidence is more (less) indicative of fraud?
- We examine the assessed likelihood of fraud (Attempted Fraud), and the differences between the assessed likelihood of Attempted Fraud and the average of the assessed likelihoods of Within-GAAP Earnings Management and High-Quality Attempt (Fraud vs. Not Misstated).
- Prompt is the only factor that is significant.
- Evidence is not significant, and neither is the EvidencePrompt interaction.
- Furthermore, in both two-way ANOVAs, the Evidence main effect accounts for only 18.2% (Attempted Fraud) and 4.4% (Fraud vs. Not Misstated) of the total between-cell variance.
- We examine all of the simple main effects associated with this ANOVA, again reporting p-values for Fisher’s LSD test as well as for Tukey’s HSD test.
- While the simple effect of Evidence clearly fails to attain statistical significance at an alpha of 0.05 for both dependent variables at the doubtful level of the skepticism Prompt for Fisher’s LSD, it approaches significance at the neutral level of the skepticism Prompt for Attempted Fraud.
- However, this finding is diminished in that Evidence fails to obtain significance at conventional levels for the Fraud vs. Not Misstated dependent measure.
- Furthermore, using the Tukey HSD test, no Evidence effect is significant.
- What we observe instead is that the skepticism Prompt affects how highly auditors with weaker WTDs assessed fraud risk, as they assess it higher when reminded to be doubtful than when reminded to be neutral.
- Notably, the Prompt main effect is significant in each of the two-way ANOVAs without any EvidencePrompt interaction.
- Overall, these results provide only limited evidence that our neutral skepticism prompt can be helpful to auditors with weaker WTDs.
 
# 4. Experiment 2
- In Experiment 1, we find that, as hypothesized, auditors evaluate Evidence objectively, but only when they have stronger WTDs.
- However, in all of the Experiment 1 conditions auditors were prompted to exercise professional skepticism.
    - Thus, an alternative interpretation of Experiment 1 findings is that the objective evaluation of evidence requires a combination of stronger WTDs and a skepticism prompt, rather than stronger WTDs only.
- Experiment 2 thus tests our hypothesis without use of skepticism prompts.
- We also designed the new experiment such that its dependent measures go beyond the assessment of fraud risk (i.e., a judgment) to include action plans to address fraud risk (i.e., a choice).
    - We wanted to do so because, unless auditors undertake different actions to address fraud risk, the chance of fraud detection is unlikely to change.
- In Experiment 2, the participants have less audit experience and there are new case materials and new dependent variables which span both the assessment and plans to address fraud risk, all of which subject our hypothesis to a reproducibility test.
- Graduate students with an average (standard deviation) age of 26.9 (8.2) from a large state university participated.
    - These participants had completed an audit internship and/or were enrolled in a masters-level auditing course.
    - We adapt experimental materials from two prior studies [Griffith et al. 2015; Kadous and Zhou 2019].
    - Participants assess and address the risk that management is using unreasonable (overly aggressive) revenue projections to support their assertion that goodwill is properly valued (not impaired).
- Participants view the following audit evidence: summary of discussions with management regarding the revenue projections, historical revenue projections, peer company revenue projections, a sensitivity analysis, and market and industry information.
- Next, participants assess the reasonableness of management’s revenue growth projections and choose an action to address concerns about its reasonableness.
- Last, participants complete a post-test, including WTD items.
 
### 4.1. Experimental Design
- We use a 22 between-subjects design and manipulate Evidence at more indicative versus less indicative of overly aggressive revenue projections.
- In the more indicative condition, for example, management’s projections are more favorable than those of peer firms, while in the less indicative condition they are in line with those of peer firms.
- The second factor is a measured variable—that is, a median split of participants’ WTDs (stronger vs. weaker).
- For dependent variables, participants assess the reasonableness of management’s revenue projections and address misstatement risk arising from management’s revenue projections by choosing a Next Action.
- Our focus is whether the participants are sufficiently unsure about the reasonableness of revenue projections to warrant a discussion of their concerns with their senior associate.
- We code chosen actions as more appropriate or less appropriate.
- Two of these responses are more appropriate for more indicative Evidence (i.e., the third or fourth option), and two others are more appropriate responses for less indicative Evidence (i.e., the first or second option).
- We infer more objective evaluation of evidence in assessing and addressing fraud risk if participants choose more appropriate responses.
- Hypothesis 1 predicts that auditors will choose objectively more appropriate actions based on Evidence, but only if they have stronger WTDs.
- We perform no manipulation check on Evidence as the findings below demonstrate a successful manipulation.
 
### 4.2. RQ1 variation in the strength of auditors’ WTD
- We first investigate RQ1, again observing considerable variation in the strength of auditors’ WTDs, ranging from 32 to 115.
- Of the 71 participants, 39 (54.9%) have prior audit experience, and of these 39, there is an average (standard deviation) of 0.9 (2.1) years of audit experience.
- Audit experience is not correlated with participants’ WTD score.
- Furthermore, the average WTD strength does not differ statistically with respect to Experiment 1’s audit senior participants and the less experienced auditors in this experiment.
- We again use a median split to classify auditors as having stronger WTDs or weaker WTDs to enable the use of planned contrasts.
- Consistent with random assignment, WTD strength does not differ across Evidence conditions.
- Descriptive statistics for auditors with stronger WTDs are mean = 50.24 and standard deviation = 7.87, and for auditors with weaker WTDs, mean = 81.05 and standard deviation 14.24, in line with means and standard deviations in Experiment 1.
 
### 4.3. Test of Hypothesis 1’s reproducibility
- We observe no significant effects in the two-way ANOVA.
- Instead, most participants express high uncertainty about the reasonableness of these projections.
- When asked, “How likely is it that the five-year revenue projections are reasonable?” the average response falls between 5 and 6.
- In fact, over half (80%) of the participants respond within 1 point (2 points) of the midpoint 5 as likely as not on the 11-point scale.
- The more critical dependent variable asks participants how they would address concerns about the reasonableness of management’s revenue projections.
- The planned contrast using weights of 0 less indicative Evidence, weaker WTD; 0 more indicative Evidence, weaker WTD; 1 less indicative Evidence, stronger WTD; and +1 more indicative Evidence, stronger WTD.
- As in Experiment 1, these contrast weights fit the predicted pattern: auditors respond to Evidence in their choice of additional procedures but only when they have a stronger WTD.
- The visual fit predominantly supports Hypothesis 1, as does the significant contrast test, the insignificant semi-omnibus test, and relatively small contrast variance residual of 26.9% [Guggenmos et al. 2018].
- Importantly, simple effects show that stronger WTD auditors follow up with their senior associate more often when Evidence suggests that management’s revenue projections are overly aggressive, but that weaker WTD auditors do not do so.
- From an audit effectiveness perspective, it is notable that the simple effect of stronger versus weaker WTD is significant given Evidence that is more indicative of fraud.
- Collectively, these findings demonstrate the reproducibility of our hypothesis, provide evidence that it holds in the absence of skepticism prompts (RQ2.3), and strengthen the validity of the WTD construct.
- Having shown that auditors (fail to) objectively evaluate evidence if they have stronger (weaker) WTDs, we next probe whether WTD coheres as a multifaceted construct and diverges from HTS.

# 5. Further analyses to assess WTD construct and divergent validity
- To further validate the multifaceted nature of our WTD construct, we perform a **confirmatory bi-factor analysis**.
    - A **bi-factor model** is a hierarchical factor model that determines whether a construct includes a higher-order general factor that captures a commonality threading among two or more lower-order subfactors.
    - In selecting each of our 22 scale items, we anticipated that they would entail a higher-order general wise thinking factor in addition to a small number of subfactors [Chen et al. 2012].
    - Of these 22 items, we categorize 10 into a subfactor that we call **reflective thinking**, 10 into one that we call **evidence-based beliefs & doubts**, and 2 into a subfactor that we call **counterfactual thinking**.
    - Other subfactors could exist, but we keep our model simple, as is appropriate for new constructs [Churchill 1979].
- Because empirical assessment of construct validity using bi-factor analyses is more powerful and stable with larger sample sizes, we use 298 participants by pooling the 87 audit seniors in Experiment 1, the 71 graduate audit students in Experiment 2, and an additional 140 undergraduate participants in Experiment 3, who completed an exploratory study during fall 2015 in which they filled out both the WTD and HTS questionnaires.
    - Pooling across these three groups is appropriate conceptually and pragmatically.
    - Conceptually, the WTD construct is invariant across different adults; its meaning does not differ between audit seniors and audit staff.
    - Indeed, nearly identical WTD means and standard deviations obtain across these groups.
- The model fit measures exceed conventional cutoffs for a good fit [Boateng et al. 2018; Kenny 2020], and 11 of our 22 items (50%) load on a higher-order wise thinking (general) factor.
- In addition, 7 of the 10 items categorized as reflective thinking load on that subfactor, 8 of the 10 items categorized as evidence-based beliefs items load on that subfactor, and 1 of the 2 items categorized as counterfactual thinking items loads on that subfactor.
- Overall, the confirmatory bi-factor analysis supports the conclusion that our scale’s 22 items collectively capture a multifaceted WTD construct consisting of a general factor as well as three subfactors.
 
- Last, having already shown WTD strength is only very weakly correlated with tacit managerial knowledge, we next examine if a distinction can be drawn between the WTD construct and the **Hurtt Trait Skepticism (HTS) construct**.
    - Prior research shows that certain advantages are associated with having higher HTS.
    - McAllister et al. [2020] finds that having a higher HTS auditor on an audit team helps other team members consider case information they otherwise overlook.
    - Furthermore, HTS scores are positively correlated with a longer auditing career trajectory [Cohen et al. 2017].
- Nonetheless, **trait skepticism** does not fully capture **professional skepticism** in that it largely omits **situational skepticism**.
    - Situational skepticism captures the idea that auditors should heighten their skepticism in some situations as compared to others [Nolder and Kadous 2018].
    - Counter to this idea, Quadackers et al. [2014] find that regardless of their HTS score, auditors assess fraud risk to be higher in response to a stark manipulation by which management either aggressively manages earnings, frequently gets into disputes with external auditors, and emphasizes productivity; or strives to report accurately, works harmoniously with external auditors, and emphasizes integrity.
    - Auditors with higher versus lower HTS scores also assess fraud risk to be higher regardless of which management type they face.
    - In other studies, higher HTS scores reportedly have no significant effect on auditors’ evaluation of evidence [Rasso 2015, 47–49].
 
- In short, we know of no study that reports findings consistent with higher HTS scores enabling auditors to more objectively evaluate evidence.
    - Thus, the observed effects of higher HTS on auditors’ evidence evaluation diverge from that which we observe in two separate experiments herein.
    - We find that auditors objectively evaluate Evidence that is less versus more indicative of potential fraud when assessing fraud risk and planning follow-up procedures to address fraud risk, but only when they have stronger WTDs.
    - As such, higher HTS scores and stronger WTD scores would appear to be capturing decidedly different constructs.
- Nonetheless, to further investigate the possibility that HTS and WTD capture essentially identical constructs, we perform further untabulated analyses using undergraduate senior participants from a large state university from the aforementioned Experiment 3.
    - They completed our 22-item WTD scale and the 30-item HTS scale in a post-test.
    - Four participants failed to fill out all measures, leaving 136 observations.
    - To begin, we simply regress participants’ HTS scores on their WTD scores.
    - Unsurprisingly, HTS scores explain some of the variation in WTD (R2 = 21%, p < 0.001), but the larger point is that they fail to explain most of the variation in WTD (i.e., 79%, 1 – R2 ).
- We next conduct a factor analysis on 52 items, 22 from the WTD scale and 30 from the HTS scale.
    - We use promax rotation, which allows extracted factors to be correlated.
    - Following convention [Boateng et al. 2018; Nunnally 1978], we extract 10 factors as they have loadings of an absolute value of at least 0.40.
    - Of these 10, 6 factors load exclusively on HTS items.
    - Furthermore, the remaining four factors load exclusively on WTD items.
    - Finally, only 1 of the 24 interfactor correlations between the 6 HTS factors and the 4 WTD factors is even moderately strongly correlated (i.e., r ≥ +0.40, at +0.44).
    - Of the 23 remaining interfactor correlations, 9 are weakly correlated (i.e., +0.20 ≤ r ≤ +0.39) and 14 are very weakly correlated (+0 ≤ r ≤ +0.19).
- We conclude that HTS and WTD readily diverge from one another empirically.
 
# 6. Concluding comments
- Audit theory and standards prescribe that auditors should evaluate evidence objectively and describe objective evaluation as indispensable for the proper exercise of professional skepticism.
- However, both anecdotal inspection findings and empirical findings from numerous prior experimental studies indicate that auditors, at least on average, fail to objectively evaluate evidence.
- This paper contributes to the literature by developing the hypothesis that auditors objectively evaluate evidence, provided that they have relatively strong WTDs.
- We find support for this hypothesis across two experiments that differ with regard to inclusion or exclusion of a prompt to exercise professional skepticism.
    - The experiments also use different audit contexts that feature potential fraud as well as different dependent variables and different participants.
    - In both experiments, auditors objectively evaluate whether evidence is more or less indicative of fraud only when they have stronger WTDs, demonstrating the reproducibility of the audit judgment-quality benefits of stronger WTDs.
- This finding has potential implications for standard setters and audit firms.
    - In particular, quality control standards and audit working paper review processes might benefit from a revision that takes into account our theory-consistent findings that auditors do not objectively evaluate evidence unless they have stronger WTDs.
- A second contribution is identification of WTD as a new construct, defining it as a thinking disposition by which individuals naturally engage in balanced revision of prior beliefs and doubts about target phenomena by thinking openly and reflectively about available evidence [Meacham 1983; Kitchener and Brenner 1990; Ardelt 2003; Staudinger and Glück 2011a].
- We further contribute to the literature by showing that both inexperienced and experienced auditors vary considerably in terms of the strength of their WTD.
- Finally, we perform confirmatory bi-factor analyses to demonstrate that a general factor threads through our 22 items used to measure WTDs, and we differentiate WTD from HTS.
    - These findings have potential practical implications in that weaker WTD auditors likely have difficulty responsibly exercising professional skepticism, as they struggle to objectively evaluate evidence.
- We close by considering opportunities to extend this research.
    - While we introduce the WTD construct to the accounting literature with an aim of showing that stronger WTDs enable auditors to objectively evaluate evidence, stronger WTDs likely also improve other facets of auditor task performance.
    - Do stronger WTDs help auditors to allocate scarce cognitive or budgetary resources while performing complex tasks, reduce auditor overconfidence, or enable fuller learning from past deficiencies in professional judgments?
    - As WTDs are one component of a rich cognitive architecture, researchers could examine how WTDs interact with other forms of auditor knowledge (e.g., technical or industry) or practical intelligence (e.g., tacit knowledge, systems thinking, emotional intelligence) to improve auditor judgments and decisions.
    - To build on our finding that WTD strength varies significantly among audit staff and seniors, future studies can examine how WTD strength varies within and across audit managers and partners, as well as within and across auditors receiving more favorable or less favorable performance evaluations.
    - Consequently, to further our understanding of how audit teams interact, one could examine under what conditions, if any, auditors with stronger WTDs compensate for other team members with weaker WTDs.
    - Related future research could examine instructional or enculturation methods that strengthen auditors’ WTDs as well as endeavor to identify better measures of WTD strength.
    - As an example, one could use a multi-method approach by pairing our WTD survey with participants’ reactions to scenarios depicting complicated life problems and solicit participants’ goals, beliefs, doubts, and possible courses of action.
    - Trained judges could then code participants’ responses [Staudinger and Glück 2011b].
    - While our studies conveniently provided auditors with evidence of fraud, future investigations could examine whether stronger WTDs improve auditors’ evidence search or mitigate over-aversion or over-eagerness to use artificial intelligence–based audit evidence.
    - Finally, future research could examine whether the failure of auditors with weaker WTDs to objectively evaluate evidence can be overcome by interventions that induce deliberative mindsets such as situational prompts or advisor roles [Griffith et al. 2015; Bauer et al. 2020].
 
---
 
# Executive summary of 1. Introduction
- Audit standards require objective evaluation of evidence, but studies suggest auditors often fail to do so.
- We introduce wise-thinking dispositions (WTDs) and theorize that auditors with stronger WTDs objectively evaluate evidence.
- Thinking dispositions guide the use of intellect, knowledge, and abilities.
- The study aims to show that auditors vary in WTD strength, and auditors with stronger WTDs objectively evaluate evidence.
- The study also examines the role of skepticism prompts in influencing the relationship between WTDs and objective evaluation of evidence.
 
# Executive summary of 2. Background and Hypotheses
- Audit standards emphasize objective evaluation of evidence and professional skepticism.
- Studies show that auditors often fail to adapt their plans appropriately in response to evidence.
- We introduce WTDs to differentiate auditors in terms of their propensity for objective evaluation of audit evidence, addressing the limitations of focusing solely on "the cult of the average".
- We present *H1*: Auditors’ fraud risk assessments and planned actions will reflect a higher (lower) risk of fraud when evidence is more (less) indicative of fraud, but only when they have stronger WTDs.
- The section poses research questions related to the variation in WTD strength across auditors and whether the nature of external professional skepticism prompts constitute boundary conditions for *H1*.
 
# Executive summary of 3. Experiment 1
- Experiment 1 involved 87 auditors from a Big 4 firm, focusing on their evaluation of nonrecurring journal entries.
- A 2x2x2 mixed design was used, manipulating Evidence (more/less indicative of fraud) and skepticism Prompt (doubtful/neutral), with WTD strength as a measured variable.
- The study used specific scales (3D-WS and AOMT) to measure WTDs, scoring them using the sum of squared deviations.
- Participants assessed the likelihood of management engaging in fraud, earnings management, or high-quality accounting.
- The results indicated a successful manipulation of the skepticism prompt, and considerable variation in auditors' WTD strength.
- Consistent with *H1*, auditors assessed a higher (lower) risk of fraud when evidence was more (less) indicative of fraud, but only if they had stronger WTDs.
- Stronger WTD auditors' express greater (lesser) concern about fraud risk when Evidence is more (less) indicative of fraud regardless of skepticism Prompt level.
 
# Executive summary of 4. Experiment 2
- Experiment 2 tested the hypothesis without skepticism prompts, using graduate students with audit experience.
- Participants assessed the reasonableness of revenue projections and chose an action to address misstatement risk.
- The study employed a 2x2 between-subjects design, manipulating Evidence (more/less indicative of overly aggressive projections) and measuring WTDs.
- Results showed a significant interaction between WTD strength and Evidence in auditors' choice of actions.
- The study provides evidence that the proposed effects related to WTDs also hold without the use of skepticism prompts.
 
# Executive summary of 5. Further analyses to assess WTD construct and divergent validity
- The section provides further analyses to assess WTD construct and divergent validity.
- Confirmatory bi-factor analysis using data pooled from the three experiments indicated that a general factor threads through our 22 items used to measure WTDs.
- The HTS and WTD readily diverge from one another empirically.
 
# Executive summary of 6. Concluding comments
- This paper contributes to the literature by developing the hypothesis that auditors objectively evaluate evidence, provided that they have relatively strong WTDs.
- Standard setters and audit firms could take our theory-consistent findings that auditors do not objectively evaluate evidence unless they have stronger WTDs into account.
- A second contribution is identification of WTD as a new construct, defining it as a thinking disposition by which individuals naturally engage in balanced revision of prior beliefs and doubts about target phenomena by thinking openly and reflectively about available evidence.
- Finally, the study presents opportunities to extend this research.

</details>

```
title: Promoting Proactive Auditing Behaviors
authors: Mark E. Peecher, Michael A. Ricci, Yuepin (Daniel) Zhou
journal: Contemporary Accounting Research
published: 2024
```
 
# Executive Summary
- This study introduces the concept of **proactive auditing behaviors** to the accounting literature and investigates their antecedents.
- **Proactive auditing behaviors** are defined as anticipatory actions that auditors take, without instruction, to improve audit quality by going above and beyond the task at hand.
- Key **theoretical** / **conceptual framework** discussions
    - **Proactive behaviors**: Are relatively self-starting, change oriented, and future focused [Parker et al., 2010].
    - **Autonomy**: The extent to which a job allows freedom, independence, and discretion to schedule work, make decisions, and choose the methods to perform tasks [Morgeson and Humphrey, 2006].
    - **Tacit knowledge**: Knowledge about how to manage themselves and others when performing socially interactive tasks [Tan and Libby, 1997; Bol et al. 2018].
    - **Regulatory focus theory**: Identifies two distinct cognitive foci: a promotion focus on achieving positive outcomes and a prevention focus on avoiding negative outcomes [Crowe and Higgins, 1997].
- **H1**: Auditors will be more proactive when they have both higher autonomy and higher tacit knowledge compared to when they lack either or both of these factors.
- Key **findings** / arguments
    - An experiment involving audit seniors completing a case study with opportunities for proactive behavior.
    - **Autonomy and Tacit Knowledge**: The combination of higher autonomy and higher tacit knowledge is key for auditor proactivity.
    - **Proactive Behaviors**: The four seeded proactive behaviors (following up on potential control deficiency, coordinating with client management, updating staff on client accounting policy changes, and developing staff) coalesce as one construct.
    - **Regulatory Focus**: The interaction of autonomy and tacit knowledge holds across both promotion and prevention regulatory foci.

#proactive_behaviors #autonomy #tacit_knowledge #audit_quality #coaching #client_coordination #regulatory_focus
 

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- This paper investigates the antecedents of **proactive auditing behaviors**, a new construct in the auditing literature.
    - **Proactive auditing behaviors** are anticipatory actions taken by auditors, without instruction, to improve current or future audit quality by exceeding the required tasks.
- Regulators and practitioners agree that proactive behaviors are crucial for audit quality but are often lacking.
    - Auditors operate in complex, dynamic environments [Peecher et al., 2013; Bol et al., 2018], making complete supervisory directives impossible [Knechel et al., 2013].
    - Current revisions to audit quality control standards aim to encourage a more proactive approach [IAASB, 2020; AICPA, 2022; PCAOB, 2022a; Hayne et al., 2023].
    - Audit firms are trying to hire more proactive employees and increase proactivity among existing auditors [e.g., Deloitte, 2017, 2019; KPMG, 2017, 2021; EY, 2020; PwC, 2020].
- Prior studies confirm the scarcity of auditor proactivity.
    - Auditors often fail to take helpful, non-standard actions [Hammersley et al., 2011; Griffith et al., 2015; PCAOB, 2017; Kadous and Zhou, 2019], coordinate with clients [Barr and McNeilly, 2003; Hatfield et al., 2022], or coach junior auditors [Andiola et al., 2021a].
- This study aims to experimentally test the conditions under which auditors are more likely to engage in specific proactive auditing behaviors:
    - Responding to evidence with out-of-task implications.
    - Coordinating with client management to acquire evidence.
    - Two forms of coaching staff auditors.
- The argument is that proactivity unifies these behaviors and can be promoted by a combination of higher autonomy and higher tacit knowledge.
    - Higher autonomy gives auditors discretion [Kirkman and Rosen, 1999; Hyatt and Prawitt, 2001; Proell et al., 2022; Ball, 2009], but doesn't provide the knowledge to navigate social situations.
    - Tacit knowledge provides auditors with knowledge to manage themselves and others [Tan and Libby, 1997; Bol et al., 2018].

## 1.1. Autonomy and Tacit Knowledge
- Autonomy can be influenced by audit firm methodologies [Kirkman and Rosen, 1999; Hyatt and Prawitt, 2001; Proell et al., 2022] and regulatory standards [e.g., Ball, 2009].
- Management studies link autonomy and proactivity [Grant and Ashford, 2008; Morgeson and Humphrey, 2006], but auditing is more complex than previous settings.
    - Higher autonomy alone is unlikely to be enough; tacit knowledge is also crucial.
- This study posits that it is the combination of higher autonomy and higher tacit knowledge, rather than either alone, that increases auditor proactivity.
    - Auditors with higher tacit knowledge will better identify how to be proactive.
- This study also explores the generalizability of the theory over different auditor regulatory foci.
    - Regulatory focus theory identifies promotion (achieving positive outcomes) and prevention (avoiding negative outcomes) foci [Crowe and Higgins, 1997].
- The study uses a 2 × 2 × 2 experiment manipulating autonomy and regulatory focus, and measuring tacit knowledge.
    - Auditors review a staff auditor's work and prepare the staff for an upcoming task, with cues seeding opportunities for proactive behavior.
    - The study measures the total number of seeded proactive behaviors auditors describe.
- The results indicate that the combination of higher autonomy and higher tacit knowledge is key for auditor proactivity.
    - The four seeded proactive behaviors coalesce as one construct, and the interaction holds for both regulatory foci.
 
## 1.2. Contributions
- This study contributes to theory and practice in three ways.
- First, the study addresses the scarcity of proactive behaviors and the lack of evidence on their promotion.
    - Prior studies focus on the consequences of actions, not antecedents [e.g., Andiola and Bedard, 2018; Andiola et al., 2019; Andiola et al., 2021b; Saiewitz and Kida, 2018; Hatfield et al., 2022].
- Second, the study highlights the cost of lower autonomy or tacit knowledge – scarcer proactivity.
    - Audit firms can design programs and controls that champion autonomy [Stuart and Prawitt, 2012; Boland et al., 2019; Bamber and Iyer, 2009; Mocadlo et al., 2020; FAR, 2020; Peecher et al., 2023].
    - Firms should develop auditors' tacit knowledge [Bol et al., 2018], especially given remote work arrangements and turnover [Johnson and Pike, 2020; EY, 2021; Proell et al., 2023].
- Third, the study extends prior management studies on proactivity by providing causal evidence that autonomy can increase auditor proactivity.
    - It also demonstrates that lower tacit knowledge is a boundary condition preventing higher autonomy from increasing proactivity [Grant and Ashford, 2008; Parker et al., 2010; El Baroudi et al., 2019].
 
# 2. Theory and hypothesis development
- **Proactive behaviors** are anticipatory actions employees take to impact themselves and/or their environments [Grant and Ashford, 2008].
    - Behaviors are more proactive when they are self-starting, change oriented, and future focused [Parker et al., 2010].
- Proactive behaviors vary depending on the job [Crant, 2000; Parker et al., 2019].
    - Management research examines feedback seeking [Anseel et al., 2015], speaking up [Morrison, 2011], social network building [Saks and Ashforth, 1996], career management [Seibert et al., 2001], and issue selling [Dutton and Ashford, 1993].
    - Some behaviors viewed as proactive in other professions are not in auditing.
- The study focuses on specific proactive auditing behaviors: coaching junior auditors, coordinating with clients, and responding to evidence with out-of-task implications.
- These behaviors have received little attention in management, but are relevant to auditing.
    - There are concerns about a lack of coaching [Westermann et al., 2015], ineffective client coordination [Guénin-Paracini et al., 2015; Dodgson et al., 2020], and unresponsiveness to new evidence [PCAOB, 2017, 2018].
    - These behaviors require auditors to take initiative to improve current or future outcomes.
- The prior auditing literature has not considered proactive auditing behavior as a unifying construct.
    - Many streams focus on judgments and judgment processes [e.g., Libby and Frederick, 1990; Hammersley, 2006; Wilks, 2002; Piercey, 2011; Zhou, 2023; Zimbelman, 1997; Simon et al., 2018].
    - Some studies focus on required or prohibited actions, rather than proactive actions.
- Some studies investigate behaviors sharing attributes with proactive behaviors, but consider them individually.
    - The literature on coaching focuses on consequences [Westermann et al., 2015; Andiola et al., 2021b], with limited understanding of antecedents [e.g., Andiola et al., 2021b].
    - The literature on client coordination focuses on consequences [Saiewitz and Kida, 2018; Hatfield et al., 2022], without addressing antecedent conditions.
- This study aims to provide evidence about conditions under which auditors perform quality-enhancing proactive behaviors: coaching, client coordination, and responding to out-of-task evidence.
    - The study draws on theory from management and psychology to group these behaviors together.

## 2.1. Theorizing how autonomy and tacit knowledge interact to encourage proactivity
- Theory posits that autonomy is necessary for proactivity because it causes employees to feel like they have the discretion to be proactive [Grant and Ashford, 2008].
    - Autonomy is defined as "the extent to which a job allows freedom, independence, and discretion to schedule work, make decisions, and choose the methods to perform tasks" [Morgeson and Humphrey, 2006, 1323].
- Management studies suggest higher autonomy increases employee proactivity in some settings.
    - These studies document that autonomy is positively correlated with self-reported proactive behaviors [Axtell et al., 2000; Frese et al., 2007] and higher perceived ability to be proactive [Parker, 1998; Axtell and Parker, 2003].
    - However, these studies are limited and reverse causality is possible.
- Settings in prior management studies differ markedly from auditing.
    - These industries vary greatly from the auditing profession in terms of job complexity, underlying accountability and incentive structures, legal liability pressures, and the extent of knowledge necessary to act proactively.
- This study theorizes that the combination of autonomy and tacit knowledge, rather than autonomy alone, will be sufficient to improve auditor proactivity.
    - Tacit knowledge is gleaned from experience [Wagner and Sternberg, 1985] and involves knowledge about managing oneself, others, and tasks [Wagner and Sternberg, 1987].
    - Audit firms did not actively reward or develop tacit knowledge [Tan and Libby, 1997, Bol et al., 2018].
- Tacit knowledge captures general knowledge about acting in social environments.
    - Proactive behaviors are self-starting and have interpersonal dependencies.
    - Higher tacit knowledge helps auditors identify situations when proactive behaviors are appropriate.
- Therefore, higher autonomy likely causes auditors to feel like they have the discretion to be proactive, but it is unlikely to be sufficient to encourage proactivity unless auditors also have higher tacit knowledge.
 
- *H1*: Ceteris paribus, auditors will be more proactive when they have both higher autonomy and higher tacit knowledge compared to when either or both of these factors are absent.
 
## 2.2. Exploring the generalizability of our theory over different regulatory foci
- This section considers whether the theory about interactive effects generalizes over different regulatory foci.
    - Auditors work under time-budget constraints and face incentives that sometimes focus more on efficiency than effectiveness [Peecher, 1996; Lambert et al., 2017; Bennett and Hatfield, 2017].
    - A lack of motivation could be a barrier to auditor proactivity.
- Theory from psychology suggests adopting a particular regulatory focus might affect motivation.
    - **Regulatory focus** concerns people's orientation toward goals [Higgins, 1997].
    - People can pursue goals using a promotion focus (achieving positive outcomes) or a prevention focus (avoiding negative outcomes) [Crowe and Higgins, 1997; Higgins, 2000; Higgins and Cornwell, 2016].
- Proactive behaviors align with a promotion focus, but a prevention focus may also motivate quality-enhancing behaviors.
 
- *Research Question*: Do the predicted interactive effects of autonomy and tacit knowledge on auditor proactivity hold across different regulatory foci? More broadly, does regulatory focus exert any effects on auditor proactivity?
 
# 3. Research method
- This study uses a 2 × 2 × 2 experiment, manipulating autonomy and regulatory focus, and measuring tacit knowledge.
- Participants are audit seniors from eight public accounting firms, including all the Big 4, with an average of 40 months of experience.
 
## 3.1. Task
- The online case provides background information about an electronics manufacturer client.
    - The audit team is finishing interim substantive testing of revenue and preparing for year-end accounts receivable testing.
- Auditors perform two tasks with seeded opportunities to be proactive.
    - **Task 1**: Review a staff auditor's interim revenue testing. The staff couldn't obtain purchase orders for some items but confirmed sales with customers.
        - This suggests a potential control deficiency, even though there is no misstatement.
        - Auditors have an opportunity to proactively re-consider the effectiveness of controls, beyond the scope of their assigned task.
    - **Task 2**: Prepare the staff auditor for year-end accounts receivable testing.
        - **Opportunity 1**: Client management is known for getting frustrated by last-minute requests, seeding an opportunity to proactively coordinate confirmations with the client [Hatfield et al., 2022].
        - **Opportunity 2**: Client management recently changed its AR aging policy, seeding an opportunity to proactively keep the staff up-to-date.
        - **Opportunity 3**: The staff auditor is inexperienced with AR, seeding an opportunity to proactively invest in the staff's professional development [Andiola et al., 2021b].
- In summary, the case includes four seeded opportunities: (1) follow-up about the potential control deficiency, (2) coordinate confirmations with the client, (3) keep the staff up-to-date about the policy changes, and (4) invest in the staff's professional development.
- The study is designed with feedback from experienced representatives and a pilot test with audit seniors.
- After encountering the opportunities, auditors describe what they would do next and complete a post-experimental questionnaire.

## 3.2. Independent variables
- **Autonomy** is manipulated using a multi-faceted approach.
    - Describing the team's culture [Morrison, 2006; Morgeson and Humphrey, 2006; Rank et al., 2007].
    - Varying language used throughout the case [Sheldon et al., 2018].
    - Providing higher autonomy participants with the illusion of choice [Finkelstein and Fishbach, 2010].
- **Tacit knowledge** is measured by adapting Bol et al.'s [2018] approach.
    - This measure assesses the degree to which auditors’ reactions to work-related scenarios converge with the reactions of audit experts.
    - Five questions from Bol et al. [2018] are used, and a median split is performed.
- **Regulatory focus** is manipulated by asking auditors to list three negative (positive) outcomes of poor (excellent) job performance [Levine et al., 2000; Aaker and Lee, 2001; Freitas et al., 2002; Freitas et al., 2002; Wang and Lee, 2006; Hammersley et al., 2021; Hong, 2022].
 
## 3.3. Dependent variables
- After encountering the opportunities, auditors can select from a list of eight options.
    - The purpose is to elicit a detailed discussion of what auditors wanted to do next, without forcing them to take any action or asking them about the seeded behaviors.
- Auditors select one option at a time until they indicate they are finished.
    - **Options 1 and 2**: Sign off on the staff's workpaper, indicating no further action.
    - **Options 3 to 8**: Provide review notes, provide instructions, talk to my manager, talk to the controller, perform an action that is not listed, indicating a general action.
- When auditors select general action options, they respond to open-ended questions about what specifically they wanted to do, providing an opportunity to describe the seeded proactive behaviors.
- Auditors' responses are divided into thought units [Koonce et al., 1995] and coded into six categories:
    - Control Deficiency Follow-up.
    - Coordinating with Client.
    - Keeping Staff Up-to-Date.
    - Staff Development.
    - Other Valid Behaviors.
    - Invalid Behaviors.
- Two research assistants independently coded, and coding differences were reconciled.
- Four binary variables indicate whether participants want to perform each of the seeded proactive behaviors.
- The primary dependent variable, Total Proactive Behaviors, is the sum of the four binary variables.
 
# 4. Results
## 4.1. Validating the proactivity of seeded behaviors
- Auditors assessed the proactivity of various behaviors on 11-point Likert scales.
- The proactivity of three behaviors similar to the seeded proactive behaviors was rated significantly higher than a baseline behavior (follow your manager’s guidance).
- These three behaviors were all rated significantly higher than the scale midpoint (i.e., "moderately proactive").

## 4.2. Manipulation checks
- Auditors indicated significantly higher freedom in the higher autonomy condition, suggesting attention to the manipulation.
- All outcomes classified as positive (negative) originated from participants in the promotion-focused (prevention-focused) condition.

## 4.3. Hypothesis test
### 4.3.1. Testing H1 using total proactive behaviors
- The overall mean of Total Proactive Behaviors is 1.51.
 
- *H1*: predicts an Autonomy x Tacit Knowledge ordinal interaction in which auditors engage in more proactive behaviors when they have both higher autonomy and higher tacit knowledge than when they lack either or both of these factors.
- The primary statistical test of H1 is a planned contrast test: +3 for higher Autonomy/higher Tacit Knowledge and -1 for the other three combinations.
- The planned contrast is significant (p < 0.01) and the between-cells residual variance test is not significant.
- Follow-up pairwise comparisons further support H1.
- While the ANOVA model shows a significant main effect of autonomy, autonomy alone does not significantly increase auditor proactivity.
- Similarly, while the ANOVA model shows a main effect of tacit knowledge, higher tacit knowledge alone does not significantly increase auditor proactivity.
- Taken together, the combination of higher autonomy and higher tacit knowledge is key for auditor proactivity.

### 4.3.2. Robustness tests of H1 using each seeded proactive behavior
- Auditors with higher autonomy and higher tacit knowledge are consistently more likely to describe each individual proactive behavior than are auditors in the other three conditions.
- Applying the same directional contrast, there is support for H1 using each behavior: “Control Deficiency Follow-Up”, “Coordinate with Client”, “Keep Staff Up-to-Date”, and “Staff Development”.
- Thus, the seemingly distinct auditing behaviors share common antecedent conditions and represent examples of the broader construct—proactive auditing behaviors.

## 4.4. Supplemental analyses
### 4.4.1. Generalizability of our theory over different regulatory foci
- To explore the generalizability, H1 is tested using the same planned contrast within each regulatory focus condition.
- *H1* is robust across both the promotion and prevention focus conditions.
- The ANOVA shows no evidence of a main effect of regulatory focus nor evidence of two-way or three-way interactive effects.

### 4.4.2. Audit efficiency
- There is no evidence that autonomy increases behaviors coded as invalid, regardless of whether autonomy is combined with higher tacit knowledge, a certain regulatory focus, or both.
- Scaling Total Proactive Behaviors by all behaviors described shows that the combination of higher autonomy and higher tacit knowledge increases the proportion of proactive behaviors auditors identify.
- Therefore, the independent variables promote quality-enhancing proactive behaviors, but not invalid or non-proactive behaviors.
 
# 5. Conclusion
- In order to achieve high audit quality, it is critical for auditors to be proactive.
- Despite being critical for audit quality, proactive behaviors remain relatively scarce in practice and the determinants of auditor proactivity are poorly understood.
- This study provides theory and empirical evidence about how the combination of autonomy and tacit knowledge are key for auditor proactivity.
- This advances theory on determinants of proactivity, informs practitioners about ways to improve auditor proactivity, and paves the way for future research to study auditor proactivity.
 
---
 
# Executive summary of 1. Introduction
- This section introduces **proactive auditing behaviors** as a crucial element for ensuring high audit quality in complex and dynamic environments.
- It highlights the **shortage** of such behaviors, the efforts by regulators and firms to promote them, and the **lack of understanding** regarding their antecedents.
- The section sets the stage for the experimental investigation, which aims to identify conditions that encourage specific **proactive behaviors**, emphasizing the roles of **autonomy** and **tacit knowledge**.
 
# Executive summary of 2. Theory and hypothesis development
- This section defines **proactive behaviors** and discusses their context-dependent nature, focusing on specific auditing behaviors like **coaching**, **client coordination**, and responding to **out-of-task evidence**.
- It emphasizes that prior auditing literature has not considered proactive auditing behavior as a unifying construct, and introduces the theory that the combination of **autonomy** and **tacit knowledge** will encourage auditor proactivity.
- Finally, it proposes **H1**, which states that auditors will be more proactive when they have both higher autonomy and higher tacit knowledge compared to when either or both of these factors are absent.
- Additionally, it poses a research question about the generalizability of the theory across different **regulatory foci**, such as promotion and prevention.
 
# Executive summary of 3. Research method
- This section details the study's methodology, including the **2 × 2 × 2 experiment** that manipulates **autonomy** and **regulatory focus**, while measuring **tacit knowledge**.
- It describes the participants as **audit seniors** from various firms and outlines the **online case** they completed, which included **two tasks** with opportunities for **proactive behavior**.
- The section also explains how the independent and dependent variables were measured, including the use of **open-ended questions** and **coding** to assess auditors' proactive responses.
 
# Executive summary of 4. Results
- This section presents the results of the study, including the validation of the proactivity of seeded behaviors and manipulation checks.
- It reports that auditors with higher autonomy and higher tacit knowledge were more likely to describe each individual proactive behavior than auditors in the other three conditions, supporting the hypothesis.
- The results also demonstrate the robustness of the findings across both promotion and prevention regulatory foci.
- Finally, it discusses audit efficiency and finds that the independent variables promote quality-enhancing proactive behaviors without increasing invalid or non-proactive behaviors.
 
# Executive summary of 5. Conclusion
- This section summarizes the study's key findings and contributions, emphasizing the importance of proactive behaviors in achieving high audit quality.
- It highlights that auditors are more proactive when they have both higher autonomy and higher tacit knowledge, and that this combination is key for auditor proactivity.
- The section also offers implications for practice and suggests future research opportunities, such as exploring other antecedents and examining the effects of remote work and new technologies on auditor proactivity.

</details>

```
title: Improving Experienced Auditors’ Detection of Deception in CEO Narratives
authors: Jesse N. Hobson, William J. Mayew, Mark E. Peecher, and Mohan Venkatachalam
journal: Journal of Accounting Research
published: 2017
```
 
# Executive Summary
- This study investigates whether providing experienced auditors with a **negative affect instruction** (explaining that **cognitive dissonance**, the negative, uncomfortable emotion a person feels when saying something untrue, is a cue for detecting deception) improves their ability to detect deception in CEO narratives from earnings conference calls.
- We find that without the instruction, auditors are more accurate at identifying nonfraudulent companies but perform no better than chance for fraudulent companies.
- Providing the **negative affect instruction** improves accuracy for both fraudulent and nonfraudulent companies.
- We also find that instructed auditors provide more extensive descriptions of **red flags** for fraudulent companies and more accurately identify sentences in narratives that pertain to underlying frauds.
- key **theoretical** / **conceptual framework** discussions
    - **Motivated reasoning theory**: decision makers subconsciously pursue directional goals, but only if they can maintain their “illusion of objectivity."
    - **PEDMIN (Primary Error Detection and Minimization) Testing Strategy**: Auditors learn to habitually minimize false positives (mislabeling a clean company as fraudulent) since errors from making a “fraud exists” judgment are personally and saliently costly.
- key **findings** / arguments including 
    - Company type and **negative affect instruction** jointly influence experienced auditors’ deception detection accuracy in the form of an ordinal interaction in which they are least accurate on fraud companies when provided with no instruction and most accurate in the other three experimental conditions.
    - Without **negative affect instruction**, auditors are more accurate for nonfraud companies than for fraud companies.
    - When evaluating fraud companies, auditors are more accurate if they receive the **negative affect instruction** than if they do not receive the instruction.
    - Instructed auditors apply more of their audit-related knowledge and effort regarding **red flags** for fraud companies.
    - **Negative affect instruction** significantly improves accuracy at the sentence level for fraud sentences.
 
#deception_detection #earnings_conference_call #fraud #audit #cognitive_dissonance #motivated_reasoning #professional_skepticism #red_flags


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Auditors are seldom the first to detect fraud [Dyck, Morse, and Zingales, 2010], so improving their **deception detection capability** is of interest [PCAOB, 2007, 2013; CAQ, 2010; Hogan et al., 2008; Christensen et al., 2016]. We examine experienced auditors' **deception detection** in CEO narratives from earnings conference calls, which the PCAOB recommends auditors review [PCAOB, 2010].
- The psychology literature suggests that experienced professionals often perform poorly at deception detection [Bond and DePaulo, 2006, 2008; Vrij et al., 2006]. However, the audit setting is unique because auditors often repeatedly interact with the same clients.
- Theory and research suggest auditors learn to avoid false positives about fraud rather than evaluate evidence objectively [Friedrich, 1993]. Audit supervisors penalize subordinates who act on skeptical beliefs [Brazel et al., 2016], and auditors use motivated reasoning [Bazerman, Morgan, and Loewenstein, 1997; Kadous, Kennedy, and Peecher, 2003], collect less evidence to avoid conflict [Nelson, 2009; Bennett and Hatfield, 2013], and become more reluctant skeptics with experience [Shaub and Lawrence, 1999].
    - Experienced auditors learn to discount audit evidence that may cause conflict with management, even when it suggests elevated fraud risk.
    - We predict that experienced auditors will more accurately identify nonfraud companies than fraud companies when analyzing CEO earnings call narratives.
- We also predict that a **negative affect instruction** (explaining that **cognitive dissonance** is a cue for detecting deception) will improve auditor accuracy for fraud companies. The instruction should help auditors better identify fraud companies by reducing their learned tendency to discount fraud cues.
- We test these predictions in an experiment where 31 experienced auditors judge excerpts from four unique CEOs from quarterly conference calls. Some auditors receive the **negative affect instruction**. We classify excerpts as fraudulent if the company's financial statements were later restated and linked to fraud.
- We find that auditors receiving no instruction are more accurate for nonfraud firms but not for fraud firms. Auditors receiving the instruction achieve above-chance accuracy for both. Instructed auditors also provide more extensive descriptions of perceived red flags for fraud companies and more accurately identify specific sentences pertaining to frauds.
    - Instructing experienced auditors to be alert for **cognitive dissonance** in CEO narratives can activate **deception detection** capabilities.
- This study sets the stage for future work on how interventions affect the accuracy of deception judgments made by auditors with less experience or by forensic auditors. Future work could also investigate narratives in shareholder meetings, road show presentations, and client–auditor interactions.

# 2. Related Research and Hypotheses
## 2.1. Auditors, Incentives, and Fraud Detection
### 2.1.1. Existing Evidence on Auditor Fraud Detection
- Social psychology shows that individuals generally fail to exceed chance levels when detecting deception [Bond and DePaulo, 2006, 2008; Vrij, 2008], and experienced professionals rarely perform better than novices.
- Financial-statement auditors are responsible for providing reasonable assurance that financial statements are free of material misstatement due to fraud. However, prior work on experienced auditors’ capability to detect deception is sparse [Ariail, Blair, and Smith, 2010].
- Some research provides indirect evidence that experience enhances auditors’ detection of financial fraud [Brazel, Carpenter, and Jenkins, 2010; Knapp and Knapp, 2001].
- Auditor interviews of management and resulting narratives are critical audit evidence [PCAOB, 2010], but they are confidential. We study management narratives that occur publicly during Q&A portions of earnings conference calls, which Auditing Standard No. 12 recommends auditors observe [PCAOB, 2010].
    - Q&A between analysts and management provide information about the company [Hollander, Pronk, and Roelofsen, 2010; Matsumoto, Pronk, and Roelofsen, 2011; Mayew and Venkatachalam, 2012; Price et al., 2012; Blau, Delisle, and Price, 2015], including cues that can facilitate detection of financial statement fraud [Hobson, Mayew, and Venkatachalam, 2012; Larcker and Zakolyukina, 2012].
 
### 2.1.2. Auditor Incentives of Skepticism for Fraud
- Shaub and Lawrence [1999] theorize that auditors become less skeptical with more experience because there are few rewards and numerous costs to skeptical actions. Auditors are penalized for failing to find fraud but not rewarded for detecting it [Peecher, Solomon, and Trotman, 2013].
- Auditors who blow the whistle lose their client 50% of the time [Dyck, Morse, and Zingales, 2010; Doty, 2014] and are excluded from monetary rewards from the Dodd-Frank Act. The costs of investigating potential fraud are immediate, and auditors are motivated to meet their budgets [Audit Analytics Staff, 2014; Doty, 2014].
- Expressed suspicion of fraud requires additional audit procedures, causing tension and budget overruns. Auditors who skeptically investigate fraud risks can earn lower performance evaluations [Brazel et al., 2016].
- These incentives and experiences have psychological learning effects.
    - Auditors learn to follow a **primary error detection and minimization (PEDMIN)** testing strategy [Friedrich, 1993].
        - Rather than learning to render accurate deception judgments, auditors learn habitually to minimize false positives.
        - Errors from making a "fraud exists" judgment are personally and saliently costly [Shaub and Lawrence, 1999]. Auditors learn to overlook "red flag" indicators of fraud.
    - Much of the learning by auditors to avoid false positives operates at the subconscious level through **motivated reasoning** [Moore, Tanlu, and Bazerman, 2010].
        - Auditors subconsciously pursue directional goals that trigger criticism of contrary evidence but ready acceptance of supportive evidence [Kunda, 1990; Ditto and Lopez, 1992].
        - Auditors' use of **motivated reasoning** [Nelson, 2009; Lee, Welker, and Wang, 2013] impairs their objectivity [Kadous, Kennedy, and Peecher, 2003] and reduces their propensity to speak up [Clor-Proell and Nelson, 2007].
 
## 2.2. Negative Affect Instruction to Improve Fraud Detection
- To remedy auditors’ overlooking or explaining away fraud cues, we consider **motivated reasoning** theory, which holds that decision makers subconsciously pursue directional goals, but only if they can maintain their “illusion of objectivity” [Pyszczynski and Greenberg, 1987; Kunda, 1990; Kadous, Kennedy, and Peecher, 2003].
- We derive an instruction that makes it unreasonable for experienced auditors to overlook fraud cues. Our instruction informs auditors that deceivers often experience **negative affect** from **cognitive dissonance**.
    - We choose this instruction because deception frequently leads to **negative affect** [Ekman, 1985; Harmon-Jones, 2000; DePaulo et al., 2003].
    - Research shows that in the sample narratives, dissonance markers in the speech of the CEOs are present and associated with financial misreporting [Hobson, Mayew, and Venkatachalam, 2012].
    - Psychology research indicates that peak performance for the perception of affective states occurs between the ages of 40 and 60 [Hartshorne and Germine, 2015], which is the age range of experienced auditors.
- Instructing auditors to attend to management's **negative affect** will mitigate **motivated reasoning's** "illusion of objectivity" that otherwise subconsciously causes auditors to overlook fraud cues in CEO narratives of fraud companies. This instruction enables experienced auditors to tap into their accumulated audit knowledge in ways that facilitate better **deception detection** in fraud companies.
    - This instruction should result in little improvement in experienced auditors’ deception judgments for nonfraud companies.
    - If the **negative affect instruction** causes auditors to more objectively evaluate **red flag** fraud cues, the accuracy rate among instructed auditors judging fraud companies will approach the accuracy rates of both instructed and uninstructed auditors judging nonfraud companies.
 
- *H1A*: Company type and **negative affect instruction** jointly influence experienced auditors’ deception detection accuracy in the form of an ordinal interaction in which they are least accurate on fraud companies when provided with no instruction and most accurate in the other three experimental conditions.
 
- *H1B*: When experienced auditors do not receive the **negative affect instruction**, they are more accurate for nonfraud companies than for fraud companies.
 
- *H1C*: When evaluating fraud companies, experienced auditors are more accurate if they receive the **negative affect instruction** than if they do not receive the instruction.
 
# 3. Method
## 3.1. Participants
- We gathered 124 observations from 31 current or retired audit professionals at multiple large public accounting firms with an average of 24 years in the audit, assurance, and/or fraud/forensic services.
    - All but four participants are Certified Public Accountants (CPAs).
    - Participants spend approximately two hours on the task.
- We examine a participant pool with a high level of experience since expertise often takes very extensive and deliberate experience [Ericsson, Krampe, and Tesch-Römer, 1993]. Further, participants with this level of audit experience have the necessary experience processing CEO and CFO narratives.
 
## 3.2. Speech Corpus Selection
- Each auditor provides deception judgments on excerpts from the Q&A portion of a quarterly earnings conference call for four public companies.
    - Due to time constraints, we ask participants to evaluate only four companies.
    - For each participant, we randomly draw four companies from a population of 10 companies, five with deceptive discussions and five without.
    - Auditors are informed of this fraud rate, which follows the vast majority of deception detection experiments in the literature [Levine et al., 2014].
- The 10 company quarters are a subset of the 1,572 company-quarter earnings conference calls studied in Hobson, Mayew, and Venkatachalam [2012], which originally were broadcast during calendar year 2007.
    - Each CEO in the sample is male, and each of the 10 conference call narrative excerpts are the first five minutes of analyst questions to the CEO and the CEO responses to those questions.
- Following Hobson, Mayew, and Venkatachalam [2012], we characterize narratives as deceptive if the company’s financial statements being discussed in the call were restated and any of the following “irregularity conditions” hold: the restatement was deemed fraudulent, a regulatory investigation followed the restatement, or a class action lawsuit followed it.
    - We assume that CEOs of fraudulent companies have knowledge of the fraud and express **negative affect** in some area of the narrative excerpt.
- We create the population of 10 calls using two selection criteria.
    - First, we require that fraudulent companies do not systematically differ from nonfraud companies on known financial statement predictors of fraud. To do so, we calculate a financial statement–based fraud score or “F-score” [Dechow et al., 2011].
    - Second, we require the call narratives to come from companies that are not generally known to be fraudulent, by dropping fraudulent observations and their related pair based on survey responses from accounting doctoral students indicating familiarity the company’s fraud.
 
## 3.3. Speech Corpus Preparation
- We manually transcribe each conference call excerpt rather than rely on commercial transcription [Larcker and Zakolyukina, 2012]. Additionally, while Hobson, Mayew, and Venkatachalam [2012] isolate only the voice of the CEO in the conference call, we include the analyst’s question(s) to which the CEO is responding. A generic, computerized male or female voice (based on the gender of the analyst) reads the analysts’ questions.
 
## 3.4. Procedure and Variables
- We manipulate company type (fraudulent or not) and the presence or absence of the **negative affect instruction**. The manipulation of whether or not auditors are instructed to consider management’s negative affect has three parts.
    - First, initial instructions for all participants preceding each company evaluation states “Note: Research indicates that certain cues in what a CEO says and how s/he says it can help in the detection of deception.”
    - Second, half of our participants randomly receive the following: "One cue found to be useful in detecting deception in these CEO responses is cognitive dissonance. Cognitive dissonance is the negative, uncomfortable emotion a person feels when they are saying something that they know is not true. Those experiencing cognitive dissonance feel uncomfortable, uneasy, and bothered."
    - Third, after answering our principal dependent measure, participants instructed about negative affect assess “how much cognitive dissonance the CEO felt during this excerpt of the conference call.”
- We also manipulate conference call medium at two levels because conference calls are publicly available as both transcripts and audio files. Some auditors receive the conference call excerpt only as a transcript while others receive the transcript and accompanying audio.
- The auditors’ evaluation of the example and each of the four real companies consist of three parts. Part 1 contains background company information including the company name, the quarter being reported, a short business overview, and the three basic financial statements. Part 2 presents the conference call excerpt and an area to record **red flags**.
    - We tell auditors that a **red flag** exists any time they feel that CEO comments are suspicious, give them pause, or require additional investigation. We ask them to read/listen to the entire transcript.
    - We tell all auditors to assume they are the audit engagement partners for the companies they analyze.
- In Part 3, we collect responses from auditors. As auditors read/listen to the transcript, we ask them to note the timestamp/line number of each **red flag**, what was being discussed and why it was a **red flag**.
- We next solicit our primary dependent measure by asking, “Next, provide an overall judgment of whether it is more likely than not that fraud was being committed at this company during this quarter. That is, did this company later restate this quarter’s financial results due to one or more of the following: fraudulent financial results, a regulatory investigation, or a class action lawsuit?” Auditors respond, “Yes, fraud was likely being committed during this quarter” or “No, fraud was not likely being committed during this quarter.”
- We sequentially present all four evaluated companies in this manner. After evaluating the fourth company, auditors complete a postexperiment questionnaire that asks them about work experience and experience detecting deception.
 
# 4. Results
## 4.1. Descriptive Statistics
### 4.1.1. Descriptive Statistics
- Each of the 31 auditors provides four judgments, one per conference call, yielding 124 responses. We eliminate three observations because auditors indicate they are familiar with fraud at the company, leaving 121 total observations and 31 auditors.
    - Average audit experience is 23.62 years.
    - The average length of the conference call narrative provided is 7.26 minutes; the financial statement–based fraud score F-score is 1.89, and 52% of the conference calls auditors reviewed are deceptive.
    - There is no statistical difference in average audit experience, length of the conference call excerpt, F-score, or the number of fraudulent companies between auditors not instructed or instructed on **negative affect**.
 
### 4.1.2. Chance Benchmark for Accuracy Rates
- As a baseline, we first test whether uninstructed auditors’ deception judgments are better than chance rates. The overall accuracy rate of 63% for uninstructed auditors is statistically greater than chance levels of 50%.
    - This accuracy rate is high relative to reported accuracy rates for deception judgments by experts in prior meta-analysis [Bond and DePaulo, 2006].
    - Consistent with our theory, the overall 63% accuracy rate for uninstructed auditors is driven by accuracy on nonfraud companies.
        - Their accuracy rates on nonfraud companies (83%) are far better than chance while their accuracy rates on fraud companies (43%) does not statistically differ from chance.
 
## 4.2. H1—Effects of Company Type and Instruction on Accuracy Rates
- *H1A* predicts that auditors’ accuracy rates will be lowest when judging fraud companies without instruction and highest when judging fraud companies with instruction or when judging nonfraud companies, with or without instruction.
 
- *H1B* predicts that uninstructed auditors will more accurately classify nonfraud companies than fraud companies, and *H1C* predicts that experienced auditors will be more accurate when instructed than when uninstructed in evaluating fraud companies.
- We test the H1A ordinal interaction using an a priori linear contrast where auditors are equally highly accurate (contrasts weights = 1) unless evaluating fraud companies without the **negative affect instruction** (weight = –3).
    - The test statistic for this contrast is significant, confirming *H1A*.
    - The simple effect of Company Type given No Instruction is significant, indicating that uninstructed auditors’ accuracy significantly decreases when evaluating fraud companies (43%) relative to nonfraud companies (83%).
    - For *H1C*, the simple main effect of **Negative Affect Instruction** given Fraud Company is significant, indicating that instructed auditors (70%) outperform uninstructed auditors (43%) when evaluating fraud companies.
 
## 4.3. Negative Affect Instruction and Improved Deception Detection
- The **negative affect instruction** appears to unlock auditor ability, improving fraud detection. To more fully characterize the effects of the **negative affect instruction**, we conduct two analyses that exploit data underpinning the overall auditor fraud judgments above.
 
### 4.3.1. Effect of Negative Affect Instruction on Application of Audit-Related Knowledge
- While examining each transcript, auditors were asked to note the location of the **red flag** in the narrative, the topical issue of the **red flag**, and why it was a **red flag**.
    - As a collection, the content of the text box captures the extent to which auditors are applying their audit-related knowledge for the identification, description, and justification of fraud cues.
    - We use the number of characters supplied in the text box as a proxy for the extent to which auditors leverage their audit-related knowledge for the purpose of fraud detection.
- If improvements in fraud detection occur due to the **negative affect instruction's** unlocking of auditor ability, we should observe the largest amount of text box content for fraud companies when the **negative affect instruction** is provided.
- The average number of characters supplied by instructed auditors evaluating a fraud company (361.18) is larger than that of instructed auditors evaluating a non-fraud company (168.90), uninstructed auditors evaluating fraud company (255.13), and uninstructed auditors evaluating a nonfraud company (169.93).
    - Simple main effects analysis reveals instructed auditors evaluating a fraud company provide significantly more text box content than when they evaluate a nonfraud company.
    - The number of characters supplied is marginally higher for instructed versus uninstructed auditors when evaluating a fraud company, but not when evaluating a nonfraud company.
- These results indicate that the **negative affect instruction** does not indiscriminately increase the extent of **red flags** that instructed auditors find. Rather, auditors identify substantially more **red flags** precisely when the company is both a fraud company and when the auditor receives instruction.
 
### 4.3.2. Effect of Instruction on Sentence-Level Red Flag Accuracy Among Fraud Companies
- Because we know the specific locations within the narrative that caused concern for our auditors, we can refine our analysis of fraud detection to assess whether instructed auditors more accurately identify sentence-specific fraudulent statements by CEOs in the conference call narratives of fraud companies.
- To execute this analysis, we assess auditor’s fraud sensitivity at the sentence level for each of the five fraud companies.
    - First, we classify each CEO sentence in the transcript of each fraudulent company as fraudulent or not fraudulent, depending on whether the topic of the sentence pertains to the topic of the fraud.
    - Next, the subjective coding of sentence topics is compared with whether the sentence overlaps with a location of the narrative identified by the auditor as a **red flag**.
    - Finally, we define Red Flag Accuracy as 1 if the auditor identifies (does not identify) a **red flag** in a fraudulent (nonfraudulent) sentence, and zero otherwise.
- We find significant results using a linear contrast, with weights corresponding to experienced auditors’ deception judgments being equally highly accurate at the sentence level, unless they are evaluating fraud sentences without the **negative affect instruction**.
    - Simple effects analysis also shows that **negative affect instruction** significantly improves accuracy at the sentence level for fraud sentences, but not for nonfraud sentences.
- The results indicate the positive influence of the **negative affect instruction** on **deception detection** extends to the sentence level.
 
## 4.4. Alternative Explanations
### 4.4.1. 50/50 Fraud Rate
- The ideal deception detection study would have high internal validity but also high external validity by using a fraud rate that matches the fraud rate in the field [Levine et al., 2014].
- However, we do not have precise estimates on the true rate of fraudulent financial reporting. Moreover, if we were to use a particularly low rate of seeded fraud, a large fraction of our scarce audit subject resource would be assigned to nonfraud companies, inhibiting our ability to study their **deception detection** capabilities. As such, we use a 50/50 fraud rate.
- Although it is possible that use of a lower fraud rate at the company level would improve both instructed and uninstructed auditor’s accuracy in identifying fraud companies, such across the board improvement would not explain our ordinal interaction.
- For any given judgment, uninstructed auditors would tend to classify companies as being nonfraudulent (fraudulent) more (less) often than would instructed auditors.
- However, for this to be the case, instructed auditors must naively select higher rates of fraud without explicitly differentiating between fraud and nonfraud companies. Our theory predicts accuracy differentiation based on company type, and contrary to this alternative explanation, we find three sets of evidence for differentiated judgments.
    - First, uninstructed auditors judge just 31% of companies to be deceptive, which is significantly less than the 50% rate of instructed auditors.
        - However, this higher rate of selecting fraud is driven by instructed auditors’ higher rates for fraud companies, since there is no significant difference between the rate of fraud judgments between instructed and uninstructed auditors judging nonfraud companies.
        - This indicates that instructed auditors accurately discriminate in their fraud judgment rates, as predicted by our theory. Additionally, overall accuracy is not correlated with the number of times the auditor judged a company to be fraudulent for instructed auditors.
    - Second, the rate of judging a sentence as fraudulent is not different for instructed and uninstructed auditors for non-fraud sentences.
        - If subjects systematically responded to the instruction by judging sentences as **red flags**, we would expect a difference.
        - Both instructed and uninstructed auditors increase their rate of fraud judgment for fraudulent sentences, but instructed auditors do so to a greater extent, as predicted by our theory.
        - Additionally, if the instruction prompts specific attention to the stated seeded fraud rate, we should not observe our sentence-level results because the sentence-level fraud rate among fraud companies was not provided to auditor subjects. Nonetheless, we observe that accuracy with respect to identifying fraudulent sentences increases when auditors are instructed.
    - Finally, we find that instructed auditors differentiate by providing more extensive descriptions for fraud companies. That is, instructed auditors appear to apply more of their audit-related knowledge and effort regarding **red flags** for fraud companies, which demonstrates explicit differentiation between the presence and absence of fraud.
 
### 4.4.2. Medium
- We manipulate medium of communication (audio vs. transcript) to address potential generalizability issues, given the availability of both transcripts and audio in the marketplace. We make no particular predictions with respect to the effects of medium on our results.
- To explore whether the conference call format plays any role in deception detection, we examine whether the results are robust to different mediums.
- Neither including medium as a covariate nor as an interacted variable in a three-way analysis changes our inferences, yields a significant effect for the medium variable, or yields a significant effect for any of its interactions.
 
# 5. Conclusion, Limitations, and Implications
- We experimentally examine the joint, interactive effects of company type (fraud vs. nonfraud) and **negative affect instruction** (present vs. absent) on how accurately participants with many years of audit experience detect deception in CEO narratives.
- Auditors, when uninstructed about the association between **negative affect** and deception, classify companies accurately 63% of the time overall, significantly outperforming chance (50%). However, uninstructed auditors achieve this above-chance accuracy only because their accuracy rates for nonfraud companies (83%) significantly exceed that for fraud companies (43%), which does not statistically differ from chance levels. This pattern is consistent with our theory that auditors experientially become more attuned to avoiding false positives than false negatives regarding deception associated with fraud.
- We develop a theory-based instructional remedy to help auditors overcome their experientially learned avoidance of false positives. Our instruction explicitly encourages auditors to attend to management’s **negative affect** in order to mitigate their subconscious “illusion of objectivity” that otherwise causes auditors to downplay fraud cues from CEO narratives. We find instructed auditor accuracy levels for fraud companies improves from 43% to 70%, consistent with auditors having the ability to detect deception.
- In supplemental findings, we observe that instructed auditors more extensively characterize **red flags** among fraudulent companies relative to nonfraudulent companies. Additionally, instructed auditors show greater fraud sensitivity by accurately identifying fraud-related topics at the sentence-specific level. Thus, overall, we document that when provided with a **negative affect instruction**, participants with extensive audit experience possess sufficient audit-related knowledge and **deception detection** skill to detect fraud. Absent this simple, low cost instruction, however, even these very seasoned auditors are unable to detect deception from CEO conference calls narratives.
- Our research is subject to potential limitations, which also serve as opportunities for future research.
    - First, we ask auditors to imagine they are on their own audit engagement when the companies they evaluate are not their actual clients [Smith and Kida, 1991].
    - Second, since our experimental participants have numerous years of audit experience, we cannot necessarily conclude that the success of the **negative affect instruction** would extend similarly to auditors having significantly less audit-related knowledge.
    - Third, we study auditor’s assessment of earnings call narratives. While the majority of public companies hold earnings calls, some public and most private companies do not.
- This research contributes to both audit practice and research.
    - From a practice standpoint, we show that auditors with substantial audit experience can ascertain fraud cues from conference calls. Conference calls are publicly available for many companies, making them potentially useful for vetting new clients.
    - From a research perspective, we demonstrate a setting in which individuals successfully detect deception from narratives. In particular, we show a setting in which instructed auditors with extensive audit experience (i.e., experts) perform well in avoiding false positives. We also highlight the need for additional research examining experienced auditors’ reluctance to suspect a company is engaging in fraud, as our uninstructed auditors commit numerous false negatives, which likely is disconcerting to regulators, investors, and other financial statement users. Finally, we identify an inexpensive and useful remedy—instructing auditors to consider the **negative affect** of the speaker—to aid experienced auditors in mitigating these false negative errors.
 
---
 
# Executive summary of 1. Introduction
- Auditors are seldom the first to detect fraud, so improving their **deception detection capability** is of interest. We examine experienced auditors' **deception detection** in CEO narratives from earnings conference calls, which the PCAOB recommends auditors review.
- The psychology literature suggests that experienced professionals often perform poorly at deception detection.
- Theory and research suggest auditors learn to avoid false positives about fraud rather than evaluate evidence objectively. Experienced auditors learn to discount audit evidence that may cause conflict with management, even when it suggests elevated fraud risk.
    - We predict that experienced auditors will more accurately identify nonfraud companies than fraud companies when analyzing CEO earnings call narratives.
- We also predict that a **negative affect instruction** will improve auditor accuracy for fraud companies by reducing their learned tendency to discount fraud cues.
- We find that auditors receiving no instruction are more accurate for nonfraud firms but not for fraud firms. Auditors receiving the instruction achieve above-chance accuracy for both. Instructed auditors also provide more extensive descriptions of perceived **red flags** for fraud companies and more accurately identify specific sentences pertaining to frauds.
 
# Executive summary of 2. Related Research and Hypotheses
- Individuals generally fail to exceed chance levels when detecting deception, and experienced professionals rarely perform better than novices. However, financial-statement auditors are responsible for providing reasonable assurance that financial statements are free of material misstatement due to fraud. We study management narratives that occur publicly during Q&A portions of earnings conference calls.
- Auditors become less skeptical with more experience because there are few rewards and numerous costs to skeptical actions. These incentives and experiences have psychological learning effects:
    - Auditors learn to follow a **primary error detection and minimization (PEDMIN)** testing strategy, habitually minimizing false positives.
    - Much of the learning by auditors to avoid false positives operates at the subconscious level through **motivated reasoning**.
- To remedy auditors’ overlooking fraud cues, we consider **motivated reasoning** theory and derive an instruction that makes it unreasonable for experienced auditors to overlook fraud cues.
    - Our instruction informs auditors that deceivers often experience **negative affect** from **cognitive dissonance**.
- Instructing auditors to attend to management's **negative affect** will mitigate **motivated reasoning's** "illusion of objectivity" and enable experienced auditors to better detect deception in fraud companies.
- We propose three hypotheses:
    - *H1A*: Company type and **negative affect instruction** jointly influence experienced auditors’ deception detection accuracy in the form of an ordinal interaction.
    - *H1B*: When experienced auditors do not receive the **negative affect instruction**, they are more accurate for nonfraud companies than for fraud companies.
    - *H1C*: When evaluating fraud companies, experienced auditors are more accurate if they receive the **negative affect instruction** than if they do not receive the instruction.
 
# Executive summary of 3. Method
- We gathered 124 observations from 31 current or retired audit professionals with an average of 24 years in the audit.
- Each auditor provides deception judgments on excerpts from the Q&A portion of a quarterly earnings conference call for four public companies.
    - For each participant, we randomly draw four companies from a population of 10 companies, five with deceptive discussions and five without.
    - We characterize narratives as deceptive if the company’s financial statements being discussed in the call were restated and any of the following “irregularity conditions” hold: the restatement was deemed fraudulent, a regulatory investigation followed the restatement, or a class action lawsuit followed it.
- We manually transcribe each conference call excerpt and include the analyst’s question(s) to which the CEO is responding.
- We manipulate company type (fraudulent or not) and the presence or absence of the **negative affect instruction**, which includes defining cognitive dissonance and linking it to deception.
- The auditors’ evaluation of the example and each of the four real companies consist of three parts: background company information, the conference call excerpt and an area to record **red flags**, and a section to collect responses from auditors, including their overall judgment of whether fraud was being committed.
 
# Executive summary of 4. Results
- The overall accuracy rate of 63% for uninstructed auditors is statistically greater than chance levels of 50%.
    - However, this accuracy rate is driven by accuracy on nonfraud companies.
- Our results confirm *H1A*, *H1B*, and *H1C*.
- The average number of characters supplied by instructed auditors evaluating a fraud company is larger than that of instructed auditors evaluating a non-fraud company, uninstructed auditors evaluating fraud company, and uninstructed auditors evaluating a nonfraud company. Auditors identify substantially more **red flags** precisely when the company is both a fraud company and when the auditor receives instruction.
- **Negative affect instruction** significantly improves accuracy at the sentence level for fraud sentences, but not for nonfraud sentences.
- Use of a 50/50 fraud rate does not drive the pattern of results.
    - Instructed auditors accurately discriminate in their fraud judgment rates, as predicted by our theory.
    - The rate of judging a sentence as fraudulent is not different for instructed and uninstructed auditors for non-fraud sentences.
    - Instructed auditors differentiate by providing more extensive descriptions for fraud companies.
- Manipulation of medium of communication (audio vs. transcript) does not affect results.
 
# Executive summary of 5. Conclusion, Limitations, and Implications
- We find that auditors, when uninstructed, classify companies accurately 63% of the time overall, significantly outperforming chance. However, this above-chance accuracy is only because their accuracy rates for nonfraud companies significantly exceed that for fraud companies, consistent with our theory that auditors experientially become more attuned to avoiding false positives than false negatives regarding deception associated with fraud.
- We develop a theory-based instructional remedy to help auditors overcome their experientially learned avoidance of false positives.
    - Instructed auditor accuracy levels for fraud companies improves from 43% to 70%, consistent with auditors having the ability to detect deception.
- Overall, we document that when provided with a **negative affect instruction**, participants with extensive audit experience possess sufficient audit-related knowledge and **deception detection** skill to detect fraud.
- Our research is subject to potential limitations, which also serve as opportunities for future research.
- From a practice standpoint, we show that auditors with substantial audit experience can ascertain fraud cues from conference calls, potentially useful for vetting new clients. From a research perspective, we demonstrate a setting in which individuals successfully detect deception from narratives.

</details>

# Hongyan Liang
- Senior Lecturer of Business Administration
### education
- Ph.D., Business Administration (Operations Management), Kent State University, 2017
- M.A., Financial Economics, Ohio University, 2012
- B.A., Economics, Shanxi University, 2010
### research interest
- Supply chain coordination; Production and operations management; Closed-loop supply chain; Value-added recovery; Pricing management; Product returns, Third party reverse logistics; Supply chain delivery performance; Risk management
### teaching
- Supply Chain Management; Operations Management/Research; Management Science; Business Analytics; Data Analytics; Statistics; Quantitative Methods & Analysis; Purchasing and Supply Management; Strategic Sourcing; Project Management; SAP/ERP Solutions

```
title: Racial Disparities in Conforming Mortgage Lending: A Comparative Study of Fintech and Traditional Lenders Under Regulatory Oversight
authors: Zilong Liu, Hongyan Liang
journal: FinTech
published: 2025
```
# Executive Summary
- This study investigates racial and ethnic disparities in mortgage lending across different lender types (large banks, fintech lenders, non-bank lenders, small banks, and credit unions) using HMDA data from 2018 to 2023.
- The analysis focuses on loan approval rates, rate spreads, and origination charges.
- Key **theoretical** / **conceptual framework** discussions:
    - **Regulatory Oversight**: The study examines how varying levels of regulatory oversight impact lending outcomes for minority borrowers. Large banks, subject to stringent regulations, are compared against non-bank and fintech lenders, which often operate under lighter regulatory constraints.
    - **Algorithmic Decision-Making**: The role of fintech innovations, particularly algorithmic underwriting, in mitigating or perpetuating racial disparities is assessed. The study evaluates whether automated decision-making reduces human bias or simply replicates existing inequities in pricing models.
    - **Fair Lending Practices**: The research evaluates the extent to which different lender types adhere to fair lending laws, such as the Equal Credit Opportunity Act (ECOA) and the Fair Housing Act, in their mortgage lending practices.
- Key **findings** / arguments including:
    - Black and Hispanic borrowers face less favorable terms than White borrowers, with disparities varying by lender type.
    - Large banks have relatively equitable pricing but higher denial rates for minorities.
    - Credit unions offer the lowest rate spreads overall but penalize minority borrowers more severely in pricing.
    - Fintech lenders charge higher rate spreads and fees but show smaller credit access disparities for minority borrowers.
    - Non-bank and small banks display mixed results, with inconsistencies in their treatment of minorities.
    - Technological innovations alone do not eliminate systemic inequities; enhanced regulatory oversight, transparency in algorithmic decision-making, and stricter enforcement of fair lending practices are necessary.
    - Specifically, fintech lenders impose smaller additional rate spreads on minority borrowers (except large banks).
    - Credit unions, while generally offering lower rate spreads, surprisingly impose the largest rate spread penalties on minorities—particularly Black and Hispanic borrowers.
    - Large banks generally exhibit more equitable outcomes, and the negative coefficient for the minority variable indicates that minority borrowers, on average, face lower rate spreads than White borrowers within large banks.
    - Overall, the findings suggest that Fintech lenders, while charging the highest origination fees overall, offer more favorable terms to minority borrowers relative to White borrowers.
- In the logistic regression to predict mortgage denial rates, fintech lenders reduce the disadvantage faced by minority applicants, while non-banks and small banks produce more varied outcomes, and credit unions maintain a disadvantage, though one less severe than that at large banks.

#mortgage_lending #racial_disparities #fintech_lenders #regulatory_oversight #fair_lending #hmda_data #lender_types

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Racial and ethnic disparities persist in U.S. mortgage lending despite policy attention and regulatory oversight [1–3].
    - Black and Hispanic borrowers face higher denial rates, elevated interest rates, and greater financial burdens compared to White borrowers [1–3].
    - These disparities restrict homeownership, impede wealth accumulation, and undermine fair lending laws [4,5].
- The mortgage market has evolved, with non-bank and fintech lenders gaining market share [6–8].
    - Fintech innovations can potentially reduce racial disparities [9], but recent research shows they don't inherently eliminate disparities and may entrench them [10,11].
    - Non-bank lenders, with lighter regulation, may perpetuate inequities [12].
    - Large banks, with rigorous regulation, may have stronger incentives to adhere to fair lending standards [5,7].
- This study analyzes how lender type and regulatory environment shape minority borrowers' experiences.
    - We use HMDA data from 2018 to 2023 to analyze disparities in loan approval rates, rate spreads, and origination charges across lender types.
    - We control for borrower- and loan-level characteristics, as well as geographic and temporal factors.
- Large banks offer relatively equitable rate spreads but have higher denial rates for minorities.
- Credit unions offer the lowest rate spreads but impose the steepest penalties on minority borrowers.
- Fintech lenders impose fewer minority-specific penalties and improve approval odds for minority borrowers compared to large banks.
- Non-bank lenders and small banks produce more varied outcomes.
- New lender types and algorithmic decision-making tools don't guarantee more equitable lending practices.
    - Persistent disparities remain, highlighting the limitations of market-driven solutions.
    - Addressing systemic inequities requires sustained policy interventions, robust regulatory oversight, transparent algorithms, and fair lending law enforcement.
- We contribute to the literature by providing a comprehensive analysis of racial disparities across five lender types using HMDA data from 2018–2023.
    - We reveal nuanced patterns in credit access, pricing, and costs.
    - Our findings shed light on the limits of technological innovation and underscore the critical role of enhanced regulatory frameworks.
- We test the following two hypotheses:
    - *H1* (approval disparities): Even after controlling for borrower creditworthiness, Black and Hispanic borrowers will exhibit lower loan approval rates than White borrowers, with the magnitude of these disparities varying by lender type.
        - Rationale: Prior research suggests that racial disparities in mortgage lending persist even after accounting for creditworthiness, indicating potential biases in lending practices. Lender type may influence the extent to which these biases manifest.
    - *H2* (cost disparities): Conditional on loan approval, Black and Hispanic borrowers will face higher borrowing costs (rate spreads and/or origination charges) than similarly qualified White borrowers, with varying degrees of pricing penalties observed across different lender types.
        - Rationale: Even if minority borrowers are approved for loans, they may still face higher costs due to discriminatory pricing. The degree of these pricing penalties may vary depending on the lender type, reflecting different lending models and regulatory environments.
- By evaluating these hypotheses, we aim to ascertain whether newer lending models, such as fintech, can mitigate racial and ethnic disparities, or whether existing biases merely manifest in different ways under varying regulatory and operational structures.

# 2. Literature Review
- A large body of research documents persistent racial and ethnic disparities in the U.S. mortgage market.
    - Black and Hispanic applicants are more likely to be denied loans, charged higher interest rates, and face steeper financial costs than similarly situated White borrowers [2,3,13].
    - Racial disparities in mortgage lending are neither historical relics nor solely attributable to observable risk factors [1,5].
- Transformations in the mortgage industry have prompted new lines of inquiry.
    - Non-bank lenders have surged in market share, raising concerns that their less stringent regulatory environment may facilitate new or persistent forms of discrimination [6,12].
    - While some non-bank lenders broaden access, they often do so at higher cost or under less transparent pricing regimes, potentially widening disparities for minority borrowers [7].
- Fintech lending has also attracted scrutiny.
    - Proponents argue that automated decision-making can minimize human bias [9].
    - Empirical evidence suggests that algorithmic models may simply replicate existing biases [10,11,14].
    - Fintech innovations may streamline application processes but do not inherently guarantee more equitable outcomes.
- Persistent disparities remain across racial and ethnic groups despite the expansion of fintech lending platforms.
    - Haupert [15] reveals that fintech lenders are less likely to issue subprime loans in highly Black-segregated areas, but disparities persist, especially among Hispanic borrowers.
    - Fourcade and Healy [16] argue that algorithmic scoring systems reinforce existing social stratifications.
    - Haupert [17] indicates that while fintech lenders reduce racial disparities in loan approvals, they continue to distribute subprime credit disproportionately to minority neighborhoods.
    - These insights underscore the need for regulatory scrutiny to ensure fintech models do not perpetuate systemic biases under the guise of algorithmic neutrality.
- Large banks—often subject to more intensive oversight—provide a useful point of comparison.
    - Regulatory examinations incentivize them to implement standardized, risk-based underwriting procedures; reduce discretion; and comply with fair lending laws [5].
    - This more stringent regulatory environment may help large banks produce more equitable results.
- Credit unions, small banks, and other community-based institutions might be expected to serve minority borrowers effectively.
    - Studies suggest that personalized decision-making can introduce subjectivity and bias, undermining equitable outcomes [13].
    - Even lenders with ostensibly community-focused missions may inadvertently perpetuate disparities if they rely on less standardized assessment tools.
- The historical context of mortgage-lending disparities reveals entrenched patterns of racial exclusion and financial exploitation.
    - Faber [18] found that Black and Latino borrowers were disproportionately targeted for high-cost loans during the housing boom, even among wealthier minority applicants.
    - Hammel and Nilsson [19] analyzed the aftermath of the foreclosure crisis and highlighted that high-foreclosure minority neighborhoods faced sustained barriers to mortgage credit access.
    - While fintech and non-bank lenders have expanded access to mortgage credit, the persistence of pricing disparities suggests that historical patterns of exclusion continue to shape borrower experiences today.
- Loya [20] highlights racial stratification within the Latino borrower population.
    - Black Latinos face higher loan rejections and costlier mortgage products compared to White and Asian Latinos.
- Sanchez-Moyano [21] explores how geographic patterns and neighborhood racial compositions influence Hispanic homeownership disparities.
- Haupert and Lee [22] investigate the role of fintech lenders in subprime mortgage lending across immigrant gateway metropolitan areas.
    - While fintech lenders may reduce disparities in some contexts, they can still reflect racial disparities based on neighborhood demographics.
- Overall, the literature indicates that racial and ethnic disparities in mortgage lending persist across evolving market structures and technological advances.
    - While emerging lender types have shifted the competitive landscape, regulatory frameworks and enforcement remain pivotal in shaping equitable outcomes.
    - Investigating lender type in conjunction with borrower race and ethnicity can yield deeper insights into where and why disparities arise, informing the development of policies and regulatory interventions that promote fair and inclusive access to mortgage credit.

# 3. Data and Sample
- Our study analyzes HMDA data from 2018 to 2023, focusing on conventional first-lien 30-year fixed-rate conforming mortgages for owner-occupied, single-family properties.
- To ensure consistency, relevance, and replicability, we applied several filters to the data:
    - Included only loans for owner-occupied properties where occupancy type equals 1.
    - Retained records categorized as “Single Family (1–4 Units): Site-Built” for dwelling type.
    - Limited loan purposes to home purchase, refinancing, or cash-out refinancing.
    - Restricted to first-lien loans by including only records where lien status equals 1.
    - Excluded reverse mortgages and open-end lines of credit.
    - Removed loans for business or commercial purposes.
    - Filtered out loans with features such as negative amortization, interest-only payments, or balloon payments.
    - Included only loans meeting conforming loan limits.
    - Restricted the sample to those with a loan term of 360 months.
    - Excluded applications with missing or unspecified race or ethnicity.
- This detailed filtering process reduced the dataset to 29,338,620 mortgage applications.
- We included observations with missing values (property values and incomes) rather than excluding them.
    - Excluding these observations would disproportionately remove rejected applications, reducing the sample size from 29 million to 23 million and limiting our ability to analyze approval disparities.
    - Retaining all observations ensures that both approved and rejected loans are represented.
- We classified lender types using a structured approach:
    - Banks were identified by the presence of a bank charter, a Regulatory Reporting System ID (RSSD), or a bank parent.
    - Lenders without these identifiers were classified as non-banks.
    - Fintech lenders were identified based on prior research by Buchak et al. [6] and Fuster et al. [7].
        - Classified non-banks as fintech lenders if their mortgage-lending process up to the preapproval stage could be completed entirely online and if their business model relied predominantly on online channels.
    - Large banks were identified using the Federal Reserve’s Comprehensive Capital Analysis and Review (CCAR) list.
    - Banks not subject to CCAR were categorized as small banks.
    - Credit unions in the HMDA data are identified by an agency code of 5, which corresponds to the National Credit Union Administration (NCUA).
- Figure 1 illustrates the evolving market shares across various lender types from 2018 to 2023.
    - Non-bank lenders increased their market share from 41.9% in 2018 to 47.6% in 2023.
    - Fintech lenders also show consistent growth, rising from 9.5% to 12.5%.
    - Large banks’ share fell sharply from 21.8% in 2018 to 12.8% in 2023.
    - Small banks declined from 14.3% in 2020 to 12.8% in 2023.
    - Credit unions maintained a relatively steady contribution, accounting for 7–9% of the market.
- Table 1 presents summary statistics for the sample of 29,338,620 mortgage loans analyzed, covering the period from 2018 to 2023.
    - The average loan amount is USD 292,522.
    - The mean combined loan-to-value (CLTV) ratio is 76.14%.
    - The average interest rate is 4.01%.
    - The mean rate spread is 0.30%.
    - Mean total loan costs are USD 4514, and mean origination charges are USD 2263.
- Demographic and geographic characteristics, based on HMDA data, indicate that the average percentage of minority residents in the census tracts where mortgage applications originated is 31.41%.
    - The average tract-to-MSA income percentage is 114.35%.
    - The median family income across all MSAs in the dataset is USD 83,993.
- The sample reflects a diverse lender composition:
    - Non-bank lenders hold the largest share (44.50%).
    - Small banks (18.30%), large banks (15.80%), fintech lenders (12.50%), and credit unions (8.50%).
- A majority of loans (53.00%) finance home purchases.
- Borrower characteristics show that 38.20% of loans are taken jointly.
- In terms of race and ethnicity, White borrowers constitute 72.30% of the sample.

# 4. Loan Approval Rates
- We begin our analysis by examining unconditional approval rates to establish a baseline understanding of racial and ethnic disparities across different lender types.
- Table 2 presents loan approval rates by lender type and borrower race/ethnicity, segmented by purchase, refinance, and overall loans.
    - In addition to the raw approval rates, the table provides the differences in approval rates between minority borrowers and White borrowers.
- Across all lender types, White borrowers consistently achieve the highest approval rates.
    - Credit unions approve White applicants at an overall rate of 87.99%.
    - Hispanic (75.53%) and Black (66.77%) borrowers encounter lower overall approval rates.
    - Black borrowers face a particularly steep shortfall in refinance loans (60.26%).
- Approval decisions are influenced by borrower-level attributes that can differ systematically by race and ethnicity.
    - Some of these key factors are incorporated into the regression analysis in later section.
- While fintech and non-bank lenders exhibit relatively more inclusive outcomes for many minority groups, Black and Hispanic applicants still experience significant approval gaps compared to White borrowers.
    - Even at fintech lenders, Black borrowers’ all-loan approval rate lags behind that of White borrowers by nearly 10 percentage points.
- In contrast, credit unions, large banks, and small banks tend to have even wider disparities for Black and Hispanic borrowers, often reaching double-digit approval-rate gaps.
- These findings indicate that although some newer or technology-driven lending models appear to reduce inequities for certain groups, Black and Hispanic borrowers remain at a disadvantage across all lender types.
    - The persistent nature of these disparities underscores the need for targeted interventions and rigorous enforcement of fair lending regulations.

# 5. Pricing and Cost Components
- Having examined unconditional approval rates, we now turn to descriptive measures of pricing and cost components to understand how disparities extend beyond simple acceptance or rejection decisions.
- Table 3 provides insights into interest rates, rate spreads, origination charges, and discount points by lender type and race/ethnicity.
    - While fintech lenders often show smaller differences in headline pricing terms like interest rates and rate spreads, Black and Hispanic borrowers still incur noticeably higher origination charges and discount points compared to White borrowers.
    - Even with fintech lenders, Black borrowers face substantially higher loan-related fees.
- Similarly, for non-bank lenders, interest rates may not differ drastically, but Black and Hispanic borrowers shoulder some of the steepest additional costs.
- Large banks and credit unions, despite occasionally offering more favorable interest terms, fail to fully close the gap in fees, placing a disproportionate financial burden on these borrowers.
- Overall, no lender category successfully eliminates cost-based inequities for Black and Hispanic borrowers.
- Taken together, Tables 2 and 3 reveal that, on average, Black and Hispanic borrowers face notable disadvantages in both loan approval rates and loan costs relative to White borrowers across various lender types.
    - Although fintech and non-bank lenders show somewhat narrower disparities, particularly in approval-rate and interest-rate terms, these gaps do not disappear entirely, especially when considering origination charges and discount points.
- It is important to note that these patterns represent unconditional means and do not control for borrower risk factors.
- To better understand the drivers of these disparities and identify whether differences persist after adjusting for relevant risk characteristics, we proceed with a multivariate regression analysis in the following sections.

# 6. Regression Analysis
- While the unconditional statistics in Tables 2 and 3 demonstrate persistent disparities, they may conflate differences in borrower risk and property characteristics with discrimination.
- To disentangle these effects, we next present regression analyses that control for a range of relevant factors, such as income, loan characteristics, state, and year fixed effects.
    - This approach aims to provide a deeper understanding of the underlying drivers of these disparities.
- We examine three outcomes: rate spread, reflecting the cost of borrowing; origination charge, capturing upfront loan costs; and loan-application denial, indicating access to credit.
- This comprehensive approach isolates the underlying drivers of observed disparities.

## 6.1. Rate Spread Regression Analysis
- Table 4 reports the results from a series of fixed-effects linear regressions of the following form:
    - RateSpread{i,c,t} = α + β1Xi + β2 Li + β3(Minority × LenderType) + γc + δt + ε{i,c,t}
    - where RateSpread{i,c,t} is the difference between the loan’s Annual Percentage Rate (APR) and the average prime offer rate for loan i, in county c and year t.
    - The vector Xi includes borrower and loan characteristics.
    - Li represents indicator variables for the lender type, with large bank serving as the baseline category.
    - County and origination-year fixed effects (γc and δt) control for geographic and temporal variations.
    - The interaction terms Minority × LenderType capture how the pricing difference for a minority borrower relative to a White borrower changes across lender types.
- Each column in Table 4 restricts the sample to White borrowers and one minority group.
- The results provide a comprehensive understanding of rate spread differences across firm types and borrower groups.
    - First, examining the firm-type variable alone, credit unions are associated with significantly lower rate spreads, underscoring their role in offering more favorable loan terms.
    - In contrast, fintechs, non-banks, and small banks charge higher rate spreads than large banks.
    - Among all firm types, credit unions provide the most valuable cost savings to consumers.
- The minority dummy variable alone captures how minority borrowers compare to White borrowers within large banks (the baseline group).
    - The negative coefficient for the minority variable indicates that minority borrowers, on average, face lower rate spreads than White borrowers within large banks.
    - This outcome suggests that large banks may implement targeted pricing strategies or take deliberate actions to offer competitive terms to minority borrowers.
- The interaction terms indicate how minority borrowers are treated relative to White borrowers within each financial firm type.
    - For example, the positive coefficient for the minority–fintech interaction demonstrates that rate spreads are higher for minority borrowers than for White borrowers at fintechs.
    - However, the disparity at fintechs is smaller compared to other institution types.
    - The interaction term minority × fintech indicates that the rate spread gap between Black and White borrowers is 15.96 basis points, significantly smaller than the 27.61 basis point gap at credit unions.
    - This suggests that fintech lenders apply more standardized pricing, reducing the discretionary bias observed in traditional institutions.
- Ranking the interaction coefficients for Black and Hispanic borrowers reveals that fintechs impose the smallest additional rate spread, followed by small banks.
    - Non-banks exhibit a larger pricing disparity than both fintechs and small banks.
    - Interestingly, credit unions impose the highest additional rate spread on minority borrowers, challenging their reputation for consumer-friendly practices.
- Borrower and loan characteristics further influence rate spreads.
    - Larger loan amounts are associated with lower rate spreads.
    - Higher property values have a minor yet favorable effect on rate spreads.
    - Higher borrower income significantly reduces rate spreads.
    - Debt-to-income ratios are strongly correlated with rate spreads.
- The results demonstrate that although fintech lenders impose statistically significant rate spread penalties on minority borrowers, these penalties are smaller compared to other financial institutions (except large banks).
- Credit unions, while offering the lowest overall rate spreads to all borrowers, surprisingly impose the largest rate spread penalties on minorities—particularly Black and Hispanic borrowers.
- Large banks, generally exhibit more equitable outcomes.
- Minority borrowers at large banks often receive rate spreads that are equal to or even slightly lower than those of White borrowers.
- Other lender types, particularly credit unions, non-banks, and small banks, continue to impose disproportionate costs on minority borrowers.
- Additionally, we conducted robustness checks using the interest rate as the dependent variable instead of the rate spread.
    - The results were quantitatively similar.

## 6.2. Origination-Charge Regression Analysis
- Table 5 reports the results from a series of fixed-effects linear regressions of the following form:
    - Origniation_Charge{i,c,t} = α + β1Xi + β2 Li + β3(Minority × LenderType) + γc + δt + ε{i,c,t}
    - where Origniation_Charge{i,c,t} is the upfront fee covering the lender’s costs for processing, underwriting, and approving loan i, in county c and year t.
    - The vector Xi includes borrower and loan characteristics.
    - Li represents indicator variables for the lender type, with large bank serving as the baseline category.
    - County and origination-year fixed effects (γc and δt) control for geographic and temporal variations.
    - The interaction terms Minority × LenderType capture how the pricing difference for a minority borrower relative to that of a White borrower changes across lender types.
- The results provide insights into how origination charges vary across firm types and borrower groups.
- For firm types alone, credit unions are associated with the lowest origination charges, as indicated by the negative coefficient compared to large banks.
    - In contrast, all other firm types charge higher origination costs than large banks, with fintechs imposing the highest origination charges.
- The minority variable alone captures how minority borrowers compare to White borrowers within large banks (the baseline group).
    - The positive coefficients for Black, Hispanic, and other minority groups suggest that large banks charge minorities higher origination costs than White borrowers.
- Examining the interaction terms reveals nuanced results across different financial firm types.
    - Fintechs, for example, offer lower origination charges to minority borrowers compared to White borrowers, as reflected by the negative interaction coefficients.
    - The interaction term minority × fintech is negative for Black borrowers, indicating a reduction in origination charges of approximately USD 505 compared to White borrowers, all else being equal.
- For other financial firm types, the results are mixed.
    - Black borrowers consistently receive lower origination charges across all firm types, whereas Hispanic borrowers tend to be charged higher origination costs.
- Overall, the findings suggest that Fintech lenders, while charging the highest origination fees overall, offer more favorable terms to minority borrowers relative to White borrowers.
    - In contrast, other financial institutions exhibit more varied and less consistent pricing patterns for minority groups.

## 6.3. Approval-Rate Logit Regression
- Similarly, to better understand denial decisions and verify whether the patterns observed in raw approval rates persist after controlling for confounding variables, we estimate logit models that adjust for borrower- and loan-level attributes.
- Table 6 presents logistic regressions of the following form:
    - logit(P(Denial{i,c,t} = 1)/(1 − P(Denial{i,c,t} = 1))) = α + β1Zi + β2 Li + β3(Minority × LenderType) + γs + δt
    - where P(Denial{i,c,t} = 1) is an indicator equal to 1 if loan iii is denied and 0 otherwise.
    - Similar to Table 4, we include a rich set of controls, Zi.
    - We also control for state and origination-year fixed effects (γs, δt).
    - The interaction terms (Minority × LenderType) identify whether a given lender type increases or decreases the likelihood of denial for minority applicants relative to White applicants, after holding observable borrower and loan characteristics constant.
- The four columns each focus on a distinct minority group compared to White borrowers.
- Starting with the lender-type variable, when using large banks as the baseline, the coefficients for credit unions, fintechs, non-banks, and small banks are all negative and statistically significant.
    - This suggests that relative to large banks, these other lending institutions are generally associated with lower rejection rates.
- Focusing next on minority status, the coefficient for the “minority” variable itself is positive and highly significant.
    - This reflects the baseline scenario for large banks: minority borrowers experience a higher likelihood of loan rejection compared to non-minority (White) borrowers.
- However, once we include interaction terms between minority status and lender type, the picture becomes more nuanced.
    - For Fintech lenders, the interaction term with minorities is negative and significant.
    - This indicates that when minority borrowers seek loans from Fintech firms, the difference in rejection rates compared to White borrowers actually diminishes relative to what they would face at a large bank.
- In contrast, the interaction results for non-banks and small bank are mixed.
- Credit unions, on the other hand, resemble large banks in their treatment of minorities—minority applicants are still more likely to be rejected—but the magnitude of the disadvantage is smaller than it is at large banks.
- In summary, relative to large banks, all other lender types initially appear more accommodating, offering lower rejection rates overall.
- For minority borrowers, large banks present a significant hurdle, showing a marked increase in rejection likelihood.
- Interactions with lender type modify these outcomes: fintech lenders, in particular, reduce the disadvantage faced by minority applicants, while non-banks and small banks produce more varied outcomes, and credit unions maintain a disadvantage, though one less severe than that at large banks.

# 7. Results summary (Tables 4–6)
- The regression results from Tables 4–6 provide a complex and nuanced lending landscape where no single lender type uniformly provides the most favorable outcomes for minority borrowers.
- Although large banks serve as a baseline, the analysis shows that differences in pricing, fees, and rejection rates vary significantly across lender types and borrower groups once relevant borrower and loan characteristics, as well as local and temporal factors, are taken into account.
- For rate spreads, large banks tend to offer relatively equitable or even slightly more favorable rate spreads to minority borrowers compared to Whites.
    - Credit unions provide the lowest overall rates, but they paradoxically impose the largest minority penalties when considering the interaction terms.
    - Other lenders, including fintechs and non-banks, generally charge higher spreads than large banks but do not penalize minority borrowers as heavily as credit unions do.
- For origination charges, credit unions stand out by offering the lowest upfront charges.
    - Fintechs charge high origination fees overall but are relatively more favorable toward minority borrowers, reducing some disparities in these upfront costs.
- For credit access, large banks appear relatively more stringent overall and impose a significant minority penalty in terms of denial rates.
    - Other lender types—fintechs, non-banks, small banks, and credit unions—are associated with lower denial rates on average.
    - Fintechs notably reducing the minority denial gap.
- Table 7 presents a summary of how different racial and ethnic groups are treated by various lender types based on interaction-term results from regression analysis.
- For Black borrowers, large banks impose stricter credit access, but once approved, they provide favorable rate spreads, albeit with higher origination charges.
    - Fintech lenders offer easier access to credit but impose less favorable terms.
    - Non-bank and small-bank lenders present stricter credit access with consistently higher spreads and origination fees.
    - Credit unions also exhibit stricter credit access and impose higher costs in both spreads and origination charges.
- For Hispanic borrowers, large banks enforce stricter approval standards but offer favorable rate spreads for those approved, although origination charges remain higher.
    - Fintech lenders improve approval odds for Hispanic borrowers but impose higher spreads and origination fees.
- For Asian borrowers, large banks exhibit slightly stricter approval standards but offer favorable rate spreads and origination charges once approved.
    - Fintech lenders provide easier credit access with favorable origination charges, but spreads are slightly higher compared to White borrowers.
- For Other Minorities, large banks impose stricter approval standards but provide more equitable spreads and only slightly higher origination charges once approved.
    - Fintech lenders offer easier credit access with favorable origination charges but impose higher spreads.
- In essence, the overall conclusion is that differences across lender types cannot be fully explained by borrower risk factors alone.
    - These findings highlight the importance of scrutinizing individual lender practices and market structures to ensure that neither cost savings nor broader access comes at the expense of equitable treatment for minority borrowers.

# 8. Discussion and Implications
- Our analysis of HMDA data from 2018 to 2023 reveals that racial and ethnic disparities persist across different lender types, despite controlling for a range of borrower- and loan-level factors.
- While some lenders expand credit access or reduce specific cost penalties for minority borrowers, no single lender category systematically eliminates disparities in mortgage pricing or approval decisions.
- Large banks in our dataset tend to offer relatively equitable rate spreads once minority borrowers are approved.
    - However, our results also show higher denial rates for Black and Hispanic applicants at these institutions.
- Fintech lenders improve approval odds for minority borrowers but charge comparatively higher interest rate spreads and fees.
- Credit unions provide the lowest average rate spreads but impose the steepest pricing penalties on Black and Hispanic borrowers.
- Similarly, small banks and non-bank lenders exhibit mixed or inconsistent patterns.
- Several mechanisms may help explain these results:
    - First, heightened federal oversight likely incentivizes large banks to adopt standardized, risk-based pricing models that reduce some forms of cost discrimination but also tighten credit eligibility thresholds.
    - Second, smaller, community-based lenders frequently rely on discretionary lending decisions.
    - Finally, non-bank lenders tend to approve more minority borrowers but at cost structures that may compound financial burdens.
- Because different lender types exhibit distinct patterns of approval and pricing disparities, policy solutions will require a tailored and multifaceted approach:
    - First, extending or refining the scope of federal fair-lending regulations to encompass non-bank mortgage lenders could ensure that they undergo similarly rigorous fair-lending exams as large banks.
    - Second, transparent algorithmic audits could address the persistent cost gaps observed for minority borrowers at fintech lenders.
    - Third, strengthening oversight and training for credit unions and small banks can help align their community-focused missions with equitable pricing.
- Although HMDA provides a broad lens on mortgage activity, it lacks direct credit metrics that could further illuminate whether risk-based pricing alone explains the observed differentials.
- In sum, our study confirms that the U.S. mortgage market remains stratified along racial and ethnic lines, even amid the rise of technology-focused lenders and diverse institutional structures.

# 9. Conclusions
- This study provides nuanced insights into how different lender types impact mortgage outcomes for minority borrowers.
- Using HMDA data from 2018 to 2023 and controlling for borrower and loan characteristics, we examine disparities in credit access, rate spreads, and origination charges.
- Our findings suggest that large banks impose significant barriers to credit access for minority borrowers, reflected in higher denial rates compared to White borrowers.
    - However, once approved, large banks generally offer better pricing for minority borrowers, with lower rate spreads and fewer pricing penalties.
    - Despite this, minority borrowers often face higher upfront origination charges.
- Fintech lenders show greater inclusivity in regard to credit access.
    - Denial disparities for minority borrowers are narrower, and origination charges for minorities are notably lower.
    - However, fintech lenders impose higher rate spreads on approved minority borrowers.
- Credit unions exhibit mixed results.
    - They offer the lowest overall rate spreads and origination charges, but their treatment of minority borrowers varies widely.
    - Hispanic borrowers often face significant pricing penalties, while Black borrowers benefit from reduced costs and more favorable terms.
- Small banks provide moderate credit access, with denial rates less stringent than those of large banks.
    - However, the pricing outcomes for minority borrowers are inconsistent.
- Non-bank lenders offer relatively higher approval rates for minorities, improving credit access.
    - However, their pricing and cost structures remain uneven.
- In sum, this study reveals that while fintech and non-bank lenders improve credit access for minority borrowers, they fail to eliminate disparities in pricing and costs.
- Large banks demonstrate relatively equitable pricing once minority borrowers are approved, but their stringent credit access standards remain a significant hurdle.
- Credit unions, despite offering lower baseline costs, impose severe penalties on certain minority groups, particularly Hispanic borrowers.
- Small banks and non-banks display inconsistent practices that result in varied outcomes for different minority groups.
- These findings highlight the need for enhanced regulatory oversight, standardized lending practices, and transparent algorithms to address persistent disparities in the mortgage market.

---

# Executive summary of 1. Introduction
- Persistent racial and ethnic disparities in U.S. mortgage lending hinder homeownership and wealth accumulation.
- The evolving mortgage market includes non-bank and fintech lenders, with varying impacts on these disparities.
- Fintech innovations offer potential but don't guarantee equitable outcomes, while large banks face regulatory pressure to adhere to fair lending standards.
- This study analyzes lender type and regulatory environment using HMDA data from 2018 to 2023, focusing on loan approval rates, rate spreads, and origination charges.
- Key findings include: large banks have equitable pricing but higher denial rates for minorities; credit unions have lower spreads but penalize minorities more severely; fintech lenders improve access but charge higher rates and fees.
- The study concludes that technological innovation alone isn't sufficient, requiring policy interventions, regulatory oversight, transparent algorithms, and fair lending enforcement.
- *H1* examines approval disparities, predicting lower approval rates for Black and Hispanic borrowers varying by lender type.
- *H2* examines cost disparities, predicting higher borrowing costs for these groups varying by lender type.

# Executive summary of 2. Literature Review
- Extensive research confirms ongoing racial and ethnic disparities in U.S. mortgage lending.
- Non-bank lenders' growth raises concerns about discrimination due to less stringent regulation.
- Fintech lending's use of algorithmic models may replicate existing biases, despite streamlining application processes.
- Studies show that fintech lenders are less likely to issue subprime loans in highly Black-segregated areas, but disparities persist, especially among Hispanic borrowers, requiring scrutiny to prevent algorithmic neutrality from perpetuating biases.
- Large banks' stringent oversight promotes equitable results, while community-based institutions' personalized decision-making can introduce bias.
- Historical context reveals entrenched racial exclusion, with higher-income minorities targeted for subprime loans and high-foreclosure minority neighborhoods facing access barriers.
- Fintech and non-bank lenders' expanded access doesn't eliminate pricing disparities, indicating persistent patterns of exclusion.
- Racial stratification among Latino borrowers, geographic patterns, and neighborhood racial compositions further influence disparities.
- Regulatory frameworks and enforcement remain crucial for equitable outcomes.

# Executive summary of 3. Data and Sample
- The study uses HMDA data from 2018 to 2023, focusing on conventional first-lien mortgages for owner-occupied, single-family properties.
- Rigorous data filtering ensures consistency and relevance, resulting in a dataset of 29,338,620 mortgage applications.
- Missing values for property values and incomes are included to avoid disproportionately removing rejected applications.
- Lender types are classified based on bank charters, regulatory IDs, and online lending processes, distinguishing between traditional banks and fintech lenders.
- Market share analysis shows increasing dominance of non-bank and fintech lenders, while large banks' share declines.
- Summary statistics reveal average loan amount of USD 292,522, CLTV ratio of 76.14%, and interest rate of 4.01%.
- Demographic data indicates 31.41% minority residents in mortgage application areas, with median family income across MSAs at USD 83,993.
- Non-bank lenders hold the largest share (44.50%), with purchase loans dominating (53.00%), and White borrowers constituting 72.30% of the sample.

# Executive summary of 4. Loan Approval Rates
- Unconditional approval rates are examined to understand racial and ethnic disparities across lender types.
- White borrowers consistently achieve the highest approval rates across all lender types, with Black and Hispanic borrowers facing significantly lower rates.
- Fintech and non-bank lenders exhibit more inclusive outcomes for many minority groups, but Black and Hispanic applicants still experience substantial approval gaps.
- Credit unions, large banks, and small banks tend to have wider disparities for Black and Hispanic borrowers, often reaching double-digit approval-rate gaps.
- Some newer lending models reduce inequities for certain groups, Black and Hispanic borrowers remain at a disadvantage across all lender types, highlighting the need for targeted interventions and rigorous enforcement of fair lending regulations.

# Executive summary of 5. Pricing and Cost Components
- This section describes the pricing and cost components to examine how disparities extend beyond simple acceptance or rejection decisions.
- Fintech lenders often show smaller differences in headline pricing terms but Black and Hispanic borrowers still incur noticeably higher origination charges and discount points compared to White borrowers.
- Non-bank lenders may not differ drastically in interest rates, but Black and Hispanic borrowers shoulder some of the steepest additional costs.
- Large banks and credit unions occasionally offer more favorable interest terms but fail to fully close the gap in fees, placing a disproportionate financial burden on these borrowers.
- Black and Hispanic borrowers face notable disadvantages in both loan approval rates and loan costs relative to White borrowers across various lender types.
- While fintech and non-bank lenders show somewhat narrower disparities, particularly in approval-rate and interest-rate terms, these gaps do not disappear entirely.
- Patterns represent unconditional means and do not control for borrower risk factors.

# Executive summary of 6. Regression Analysis
- Regression analyses control for borrower risk factors to disentangle them from discrimination, examining rate spread, origination charge, and loan-application denial.
- **6.1.** Rate Spread Regression Analysis
    - Credit unions offer lower rate spreads, while fintechs, non-banks, and small banks charge higher spreads than large banks.
    - Minority borrowers face lower rate spreads than White borrowers within large banks.
    - Fintechs impose smaller additional rate spread penalties on minority borrowers (except large banks).
    - Credit unions surprisingly impose the largest rate spread penalties on minorities.
    - Larger loan amounts and higher borrower income reduce rate spreads, while higher debt-to-income ratios increase them.
- **6.2.** Origination-Charge Regression Analysis
    - Credit unions have the lowest origination charges, while fintechs charge the highest.
    - Large banks charge minorities higher origination costs than White borrowers.
    - Fintechs offer lower origination charges to minority borrowers compared to White borrowers.
- **6.3.** Approval-Rate Logit Regression
    - Credit unions, fintechs, non-banks, and small banks are associated with lower rejection rates than large banks.
    - Minority borrowers face a higher likelihood of loan rejection compared to White borrowers at large banks.
    - Fintech lenders reduce the disadvantage faced by minority applicants.

# Executive summary of 7. Results Summary (Tables 4–6)
- The lending landscape is complex, with no single lender uniformly providing the most favorable outcomes for minority borrowers.
- Large banks offer equitable rate spreads but have stringent credit access.
- Credit unions provide the lowest overall rates but impose the largest minority penalties.
- Fintechs charge higher spreads but do not penalize minorities as heavily as credit unions.
- Differences across lender types cannot be fully explained by borrower risk factors alone.

# Executive summary of 8. Discussion and Implications
- Racial and ethnic disparities persist across different lender types, despite controlling for borrower and loan-level factors.
- While some lenders expand credit access, no single lender category eliminates disparities.
- Large banks offer equitable rate spreads but have higher denial rates, fintech lenders improve approval odds but charge higher rates and fees, and credit unions offer lower rates but impose pricing penalties on minority borrowers.
- Mechanisms explaining these results include federal oversight incentivizing standardized pricing, reliance on discretionary lending, and approval of higher-risk applicants at costlier structures.
- Policy solutions require tailored approaches, including fair-lending regulations for non-bank lenders, algorithmic audits for fintech lenders, and oversight for credit unions and small banks.
- The absence of direct credit metrics presents a limitation.

# Executive summary of 9. Conclusions
- The study provides insights into how different lender types impact mortgage outcomes for minority borrowers using HMDA data from 2018 to 2023.
- Large banks impose barriers to credit access but offer better pricing once approved, though with higher origination charges.
- Fintech lenders show greater inclusivity, but denial disparities exist, origination charges are lower, and rate spreads are higher.
- Credit unions exhibit mixed results, with significant pricing penalties for Hispanic borrowers and more favorable terms for Black borrowers.
- Small banks provide moderate credit access but inconsistent pricing outcomes.
- Non-bank lenders offer higher approval rates but uneven pricing and cost structures.
- Enhanced regulatory oversight, standardized lending practices, and transparent algorithms are needed to address persistent disparities.

</details>

```
title: Permanent layoff and consumer credit card loss forecasting
authors: Zilong Liu and Hongyan Liang
journal: Managerial Finance
published: 2023
```

# Executive Summary
- This research investigates the relationship between credit card charge-off rates and the unemployment rate (UR), particularly focusing on the impact of permanent versus temporary layoffs.
- We find that the spike in UR during COVID-19 was mainly due to a surge in temporary layoffs.
- The credit card charge-off rate is primarily driven by permanent UR, with temporary UR having little predictive power.
- During recessions, permanent UR is a stronger indicator than total UR.
- **Theoretical framework**:
    - Decomposition of Unemployment Rate:
        - Total UR = Permanent UR + Temporary UR, allowing for analysis of each component's influence.
    - Regime-Switching Model:
        - Accounts for shifts in the relationship between UR and credit loss across different economic states (recession vs. non-recession).
- **Key Findings**:
    - Variance in total UR is primarily driven by permanent layoffs (75% vs 25% for temporary).
    - Permanent UR exhibits a higher correlation with credit card charge-off rates than temporary UR.
    - In non-recessionary periods, total UR has a significant impact on credit loss, but this predictive power weakens during recessions.
    - Permanent UR shows stronger predictive power for credit loss during recessions.
    - Temporary UR has little explanatory power in either recessionary or non-recessionary periods.
- The findings suggest using permanent UR for credit risk modeling, particularly in times of economic uncertainty like the COVID-19 pandemic. This approach can improve loss forecasting and CECL reserve models for banks, regulators, and policymakers.

#credit_risk #macroeconomics_indicators #loss_forecasting #covid_19 #unemployment_rate #permanent_layoff #temporary_layoff #credit_card_chargeoff #regime_switching_model

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Credit risk management is crucial for financial institutions, highlighted by the 2008-2009 GFC. Understanding credit risk drivers is essential for banks, regulators, and policymakers [Virolainen, 2004; M€ahlmann, 2005; Hackbarth et al., 2006; Pesaran et al., 2006; Jakubik, 2007; Bonfim, 2009; Castro, 2013; Djeundje and Crook, 2018; Breeden and Crook, 2022].
- Macroeconomic indicators play a key role in loss forecasting models and are mandated by regulators for stress testing. The CECL approach also requires a forward-looking view, considering future economic conditions (ASU 2016-13).
- Prior studies indicate that the unemployment rate (UR) is a leading factor in explaining credit loss for consumer loans, reflecting consumers' ability to meet financial obligations [Castro, 2013; Taghiyeh et al., 2021]. An increase in UR typically correlates with increased difficulty in credit card payments due to income loss.
- The COVID-19 pandemic significantly disrupted economic activities, leading to job losses. Simultaneously, government and financial institutions implemented relief measures. This macroeconomic shift led to high volatility in labor market variables. For instance, unemployment initial claims surged by 2000% from March to April 2020.
- Despite labor market volatility, credit loss rates remained relatively stable, possibly due to government relief and corporate forbearance programs. This created a disconnect between unemployment and credit card loss during the COVID-19 period.
- While unemployment is used for credit risk modeling, its relationship with credit loss seemed to weaken during COVID-19.
- The peak in unemployment during COVID-19 was primarily driven by temporary layoffs, while permanent job losses saw less dramatic change.
- Research Question: **Whether credit loss is driven by temporary or permanent layoffs**, which is particularly important given the growing divergence between them under COVID-19.
    - COVID-19 has led to a surge in temporary layoffs, creating a divergence between temporary and permanent unemployment.
    - Traditional models relying on total unemployment may not accurately reflect the current economic reality.
    - Understanding the distinct impacts of temporary and permanent layoffs can enhance the accuracy and stability of credit risk models.
- The primary question we examine in this paper is how total unemployment, as well as its two basic components – permanent and temporary layoffs, affects credit card charge-off rates over time.
- We investigate consumer credit card loans, which are likely to be affected by labor market conditions.
- We separate total unemployment into permanent and temporary layoffs, based on the temporary layoff ratio from the Bureau of Labor Statistics (BLS).
- We first decompose UR into two components – permanent and temporary UR and find that the variance in total UR is primarily driven by permanent layoffs.
- In addition, permanent UR is more closely correlated to total UR than temporary UR as indicated by a higher correlation coefficient.
- We then examine how credit card charge-off rate is correlated with UR as well as its two components over time.
- Permanent UR along with its transformations exhibited a higher correlation with credit loss than that of temporary UR.
- We also test the predictive power of permanent and temporary UR on credit loss by using several feature selection algorithms.
- These results suggest that permanent UR also has a dominant power in explaining the credit loss while temporary UR has little.
- Finally, we use a regime-switching model to test the relationship between UR and charge-off rate under different economic states.
- The results indicate that total UR has a large explanatory power to credit loss in non-recessionary periods while the predictive power weakens during recessions.
- In contrast, permanent UR exhibits a stronger predictive power for credit loss in recessions. Temporary UR has little explanatory power to credit loss in both regimes.
- We find permanent unemployment has dominant power in explaining the credit loss while temporary unemployment has little predictive power.
- The credit card charge-off rate is even more closely related to permanent unemployment than total unemployment.
- Our finding also helps explain the recent disconnection between total unemployment and credit charge-off under COVID-19, because the spike in UR is mainly driven by a surge in the temporary layoff, which has little explanatory power to credit loss.
- Our finding can also be explained by business intuition as cardholders who are on temporary layoff can still fulfill the debt obligation as long as they return to work and do not turn into permanent job losers.
- As a result, an increase in temporary layoff does not necessarily lead to a higher credit loss.
- The uncertainty about the economic outlook is quite large under COVID-19, ensuring the stability and accuracy of the loss forecasting framework have been considered imperative for banks, regulators, and policymakers.
- Federal Reserve has heightened the standard on bank monitoring and highlighted the critical importance to explore a more robust relationship between macroeconomic variables and credit loss that can be used during the interim of the COVID-19 pandemic.
- Our research helps to address the concern.
- The findings in our research can be applied to the credit card loss forecasting and CECL reserve models.
- In addition, our research also has implications for banks, macroeconomic data vendors, regulators, and policymakers.

# 2. Data and methodology
- We use quarterly observations of charge-off rates on credit card loans of all commercial banks from Q1 1990 to Q4 2020.
- This period covers multiple business cycles, including the 1990 oil price shock, 2000 Internet bubble, 2007–2008 GFC, and part of the COVID-19 pandemic.
- Historical macroeconomic variables were directly downloaded from the Federal Reserve, which provides data and forecasting series for 28 core macroeconomic indicators (14 domestic and 14 international) for bank stress testing.
- We only use the 14 domestic macroeconomic variables.
- We also obtain historical temporary layoff percentage information from the Current Population Survey (CPS) maintained by the US Bureau of Labor Statistics.
- For each selected macroeconomic variable, we considered the following transformations:
    - logarithmic transformation
    - year-over-year (YoY) change
    - quarter-over-quarter (QoQ) change
    - 1 to 4 quarters lags

## 2.1. Unemployment rate decomposition and credit loss
- The CPS survey contains the percent of job losers on temporary layoff among total unemployed people.
- Therefore, we decompose the total unemployment rate into two components:
    - permanent unemployment rate
    - temporary unemployment rate
- URt = URt * (1 - TempRatiot) + URt * TempRatiot = PermURt + TempURt, where URt, PermURt, and TempURt represent total, permanent, and temporary UR, respectively, and TempRatiot stands for the temporary layoff ratio.
- We can then study the variance of the total unemployment rate through the temporary layoff and permanent layoff as follows:
    - VAR(URt) = COV(PermURt, URt) + COV(TempURt, URt)
    - 1 = COV(PermURt, URt) / VAR(URt) + COV(TempURt, URt) / VAR(URt)
- Where VAR and COV are variance and covariance operators.
- To understand the portion of total UR variance driven by permanent UR and temporary UR, one only needs to regress permanent UR and temporary UR on total UR, and draw inferences based on the slope coefficients.
- Therefore, we ran the regressions and found the permanent UR accounts for 75% of variance in the total UR while temporary UR only accounts for 25% of variance in the total UR.
- Total unemployment and credit card charge-off rates followed a similar trend over time and peaked in 2010 due to the GFC, indicating a relationship between unemployment and credit card loss.
- Although both permanent UR and temporary UR are closely related to losses, the spike of temporary UR generally reverts to the normal level faster than that of permanent UR.
- These results indicate that permanent layoffs are more persistent than temporary layoffs.
- There is a huge spike in the total unemployment rate in the year 2020 which is mainly driven by the surge in temporary layoffs during COVID-19.
- The correlations between total UR and its two components, permanent and temporary UR, are 88% and 55%, correspondingly.
- These results along with the prior variance analysis suggest that the permanent UR instead of temporary UR is highly correlated with total UR and can explain most of the variances in total UR.
- There is a decreasing trend in the temporary layoff ratio from the 1990s to the 2000s, and the average temporary layoff ratio over the sample period is around 13%.
- The temporary layoff ratio is procyclical and often peaked during recessions as evidenced by the spikes in 1990, 2000, and 2008.

## 2.2. Descriptive statistics
- The average credit card charge-off rate over the sample period is 4.67%.
- On average, the total unemployment rate, permanent unemployment rate, and temporary unemployment rate for our sample are 5.91%, 5.08%, and 0.83%, respectively.
- In addition, the average real GDP growth, real disposable income, CPI inflation, and 3-month treasury rate are 2.43, 2.88, 2.4, and 2.63%, respectively.
- The correlation coefficients of the total unemployment rate, permanent unemployment rate, and temporary unemployment rate, with credit charge-off rate, are 48%, 55%, and 2%, respectively.
- The smaller coefficient on the temporary unemployment rate provides the first piece of evidence that credit card loss is more likely to be driven by the permanent unemployment rate instead of the temporary unemployment rate.
- In addition, the correlation coefficient of permanent UR (55%) is larger than total UR (48%), suggesting that the permanent UR is even more closely related to credit loss than total UR.
- GPD, personal income, CPI inflation, stock market index, and house market indices are negatively related to the credit card charge-loss rate while labor market variables and market volatility index are positively related to loss.

# 3. Correlation analysis
- This section contains the results for correlation analysis.
- In order to capture the lag-lead relationship between the loss rate and macro variables, we also consider the following transformations for each macroeconomics variables:
    - logarithm transformation
    - year-over-year (YoY) change
    - quarter-over-quarter (QoQ) change
    - 1 to 4 quarters lags
- The YoY and QoQ changes are calculated by using the logarithm growth rate instead percentage growth rate because the logarithm growth is more symmetric and less skewed than the percentage growth rate.
- The initial macro variables list only contains 16 unique variables, while after transformation there are 266 variables in total.
- We conduct two types of correlation analysis:
    - correlation without optimal transformation
    - correlation with optimal transformation
- Results are reported as follows.

## 3.1. Correlation without optimal transformation
- We compute the correlation coefficient between the loss rate and each transformed variable.
- Then we ranked each transformed variable by the correlation coefficient in descending order and counted how many times a macroeconomic variable is being selected among the top 50 list.
- The more frequent a variable is being selected in the top 100 list, the more likely this variable is highly correlated with credit loss.
- We reported the ranking results by using three different correlation measures:
    - Pearson correlation coefficients
    - Spearman correlation coefficients
    - Kendall Tau correlation coefficients
- Pearson correlation coefficient is widely used in the research as it measures the linear relationship among variables.
- In contrast, the Spearman correlation coefficient can capture the nonlinearity relationship among variables.
- Finally, Kendall’s Tau rank correlation is a nonparametric measure and is not impacted by nonlinearities as Tau relies only on the concordance of UR and loss, it can capture the correlation in the direction of loss and UR movement.
- Among all the variables, permanent UR has the highest frequency of being selected across all the three correlation measures.
- Total permanent UR ranked the second, and temporary UR has a lower frequency than permanent UR, suggesting the permanent UR transformations are more likely to be correlated with credit loss than temporary UR.

## 3.2. Correlation with optimal transformation
- For each macro variable and its transformations, we only select one transformed value that has the highest correlation coefficient with credit loss.
- Then the transformed macroeconomic variables are ranked by correlation coefficient in descending order.
- This type of correlation analysis only allows each macro variable to show up once in the top list by using its optimal transformation.
- The advantage of this analysis is that we can observe the optimal transformation as well as the magnitude of the correlation for each variable.
- We then ranked the variables based the value of their correlation coefficients in descending order.
- The optimal transformations’ correlation coefficients and p-values are also reported.
- The results show that the YoY change of permanent UR with 3-month lag has the highest correlations with loss across all three different correlation measures while total unemployment only ranked after permanent UR.
- Temporary UR ranked 4th, 7th, and 7th in term of the Pearson, Spearman, and Kendall’s tau correlation coefficients, respectively.
- These findings highlighted the importance of labor market variables, especially permanent UR, as the key indicators to forecast credit card charge-offs and are consistently with prior studies.
- In addition, although the transformed temporary UR has a weaker correlation with credit loss when compared to permanent UR, it still shows a higher correlation than many other macros variables.
- We then explore whether permanent and temporary UR are predictive for credit loss when building a model.

# 4. Feature selection
- While the correlation analysis examines which variables are more likely to be correlated with credit loss, it does show us the relative importance among different variables when building a model.
- Hence, we examine the importance of each variable among the other macroeconomics variables by using feature selection algorithms.
- In order to maintain a broad perspective, we use four types of feature selection algorithms:
    - stepwise selection
    - gradient boot machine (GBM)
    - random forecast (RF)
    - Lasso regression
- The rationale behind using four different feature selection algorithms is to include both black-box (with less interpretability) and regression-based models with a high level of interpretability.
- GBM and RF are among black-box models which can capture complex nonlinear trends with higher forecasting accuracy.
- The traditional stepwise regression has high interpretability yet suffers typically poor predication accuracy.
- Finally, the lasso penalty regression has an advantage in that it cannot only improve forecasting accuracy but also can mitigate the overfitting issue.
- To avoid the abuse and misuse of machine learning algorithms as well as the circles of “Garbage in, Garbage out”, we further narrow down the number of variables used in the feature selection algorithm based on the results from the correlation analysis.
- We only include the top 10 transformed variables that have the highest Pearson correlation coefficients.
- In addition, we exclude the total unemployment, because we are interested in comparing the impact of the permanent UR and temporary UR in credit risk models.
- We use 0.2 as the threshold for inclusion in the final model.
- Temporary UR is not selected in any algorithms, indicating this variable has low explanatory power to credit loss when building a model.
- The variables selected by the stepwise and lasso regressions are quite similar with the permanent UR being the most important variable.
- The results again show that permanent UR is ranked as the 2nd highest in terms of variable importance.
- Overall, the results show a consistent pattern that permanent UR is a highly important variable selected by all variable selection methods.
- In contrast, temporary UR has low predictive power because it is not selected by any algorithm based on variable importance.

# 5. Regime-switching models
- The previous analysis shows that the correlation between UR and credit loss varies over time.
- In particular, the temporary layoff ratio usually increases during the recession, indicating the composition of temporary and permanent changes during the recession.
- Therefore, we use a regime-switching model to explore the shifts in the time series of the correlations between UR and credit loss.
- Our intent is to examine whether permanent or temporary UR is the main driver that affects the probability of switching from one regime to another; and whether the credit loss rate is more related to permanent UR during the crisis.
- Since we only use UR and credit loss data, the data period used in the regime-switching models can go back to 1985, which is the earliest date the credit card loss data is made available.
- Previous studies (e.g. Garcia and Perron, 1996; Gray, 1996; Bekaert et al., 2001; Ang and Bekaert, 2002) employed empirical models of regime switches for modeling interest rates.
- The application of the model has also been explored for the credit risk models (e.g. Andersson and Vanini, 2008; Liu et al., 2015).
- In our study, we first estimate the constant two-state regime-switching models given by the following equations:
    - ΔCLt = as0 + a1ΔCLt-1 + as2ΔURt + et
    - ΔCLt = as0 + a1ΔCLt-1 + as2ΔPermanent URt + as3ΔTemporary URt + μt
- in which ΔCLt, ΔURt, ΔPermanent URt, and ΔTemporary URt are the YoY change in credit loss rate, UR, permanent UR, and temporary UR, respectively.
- et and μt are the residuals for the two equations.
- The as0, a1, as2 and as3 are estimated coefficients.
- The superscript S on as0; as2; and as3 indicates regime zero or regime one, such that s can be regarded as an unobserved state variable that follows a two-state, first-order Markov process.
- The transition probability matrix is:
    - X = [[p, 1-p], [1-q, q]]
- in which p = Pr(st = 0 | st-1 = 0), and q = Pr(st = 1 | st-1 = 1).
- We refer to this model subsequently as the constant transition probability regime-switching model.
- In this model, we document the statistical association in co-movement between the two variables instead of investigating economic causality.
- The probabilities from the two models follow a similar trend: we find that the probability of switching regimes reaches its two highest peaks in 2011 and 2020, which closely matches the 2008 GFC and COVID-19 pandemic period.
- We estimate the relationship between the year-over-year change of UR and credit loss at the quarterly frequency from 1986 to 2020.
- We find strong evidence of regime-shifting behavior between the two regimes.
- The probabilities in the two states are large (p = 0:955; q = 0:045), which suggests that pattern in the regime zero is more persistent.
- Both of the estimated coefficients on UR in the two regimes are positive and significant at the one percent level.
- In the first regime (regime zero), we find that the estimated coefficient on UR (a02) is 0.90 while the magnitude of a12 coefficient (0.23) on UR is about four times smaller than those in the second regime (regime one).
- This implies a substantial contrast between the regimes.
- The correlation between UR and credit card charge-off rate is 0.52 in the first regime and it decreases to 0.36 in the second regime, which suggests that the correlation between UR and charge-off rate is higher in the non-recession period.
- We estimate the regime shifting model by replacing UR with permanent UR and temporary UR.
- Interestingly, the estimated coefficient on permanent UR is 0.77 in the first regime (regime zero) while it becomes larger (2.55) and statistically significant in the second regime (regime one).
- These results imply that permanent unemployment has a larger impact on credit loss during the recession period.
- The estimated coefficients of temporary UR are non-significant in the two regimes.
- These results imply that the larger correlation between UR changes and credit loss in the second regime is mainly driven by permanent UR.
- The results show a substantial contrast in the correlations between UR, permanent UR, and credit loss in the two regimes.
- First, the UR has large explanatory power to credit loss in regime zero (non-recession) and the relationship becomes weaker during the recession.
- In contrast, permanent UR exhibits a major role in driving the credit loss in regime one (recession).
- These results imply that the smaller coefficient of UR in regime one (recession) can be explained by the sharp increase in temporary UR during the recession.
- The temporary layoff ratio is relatively stable during non-recessions while experiencing a sharp increase during the beginning of each recession.
- This procyclical pattern implies that temporary layoff and permanent layoff are divergent during a recession.
- This divergence is mainly driven by a disproportionally increase in the temporary layoff ratio, which does not necessarily lead to a higher credit loss.
- Our findings also suggest that permanent UR can be a better indicator than total UR to use during a recession because it filters out the noise in the temporary UR.
- In addition, we also estimated an extended regime-switching model with time-varying transition probabilities (TVTP) and investigate whether the GDP, can affect the probability of switching regimes.
- The TVTP model estimates are comparable to those of the constant transition probability regime-switching model, and the results are similar to those shown in Table 6.

# 6. Results discussion and implications to credit risk modelling
- In summary, we first decompose UR into two components – permanent and temporary UR and find that the variance in total UR is primarily driven by permanent layoffs.
- In addition, permanent UR is more closely correlated to total UR than temporary UR as indicated by a higher correlation coefficient.
- We then examine how credit card charge-off rate is correlated with UR as well as its two components over time.
- Permanent UR along with its transformations exhibited a higher correlation with credit loss than that of temporary UR.
- We also test the predictive power of permanent and temporary UR on credit loss by using several feature selection algorithms.
- These results suggest permanent UR also has a dominant power in explaining the credit loss while temporary UR has little.
- Finally, we use a regime-switching model to test the relationship between UR and chargeoff rate under different economic states.
- The results indicate that total UR has a large explanatory power to credit loss in non-recessionary periods while the predictive power weakens during recessions.
- In contrast, permanent UR exhibits a stronger predictive power to credit loss in recessions.
- Temporary UR has little explanatory power to credit loss in both regimes.
- Our finding is consistent with business intuition.
- Because cardholders who are on temporary layoff can still fulfill their debt obligation as long as they return to work and do not turn into permanent layoffs.
- As a result, an increase in temporary layoff does not necessarily cause a higher credit card charge-off rate.
- Our finding also helps to explain the recent disconnection between total unemployment and credit charge-off under COVID-19, because the spike in UR is mainly driven by a surge in the temporary layoffs, which do not necessarily lead to a higher loss.
- Finally, our research also has implications for banks, macroeconomic forecast vendors, regulators, and policymakers.
- While our study highlights the importance of using permanent UR for credit risk modeling, there is a limitation of focusing only on macro-level data.
- There is future scope for using macro-level data to identify the exogenous variation in permanent and temporary unemployment, which would allow us to estimate and quantify the causal effect of unemployment on credit losses.

---

# Executive summary of 1. Introduction
- Credit risk management is important for financial institutions, especially after the 2008-2009 GFC.
- Macroeconomic factors, particularly the unemployment rate (UR), are key drivers of credit risk and are used in loss forecasting.
- The COVID-19 pandemic caused volatility in macroeconomic variables, especially temporary layoffs, leading to a disconnection between total UR and credit card losses.
- **Research Question**: Does temporary or permanent unemployment drive credit card losses? This question is important because of the divergence between temporary and permanent layoffs during COVID-19.
- The paper examines how total, permanent, and temporary unemployment affect credit card charge-off rates.

# Executive summary of 2. Data and methodology
- The study uses quarterly data from Q1 1990 to Q4 2020, covering multiple business cycles.
- Data includes credit card charge-off rates and 14 domestic macroeconomic variables from the Federal Reserve.
- Temporary layoff data is obtained from the Bureau of Labor Statistics.
- We decompose total unemployment into permanent and temporary components using the temporary layoff ratio.
- The variance of total unemployment is analyzed to determine the contribution of permanent and temporary layoffs.
- Descriptive statistics and correlations of variables are also reported.

# Executive summary of 3. Correlation analysis
- The analysis examines the correlation between credit card charge-off rates and various macroeconomic variables, including permanent and temporary unemployment.
- Two types of correlation analysis are conducted:
    - Without optimal transformation: Examines the frequency with which each variable appears in the top 50 correlations.
    - With optimal transformation: Selects the transformation with the highest correlation for each variable.
- Permanent unemployment shows a higher correlation with credit card losses than temporary unemployment.

# Executive summary of 4. Feature selection
- The study uses feature selection algorithms to determine the importance of each variable in predicting credit loss.
- Algorithms used include stepwise selection, gradient boosting machine (GBM), random forest (RF), and Lasso regression.
- Permanent unemployment is consistently selected as an important variable, while temporary unemployment is not.

# Executive summary of 5. Regime-switching models
- The study employs regime-switching models to analyze how the relationship between unemployment and credit losses changes across different economic states (recession vs. non-recession).
- Total unemployment is a strong predictor of credit losses during non-recessionary periods, but its predictive power weakens during recessions.
- Permanent unemployment exhibits stronger predictive power during recessions.
- Temporary unemployment has little explanatory power in either regime.

# Executive summary of 6. Results discussion and implications to credit risk modelling
- Permanent unemployment is a more reliable indicator of credit card losses, especially during recessions, as it filters out the noise from temporary layoffs.
- The findings can improve credit risk modeling and loss forecasting for banks, regulators, and policymakers, especially during economic uncertainty like the COVID-19 pandemic.
- Future research could explore the causal effect of unemployment on credit losses using macro-level data.

</details>

# Gautam Pant
- Professor of Business Administration
### education
- Ph.D., Business Administration, University of Iowa, 2004
- M.S., Computer Science, Baylor University, 1999
- B.E., Computer Engineering, University of Mumbai, 1996
### research interest (chatgpt says...)
- Machine Learning for Biomedical Decision Making 
- AI-Human Collaboration and Algorithmic Decision Systems
- Drug Response Modeling and Combination Therapies
### teaching
- Data Science and Analytics (BADM 356), statistical inference, linear modeling, sentiment analytics, and data mining in R
- Machine Learning in Bus Res (BADM 590) Special topics in the general area of business

```
title: Pathways for Design Research on Artificial Intelligence
authors: Ahmed Abbasi, Jeffrey Parsons, Gautam Pant, Olivia R. Liu Sheng, Suprateek Sarker
journal: Information Systems Research (ISR)
published: 2024
```
 
# Executive Summary
- This editorial addresses the challenges of publishing design research on AI in IS journals, acknowledging the rapid evolution of AI and the need for clear guidelines.
- Six key **impediments** are identified:
    - *I1*. **Moving target:** IS publication speed lags behind rapid AI advancements, leading to outdated benchmarks.
    - *I2*. **Incommensurable perspectives:** Disconnect between conceptual DSR and technical artifact development hinders shared evaluation standards.
    - *I3*. **Idiosyncratic instantiation:** Lack of domain adaptation beyond simple application diminishes design contributions and generalizability.
    - *I4*. **Prediction dilemma:** Difficulty convincing reviewers of the practical value and IS connection of predictive/prescriptive artifacts.
    - *I5*. **Computation over representation:** Skewed focus towards computational aspects undervalues representational modeling research.
    - *I6*. **Generative artifact divide:** Limited access to compute resources and lack of objective evaluation metrics hinder generative AI research.
- The **LLM explosion** is used as a motivating example to highlight how these impediments are amplified in cutting-edge AI areas.
- A **pathways framework** is proposed, comprising:
    - **Abstraction spectrum:** A continuum of contribution types, ranging from emergent design insights to formal design theory.
        - **Emergent Design Insights:** Grounded in data science and analytics. *Example*: Lee et al.’s [2024] concept mining tool mapping and desiderata. Goal is to establish valuable, generalizable insights with strong methodology.
        - **Novel Problem Design Formulations:** Defining and framing new problems requiring IS attention. *Example*: Macha et al.’s [2024] personalized data obfuscation based on consumer risk/advertiser utility. Goal is to highlight the significance of the problem space itself as a key contribution.
        - **Theory-Guided Design:** Using established theories to inform artifact design and functionality. *Example*: Lee and Ram’s [2024] using argumentation theory for false claim detection. Goal is to ground artifact design in theoretical foundations.
        - **Design Principles:** Deriving and articulating generalizable principles for artifact design. *Example*: Lukyanenko et al. [2019] deriving principles relating information quality to user-generated content. Goal is to extract reusable lessons for future design efforts.
        - **Formal Design Theory:** Developing and testing formal theories about artifact design and its effects. *Example*: Li et al.’s [2020] design theory for behavioral ontology learning. Goal is to offer a comprehensive and testable understanding of the artifact's role and impact.
    - **Artifact typology:** Categorizing AI design artifacts into five distinct types, each with distinct evaluation norms.
        - **Knowledge Models:** Conceptual models, ontologies, and representation frameworks (e.g., Etudo and Yoon’s terror beliefs ontology [2024]). Evaluated based on completeness, consistency, and usefulness for knowledge representation.
        - **Data Management Models:** Frameworks and systems for managing data quality, access, and governance. Evaluated based on data integrity, efficiency, and compliance.
        - **Prescriptive Models:** Models recommending actions to optimize desired outcomes (e.g., Macha et al.’s personalized obfuscation [2024]). Evaluated based on optimality of recommendations, feasibility, and ethical considerations.
        - **Predictive Models:** Models predicting future events or outcomes (e.g., Liu et al.’s employee migration prediction [2020]). Evaluated based on predictive accuracy, robustness, and interpretability.
        - **Generative Models:** Models creating new content, such as text, images, or audio. Evaluated based on quality, creativity, and relevance of generated content.
    - **Desirable traits:** Emphasizing rich domain adaptation, socio-technical alignment, and engagement with IS discourse.
        - **Rich Domain Adaptation:** Tailoring artifacts to specific contexts and demonstrating nuanced findings. Goal is to go beyond applying standard methods and providing insights that are particularly relevant to the application context.
        - **Socio-Technical Alignment:** Connecting artifacts to social, organizational, and ethical considerations. Goal is to ensure the artifact contributes to broader societal goals.
        - **Engagement with IS Discourse:** Drawing upon existing IS literature and building a cumulative tradition. Goal is to ensure the artifacts have a IS signature.
- The framework helps researchers overcome impediments by:
    - Addressing *I4* (prediction dilemma), Lee and Ram [2024] used argumentation theory and user experiments to improve trust in false claim detection.
    - Addressing *I5* (computation over representation), Etudo and Yoon [2024] reframed ontology development as a prerequisite for ML.
- By adopting these pathways, IS researchers can produce impactful and relevant design research in the age of AI.

#design_research #artificial_intelligence #information_systems_research #pathways #design_artifacts #large_language_models #sociotechnical_axis #abstraction_spectrum #artifact_typology #domain_adaptation

 
<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The confluence of data abundance, computational power, human adaptation, and uncertain impacts of transformative information technologies (ITs), especially **artificial intelligence (AI)**, is creating new sociotechnical phenomena.
- IS research is engaging with AI in various ways, including understanding its adoption, use, and consequences [Lebovitz et al. 2021, Bauer et al. 2023, Rhue 2023, Susarla et al. 2023].
- However, a smaller body of IS research adopts a **design perspective** on AI, prescribing solutions via design artifacts, insights, guidelines, principles, and/or theories.
- Given the nascent state of design research, especially regarding AI systems, confusion exists about expectations for IS design research manuscripts.
- This editorial addresses challenges in publishing design research related to AI in IS journals and examines viable pathways to make such work more acceptable.
- Design research is essential to the socio-technical character of IS, serving as the discipline’s “axis of cohesion” [Sarker et al. 2019].
- Prior IS design research has laid the foundation for future IS research [Rai 2018].
- At ISR, design-oriented editors and submitted papers have increased, and publications with ML and artifact development have risen [Gupta 2022, Tarafdar et al. 2022].
- Despite positive outcomes, the proportion of design-oriented publications remains low [Gupta 2018, Sarker et al. 2019, Tarafdar et al. 2022].
- Tremblay et al. [2018] found an average of 11 peer-reviewed design science research (DSR) publications per year across the Association for Information Systems basket of eight journals from 2004 to 2017.
- The design perspective is crucial for IS thought leadership on AI-related sociotechnical challenges and opportunities [Bailey and Barley 2020, Benbya et al. 2021, Berente et al. 2021, Abbasi et al. 2023].
- The editorial has three main objectives:
    1. Shed light on impediments to publication success of IS design research on AI.
    2. Promote awareness of design-oriented research in IS such that it has a unique signature and contributes to a cumulative tradition [Abbasi et al. 2016, Sarker et al. 2019].
    3. Show potential pathways for design-oriented AI research to navigate impediments in the review process.
- The discussion is motivated by major trends in AI and their interaction with the design research landscape.
- AI trends include the fast pace of change in ML, advancements in general-purpose foundation models, and enhanced generative capabilities.
- Design researchers must navigate a fragmented landscape with varying perspectives and limited discussion of how guidelines relate to specific artifact types.
- The specific examples focus primarily on five types of artifacts: knowledge, data management, prescriptive, predictive, and generative models.
- These are aligned with the IS data science and representation, optimization, and computational design genres [Agarwal and Dhar 2014, Rai 2017b, Gupta 2018].
- These artifact types are directly aligned with and affected by AI trends.
- They constitute the most common categories of design-oriented submissions at ISR.
- These artifact types are within the expertise of the authors.
- With the growing impetus on learning representations and the management of machine learning and AI [Berente et al. 2021], research interest and submissions in these areas are likely to grow.
 
# 2. Design Research in IS—Some Impediments
## 2.1. The “Igon Value Problem” and the Important Role of Design Perspectives on AI
- The “**Igon Value Problem**” refers to discussing technical/scientific concepts without the necessary technical grounding, resulting in banal or incorrect generalizations [Pinker 2009].
- Emerging IS research on AI produces theoretical work on responsible AI tenets but sometimes lacks depth in understanding the technical phenomena.
- For example, AI is routinely referred to as "the AI," as if it is a monolithic entity.
- The term “**GenAI**” is commonly misused, mistakenly referring to AI-enabled technologies that are not actually generating anything in the focal context being studied.
- Such issues can negatively impact the quality and distinctiveness of theories/knowledge produced in IS.
- Welcoming and integrating design perspectives on AI into the conversation is crucial to ensure an IS cumulative tradition rooted in appropriate technical foundations.
- IS design research has a long and rich history [Nunamaker and Whinston 1972].
- The tradition has drawn upon Simon’s ideas on the science of design [Simon 1988, 1996] and incorporated its extensions to IT artifacts [March and Smith 1995], pioneering work on IS design theories [Walls et al. 1992], conceptual modeling [Storey 1993, Wand et al. 1995], expert systems [Aiken et al. 1991, Chang and Woo 1994], data quality [Wang et al. 1995], data mining [Dhar and Tuzhulin 1993], systems development [Nunamaker et al. 1990], and information retrieval [Chen and Dhar 1991].
- DSR proliferated over the past 20 years [Baskerville 2008, Tremblay et al. 2018, Leroy et al. 2023], including guidelines for authors [Hevner et al. 2004, Gregor and Hevner 2013], methodologies [Peffers et al. 2007], and articles categorizing DSR into genres [Baskerville et al. 2015, Rai 2017b].
- Others have highlighted design-oriented opportunities pertaining to IS data science, analytics, and machine learning [Shmueli and Koppius 2011, Chen et al. 2012, Agarwal and Dhar 2014, Padmanabhan et al. 2022, Samtani et al. 2023].
- Despite increased attention to design research, authors encounter roadblocks to publishing AI-focused design research in IS.
 
## 2.2. Impediments to Design-Oriented Research
- A key set of impediments to publishing AI-focused design research in IS was collated from audience member questions from editors’ panels and development workshops held at the INFORMS Information Systems Society (ISS) and College on AI (CAI).
- Keynotes and panels at the Workshop on Information Technologies and Systems (WITS) and International Conference on Design Science Research on Information Technologies and Systems (DESRIST) were also considered.
- This information was supplemented with the authors’ editorial experiences.
- At ISR, the editors have handled approximately 300 manuscripts and have accepted over 30 papers.
- The list of impediments is an amalgamation of collective experiences, illustrative rather than exhaustive, and may reflect personal opinions as authors, mentors, reviewers, and editors.
- Table 1 presents six impediments along with illustrative reviewer demands and scenarios.
- Commentary is included on the implicit tensions in the author-reviewer-editor triangle.
- The comments highlight unreasonable demands made in the peer review process, presenting roadblocks for IS design research submissions related to AI.
- Some common issues that all manuscripts face exist [Rai 2017a].
- The intentionally selected comments underscore unique aspects of IS design research: assumptions about the (non-)stationarity of the artifact and its field deployment, the balance between design knowledge and novelty/rigor, and expectations and stopping rules on evaluation and benchmarking.
- Advancements in large language models (LLMs) show how the increased emphasis on artifacts involving ML and AI further amplifies these challenges for authors.
- A pathways framework for design-oriented AI research is presented in Section 3.
- Section 4 discusses how understanding the pathways can help address these impediments.
- The identified impediments are complex and cannot be made to disappear overnight; they require awareness and scholarly discussion and debate.
 
### 2.2.1. I1. Moving Target: The Velocity of the State of the Art
- The **publication velocity and volume** contrast between IS and its reference disciplines is pronounced for design research.
- This makes authors susceptible to a never-ending cycle of evaluating emergent benchmarks and justifying the relative operational utility, novelty, and contributions of their work.
- Reviewers tend to use “**query-based reviewing**”—querying academic databases to identify new related articles as evidence of limited contribution.
- Authors fall into the trap of framing their work using the **common task framework** [Donoho 2017, Abbasi et al. 2023] that focuses on performance metric outcomes on well-known data sets.
 
### 2.2.2. I2. Incommensurable Perspectives: The Conceptual vs. Algorithmic Divides
- Research on AI, design, and data science is **bimodal**.
- This creates fragmented bodies of knowledge nested within the same journals and field/discipline but almost always orthogonal and never interacting.
- A similar bimodality manifests itself in differences between conceptual DSR methods and those actually developing artifacts [Tremblay et al. 2018, Tarafdar et al. 2022].
- The "to field or not to field" question is tricky when simultaneously faced with artifact revamp and field evaluation questions.
 
### 2.2.3. I3. Idiosyncratic Instantiation: Confounding Domain Application with Adaptation and Balancing Context and Generalization
- Two related tensions exist.
- First, submissions might focus more on **domain application** and less on **domain adaptation**.
- In the absence of proper adaptation, the evaluation and contributions are susceptible to questions such as “What’s new here?”
- Second, as contextualization is often considered opposed to generalization [Vom Brocke et al. 2020], domain adaptations receive scrutiny from critical evaluators related to applicability to other contexts.
- Questions of relevance to IS versus specialty journals commonly arise during editorial screening.
 
### 2.2.4. I4. Prediction Dilemma: The Impediments for Predictive and Prescriptive Design Research
- Despite growing interest [Shmueli and Koppius 2011, Yarkoni and Westfall 2017, Hofman et al. 2021], predictive and prescriptive artifact contributions in top IS journals remain few.
- The key dilemma that authors face can be summed up using the classical Hume’s paradox of induction [Hume 1777], which questions the efficacy of predictions induced from prior knowledge.
- Authors and reviewers must grapple with tensions attributable to user perceptions ranging from tautological and trivial versus counterintuitive, nontrivial, and valuable.
- Assumptions of what is deemed novel, interesting, and practically valuable with respect to key predictors, prediction tasks, and predictive power often differ across scholars.
 
### 2.2.5. I5. Computation over Representation: Representational Models in the Tsunami of Machine Learning
- With the latest wave of technological advancements in AI, the importance of **representational modeling** has grown.
- IS representational modeling research is needed to understand, formalize, and theorize about the role of knowledge in the construction of learning representations and architectures for modern AI models, systems, and governance frameworks [Lukyanenko et al. 2019, Kaur et al. 2022].
- Authors of representational modeling papers face two key challenges:
    - Design expertise in the peer review process has tended to skew toward computational topics/methods.
    - A large body of representation research has focused on conceptual modeling of the physical world [Recker et al. 2021].
 
### 2.2.6. I6. Generative Artifact Divide: Design in the Age of Access and Compute Disparities
- Until 10 years ago, generative models were mostly constrained to structured data types [Donoho 2017].
- Two big breakthroughs happened in the past decade that have made generative models much more salient, versatile, and mainstream:
    - The development of generative adversarial networks capable of generating realistic images and videos [Goodfellow et al. 2014, 2020].
    - The growth of LLMs.
- IS design research related to generative models must grapple with **accessibility and compute divides** [Luitse and Denkena 2021].
- How to evaluate the quality of modern generative artifacts is nontrivial.
 
## 2.3. Using the Large AI Model Literature as a Motivating Example
- The large AI model literature is used to illustrate the aforementioned impediments and to underscore the specific opportunities and general need for greater IS design research guidance in the era of AI.
- Modern AI is powered by ML models.
- In general, the ability of a model is affected by three elements: the algorithm it uses, the data it is trained on, and the quantity of parameters it uses.
- Over the 10-year period from 2013 to 2023, exponential growth occurred in the amount of data and number of parameters used in modern AI models [Sastry 2018, Ananthaswamy 2023].
- The LLM explosion over this 10-year period culminated with the introduction of disruptive models, such as ChatGPT, BARD, Minerva, and LLaMA.
- Although LLMs represent one sliver of the AI space, they are important for IS design research.
- IS has a long tradition of research at the intersection of humans, organizations, language, and communication [e.g., Lyytinen 1985, Storey 1993, Chang and Woo 1994].
- Furthermore, six of the INFORMS ISS Design Science Award winners over the past 15 years have been text artifacts.
- Figure 2 shows the 10-year journey for LLMs, depicting three major periods of advancement:
    - **Static word embeddings (2013–2015)**, such as word2vec, GloVe, and FastText [Mikolov et al. 2013, Pennington et al. 2014, Bojanowski et al. 2017].
    - **Contextualized word embeddings (2016–2018)** with models such as ELMo [Peters et al. 2018].
    - **Transformer models** capable of attending to larger windows of text (e.g., BERT, RoBERTa, GPT, LLaMA-2, and Gemini) [Devlin et al. 2018, Liu et al. 2019, Brown et al. 2020, Touvron et al. 2023].
- These are commonly referred to as **foundation models**.
- Table 2 uses the LLM explosion to illustrate the six aforementioned impediments.
 
# 3. A Pathways Framework for Design-Oriented AI Research
- The aforementioned impediments are synthesized into a **pathways framework** for taking a design research perspective on AI research in IS (Figure 3).
- The goal of the framework is to illustrate potential combinations of contributions and positioning that embrace the plurality of artifacts and abstractions applicable to design research on AI and provide guidance for authors, reviewers, and editors.
- The aim is to foster a new wave of design perspectives on AI that are innovative and forward looking while maintaining the distinctiveness of AI-related design research in IS.
- As shown in Figure 3, beginning with an artifact type, researchers should consider the norms for those artifacts as well as how and why their work is well suited for IS.
- Authors should also consider at least one form of abstraction.
- The artifact typology includes representation, computational, prescriptive, and generative artifacts.
- Consideration of the sociotechnical axis is intended to foster a distinctive IS approach and a cumulative tradition.
- Abstraction attempts to tackle the moving target problem, whereas the abstraction spectrum itself bridges disparate perspectives on design contributions.
- The hope is to overcome fragmented conversations through a shared understanding of abstractions that balance inclusion and shelf-life concerns, while maintaining rigor through a compensatory perspective.
- Given major advancements in general-purpose AI through foundation models, design artifacts should consider going “the last research mile” [Nunamaker et al. 2015] through rich domain adaptation.
- The remainder of this section describes the abstraction spectrum, artifact typology, and desirable traits for IS design-oriented research.
- These components of the framework relate to necessary (but insufficient) characteristics for design research in IS.
- Nine articles are used to highlight how framework components may manifest in design research oriented toward AI.
 
## 3.1. An Abstraction Spectrum
- Effective abstraction is key to ensuring a healthy shelf life of IS design research papers in the high-velocity era of AI.
- The DSR guidelines and methods body of literature has provided suggestions on genres, positioning, and types of abstractions.
- Adding the IS data science community perspective, there is a need for a more inclusive abstraction spectrum.
- DSR design abstractions include the use of theory-guided design, the use or derivation of design principles, and formal IS design theories [Walls et al. 1992].
- An important aspect of the framework is to extend the DSR perspective of abstraction with the IS data science and analytics perspective, where the design contributions to knowledge might be more emergent.
- In the era of AI, a plethora of new problem spaces in need of discovery and formulation are anticipated [Gregor and Hevner 2013].
- In such situations, the IS data science perspective will have an important part to play.
- Examples of abstraction types pertaining to IS data science include salient design-related insights or novel problem design formulations.
- For instance, in building their Guided Diverse Concept Miner tool, Lee et al. [2024] proposed a two-dimensional mapping of concept mining artifacts based on levels of managerial domain knowledge and guidance provided to the tool.
- Similarly, Liu et al. [2020] showed that their proposed human capital overlap metrics were predictive of future employee migration patterns.
- With respect to novel problem designs, Macha et al. [2024] proposed a new problem formulation for personalized data obfuscation.
- Zhang and Xu [2024] formulated a new fairness assessment problem in catastrophe insurance using concepts from machine learning.
- In regard to theory-guided design abstractions, Lee and Ram [2024] used argumentation theory and structural balance theory as a metaphor to design 12 features capable of enhancing false claim detection capabilities for transformer-based LLMs.
- Lukyanenko et al. [2019] derived important design principles from propositions pertaining to the interplay between class- versus instance-based IS and consumer- versus contributor-centric information quality.
- Li et al. [2020] extended the ontology learning from text literature to propose an IS design theory for behavioral ontology learning from text.
- The abstraction spectrum ranging from emergent design insights to formal design theory is analogous to the computationally intensive theory-building continuum in Miranda et al. [2022].
- An important goal of having the abstraction spectrum is to acknowledge the multifaceted nature and type of abstractions and to suggest a compensatory approach to assessing contributions.
- For instance, editors evaluating research with salient design insights or new design problems may consider using the “three ‘I’ principles” of interestingness, impact, and integration.
 
## 3.2. Artifact Typology
- The artifact typology focuses on five types of artifacts closely related to design-oriented AI: knowledge, data management, prescriptive, predictive, and generative models.
- Most existing guidelines for conducting and evaluating design research fail to consider the heterogeneous and nuanced nature of different types of design artifacts [Rai 2017a, b].
- The purpose of the artifact typology is to create a mindfulness for artifact type-specific differences and develop pathways for author-reviewer-editor triads that are conditioned on these differences.
- **Knowledge models** include conceptual models, representation theoretic perspectives, formal and lightweight ontologies, and other forms of knowledge capture and representation [Li et al. 2020, Chua et al. 2022].
- Related to design perspectives on AI, Chua et al. [2022] noted different ways to conceptualize requirements for data consumption use cases involving data science and big data artifacts.
- Etudo and Yoon [2024] developed a terror beliefs ontology along with a system to automatically populate it.
- Prior IS work has also contributed extensively to understanding **data quality and data management** [Storey 1993, Wang et al. 1995, Burton-Jones et al. 2005].
- More recently, Chua et al. [2022] introduced the 5Cs of conceptualize, collect, curate, consume, and control.
- Lukyanenko et al. [2019] examined the implications of instance-based and class-based data collection for key information quality dimensions.
- The computational and optimization-related part of the artifact typology spans predictive, prescriptive, and generative artifacts.
- **Predictive research** and **prescriptive research** are cornerstones of the sciences of the artificial and the “science of design” [Simon 1988, 1996], and they are central to the artifact typology for IS design research [Shmueli and Koppius 2011; Rai 2017a, b].
- Several recent studies have proposed predictive [Li et al. 2020, Lee and Ram 2024] and prescriptive models.
- For instance, Liu et al. [2020] predict employee migrations across big tech companies.
- Yang et al. [2023] detect Big Five personality traits from user-generated text.
- In the case of prescriptive artifacts, Macha et al. [2024] propose a prescriptive model-based personalized obfuscation scheme, and Zhang and Xu [2024] propose a conceptual framework for assessing the fairness of risk-load calculation in catastrophe insurance.
- Suarez et al. [2024] design a decision support artifact for wildfire management.
- Zhang et al. [2024] propose a model that combines predictive and prescriptive methods for future resource planning and allocation.
- Authors (and review teams) should be mindful of the norms for different types of design artifacts and should position their work accordingly.
 
## 3.3. Desirable Traits—Rich Domain Adaptation and Axis Alignment
- In an era where many disciplines are engaged in some form of AI research, the need and possibilities for IS design research to connect with the sociotechnical axis of cohesion [Sarker et al. 2019] have never been more salient.
- AI-enabled systems are swapping human intelligence for parameters; hence, when applied to organizational phenomena embedded in a specific context, they cannot reason effectively.
- Therefore, domain adaptation, problem contextualization, cognitive principles, and pairing with human knowledge, including theories of language and communication, ontologies, and lexicons, remain opportunities.
- Ideally, the field would see more papers that are integrative with respect to their sociotechnical connection [Sarker et al. 2019].
- For instance, Etudo and Yoon [2024] used collective action framing theory and social movement ideology to guide the design of their deep learning artifact.
- Liu et al. [2020] use the resource-based view of the firm to guide the design of their tech labor market competition predictive model.
- Lukyanenko et al. [2019] used the information quality literature to form a set of theoretical propositions about how users would respond to different system configurations for crowdsourced data collection.
- Another opportunity related to the sociotechnical axis of cohesion is the nature of outcomes.
- Sarker et al. [2019] observed that a very small proportion of published studies examined included humanistic goals.
- Zhang and Xu [2024] examined the fairness of rate making in catastrophe insurance and found disparate impact against minorities.
- The ability of AI systems to simultaneously engender biases and help uncover them has important design implications [Caliskan et al. 2017; Garg et al. 2018; Guo et al. 2022; Lalor et al. 2022, 2024].
- Going forward, design research needs to actively consider both humanistic and instrumental outcomes.
- Another important goal for IS design research is to be part of the disciplinary discourse without being limited by it.
- For instance, Yang et al. [2023] draws upon work related to the role of personality in technology use and prior text artifact design studies.
 
## 3.4. Design Research—Necessary Characteristics
- Design research requires contributions that are novel in designing new constructs, models, methods, or instantiations or where there is novelty in the adaptation to a domain.
- The Gregor and Hevner [2013] quadrants highlight three different ways in which novelty in artifact design and/or its application can meet requirements for a design contribution: improvement, exaptation, and invention.
- Novelty should be a necessary condition for publication, even though the extent of novelty required may differ based on the artifact and abstraction.
- Another important consideration is the depth of downstream implications in the form of a field deployment or practical impact.
- A good practice is to understand the possible impact of a design artifact: that is, recognizing and explicating the unit of impact in terms of how the research may inform strategy or policy.
- IS aspires to provide broader thought leadership in various technology-enabled contexts, making downstream impact an important consideration.
 
# 4. How Our Discussion on the Pathways Can Alleviate Common Roadblocks
- This section provides broad guidelines for authors, reviewers, and editors based on the pathways.
- It also discusses how example papers navigated these impediments.
 
## 4.1. Mindfulness Regarding Artifact Typology
### 4.1.1. Guidelines for Authors, Reviewers, and Editors
- From the onset of a design-oriented AI research project, authors should be mindful that artifacts geared toward data and representation are different from ones focusing on computation and optimization.
- Authors should familiarize themselves with the literature, norms, and unique impediments associated with each artifact type.
- Review teams should be mindful not to privilege computation over representation (or vice versa).
- Editors will need to exercise discretion when working with authors and reviewers to circumvent the generative artifact divide.
 
### 4.1.2. Example: Combatting the “Prediction Dilemma” with a Sociotechnical Perspective
- In assessing a predictive artifact to detect false information claims in text appearing in online settings [Lee and Ram 2024], the review team raised concerns about the contribution to the IS cumulative tradition.
- The authors incorporated argumentation theory as part of their theory-guided design and showed how it relates to existing IS NLP literature.
- They addressed the second concern by conducting a user experiment, which showed that users assessing the truth of online claims had greater trust and satisfaction with ML explanations when using their proposed artifact relative to general-purpose LLMs.
 
### 4.1.3. Example: Mitigating “Computation over Representation” with the Artifact Typology
- When developing their belief ontology, Etudo and Yoon [2024] had to weather the review team’s concerns related to the lack of novelty from a pure computational design perspective.
- They did so by arguing that large-scale collection of ideology knowledge was a prerequisite, without which no ML-based detection systems could exist.
- With appropriate mindfulness from the editors, this allowed their work to be assessed through the lens of knowledge models as opposed to computational design.
 
## 4.2. Abstraction Along a Spectrum
### 4.2.1. Guidelines for Authors, Reviewers, and Editors
- Once authors have thought carefully about the type of artifact, the next important step in the research development process relates to “abstract thinking” by employing an appropriate level of abstraction, which will allow for better transferability across contexts and longer shelf life.
- Authors should explicate how the novelty, uniqueness, and/or depth of domain adaptation are greater to justify abstractions farther to the left (in Figure 3).
- Reviewers should be aware that papers cannot exhaustively address every possible benchmark under the sun or include every applicable form of evaluation.
- Editors will need to apply their judgment and discretion regarding the exact nature and extent of “compensatory calculus” and appropriate stopping criteria.
 
### 4.2.2. Example: Tackling a “Moving Target” with Abstraction
- Earlier, we described how Li et al. [2020] proposed a design theory for behavioral ontology learning from text.
- This allowed them to circumvent reviewer questions about how their TheoryOn instantiation related to the latest advancements in deep learning for text classification.
 
### 4.2.3. Example: Overcoming “Incommensurable Perspectives” via Abstraction Along a Spectrum
- When proposing their Guided Diverse Concept Miner, Lee et al. [2024] were presented with new requests to differentiate and benchmark their instantiation against the latest advancements in neural topic models.
- They developed a mapping of existing work related to their class of artifacts using a 2 × 2 of managerial guidance and managerial knowledge.
- They also included a flowchart to scope when their tool could work better than others and employed ideas from cognitive science and philosophy to derive three desiderata for their scoping.
- In the absence of design science formalism, their emergent design insight-related abstraction created a common understanding between the authors, reviewers, and editors.
 
## 4.3. Creating an IS Signature Through Desirable Traits
### 4.3.1. Guidelines for Authors, Reviewers, and Editors
- Authors can think of the artifact typology and abstraction spectrum as points A and B.
- First, in the age of the common task framework-driven foundation models, rich domain adaptation is crucial.
- Second, authors taking a design-oriented perspective on AI have an opportunity to delve deeper into relevant sociotechnical connections for their work.
- Finally, consideration of these desirable traits, artifact typology, and abstraction spectrum should not compromise the necessary but insufficient conditions of novelty, rigor, and downstream implications.
- Reviewers should consider the richness and texture afforded by domain adaption.
- Editors should adjudicate competing generalizability and depth requests from reviewers.
 
### 4.3.2. Alleviating “Idiosyncratic Instantiation” Through Richness of Domain Adaptation
- When using catastrophe insurance as the focal domain for their study, Zhang and Xu [2024] circumvented issues related to the generalizability of their contributions by (1) providing a deep exposition of their focal application domain; (2) drawing interesting parallels between their application domain and ML work on fair data valuation; and (3) connecting their domain-specific findings to IT problem settings involving collective risks and individual actions.
 
# 5. Some Pathway Examples of Design-Oriented AI Research
- Figure 4 depicts pathways for nine of the papers mentioned extensively in the previous sections.
- These pathways are meant to be illustrative, not exhaustive.
- Table 3 highlights two of the articles in Figure 4, focusing on how their pathways encompass elements of the abstraction spectrum, artifact typology, desirable traits, and necessary characteristics.
 
# 6. Implications and Concluding Remarks
- The explosion of interest in AI among researchers and practitioners has led to a variety of opportunities for IS scholars to contribute.
- This editorial highlights common impediments to publishing design research in this area and suggests various pathways that can potentially aid authors.
- It reinforces the fact that not all AI design research neatly fits into one template.
- Design-oriented research on AI holds tremendous promise for the IS discipline.
 
---
 
# Executive summary of 1. Introduction
- This section sets the stage by highlighting the increasing relevance of AI in information systems research and the unique challenges faced by design-oriented research in this field.
- It emphasizes the importance of design research for the IS discipline and the need to address the impediments hindering its publication success.
- The introduction outlines the objectives of the editorial, which include identifying impediments, promoting awareness of design-oriented research, and suggesting pathways for navigating the review process.
- The section also touches on the interaction between AI trends and the design research landscape, focusing on specific artifact types and their significance.
 
# Executive summary of 2. Design Research in IS—Some Impediments
- This section delves into the challenges and impediments encountered in design research related to AI.
- It discusses the "Igon Value Problem," which refers to a lack of technical grounding in AI discussions, and the importance of design perspectives in ensuring a cumulative IS tradition.
- The section identifies and elaborates on six major impediments to design-oriented research, including:
    - **Moving Target:** The velocity of the state of the art.
    - **Incommensurable Perspectives:** The conceptual vs. algorithmic divides.
    - **Idiosyncratic Instantiation:** Confounding domain application with adaptation.
    - **Prediction Dilemma:** Impediments for predictive and prescriptive design research.
    - **Computation over Representation:** Representational models in the tsunami of machine learning.
    - **Generative Artifact Divide:** Design in the age of access and compute disparities.
- The section uses the large language model (LLM) literature as a motivating example to illustrate these impediments and underscore the need for IS design research guidance in the era of AI.
 
# Executive summary of 3. A Pathways Framework for Design-Oriented AI Research
- This section introduces a pathways framework designed to address the impediments to design-oriented AI research in IS.
- The framework comprises three main components:
    - **Abstraction Spectrum:** Ranging from emergent design insights to formal design theory.
    - **Artifact Typology:** Including knowledge, data management, prescriptive, predictive, and generative models.
    - **Desirable Traits:** Emphasizing rich domain adaptation and alignment with the socio-technical axis.
- The section elaborates on each component, providing examples and explaining how they contribute to overcoming the identified impediments.
- It emphasizes the importance of considering artifact-specific norms, employing appropriate levels of abstraction, and creating an IS signature through desirable traits.
- The section also highlights the necessary characteristics of design research, such as novelty, rigor, and downstream implications.
 
# Executive summary of 4. How Our Discussion on the Pathways Can Alleviate Common Roadblocks
- This section provides practical guidelines for authors, reviewers, and editors on how to utilize the pathways framework to address common roadblocks in design-oriented AI research.
- It emphasizes the importance of mindfulness regarding artifact typology, abstraction along a spectrum, and creating an IS signature through desirable traits.
- The section includes examples of how specific papers have navigated these impediments by applying the principles of the pathways framework.
- The guidelines aim to promote a better understanding of the nuances of design-oriented AI research and facilitate a more constructive and effective review process.
 
# Executive summary of 5. Some Pathway Examples of Design-Oriented AI Research
- This section provides specific examples of how different papers have followed various pathways for design-oriented AI research.
- It illustrates the application of the pathways framework in practice, showcasing the diverse combinations of artifact types, abstraction levels, and desirable traits.
- The section highlights the importance of considering the specific context and characteristics of each research project when selecting an appropriate pathway.
- By providing concrete examples, the section aims to inspire and guide future research in design-oriented AI.
 
# Executive summary of 6. Implications and Concluding Remarks
- This section summarizes the key implications of the editorial for researchers, practitioners, and editors in the field of information systems.
- It reiterates the opportunities for IS scholars to contribute to the growing field of AI, while also acknowledging the challenges and impediments that must be addressed.
- The section emphasizes the importance of design-oriented research in demonstrating disciplinary distinctiveness and thought leadership in an AI-enabled world.
- It concludes by encouraging dialogue and collaboration among authors, reviewers, and editors to create impactful design research that advances the field of information systems.

</details>

```
title: Discovery of Technological Innovation Systems: Implications for Predicting Future Innovation
authors: Junho Yoon, Gautam Pant & Shagun Pant
journal: Journal of Management Information Systems
published: 2024
```

# Executive Summary
- We propose a framework for **discovering Technological Innovation Systems (TISs)** by leveraging textual information from millions of patents, using a design science approach.
- The framework aims to complement the hierarchical Cooperative Patent Classification (CPC) with dynamic TIS discovery through machine learning, accounting for horizontal relationships across technologies. The **design involves a three-stage process**:
    - **Stage 1**: Using **BERTopic** (a neural topic modeling technique) to derive **latent innovation topics** from patent text (patent title and abstract).  This identifies horizontal thematic areas across patents, regardless of their CPC classification. Six overarching topics are discovered.
    - **Stage 2**: Mapping **patents** to the innovation topics based on the highest probability assignment from the **BERTopic** model. This allows each topic to be represented as a collection of technology classes (four-character CPC codes).
    - **Stage 3**: Measuring **technology relatedness** between CPC classes using the Jensen-Shannon Divergence (JSD) of their topic distributions. For a given **focal technology**, related classes above the 90th percentile in relatedness are deemed part of its **TIS**.
- We validate our framework based on its ability to predict future innovation quantity and quality in different technology classes.
- Key **theoretical underpinnings** and **conceptual framework**:
    - **Technological Spillovers:** Technologies build on each other, affecting inter-sectoral innovation [31]. Innovation in one technology within a TIS is associated with future innovation in related technologies.
    - **BERTopic:** A neural network-based topic modeling technique that uses Bidirectional Encoder Representations from Transformers (BERT) to discover the topical innovation structure from the patent text data.
- Key **variables** used in the validation and prediction tasks include:
    - **Dependent Variables**:
        - **Future Patenting (FP)**: Cumulative patent counts for the next 5 years in a given technology class.  This represents innovation quantity.
        - **Future Citations (FC)**: Cumulative citation counts for the next 5 years in a given technology class. This represents innovation quality. Citations are scaled by the annual average.
        - **Patent Capital**: An estimate of the market value of a firm's patent stock, based on market reactions to patent announcements.  Used in the firm-level analysis.
    - **Independent Variables** (Derived from TIS Discovery):
        - **Related-Tech Innovation (RTI)**: A weighted average of past patenting intensity in *related* technology classes within a TIS, using normalized technology relatedness as weights.
        - **Boundary-Spanning Innovation (BSI)**:  Information entropy calculated for each technology class based on the probability distribution over the six innovation topics.  A higher entropy indicates a more boundary-spanning (interdisciplinary) nature.  An "Alternative BSI" is also created using the coefficient of variation (CV).
        - **Related-Tech Innovation Quality (RTIQ)**: A weighted average of past citation volume in *related* technology classes within a TIS, using normalized technology relatedness as weights.
    - **Control Variables**:
        - **Past Patenting (PP)**: Cumulative patent counts for the past 5 years in a given technology class.
        - **Past Citation (PC)**: Cumulative citation counts for the past 5 years in a given technology class. Citations are scaled by the annual average.
        - **Citation-based Related-Tech Innovation (CRTI)**: Analogous to RTI, but uses citation patterns *between* technology classes (instead of the TIS topic-based relatedness) to weight patenting volume in related classes. Captures knowledge spillovers indicated by citations.
        - **Firm-level controls**: Total assets, firm age, leverage, capital expenditure (CAPEX), cash, and R&D investment.  Used in the firm-level analysis with Patent Capital.
    - **TIS Score**: The number of a firm’s patents in related technologies based on TIS discovery as a fraction of all patents of the firm.
- Key **findings** and arguments:
    - **TIS-based innovation metrics**, leveraging patenting activity in related technology classes, are significantly associated with future innovation intensity in focal technologies.
    - Firms that focus their patent portfolios on related technologies based on TISs discovered through our framework show greater future value ascribed to their patents by the financial markets.
    - Greater levels of boundary-spanning innovation signal a higher level of aggregate innovation in the technology class.
    - Machine learning models benefit significantly from our proposed innovation metrics in predicting future patenting volume and citations for a technology class. Specifically, a neural network-based time series prediction algorithm (Long-Short Term Memory or LSTM), as well as several tree-based methods, significantly benefit from our proposed innovation metrics with consistent results across varying evaluation metrics.
- This study provides a generalizable and automated framework to facilitate TIS discovery with horizontal components that cut through the vertical CPC structure.

#innovation_system #machine_learning #patents #predictive_analytics #design_science #technology_spillovers #bert #topical_modeling #boundary_spanning

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Innovation is increasingly **boundary-spanning** and **interdisciplinary**, leading to the formulation of hybrid technologies and ecosystems [3, 26, 43].
- Each ecosystem of related technologies can be referred to as a **Technological Innovation System (TIS)** [10], which benefits from modular components that enable recombinant innovation [7, 18, 59].
- Recent examples of TISs include smart TVs and autonomous vehicles, illustrating the convergence of previously separate technologies.
- The fast-paced boundary-spanning innovation challenges specialized patent examiners relying on the hierarchical Cooperative Patent Classification (CPC) system [66].
- A **TIS** provides a systems view around a focal technology by considering its relationship with various components [10], facilitating an understanding of technological trajectory.
- We focus on the discovery of **TIS** around a focal technology to identify an ecosystem of related technologies that have interactions and inter-dependencies [8, 10].
- A systems view of related technologies facilitates diagnosing vulnerable spots in a TIS and could lead to better prognosis and prediction for future evolution [27].
- We utilize the **design science approach** [25] in crafting our artifact for TIS discovery, subsequently evaluating its practical utility for businesses and other organizations within the sphere of innovation.
- We propose a framework for the **automated discovery of TIS** and demonstrate that such discovery can serve as a tool for more accurately predicting future innovation intensity and quality in a technological class.
- The predictions can be beneficial for various stakeholders of a technology innovation system, such as firms, research labs, governments, and universities.

## 1.1. TIS Discovery and Validation
- We provide a **scalable solution** as a design science artifact by proposing a **TIS discovery framework** that applies unsupervised machine learning on text from millions of patents to operationalize technology relatedness.
- The proposed framework involves deriving a **horizontal topical innovation structure** that crosses the boundaries under the popular hierarchical technology classification system (i.e., CPC).
- Innovation topics in the horizontal structure are used to identify, for each focal technology class, its related classes. We define a group of associated technologies (i.e., focal and related) as a technological innovation system (TIS).
- The **CPC structure** starts from nine main technology sections at the highest level and drills down into more granular categories in a hierarchical structure.
- A **vertical scheme** is likely inadequate to capture a TIS with increasingly horizontally related technology components.
- We leverage textual information from millions of patents to derive an alternate topical innovation structure to discover the horizontal relations within a TIS.
- Textual information provides cues on boundary-spanning relations within a TIS that are absent in the vertical CPC structure.
- We use **BERTopic**, a state-of-the-art neural network-based topic modeling technique, to discover the topical innovation structure from patent text data.
- The text-driven structure is used for a focal technology to identify its related technologies with high relatedness over a reasonable threshold. The clique of technologies thus identified can be framed as a TIS.
- We validate the **TISs** discovered by such a framework by utilizing the theory on technological spillovers as a kernel theory governing our artifact development [31].
- We expect innovation spillovers across related technologies in a TIS and use it to validate the associations of the TIS components by estimating their utility in predicting future innovation intensity in technology classes.

## 1.2. Contributions and Outline
- The **topical innovation structure** we derive using patent data complements the extant hierarchical CPC scheme in that it realizes the explicit discovery of TISs with horizontal components.
- This study provides the following **contributions**:
    - Proposing a generalizable framework to facilitate **automated TIS discovery** with horizontal components that cut through the vertical CPC structure.
    - Firms can take advantage of our framework for identifying focal technologies with high levels of future innovation potential as well as a set of related technologies that could be part of their diverse technology portfolio.
    - From a predictive modeling standpoint [56], we tease out the predictive utility of the TIS-based innovation metrics by using some popular machine learning models.
- In practice, firms can take advantage of our framework for identifying focal technologies with high levels of future innovation potential as well as a set of related technologies that could be part of their diverse technology portfolio.
- The **TIS-based innovation metrics** we propose can be used in practice to predict the future volume of patents and citations in a technology class of interest [58].

# 2. Related Literature
- The sociology literature asserts the importance of boundary-spanning channels that allow knowledge flows among otherwise siloed communities [64].
- Collaborations among technical experts from diverse domains can lead to the emergence of new technologies and drawn attention from innovation stakeholders [63].
- The National Science Foundation (NSF) has funded interdisciplinary research initiatives since the mid-1980s to nurture impactful technologies [32].
- The innovation literature in management and economics has found empirical evidence that the recombination of distinct knowledge can engender breakthrough innovations [5, 59].
- As a result of such boundary-spanning (i.e., horizontal) trends, the boundaries between technologies have become more ambiguous than in the past.
- The current Cooperative Patent Classification (CPC) system used by USPTO and EPO has a hierarchical (i.e., vertical) structure that clearly delineates technology classes.
- The broader literature on text analysis of patents can be subdivided into three strands: novelty/impact quantification [6, 29], similarity computation [19], and subgroup categorization [33].
- Our study is focused on using the patent text to quantify the relatedness between technology classes and proposes the automated discovery of TISs with boundary-spanning technology classes.
- The current literature in design science regarding technological innovation has mainly focused on specific industries, crafting artifacts to gain insight into particular technological advancements [3, 22, 49].
- Our study differentiates itself from previous innovation-related design science research by addressing extensive technological domains based on the Cooperative Patent Classification (CPC) scheme and employing machine learning to uncover intersectoral relationships among technologies using text data.

# 3. Technological Innovation System (TIS)
- A technological innovation system (TIS) can be defined by its technological components or classes analogous to a biological ecosystem [10, 38].
- The innovation literature that adopts the component-based standpoint considers technologies in a TIS as mutually associated in terms of their evolution or external interventions [8, 27].
- Identification of a **TIS** can help diagnose vulnerable spots in a TIS or facilitate policies such as investment recommendations for technology components [8, 27].
- Stephan et al. [60] apply the value chain perspective to their study on the lithium-ion battery (LIB) innovation system.
- A few studies in the information systems (IS) literature have adopted the ecosystem analogy in the information technology context [2, 4].
- Adomavicius et al. [2] propose a framework that helps understand the IT landscape based on three roles of technologies - 1) product, 2) component, and 3) infrastructure.
- The focus of the above-mentioned empirical studies is on specific manually-identified TISs based on case studies.
- We present a framework to automatically identify TISs around a focal technology utilizing large-scale patent data with ML.
- The TISs discovered through the framework show bonafide relational cues about future innovation in the focal technology.
- We address the issue with a straightforward premise based on expected spillovers across technologies in a TIS.

# 4. Data
- To operationalize our design artifact, we leverage comprehensive patent information to extract the topical innovation structure from patents.
- We collect patent data as of 2021 from the United States Patent and Trademark Office (USPTO) because it has been widely employed in prior studies [12, 50].
- The USPTO database contains patent publication numbers, filing dates, grant dates, patent titles and abstracts, country codes, citations, CPC codes, and assignees of patents.
- For this study, we use patents granted by the USPTO that were applied between 2000 and 2017.
- The unit of analysis of our study is a technology class (four-character CPC) which forms a coherent technology field [36].

# 5. TIS Discovery Framework
- The main goal of our work is to propose a data-driven framework for discovering TISs and the validation of the artifact [25].
- We elaborate on how we build the design artifact.
- The unit of analysis of our study is a technology class (four-character CPC), e.g., “H01L” (Semiconductors).
- There are three stages in the framework that we describe next.

## 5.1. Stage 1 - Topical Innovation Structure using Neural Topic Modeling
- To derive a text-based topical innovation structure using patents, we employ **BERTopic** [21].
- **BERTopic** is a state-of-the-art neural topic modeling technique based on the bidirectional encoder representations from transformers (BERT).
- The use of BERTopic with patent text enables the discovery of latent innovation topics that are transparent.
- The transparent nature of the topics also allows us to easily label them for ease of interpretation.
- Finally, given the need to build a framework that can use millions of patent text as inputs, the stable and seamless implementation of BERTopic is an added benefit.
- The input documents used by the BERTopic are patent texts, and in particular, we use the combination of a patent’s title and abstract as its text.
- BERTopic is a topic model that enhances traditional techniques by utilizing a class-based version of TF-IDF to extract meaningful topic representations.
- In our implementation based on BERTopic, patents between 2000 and 2007 are used for training the topic model.
- We suggest using recent patents (e.g., from the previous decade) for topic modeling in order to capture the timely innovation topic structure.
- With the output from BERTopic, we categorize patents into latent innovation topics based on their text.
- A main hyperparameter for topic modeling is the number of latent topics.
- We experiment with different numbers of topics, and the normalized pointwise mutual information (NPMI) coherence scores are computed for each choice of the number of topics.
- We identify a reasonable number of latent innovation topics by considering the elbow of the curve in the plot of coherence scores against the number of topics.
- We pick six topics to categorize patent terms with a coherence score of 0.208.
- Based on the five terms for each innovation topic, we refer to Topic 1 as “Digital Communication”, Topic 2 as “Automobile”, Topic 3 as “Chemical Engineering”, Topic 4 as “Information/Signal Processing”, Topic 5 as “Semiconductor” and Topic 6 as “Healthcare”.

## 5.2. Stage 2 - Mapping Patents onto Innovation Topics
- For each patent, the BERTopic model assigns patents to the topic with the highest probability.
- In this manner, all patents are mapped to one of the six innovation topics.
- With the patent mapping, the six innovation topics can now be represented as a collection of technology classes, presenting an additional validation of the innovation topics and their labels.
- For example, we find that patents with primary CPC codes “F16H” (gearing), “B60R” (vehicles), and “B62D” (motor vehicles) mostly appear in Topic 2 (Automobile).
- The mapped CPC codes reinforce the coherent and transparent nature of the topics discovered through BERTopic.
- Once the patents are mapped into the six innovation topics, we explore the boundary-spanning nature of innovation by cross-tabulating the innovation topics and the nine top-level technology sections of the CPC.

## 5.3. Stage 3 - Discovery of TIS
- We describe the use of the topical innovation structure for measuring relatedness between technology classes (i.e., four-character CPC codes) and discovering specific technological innovation systems.
- To discover specific TISs, each technology class s is represented as a discrete probability vector ws = [Pc1, Pc2, . . ., Pc6] over the six innovation topics based on the patent frequency in each topic.
- The vector representations are then used to measure pairwise technology relatedness between technology classes (s1 and s2) by employing the Jensen-Shannon Divergence (JSD) measure [35].
- For a given focal technology class, we can sort all other technology classes by their technology relatedness with the focal class, which facilitates the search for TIS components.
- For this purpose, only the related classes above the 90th percentile on the technology relatedness metric are identified as TIS components.
- Finally, the focal and its related classes together form a TIS, as discovered by the framework.

# 6. Innovation Metrics
- We now validate the efficacy of the TIS discovery framework by estimating its usefulness in predicting future innovation intensity both in terms of quantity (i.e., patents) and quality (i.e., citations).
- We operationalize several innovation metrics based on the past patenting output in the related technologies to predict future patenting in and citations to a focal class.
- The proposed metrics build on the previously discussed spillover expectation that innovation in a focal class should be associated with the innovation taking place in the related classes.
- We also come up with a boundary-spanning innovation metric of a focal class over the six innovation topics to quantify interdisciplinarity.
- In addition, motivated by Acemoglu et al. [1], we create a citation-based predictor to model inter-class spillovers where the technology classes are linked through citations and use it as a control variable in estimations.
- The citation-based predictor also serves as an important benchmark from the literature for machine learning-based prediction experiments.
- We describe the operationalization of several innovation metrics, which serve as predictors for predicting future innovation quantity and quality in a technology class.
    - **Related-Tech Innovation (RTI):** We use the normalized technology relatedness (as weights) to compute a weighted average of the past patenting intensity in related technology classes (from TIS) for a given focal technology class.
    - **Boundary-Spanning Innovation (BSI):** To measure the boundary-spanning nature of a technology class, we calculate information entropy for each technology class with the probability distribution over the six innovation topics.
    - **Related-Tech Innovation Quality (RTIQ):** Citations to a patent are seen as a measure of its impact or quality [23]. We propose the Related-Tech Innovation Quality (RTIQ) metric for future citations using the relatedness weight in the same manner as Equation 4.
    - **Citation-based Related-Tech Innovation (CRTI):** Patent citations can also be used for modeling the associations between technology classes in terms of knowledge spillover.

# 7. Explanatory Modeling
- We evaluate the validity of our design artifact (i.e., TIS discovery framework) as suggested by [25].
- We specify the explanatory empirical models that we use to validate the TIS discovery framework and the consequent innovation metrics.
- We do so by estimating the associations of the innovation metrics derived using the TISs with the future innovation quantity (patent volume) and quality (patent citations) of the focal technology class.

## 7.1. Innovation Quantity (Patent Volume)
- We begin by regressing cumulative future patenting volume (FP) in a focal technology class over the next 5 years (from year t) on the three innovation metrics described earlier that account for patenting volume in related technologies and their boundary-spanning nature.
- Patent volume in a given time period for a technology class is simply the count of patents that have been filed in that time period and have been assigned the CPC code for the technology class.
- The coefficient of main interest is β3 corresponding to related-tech innovation (RTI) that represents past patenting intensity in the related technologies within a TIS.
- If β3 is positive and significant, it is in congruence with our argument based on technology and knowledge spillover theories that a higher past patenting intensity in the related classes is associated with more future patenting for a focal class.
- For β4, we find a positive relation between the boundary-spanning innovation (BSI) metric and future innovation, indicating a greater future innovation output in a technology class with a higher boundary-spanning innovation.
- As for the interaction term (β5), the negative interaction indicates that the boundary-spanning innovation within a technology class can have a tampering effect on output when coupled with an ecosystem of related classes with high patenting activity.
- We compute an alternative boundary-spanning innovation metric (Alternative BSI) using the coefficient of variation (CV).
- One of the strengths of our framework is that it allows for repeatability with flexible time windows in deriving innovation topics.
- We exploit this advantage by retraining topics during our data sample and consider more recent relationships across technologies over time.

## 7.2. Innovation Quality (Citations)
- For the innovation quality dimension, we model the volume of citations to future patents (FC) in a focal technology class for the next 5 years (from year t).
- We allow for five years of citation lags and scale the actual citation counts of patents by the annual average for patents filed in a given year.
- We focus on β2, the coefficient on the related-tech innovation quality (RTIQ), the metric of innovation quality in the TIS-based related technologies.
- A positive and significant coefficient would show that high-quality innovation in related technologies is associated with more high-quality innovation in a given focal class.
- Table 5 shows the estimation results for Equation 10.
- We find that the related-tech innovation quality (RTIQ) is positively associated with the future innovation quality of the focal class.

# 8. Predictive Modeling
- Our explanatory empirical analysis helped to tease out the utility of TIS discovery and the consequent innovation metrics for estimating future innovation intensity and quality within technology classes.
- Machine learning models are not bound by these constraints as their focus is squarely on prediction quality.
- Hence to understand the full utility of our TIS discovery and the proposed innovation metrics, we now build predictive models using machine learning.
- We perform two separate prediction tasks for estimating future innovation quantity and quality for a technology class.
- The setup of the target variable (i.e., annual patents or citations) is partly driven by the need to create a separate hold-out data set (also called test data) for evaluation, which is necessary to measure the prediction quality.
- The current experiments utilize a linear model with L1-regularization (commonly referred to as LASSO), in addition to a variety of decision tree-based algorithms, which include Classification and Regression Trees (CART), Random Forest, and Extreme Gradient Boosting (XGBoost).
- Moreover, Long-Short Term Memory (LSTM), a class of recurrent neural networks commonly deployed for time series forecasting, is also employed.

## 8.1. Task 1 – Innovation Quantity
- In Task 1, we train models to predict future patenting volume (FP) in a given year and technology class.
- We evaluate the trained models based on their ability to correctly predict for the year 2017 (the hold-out year whose data is not used for training).
- The resulting performances are evaluated on three metrics - root mean squared error (RMSE), root relative squared error (RRSE), and coefficient of determination (R2).
- The results with RMSE clearly show that the proposed innovation metrics significantly contribute to the performance of LSTM and the tree-based models (i.e., CART, RF, XGBoost).
- In particular, for the LSTM model, using the proposed innovation metrics yields a 40.11% improved average RMSE (p-value < 0.0001) than using only the past innovation in the technology class and the past innovation in related classes based on citations.

## 8.2. Task 2 – Innovation Quality
- In Task 2, we train models to predict future citation volume (FC) for patents filed in a given year and technology class.
- We evaluate the trained models based on their ability to correctly predict for patents filed in the year 2016 (hold-out year whose data is not used for training).
- In Table 8, we find the results largely consistent with Task 1, wherein LSTM with our innovation metrics achieves the best performance for the three evaluation metrics.
- In particular, the inclusion of the proposed innovation metrics significantly (p-value < 0.0001) boosts the performance of LSTM on RMSE as well as RRSE, and to a lesser degree on R2.

# 9. Discussion and Implications
- From a managerial standpoint, a key contribution of our work is in proposing symbiotic technology candidates in a TIS to be considered for developing firms’ patent portfolio strategy [34].
- We posit that innovating in a cluster of related technologies integrated within a TIS can constitute a more valuable patent portfolio for a firm.
- We employ the patent capital metric, measured by Woeppel [67], to test this empirically.
- We regress the patent capital of firms in the following years, from t+1 to t+3, on their current TIS scores at time t.
- Table 9 shows the estimation results of Equation 12.
- The coefficient on the TIS score (β1) is consistently positive and significant from year t+1 to t+3 after controlling for the firm-specific variables and the fixed effects for firms and years.
- Our TIS discovery framework could assist firms in designing their patent portfolio.
- Another implication for managers is through our predictive modeling that provides an ML-based technique to identify technology classes that are expected to show strong innovation intensity and quality in the near future.
- Institutional investors such as endowments, hedge funds, or actively managed mutual funds can make investment decisions in firms based on the composition of their patent portfolios.
- Our framework has the potential to be useful to various organizations, including federal research agencies like the US National Science Foundation (NSF) and the US National Institutes of Health (NIH).
- In terms of implications for academic research, our study answers the call for the automated identification of TISs [8, 10].
- Our automated TIS discovery framework could allow researchers to easily obtain additional test beds for further theoretical validations.
- Our TIS discovery framework utilizing patent text finds strong evidence of horizontal innovation that cuts through the vertical CPC structure.

# 10. Conclusion and Future Work
- Utilizing over four million patents, this study develops a design artifact for the automated discovery of TISs and provides its empirical validation through explanatory and predictive models.
- We propose several innovation metrics that capture the innovation activity in related technology classes of a TIS.
- The fact that these innovation metrics are significantly associated with future innovation intensity and quality substantiates our framework.
- By proposing the TIS discovery framework, this study answers the long-existing need for the easily scalable identification of specific TISs [10].
- We expect future empirical studies using our framework to locate a particular TIS of interest and delve into more sophisticated innovation mechanisms in that TIS.
- There also exist future opportunities to create metrics of alignment between actual and predicted investments in the TIS framework at the firm level and study the effects of such alignment on various measures of firm value such as Tobin’s Q [46].

---

# Executive summary of 1. Introduction
- Innovation is becoming increasingly interdisciplinary, blurring traditional boundaries between technology sectors.
- This trend necessitates new approaches to understanding technology ecosystems, referred to as Technological Innovation Systems (TIS).
- We propose a design science-based framework for automated TIS discovery using machine learning on patent text, aiming to complement the hierarchical Cooperative Patent Classification (CPC) system.
- The framework's practical utility is evaluated based on its ability to predict future innovation intensity and quality.

# Executive summary of 2. Related Literature
- Prior literature highlights the importance of boundary-spanning channels for knowledge flow and interdisciplinary collaborations in driving innovation.
- The current CPC system's hierarchical structure struggles to capture the horizontal trends of interdisciplinary innovation.
- Existing research on text analysis of patents focuses on novelty quantification, similarity computation, and subgroup categorization.
- Our study contributes by proposing automated TIS discovery with boundary-spanning technology classes, utilizing machine learning to uncover intersectoral relationships.

# Executive summary of 3. Technological Innovation System (TIS)
- A Technological Innovation System (TIS) is defined by its technological components, analogous to a biological ecosystem.
- Identifying a TIS can help diagnose vulnerabilities and facilitate investment recommendations.
- Prior studies have focused on manually identified TISs, whereas we present an automated framework using large-scale patent data and ML.
- Our framework addresses the research gap by providing a data-driven procedure for delineating TISs based on text-based relatedness.
- We validate TIS associations by demonstrating that future innovation in a focal technology class is positively associated with past innovation outputs of related technologies.

# Executive summary of 4. Data
- To implement our design artifact, we utilize comprehensive patent information from the USPTO.
- The dataset includes patents granted between 2000 and 2017, with various fields such as publication numbers, filing dates, titles, abstracts, CPC codes, and assignees.
- The unit of analysis is the technology class (four-character CPC), which represents a coherent technology field.

# Executive summary of 5. TIS Discovery Framework
- Our primary goal is to propose a data-driven framework for discovering TISs and validating the artifact.
- The framework consists of three stages:
    - **Stage 1:** Topical Innovation Structure using Neural Topic Modeling (BERTopic).
    - **Stage 2:** Mapping Patents onto Innovation Topics.
    - **Stage 3:** Discovery of TIS.

# Executive summary of 6. Innovation Metrics
- We validate the TIS discovery framework's efficacy by estimating its usefulness in predicting future innovation intensity and quality.
- We propose several innovation metrics based on past patenting output in related technologies, including:
    - Related-Tech Innovation (RTI).
    - Boundary-Spanning Innovation (BSI).
    - Related-Tech Innovation Quality (RTIQ).
    - Citation-based Related-Tech Innovation (CRTI).

# Executive summary of 7. Explanatory Modeling
- We evaluate the validity of our design artifact using explanatory empirical models.
- These models estimate the associations of TIS-derived innovation metrics with future innovation quantity and quality.
- We account for technology class-level heterogeneity using fixed effects.
- Results indicate that patenting volume in related classes based on the TIS framework is positively associated with future patenting volume in the focal class.
- We also find a positive relation between the BSI metric and future innovation.

# Executive summary of 8. Predictive Modeling
- We assess the utility of TIS discovery and innovation metrics using machine learning models.
- Two prediction tasks are performed: estimating future innovation quantity and quality.
- Models include LASSO, CART, Random Forest, XGBoost, and LSTM.
- Results show that LSTM with our innovation metrics achieves the best performance, significantly boosting predictive accuracy for both innovation quantity and quality.

# Executive summary of 9. Discussion and Implications
- A key contribution is proposing symbiotic technology candidates in a TIS for firms' patent portfolio strategy.
- We find that firms focusing on TIS technologies experience an increase in patent value in subsequent years.
- Our framework can assist firms in designing their patent portfolio and help prioritize investments across technology classes.
- Federal research agencies can also leverage our framework for efficient research funding allocation.

# Executive summary of 10. Conclusion and Future Work
- This study develops a design artifact for automated TIS discovery and provides empirical validation through explanatory and predictive models.
- The proposed TIS discovery framework addresses the need for scalable identification of specific TISs.
- Future studies can empirically examine how technologies in a specific TIS respond to various mechanisms.

</details>

```
title: Addressing Societal Challenges Through Analytics: An ESG ICE Framework and Research Agenda
authors: Wolfgang Ketter, Balaji Padmanabhan, Gautam Pant, T.S. Raghu
journal: Journal of the Association for Information Systems
published: 2020
```

# Executive Summary

*   This paper addresses the urgent need for **analytics-driven solutions** to tackle complex societal challenges, with a specific focus on **Environmental, Social, and Governance (ESG) factors**.
*   We introduce the **ESG ICE framework**, a novel structure organizing societal challenges around three core pillars: **Impact**, **Connectivity**, and **Equity**.
    *   **Impact** is defined as mitigating negative environmental, social, and economic consequences (e.g., reducing carbon emissions, improving public health, fostering job creation) and promoting positive societal influences through sustainable practices and responsible innovation.
    *   **Connectivity** focuses on promoting inclusion, access, and interoperability (e.g., bridging the digital divide, improving access to education and healthcare, fostering social networks) to bridge societal gaps and ensure that all individuals have access to resources and opportunities.
    *   **Equity** emphasizes ensuring fairness, transparency, and accountability in resource allocation, decision-making processes, and outcomes (e.g., addressing income inequality, eliminating discrimination, ensuring access to justice) to promote a more just and equitable society.
*   The **ESG ICE framework** provides a structured approach for **categorizing and analyzing diverse societal challenges**, empowering researchers to develop more targeted and impactful analytical solutions.
    *   For example, the framework can be used to analyze the impact of a new technology on employment levels (Impact), its accessibility to different demographic groups (Connectivity), and its potential to exacerbate existing inequalities (Equity).
*   We outline a comprehensive research agenda highlighting key areas for future investigation, including:
    *   Developing innovative analytical methods to **measure and manage ESG risks** and opportunities (e.g., using machine learning to predict environmental risks associated with business activities, creating standardized ESG metrics for performance assessment).
    *   Designing **data-driven interventions** to promote **sustainable practices** and **social responsibility** (e.g., using behavioral analytics to encourage sustainable consumption patterns, creating personalized learning platforms to improve educational outcomes).
    *   Investigating the role of **AI and machine learning** in **automating ESG reporting** and **decision-making** (e.g., using natural language processing to analyze news articles for ESG risk identification, developing transparent and explainable AI systems for equitable decision-making).
    *   Exploring the ethical considerations and potential societal implications of using analytics to address societal challenges (e.g., addressing data privacy concerns, mitigating algorithmic bias, ensuring accountability for AI-driven decisions).
*   The paper strongly **advocates for interdisciplinary collaboration** between IS researchers, domain experts in fields like environmental science and sociology, and policymakers to accelerate the development and deployment of impactful analytics solutions for the betterment of society. This collaborative effort ensures that solutions are both technically sound and socially responsible.
*   This call for action urges the IS research community to actively contribute to solving pressing societal issues through the innovative application of analytics, creating a positive and sustainable impact on the world.
*   **A major limitation** acknowledged is the potential oversimplification of complex societal issues by the ESG ICE framework.
*   Future research should focus on developing more **granular, context-specific indicators**, design principles for analytics implementation, and addressing ethical implications like **algorithmic bias**.

#esg #analytics #societal_challenges #ice_framework #impact #connectivity #equity #sustainability #social_responsibility #ai #machine_learning #research_agenda #interdisciplinary_research

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction

*   The world faces a multitude of societal challenges, ranging from environmental degradation and social inequality to economic instability and political unrest [Bloom and Van Reenen, 2010; Ferraro et al., 2015].
*   These challenges are often complex, interconnected, and dynamic, requiring innovative solutions that go beyond traditional approaches [Rittel and Webber, 1973].
*   **Analytics**, defined as the use of data, information technology, statistical analysis, quantitative methods, and computational models to gain insights and make informed decisions [Davenport and Harris, 2007], holds great promise for addressing these challenges.
    *   Analytics can help us understand the root causes of societal problems, identify patterns and trends, and develop effective interventions.
*   There's a growing recognition of the importance of **Environmental, Social, and Governance (ESG) factors** in assessing the sustainability and ethical impact of organizations and investments [Eccles and Serafeim, 2013; Friede et al., 2015].
    *   ESG factors encompass a wide range of issues, such as climate change, resource depletion, human rights, labor standards, corporate governance, and ethical conduct.
*   Traditional methods for addressing societal challenges often lack the **data-driven insights** and **predictive capabilities** needed to make informed decisions and track progress [Bollier, 2016].
*   The information systems (IS) field has a unique opportunity to contribute to solving societal challenges by developing and applying innovative analytics solutions [George et al., 2016].
    *   IS researchers possess the technical expertise, methodological skills, and interdisciplinary perspective needed to tackle complex problems that require integrating data from diverse sources, developing sophisticated analytical models, and designing user-friendly interfaces.

# 2. Conceptual Framework: The ESG ICE Framework

*   We propose the **ESG ICE framework** to provide a structured approach for categorizing and analyzing societal challenges and for developing targeted analytical solutions.
    *   The framework is based on three key pillars: **Impact**, **Connectivity**, and **Equity**.

## 2.1. Impact

*   **Impact** refers to the extent to which an activity, organization, or policy affects society, both positively and negatively.
*   It encompasses a wide range of environmental, social, and economic consequences, such as:
    *   Pollution
    *   Resource depletion
    *   Climate change
    *   Public health
    *   Job creation
    *   Economic growth
*   **Examples:**
    *   **Environmental Impact:** A company's carbon footprint or its contribution to deforestation.
    *   **Social Impact:** The effect of a new technology on employment levels or the impact of a government program on poverty reduction.
*   **Analytics can play a crucial role in measuring and managing impact** by providing data-driven insights into the consequences of different actions and policies.
    *   For instance, **life cycle assessment (LCA)** techniques, combined with machine learning, can be used to analyze the full environmental impact of a product or service, from raw material extraction to end-of-life disposal [Hendrickson et al., 2006].

## 2.2. Connectivity

*   **Connectivity** refers to the extent to which individuals, organizations, and communities are connected to each other and to resources and opportunities.
*   It encompasses issues such as:
    *   Digital inclusion
    *   Access to education
    *   Healthcare
    *   Transportation
    *   Social networks
*   **Examples:**
    *   **Digital Divide:** The gap between those who have access to the internet and those who do not.
    *   **Social Isolation:** The lack of social connections and support among certain groups of people.
*   **Analytics can help improve connectivity** by identifying underserved populations, designing targeted interventions, and facilitating collaboration and knowledge sharing.
    *   For example, **geospatial analytics** can be used to identify areas with limited access to healthcare services, allowing for the strategic deployment of mobile clinics or telehealth programs [Rushton, 2003].

## 2.3. Equity

*   **Equity** refers to the fairness and impartiality of resource allocation, decision-making processes, and outcomes.
*   It encompasses issues such as:
    *   Income inequality
    *   Discrimination
    *   Access to justice
    *   Political representation
    *   Corporate governance
*   **Examples:**
    *   **Gender Pay Gap:** The difference in earnings between men and women for similar work.
    *   **Racial Bias:** The unequal treatment of individuals based on their race or ethnicity.
*   **Analytics can help promote equity** by identifying biases in data and algorithms, designing fair allocation mechanisms, and ensuring transparency and accountability in decision-making.
    *   For example, **fairness-aware machine learning** techniques can be used to mitigate biases in predictive models used for loan applications or hiring decisions, ensuring that all individuals are evaluated fairly [Barocas and Selbst, 2016].

## 2.4. Interdependencies Between the Pillars

*   The three pillars of the ESG ICE framework are not mutually exclusive, but rather are **interdependent and interconnected**.
*   **Example:** Actions taken to reduce environmental impact (e.g., investing in renewable energy) can also improve connectivity (e.g., providing access to clean energy in underserved communities) and promote equity (e.g., creating green jobs in disadvantaged areas).
*   Analytics solutions that address societal challenges should therefore consider the **interdependencies between the three pillars** to maximize their overall impact.

# 3. Research Agenda

*   We propose a research agenda that outlines several key areas for future research on analytics for societal challenges, organized around the three pillars of the ESG ICE framework.

## 3.1. Impact

*   **Research Question:** How can analytics be used to measure and manage the environmental, social, and economic impacts of organizations, policies, and technologies?
    *   Traditional impact assessment methods are often subjective, incomplete, and backward-looking [Bollier, 2016].
    *   Analytics can provide more **objective, comprehensive, and forward-looking insights** into the consequences of different actions and policies.
*   **Specific Research Areas:**
    *   **Developing new metrics and indicators** for measuring ESG performance [Eccles and Serafeim, 2013].
        *   This includes developing **standardized and comparable metrics** that can be used to assess the ESG performance of different organizations and industries, such as a standardized measure of carbon intensity or a social impact index.
    *   **Using machine learning to predict the environmental and social risks** associated with different business activities [Ferraro et al., 2015].
        *   This could involve using **natural language processing** to analyze news articles and social media posts to identify potential ESG risks, such as reputational damage from environmental disasters or labor disputes.
    *   **Designing data-driven interventions** to promote sustainable consumption and production patterns [Arora et al., 2008].
        *   This could involve using **behavioral analytics** to understand consumer preferences and design targeted interventions that encourage more sustainable choices, such as personalized recommendations for eco-friendly products or gamified challenges to reduce energy consumption.

## 3.2. Connectivity

*   **Research Question:** How can analytics be used to improve connectivity and promote inclusion, access, and interoperability?
    *   Many individuals, organizations, and communities remain disconnected from resources and opportunities due to factors such as poverty, discrimination, and lack of infrastructure [Hilbert, 2016].
    *   Analytics can help identify these underserved populations and design targeted interventions to improve their connectivity.
*   **Specific Research Areas:**
    *   **Using social network analysis to map and analyze social connections and relationships** [Easly and Kleinberg, 2010].
        *   This could involve identifying key influencers and bridging agents who can help connect different groups of people, such as community leaders who can facilitate access to resources for marginalized populations.
    *   **Developing personalized learning platforms** that adapt to the individual needs and learning styles of students [Siemens, 2013].
        *   This could involve using **adaptive learning algorithms** to provide students with customized content and feedback, such as personalized learning paths based on their strengths and weaknesses.
    *   **Designing smart cities that leverage data and technology to improve the quality of life for all citizens** [Batty, 2012].
        *   This could involve using **sensor data** to monitor traffic patterns, air quality, and energy consumption, and then using analytics to optimize resource allocation and improve public services, such as intelligent traffic management systems or smart waste collection.

## 3.3. Equity

*   **Research Question:** How can analytics be used to promote equity and ensure fairness, transparency, and accountability?
    *   Many decisions that affect people's lives are made based on biased data, discriminatory algorithms, and opaque processes [O'Neil, 2016].
    *   Analytics can help identify and mitigate these biases and promote more equitable outcomes.
*   **Specific Research Areas:**
    *   **Developing algorithms that are fair and unbiased** [Dwork et al., 2012].
        *   This could involve using **adversarial training techniques** to develop algorithms that are robust to different types of bias, ensuring that the algorithms do not discriminate against certain groups of people.
    *   **Designing transparent and explainable AI systems** [Lipton, 2018].
        *   This could involve developing **visualization tools** that allow users to understand how AI systems make decisions, providing insights into the reasoning behind the algorithm's output.
    *   **Using blockchain technology to ensure transparency and accountability in supply chains and other complex systems** [Tapscott and Tapscott, 2016].
        *   This could involve using **smart contracts** to automate the enforcement of ethical and environmental standards, ensuring that all parties in the supply chain adhere to these standards.

## 3.4. Cross-Cutting Themes

*   In addition to the specific research areas outlined above, there are several **cross-cutting themes** that are relevant to all three pillars of the ESG ICE framework.
*   **These include:**
    *   **Data quality and governance:** Ensuring that the data used for analytics is accurate, complete, and reliable [Redman, 2013].
    *   **Ethical considerations:** Addressing the ethical implications of using analytics to address societal challenges [Floridi, 2013].
    *   **Interdisciplinary collaboration:** Fostering collaboration between IS researchers, domain experts, and policymakers [Van de Ven, 2007].

# 4. Conclusion

*   The world faces a multitude of societal challenges that require innovative and data-driven solutions.
*   The IS field has a unique opportunity to contribute to solving these challenges by developing and applying innovative analytics solutions.
*   The **ESG ICE framework** provides a structured approach for categorizing and analyzing societal challenges and for developing targeted analytical solutions.
*   The research agenda outlined in this paper highlights several key areas for future research on analytics for societal challenges.
*   We hope that this paper will inspire IS researchers to actively contribute to solving pressing societal issues through the innovative application of analytics.

# 5. Limitations and Future Research Directions

*   While the ESG ICE framework provides a useful lens for categorizing and addressing societal challenges, it is important to acknowledge its limitations.
*   One limitation is the **potential for oversimplification**.
    *   Societal challenges are often complex and multifaceted, and the three pillars of the ICE framework may not fully capture all of the nuances and interdependencies involved.
    *   Furthermore, the relative importance of each pillar may vary depending on the specific context and the perspectives of different stakeholders.
*   Another limitation is the **lack of prescriptive guidance** on how to implement analytics solutions within each pillar.
    *   The framework provides a conceptual structure for organizing research, but it does not offer specific methodologies or tools for developing and deploying analytical models.
    *   This requires researchers to draw upon a wide range of techniques and approaches from different disciplines, such as statistics, machine learning, optimization, and behavioral science.
*   **Future research** should focus on addressing these limitations and expanding the scope of the ESG ICE framework.
*   **Specific areas for future research include:**
    *   **Developing more granular and context-specific indicators** for measuring impact, connectivity, and equity.
        *   This could involve incorporating qualitative data and stakeholder perspectives to complement quantitative metrics, creating a more holistic and nuanced understanding of the societal challenges.
    *   **Developing design principles and best practices** for implementing analytics solutions within each pillar of the ICE framework.
        *   This could involve conducting case studies of successful analytics projects and identifying common patterns and strategies, developing a library of reusable components and best practices for analytics implementation.
    *   **Exploring the ethical and societal implications** of using analytics to address societal challenges.
        *   This includes addressing issues such as data privacy, algorithmic bias, and the potential for unintended consequences, developing frameworks for ethical impact assessment and mitigation.
    *   **Investigating the role of different stakeholders** in the development and deployment of analytics solutions.
        *   This could involve examining the roles of government agencies, non-profit organizations, and private sector companies in promoting the use of analytics for social good, fostering collaborations and partnerships across different sectors.
    *   **Extending the ESG ICE framework to other domains and contexts**, such as healthcare, education, and criminal justice.
        *   This could involve adapting the framework to address the specific challenges and opportunities in each domain, developing domain-specific indicators and metrics.
*   By addressing these limitations and expanding the scope of the ESG ICE framework, we can further enhance the ability of analytics to contribute to solving pressing societal issues.

---

# Executive summary of 1. Introduction

*   Societal challenges (environmental degradation, social inequality, economic instability, political unrest) require innovative solutions, and **analytics** offers great potential by providing data-driven insights and predictive capabilities.
*   There's growing importance of **Environmental, Social, and Governance (ESG) factors** in assessing sustainability and ethical impact, encompassing issues like climate change, human rights, and corporate governance.
*   Traditional methods often lack the **data-driven insights** and **predictive capabilities** needed for informed decisions, highlighting the need for more sophisticated approaches.
*   The IS field can contribute by developing innovative **analytics solutions**, leveraging its technical expertise, methodological skills, and interdisciplinary perspective.

# Executive summary of 2. Conceptual Framework: The ESG ICE Framework

*   We propose the **ESG ICE framework** to categorize societal challenges and develop targeted analytical solutions, based on three key pillars.
*   **Impact**: refers to societal effects (environmental, social, economic) of activities, organizations, and policies. Analytics measures and manages impact, using techniques like life cycle assessment (LCA) combined with machine learning.
*   **Connectivity**: refers to the extent to which individuals, organizations, and communities are connected to resources and opportunities. Analytics improves connectivity by identifying underserved populations and facilitating collaboration, using techniques like geospatial analytics.
*   **Equity**: refers to the fairness and impartiality of resource allocation, decision-making, and outcomes. Analytics promotes equity by identifying biases and ensuring transparency, using techniques like fairness-aware machine learning.
*   The three pillars are **interdependent**, and solutions should consider interdependencies to maximize their overall impact, such as renewable energy investments that improve connectivity and promote equity.

# Executive summary of 3. Research Agenda

*   We propose a research agenda organized around the **ESG ICE framework**, focusing on how analytics can address challenges within each pillar.
*   **Impact:** How can analytics measure and manage environmental, social, and economic impacts?
    *   Develop new metrics and indicators for measuring ESG performance, such as standardized carbon intensity measures.
    *   Use machine learning to predict environmental and social risks, such as using NLP to analyze news for ESG risk.
    *   Design data-driven interventions to promote sustainable consumption, such as personalized recommendations for eco-friendly products.
*   **Connectivity:** How can analytics improve connectivity and promote inclusion?
    *   Use social network analysis to map social connections and identify key influencers.
    *   Develop personalized learning platforms, using adaptive learning algorithms to customize content.
    *   Design smart cities, using sensor data to optimize resource allocation and improve public services.
*   **Equity:** How can analytics promote equity and ensure fairness?
    *   Develop algorithms that are fair and unbiased, using adversarial training techniques.
    *   Design transparent and explainable AI systems, developing visualization tools to understand decision-making.
    *   Use blockchain technology to ensure transparency and accountability, using smart contracts to enforce ethical standards.
*   **Cross-Cutting Themes:**
    *   Data quality and governance, ensuring accurate and reliable data.
    *   Ethical considerations, addressing the ethical implications of using analytics.
    *   Interdisciplinary collaboration, fostering collaboration between researchers and policymakers.

# Executive summary of 4. Conclusion

*   Societal challenges require data-driven solutions, and the IS field can contribute by developing innovative analytics.
*   The **ESG ICE framework** provides a structured approach for categorizing and analyzing challenges.
*   The research agenda highlights key areas for future research in analytics for societal impact.
*   We hope to inspire IS researchers to address societal issues through innovative application of analytics.

# Executive summary of 5. Limitations and Future Research Directions

*   The **ESG ICE framework** has limitations, including potential **oversimplification** of complex societal issues and a lack of **prescriptive guidance** for implementation.
*   Future research should address these limitations by:
    *   Developing more **granular and context-specific indicators**, incorporating qualitative data and stakeholder perspectives.
    *   Developing **design principles and best practices** for analytics implementation, based on case studies and reusable components.
    *   Exploring the **ethical and societal implications** of analytics use, addressing issues like algorithmic bias and data privacy.
    *   Investigating the role of **different stakeholders** in the development and deployment of solutions, fostering cross-sector collaborations.
    *   Extending the framework to other **domains and contexts**, such as healthcare and education, by adapting it with domain-specific metrics.


</details>

```
title: Predicting Labor Market Competition: Leveraging Interfirm Network and Employee Skills
authors: Yuanyang Liu, Gautam Pant, Olivia R. L. Sheng
journal: Information Systems Research
published: 2020
```

# Executive Summary

- We address the **gap** in the literature regarding the **identification** and **prediction** of labor market competitors.
- We use a **unique longitudinal employer-employee matched data set** derived from online profiles of 89,943 employees, tracking their careers in 3,467 public firms from 2000 to 2014.
- **Key theoretical framework**: resource-based view (RBV) of the firm
    - **RBV**: Firms are seen as resource bundles; human capital is a critical resource for competitive advantage. The **composition** of human capital (explicit and tacit knowledge) determines a firm's competitive positioning.
- **Key constructs** to measure human capital overlap:
    - **Skill-based metrics**: These capture similarities in *explicit* knowledge by analyzing skills listed on employee profiles.
        - ***SkillTermSim*** (similarity based on skill terms): Measures the cosine similarity between firms' skill vectors, where each element is a skill term, weighted by its prevalence within the firm and its rarity across all firms (SF-IFF). This captures direct overlap of specific skills, accounting for their commonness.
        - ***SkillTopicSim*** (similarity based on skill topics): Uses Latent Dirichlet Allocation (LDA) to group skills into broader topics (e.g., Programming, Marketing). Measures the cosine similarity between firms' skill topic vectors, capturing similarities in the *distribution* of broad skill categories.
    - **HCF network overlap metrics**: These capture similarities in *tacit* knowledge and organizational practices, inferred from employee flows.
        - ***UpstreamSim*** (similarity in upstream firms): Measures the cosine similarity between firms' "upstream vectors," where each element represents the number of employees hired from a specific upstream firm. This reflects similarities in the *sources* of human capital, and in turn the kind of tacit knowledge the downstream firms are acquiring.
        - ***DownstreamSim*** (similarity in downstream firms): Measures the cosine similarity between firms' "downstream vectors," where each element represents the number of employees who have left for a specific downstream firm. This reflects the market's perception of a firm's knowledge base: that similar, desirable qualities and tacit knowledge are perceived by different downstream companies that are hiring the talent from the current company.
        - ***HCFCommunitySim*** (whether firms belong to the same HCF network cluster): A binary indicator of whether two firms belong to the same community in the HCF network. This reflects a broader sense of similarity based on the overall structure of the network.
- **Key design elements**:
    - **Data**: LinkedIn profiles of employees from S&P 100 firms and their past work experiences.
    - **Method**: Predictive analytics framework using machine learning methods (KNN, Logistic Regression, SVM, CART, Random Forest, Deep Learning).
    - **Variables**: Economic metrics (revenue, employee count), product overlap metrics (SIC similarity, business description similarity), and human capital overlap metrics. Outcome variable is future employee migrations (HCF).
- **Key findings**:
    - Proposed human capital overlap metrics have **superior predictive power** over conventional firm-level economic and human resource measures.
    - **Skill-based metrics (labor overlap)**: *SkillTermSim* and *SkillTopicSim*, useful for predicting all competitors and strong competitors, with performance being stronger for strong competitors in particular.
    - **HCF network overlap metrics**: large improvements in AUC when included, with performance being stronger for predicting all competitors than for strong competitors.
    - The best performing model is **Random Forest**, achieving AUC of 0.89 for predicting labor market competitors (using HCF ≥ 1 as the threshold) and correctly identifying 96.4% of new competitors.
- The framework is incorporated into a comprehensive **2D competitor analysis** that includes both product and labor overlap between firms, providing strategic insights.

#competitor_analysis #human_capital #text_mining #network_analysis #machine_learning #resource_based_view #employee_skills #labor_market #interfirm_networks #skill_similarity #hcf_network #predictive_analytics


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Firms compete for both consumers in the product market and human capital in the labor market [Markman et al. 2009].
- Human capital is crucial for a firm's success [Grant 1996a, Beechler and Woodward 2009], and the resource-based view (RBV) suggests it's a key resource for establishing and sustaining a competitive advantage [Barney 1991, Wright et al. 1994].
- Executives increasingly emphasize attracting, motivating, developing, and retaining talent [Chambers et al. 1998, Axelrod et al. 2001, Wright and McMahan 2011], especially in the knowledge economy where employee knowledge is more valuable than tangible assets [Cliffe 1998].
- Firms rely on acquiring human assets from other firms to meet talent demands due to rapid technological changes [Cappelli 2008], and they often hire to learn from other firms [Song et al. 2003].
- Labor market competition exists across industries, not just within a product market [Gardner 2005, Higgins and Hull 2015, Wigglesworth 2015].
- The overlap between firms in product and labor dimensions has varied implications [Markman et al. 2009].
 
## 1.1. Theoretical Frameworks of Competitor Analysis
- Competitors can be identified by similarities in products (market) and resources [Chen 1996, Peteraf and Bergen 2003, Markman et al. 2009].
- A 2D framework (Figure 1) places firms in four quadrants based on product and labor similarities:
    - Quadrant I: Direct competitors (e.g., Amazon and Walmart).
    - Quadrant II: Product market competitors with dissimilar human capital endowments.
    - Quadrant III: Weak competitors with dissimilar products and labor skills.
    - Quadrant IV: Non–product market competitors with similar labor skills.
- The inclusion of the labor market dimension allows differentiation between product market competitors with similar or dissimilar human capital [Bergen and Peteraf 2002].
- Introducing new resources to production processes can disrupt established product markets [Markman et al. 2009].
- The labor overlap dimension helps identify non–product market competitors with similar labor skills, posing a stronger potential threat [Bergen and Peteraf 2002].
- Human capital is crucial for product development and diversification [Farjoun 1994], leading to potential competition across sectors.
- The 2D framework offers valuable strategic insights by differentiating potential competitors and highlighting varied implications for firms [Chen 1996, Peteraf and Bergen 2003, Markman et al. 2009].
 
## 1.2. Challenges and Research Gap
- Empirically performing 2D competitor analysis requires identifying interfirm similarities along both product and labor dimensions.
- Product market similarity can be measured by industry classifications, business descriptions [Shi et al. 2016], or website analysis [Pant and Sheng 2015], but commercial profiling companies are often incomplete [Ma et al. 2011].
- Empirically identifying firm labor market competitors has received less attention due to the unavailability of relevant data sources and the underuse of data science methods [Horton and Tambe 2015, Wright and McMahan 2011].
- Labor market competitor identification is more challenging than product markets because of cross-industry migrations of human capital.
- A predictive framework that mitigates this challenge is beneficial for HR managers, investors, and analysts.
- This paper proposes human capital overlap metrics derived from employees’ skills and career mobility patterns across firms to predict future labor market competition.
 
## 1.3. Theoretical Foundation
- RBV provides a theoretical foundation for predicting labor market competitors with human capital overlap metrics [Barney 1991, Newbert 2007].
- Firms are seen as resource bundles that, if valuable and hard to imitate, provide a competitive advantage [Hoopes et al. 2003].
- Human capital is critical to a firm’s competitive positioning [Barney 1991, Bartlett and Ghoshal 2002, Davenport et al. 2010].
- Human capital is difficult to imitate because it's a combination of individual employee skills [Wright et al. 1994].
- Human capital includes explicit knowledge (skills) and implicit knowledge (tacit knowledge) [Grant 1996a, 1996b].
- The composition of a firm’s human capital in terms of the mix of knowledge and skills (both explicit and tacit) characterizes the nature of its human capital bundle.
- Quantifying and measuring the overlap in human capital composition at two firms can measure the similarity in their competitive positioning.
- Composition can be viewed from two perspectives: explicit knowledge (skills) and employee origin (tacit knowledge).
- Measuring the overlap between firms in their competitive positioning allows prediction of future labor market competition.
 
## 1.4. Empirical Challenges
- Measuring human capital at the firm level is a fundamental challenge [Wright and McMahan 2011].
- Previous studies used surveys [Takeuchi et al. 2007, Ployhart et al. 2009], but lacked scale and granularity.
- Existing measures don't identify individual employee knowledge and skills beyond general assessments [Takeuchi et al. 2007] or proxies like education and experience [Hitt et al. 2001].
- Existing human capital measures aren't suitable for identifying labor market competitors.
- A more detailed and richer measurement of human capital is needed to analyze competition between firms.
- The study uses a longitudinal employer-employee matched dataset from over 89,000 LinkedIn users' public profiles.
- Public LinkedIn profiles provide data on interactions between firms and employees, experiences, education, and skills [Tambe and Hitt 2011].
- Employee job histories across firms can be tracked to locate employees in a firm in a particular year.
- LinkedIn users report skill terms, which can be aggregated at the firm level to construct a skill summary for each firm.
- Firm-level skill summaries provide a granular description of a firm's explicit knowledge base, representing each firm as a bundle of employee skills or knowledge.
- This skill-based measurement closely matches the RBV notion, representing a firm as a bundle of weighted skills unique to the firm.
- Interfirm labor overlap metrics based on employee skills can be used more effectively than traditional measures.
- A second challenge is the limited prior information about firms' competitor relationships in the labor market.
- Employee career data can reveal labor market competition, providing information on employee movement between firms.
- The movement of employees (HCF) is used as the labor market competition measurement.
- HCF indicates that the target firm needs a piece of the human capital available in the source firm.
- The study validates interfirm human capital overlap metrics using a predictive analytics framework.
 
## 1.5. HCF Network
- The study connects firms with direct HCF to obtain an interfirm network structure, called the HCF network, viewed as a supply chain network of human capital [Cappelli 2008].
- In the network, a firm receives human capital from upstream firms and provides human capital to downstream firms.
- Employees bring explicit and tacit knowledge to the target firm [Grant 1996b].
- Skill summaries capture explicit knowledge, while the network-based representation captures tacit knowledge flows.
- A firm’s human capital can be characterized by its HCF network relationships because these relationships signify knowledge flows.
- Similarity in upstream relationships reflects a demand for and flow of similar human capital.
- Similarity in downstream relationships reflects similar human capital at the firms.
- HCF network relationships indicate unique human resource configurations, providing cues to similarities in firms' competitive positioning based on RBV.
 
## 1.6. Literature and Contributions
- Previous literature has shown that conventional industrial classification is a poor signal of clusters derived from employee migration networks [Guerrero and Axtell 2013, Schmutte 2014].
- Structural properties in such networks are related to firm performance [Wu et al. 2018], but haven't been used for predicting labor market competition.
- The study observes significant predictive power of network-based human capital overlap metrics.
- New theories regarding interfirm competition for human capital may investigate conditions that fuel competition for different types of human capital (explicit versus tacit knowledge) and document differences among competitors from similar or dissimilar product markets [Chen and Miller 2012].
- The previous literature identified a theoretical framework for analyzing pairwise competition through product market and resource dimensions [Chen 1996, Peteraf and Bergen 2003, Markman et al. 2009].
- RBV suggests characterizing firms’ human resource bundles to identify similarities in their competitive positioning.
- These streams of work lack guidance on empirically measuring interfirm overlap along the human capital dimension.
- This study provides the following main contributions:
    - Proposes novel human capital overlap metrics based on firms’ skill endowment and their embedded HCF network structure, allowing to capture the interfirm similarities in the explicit knowledge base.
    - Explores previously unexplored network analysis based on employee migrations that shows the small-world nature of the HCF network with weak industrial homophily that quickly diminishes with link distance.
    - Provides the first study on predicting labor market competition using economic, human resources, product overlap, and human capital overlap metrics as predictors.
    - Tackles the empirical measurement of human capital overlap and validates it through a predictive analytics framework, operationalizing the 2D theoretical framework into an empirical lens.
 
# 2. Data
- The study uses data seeded from employees of Standard & Poor’s 100 Index (S&P 100) companies as of May 2015 for diversity and high-value human capital.
- Public LinkedIn profiles of employees of S&P 100 firms were obtained through the Yahoo BOSS API.
- Keywords were used to identify employee profiles with job experiences, education, and skill terms.
- Up to 1,000 profiles were obtained for each firm, resulting in 89,943 LinkedIn profiles.
- Profiles contain job experiences, education, and skill terms, including firm names and job start/end dates.
- Based on individuals’ job experiences, 75,350 different companies/organizations were identified.
- Firm-level data (revenue, size, business description) were obtained from the Compustat North America database.
- A total of 3,467 publicly held firms were included in the analysis.
- A longitudinal employer-employee matched dataset was compiled from 2000 to 2014, with detailed information from matched employer-employee records.
 
## 2.1. Robustness Checks
- Robustness checks were performed to verify the representativeness of the data.
- Firm-level statistics from the data were compared with all firms in the Compustat database.
- Table 2 shows the percentage of firms in each one-digit Standard Industrial Classification (SIC) level industry.
- The distribution of firms across different industries was similar between the data and Compustat firms.
- The average absolute value of the differences (in percentages) between firms in our data and firms from Compustat is 4.17% (2.85% when the finance sector is not considered).
- Kendall rank correlation was computed between firm sizes based on the sample and the known sizes from Compustat, confirming the data are a reasonable representation of firms in terms of relative sizes (Figure 3).
- Additional analysis on representativeness based on employee skills and business summaries of firms is described in Online Appendix B.
 
## 2.2. Data Characteristics
- The workers in the data are highly educated: about 90% report a college or higher level of education.
- The competitor analysis focuses on competition between firms for highly skilled/educated workers, who are crucial to firm success [Grant 1996b, Beechler and Woodward 2009].
- LinkedIn users are more "active" in the labor market, making the data more likely to capture employee mobility between firms.
- The lack of alternate data sources that track employee-firm interactions and employee skills is highlighted.

# 3. Human Capital Overlap Metrics
- The human capital overlap of firm pairs can be a predictor of labor market competition [Peteraf and Bergen 2003].
- Metrics that represent firm pair similarity in terms of their human capital are proposed.
- Skill-based human capital similarity, **labor overlap**, measures human capital overlap through the lens of skills possessed by employees.
- The network induced by employee migration between firms (HCF network) could provide cues to human capital overlap that is broader than the explicit skills alone, measured by HCF network overlap measures.
 
## 3.1. Labor Overlap
- 86,030 of 89,943 individual employees included in the data have reported skill terms.
- 15,998 distinct skill terms reported by more than one employee are used in the analysis.
- On average, an employee reported 14.6 skill terms.
- Reported skill terms form the basis for constructing interfirm skill-based similarity metrics.
 
### 3.1.1. Skill Term Similarity
- A skill summary for each firm is constructed by aggregating skill terms of its employees in a particular year, signifying the aggregated skill base of firms [Wright et al. 2014].
- Each firm *k* is represented as a skill vector *s_k* in R^N space, where *N* = 15,998 is the set of all skill terms across employees in the sample.
- Each element of *s_k* is computed as the product of skill frequency (SF) and inverse firm frequency (IFF).
    - *SF-IFF(s,k) = SF(s,k) × IFF(s)*
    - **Skill Frequency (SF)**: *SF(s,k)* is the number of employees at firm *k* that list the skill term *s* in their LinkedIn profile.
    - **Inverse Firm Frequency (IFF)**: *IFF(s)* is defined as *log(F/FFs)*, where *F* is the total number of firms, and *FFs* is the number of firms whose skill summary contains the skill term *s*.
- Inverse firm frequency weighs down common skills and helps to differentiate firms with respect to their skill summaries.
- The similarity in human capital at two firms is measured by the cosine of the angle between the skill vectors corresponding to firms *a* and *b*:
    - *sim(a, b) = (sa · sb) / (||sa|| · ||sb||)*
- This is called the **skill term similarity (SkillTermSim)** between firms.
- Cosine similarity measures the similarity in the relative distribution of skills between the two firms, normalizing the effect of the size of the firms.
 
### 3.1.2. Skill Topic Similarity
- The skill term similarity treats each skill term independently, but different skill terms can reflect the same overall types of skills.
- Latent Dirichlet allocation (LDA) is applied to discover the underlying skill topics in the employees' reported skill terms [Blei et al. 2003].
- Each document (skill terms reported by an individual employee) is represented as a mixture over a small number of latent topics, where each latent topic is characterized by a distribution over all the words.
- All 86,030 employees' skill terms can be represented by a document-term matrix.
- The LDA output represents each employee with a probability distribution over a set of the latent skill topics, where each skill topic is represented by a probability distribution over all 15,998 skill terms.
- The input parameter for the LDA algorithm is the number of different latent topics.
- The in-sample perplexity is calculated for each number of topics, and the number of skill topics is set as six because it provides semantically meaningful topics.
- Each of the six skill topics is represented with a distribution over the 15,998 skill terms.
- Top-10 skill terms with the highest probabilities for each of the six skill topics are used to name each skill topic category (Figure 5).
- The LDA algorithm assigns a probability distribution over the six skill topics for each of the 86,030 employees.
- Each firm *d* in a year is represented by a vector θ_d of size six, where each element of the vector is the sum of its employees' probabilities for that skill topic.
- The interfirm similarity based on the skill topic distribution of their labor is calculated as
    - *sim(a, b) = (θa · θb) / (||θa|| · ||θb||)*
- This is called the **skill topic similarity (SkillTopicSim)** between firms.
- The two labor overlap metrics are based on firms' endowments in employee skills and can be measures of input-side or resource-side proximity between a pair of firms.
- These labor overlap metrics extend previous interfirm similarity research that has largely focused on the product-side overlaps.
 
## 3.2. HCF Network Overlap
- Employee migrations between the firms included in the data allow firms to be connected using a network structure.
- The nodes of the network are firms, and the directed edges between firms are weighted by the number of employees who have moved from the source firm to the target firm up to a given year (Figure 2).
- Two firms are connected in the HCF network by the direct employee mobility between them.
- For simplicity, an undirected and unweighted version of the HCF network is used to derive some summary network measures.
- Considering all HCF over time, the network contains 3,467 nodes and 20,171 edges.
- A large connected component exists where 3,465 firms are connected through paths.
- The network has a diameter of nine and an average shortest path of 3.14 edges, showing small-world connectivity.
- The small-world observation resonates with the "boundaryless career" idea, reflecting the boundarylessness of employees' careers in terms of industries [Arthur 2014].
- The dual forces of institutional isomorphism and divergence may explain firms’ willingness to hire from both within and across industries [DiMaggio and Powell 1983, Pant and Sheng 2015, Beckert 2010].
 
### 3.2.1. Industrial Homophily in the HCF Network
- The study expects that firms that are in similar industries are closer on the HCF network, which it calls **industry locality**.
- SIC Code similarity (SICSim) between a pair of firms is defined as shown in Table 3.
- The SIC Code similarity metric is more flexible and informative than using the first two or all the digits of the SIC Code to indicate a firm's industry [Weiner 2005].
- The metric is averaged at various link distances.
- Figure 6 shows the average SIC Code similarity at various link distances on the HCF network.
- Firms that are one link away on the network have significantly higher SIC Code similarity than firms two links away, and they, in turn, have significantly higher SIC Code similarity than those three links away, and so on.
- The HCF network shows industry locality but the existing industrial similarity drops quickly with link distance.
- Despite this weakness, given the popularity of SIC Codes for identifying industries [Witten and Frank 2005], SICSim will serve as an important control variable for the study.
 
### 3.2.2. Upstream and Downstream Similarity
- The network provides complementary and global cues about labor market competition that are not captured by local firm-level variables.
- Metrics that measure the level of HCF network overlap between a pair of firms are proposed.
- An overlap in network neighborhood can signal future employee migrations.
- Weighted and directed HCF networks are used for all subsequent analyses.
- For a firm pair in the network, their similarity in their upstream and downstream firms is calculated.
- **Upstream firms** are firms from which employees have moved in the past to the focal firm.
- **Downstream firms** are firms to which the focal firm's employees have moved in the past.
- Each firm *k* is represented as an upstream vector *u_k* in R^F space, where *F* is the set of all firm nodes in the network.
- Each element *uik* of *u_k* is the number of employees who have moved in the past from firm *i* to firm *k*.
- *u_k* represents the distribution of employees who have migrated to firm *k* over all firms.
- The similarity in knowledge inputs to two firms *a* and *b* is measured by the cosine similarity between the upstream vectors *u_a* and *u_b*, called the **upstream similarity**.
- Like upstream similarity, the **downstream similarity** between a pair of firms can also be computed.
- Each firm *k* is represented as a downstream vector *d_k* in R^F space.
- Each element *dki* of *d_k* is the number of employees who have moved in the past from firm *k* to firm *i*.
- The downstream vector of a firm *k* represents the distribution of its outgoing employees over all firms, representing the different degrees to which other firms value the human capital at a focal firm *k*.
 
### 3.2.3. HCF Network Community
- A network community detection algorithm is applied to find firm communities or clusters in the network.
- An **HCF network community** is a set of firms that have dense connections between them and sparser connections with other communities.
- Firms in the same HCF network community have a greater likelihood of competing in the labor market.
- The modularity maximization algorithm is applied to find network communities [Blondel et al. 2008, Schmutte 2014].
- After identifying the network communities, the study detects whether a firm pair in a given year falls into the same community.
- An indicator variable is created (one if two firms are in the same community, zero otherwise) that records the **network community overlap** for each pair of firms.
- The network features (upstream/downstream similarity and community overlap) are all at the level of a firm pair because the focus is on the labor market competition between two firms.

# 4. Control Metrics
- A competitor analysis assumes that each firm has a unique product market profile and resource (human capital) endowment.
- The competitive relationship between two firms can be illuminated by their overlap in these two dimensions [Chen 1996, Markman et al. 2009].
- A pairwise competitor relationship between firms in the product market is associated with their similarity on the product side [Pant and Sheng 2015].
- A pairwise competitor relationship between firms in the labor market is expected to be associated with their similarity in human capital endowment [Markman et al. 2009].
- Human capital overlap metrics capture employees’ explicit and tacit knowledge.
- A direct indicator of current labor market competition between firms is the employee migrations between those firms [Gardner 2005].
- Proposed human capital overlap metrics are validated by evaluating their ability to predict future labor market competition operationalized using employee migrations.
- Economic control features are included to cue such competition [e.g., firm size, growth, etc.] and have been previously used in the human resource management literature.
- Measures of firms’ product-side similarity are included in terms of their business and industry.
- Three sets of features are calculated for the predictive analysis: basic economic metrics, product overlap metrics, and human capital overlap metrics (Table 4).
- The additional predictive power provided by the proposed human capital overlap metrics over and above the basic economic and product overlap metrics is a clear validation of the proposed human capital overlap metrics.
 
## 4.1. Basic Economic Metrics
- Variables that indicate the current economic state of firms and the previous year’s number of employees moved between firm pairs (HCF lag) provide information on future labor market competition.
- The revenue, revenue growth rate, number of employees, and growth rate in the number of employees are recorded for each firm from Compustat.
- The revenue and size of a firm indicate power and maturity in the labor market, while the corresponding growth indicates a firm’s current dynamism.
- Firm labor market competition can be skewed by firm size, so metrics controlling firm size and growth are important to evaluate additional predictive utility.
- The net HCF (number of incoming employees minus number of outgoing employees) is calculated.
- The inverse HCF (from the target to the source firm) in each year is computed for a source-target firm pair.
- These metrics serve as a set of important control variables.
- Several conventional HR metrics indicate the state of HR at a firm based on education information in public LinkedIn profiles of current employees of the firm [Hom et al. 2017, Wright and McMahan 2011].
- The average number of years current employees have been working after college (undergraduate degree) is computed, considered a proxy for the average age and experience of the employees in a firm.
- The percentage of employees with a postgraduate degree and the average rank of employees’ undergraduate universities are computed.
- These are commonly used human capital measures that provide signals on the level and quality of education among a firm’s employees [Nyberg et al. 2014].
- The economic control variables are summarized in panel A of Table 4.
 
## 4.2. Product Overlap Metrics
- Metrics are calculated that represent two firms’ overlap in terms of their products, motivated by the firm competitor analysis framework [Markman et al. 2009].
- Product overlap is unlikely to capture all the HCF between firms [Peteraf and Bergen 2003], but provides important control metrics.
- The SIC Code of firms is used because SIC Codes are reflective of the product side of firms.
- The SIC Code similarity (SICSim) measure between a pair of firms in Section 3.2 is used, providing a metric of product-side overlap between firms based on their industrial sectors.
- One limitation of SIC code is that the SIC Code of a firm may not reflect all the different and granular product spaces in which the firm operates.
- The LDA algorithm is applied to detect latent business topics from textual business descriptions of firms [Shi et al. 2016].
- Firm pair (cosine) similarity in their business topics (BusdescTopicSim) and also the (cosine) similarity in the text terms in their business summaries (BusdescTermSim) are calculated.
- Panel B of Table 4 summarizes the firm pair product market overlap measures.
- Panel C of Table 4 shows the proposed labor overlap and HCF network overlap metrics as described in Section 3.
- For prediction analysis, all predictor values are standardized to represent the number of standard deviations from their mean values.

# 5. Predictive Analysis
- Employee mobility between firms is a key reflection of interfirm labor market competition [Gardner 2002, 2005].
- The target variable of interest for the predictive framework is HCF in a given year.
- The study experiments with the proposed set of metrics for the prediction of future labor market competition, operationalized using HCF values.
- An important part of the experimentation methodology is to tease out the utility that skill- and network-based human capital overlap metrics provide above and beyond the control metrics.
 
## 5.1. Data Set Construction
- The unit of analysis for the prediction task is the source-target-year firm pair.
- A panel-type dataset is constructed where source-target firm pairs are tracked over time.
- The time span considered is from 2000 to 2014.
- Variables from year *t* − 1 (including HCF lag) are used to predict the labor market competition outcome variable in year *t*.
- Each observation represents the HCF outcome in year *t* with all predictor variables in the year *t* − 1.
- Only firm pairs with previously observed HCF between them are included, starting tracking firm pairs since their first observed HCF.
- A total of 84,733 source-target-year firm pair instances (excluding observations with missing predictor values) are included in the analysis.
- The data construction procedure allows the incremental addition of new source-target firm pairs into the data set over time.
- These first-time firm pairs can be considered as new labor market competitors.
 
## 5.2. Outcome Variable
- The outcome variable of interest is an indicator of interfirm labor market competition.
- Numeric HCF values are transformed into a binary interfirm labor market competition indicator (*Y*) depending on whether the HCF value between the source and target firms meets a threshold δ:
    - *Y = 0, if HCF < δ*
    - *Y = 1, if HCF ≥ δ*
- The study focuses on a classification problem (classifying firm pairs into competitors or non-competitors).
- The binary classification is consistent with academic literature and practice where firms are either seen as competitors or not [Peteraf and Bergen 2003].
- For all firm pairs with a positive HCF value in the data, 88.8% have HCF equal to one.
- A binary outcome variable for HCF with δ = 1 is reasonable in itself.
- Results are reported when δ = 2 is used to dichotomize the outcome variable, indicating a stronger labor market competition relationship between two firms.
- The two different δ values constitute two different definitions of labor market competition.
 
## 5.3. Predictive Models
- Using the metrics constructed in the preceding section and as summarized in Table 4, the predictive models and corresponding results are described.
- Observations from the years 2000 to 2012 are used for training the predictive models, with observations in 2011 and 2012 as the validation set for hyperparameter tuning.
- Predictions from the different models are evaluated for the observations in 2013 and 2014 (see Table 5 for a summary).
- Popular machine learning methods such as K-nearest neighbors (KNNs), regularized logistic regression, support vector machines (SVMs), and classification and regression tree (CART) are included as baseline models for prediction.
- Ensemble methods including bootstrap aggregation or bagging of logistic regression (Bag(LR)) and bagging of support vector machines (Bag(SVMs)), as well as a tree-based ensemble method, random forest (Breiman 2001), are experimented with.
- Deep learning methods such as multilayer perceptron (MLP) and convolutional neural network (CNN) are included as predictive models.
- Each of these methods is tuned by experimenting with various hyperparameter values using the validation data.
- The results presented are based on the best variation of each method identified through hyperparameter tuning and evaluated on held-out test data from the years 2013 and 2014.
 
## 5.4. Prediction Results
- Three types of metrics are considered as predictors: economic, product overlap, and human capital overlap.
- For human capital overlap, the skill-based labor overlap and the HCF network overlap are the focus of the study.
- The other economic and product overlap metrics serve as important control metrics.
- Models are evaluated first with just the economic metrics, followed by incrementally adding product overlap, labor overlap, and HCF network overlap to the predictor set.
- Table 6 shows the predictive performance of various machine learning methods (columns) with different sets of predictors (rows) in terms of the area under the receiver operating characteristics (ROC) curve (AUC).
- AUC is one of the most popular metrics for measuring the performance of classifiers, equivalent to the probability that a randomly chosen positive sample (competitors) will be ranked higher than a randomly chosen negative sample (noncompetitors) [Provost and Fawcett 2001, Fawcett 2004].
- For each model, the prediction experiment is repeated five times, where each time a random 80% of the training data is used to build a model.
- The variation of a model’s performances is evaluated by calculating the standard error of its five performances on the test set.
- The performance improvement of a given model after incrementally adding features is tested for statistical significance with the one-tailed, two-sample t-test.
- The prediction experiment is performed twice with the δ in Equation (4) set to one (all identified labor market competitors) and two (strong labor market competitors), respectively, with all the models being used for the classification problem.
- Main focus: looking at one algorithm at a time. Given an algorithm, the predictive performance improvements are examined after adding the proposed metrics.
- The predictive utility of proposed metrics across different state-of-the-art algorithms is understood.
 
### 5.4.1. Key Observations
- Models including the labor overlap metrics (SkillTermSim and SkillTopicSim) outperform models without them for δ = 1 (panel A).
- Except for CNN, the differences in AUC values are all statistically significant (the addition of proposed labor overlap metrics is generally helpful for predicting all competitors across models).
- The predictive utility of skill-based labor overlap metrics is also strong for δ = 2 (strong competitors).
- For almost all models, except CNN and Bag(SVM), a sizable and statistically significant increase in AUC is observed when the labor overlap metrics are included in addition to the control metrics (economic + product overlap).
- The results highlight the predictive utility of labor overlap metrics in their ability to capture all as well as just the strong labor market competitors.
- The predictive utility of HCF network overlap metrics is also clear from Table 6.
- Large and statistically significant improvements in AUC are seen when the HCF network overlap metrics are included for all models in panel A.
- The improvement in performance can range between 18% (Bag(LR)) and 34% (Bag(SVM)) depending on the model.
- When only the strong labor market competitors are considered, as shown in panel B, the additional predictive power provided by the network overlap is not as large.
- When the labor competition between a pair of firms is strong, the similarity in explicit skills can provide most of the predictive utility beyond the control metrics. The tacit knowledge captured by the network overlap diminishes in terms of additional utility.
- This is in contrast to panel A (δ = 1), which includes all labor market competitors, where tacit knowledge captured through network overlap provides greater additional utility.
- Strong labor market competitors are likely using similar technologies and hence require similar explicit knowledge inputs from labor.
- The predictive role of overlap in tacit skills (i.e., network overlap) diminishes for strong competitors after the similarity in explicit skills (i.e., labor overlap) is known.
- The best-performing models for δ = 1 and 2 are highlighted in boldface in the result tables.
- Overall, the best-performing model is the ensemble-based random forest (RF) using all four types of predictors.
- This is consistent for both values of δ. In particular, for δ = 1, it achieves an AUC of 0.89.
- This model can significantly outperform a random baseline classifier and correctly identify a competitor pair over a noncompetitor pair in the test set with a probability of 0.89.
- The RF with full feature set is also the most effective in predicting strong labor market competitors, as shown in panel B of Table 6.
- By using carefully crafted human capital overlap metrics derived from employee skills and the labor supply network, along with RF models, strong predictive performance can be provided for identifying future labor market competitors.
 
### 5.4.2. Dynamic Nature of Firm Labor Market Competition
- Given the dynamic nature of firm labor market competition, the predictive performance of the models is evaluated separately on just the new labor market competitors that are previously unknown.
- A set of firm pairs that exists only in the test set because their first HCF appears in 2013 or 2014.
- Such firm pairs are not included in the training data, and they all have positive HCF values only in the test set.
- There are 3,062 and 150 such new firm pairs in the test set with their first observed HCF ≥ 1 and 2, respectively.
- To evaluate the predictive performances on these new firm pairs, the same set of models is used as in Table 6.
- The output of these models is the probability of a firm pair being labor market competitors.
- Test firm pairs are classified to be labor market competitors if the output probability from the model exceeds the prior probability (of labor market competitors) in the training data (0.231 for δ = 1 and 0.020 for δ = 2, as shown in Table 5).
- The proportion of new firm pairs that are correctly identified as labor market competitors is reported in Table 7.
 
### 5.4.3. New Future Competitors
- Table 7 again observes the greatly improved performance of predicting new labor market competitors when human capital overlap metrics are included in addition to the control metrics.
- Specifically, the network overlap metrics provide strong predictive utility in identifying new firm competitor pairs for both δ = 1 and 2.
- When weaker competitors are included (i.e., δ = 1), a 200%–300% better performance is seen for several machine learning models that use of the network overlap metrics on top of control metrics and labor overlap metrics.
- When all four types of metrics are used (economic + product + labor + network), the RF model is able to correctly identify 96.4% of the new competitors.
- It is more challenging to predict new strong competitors (i.e., δ = 2), but Bag(SVM) is able to capture, on average, 81.7% of the new strong competitors.
- The evidence clearly indicates the utility of the proposed human capital metrics for predicting labor market competitors, including previously unseen ones.
- The analysis was replicated using another set of data collected in 2016, and the main results in terms of the predictive utility of the proposed metrics remain the same (see Online Appendix G).

# 6. Discussion
- The study illustrates how the proposed labor market overlap metrics and resulting predictive models can be incorporated into the 2D competitor analysis [Chen 1996, Peteraf and Bergen 2003, Markman et al. 2009], as shown in Figure 1, and support business intelligence-gathering efforts.
- To measure a firm pair’s product market overlap, the average of their BusdescTermSim and BusdescTopicSim is taken.
- To measure their labor overlap, the average of a firm pair’s SkillTermSim and SkillTopicSim is taken.
- These skill-based labor overlap metrics are a direct measure of the similarity in the explicit knowledge endowment of two firms and are consistent with the measure on the y-axis in terms of firms’ similarity in the product market that is based on the textual descriptions of firms.
- In this section, only the predictions based on the RF model with the full feature set are considered for analysis.
 
## 6.1. 2D Competitor Analysis
- For ease of visualization, Figure 7 plots only the strong competitors (i.e., δ = 2), and the size of the dot indicates the size of the HCF between them.
- The dashed lines plot the median values of the product overlap and the labor overlap given all firm pairs in the test set.
- The median lines divide the plot into four quadrants, as initially conceptualized in Figure 1.
- Most of the labor market competition appears in the upper-right quadrant, which corresponds to direct competitors that have a large overlap in both product and labor dimensions.
- When the RF model’s prediction performances (δ = 2) are considered separately for the four quadrants, the model’s AUC is 0.839 for the upper-right quadrant (direct competitors), 0.775 for the upper-left quadrant (indirect competitors), 0.718 for the lower-left quadrant (weak competitors), and 0.765 for the lower-right quadrant (potential competitors).
- The best performances are achieved among firm pairs that are direct competitors and hence present various cues to their competition through product and human capital overlap.
- The prediction performance is lowest (although still reasonable) for weak competitors, where firm pairs are expected to have smaller overlap in both outputs (products) and inputs (human capital).
- The varying levels of AUC for different quadrants inform users about the confidence they can have on the predictions based on where the unseen firm pairs of interest (to the user) fall.
- The visualization using the product and labor overlap metrics, as shown in Figure 7, also allows users to identify interesting exceptions.
- For example, there is a large HCF = 17 from Hewlett-Packard (HP) to General Motor (GM) in 2013, which is the large dot in the lower-left quadrant (weak competitors) of Figure 7.
- This is a firm pair with a low product overlap of 0.08 and a low labor overlap of 0.26.
- The overall best-performing model, RF, correctly predicts the firm pair as labor market competitors (δ = 2) using prior probability from training data as a threshold.
- This observed large HCF probably reflects GM’s agreement with HP to hire up to 3,000 HP employees already working on GM’s business starting in 2012.
- GM is a weak competitor to HP.
- The departure of employees to GM can be beneficial to HP, facilitating the creation and strengthening of further business relationships between HP and GM, and making future interorganizational endeavors more efficient [Somaya et al. 2008].
- For HP, the implications and strategic responses will be different between losing employees to a weak competitor (GM) versus a direct competitor (Apple or Dell).
- The metrics and prediction framework proposed in this paper can enrich firm competitor analysis and provide insights that may be largely ignored if competitor analysis is performed in a context devoid of the four quadrants in Figure 1.
- Depending on the quadrant where a competitor pair is predicted or identified, a firm may have vastly different strategies for acting on the discovery.
- The 2D competition analysis can be performed for each industry separately, providing a more nuanced picture of the industry-specific competitor landscape.
- Figure 8 shows such analysis.
- The left panel in Figure 8 shows a subset of firm pairs where the source firm is a drug (pharmaceutical) firm (SIC Code staring in 283), with the RF model achieving an AUC of 0.915.
- The labor market competitors faced by pharmaceutical firms are mostly other firms with high product and labor market overlap simultaneously because most firm pairs are in the upper-right quadrant of the subplot.
- The right panel of Figure 8 shows a different competitive landscape for the source firms that are in the industry of computer-related services (SIC Code starting in 737; e.g., Google, IBM, and Microsoft), with the RF model achieving an AUC of 0.882.
- Firm pairs with both high product and labor market overlap also have higher levels of HCF.
- The labor market competitors occupy a different subspace of the 2D competition plot than those from the pharmaceutical industry.
- The diverse target firms for the HCF from firms in the computer-related services industry are a reflection of the more general applicability of computer-related knowledge than the pharmaceutical-related knowledge to other industries [Joseph et al. 2012].
- The industry-level analysis presents an example of how a competitor analysis incorporating both product and labor market dimensions can provide a broader view of human capital development in different industries.
- The varying levels of AUC across different industries indicate the varying levels of confidence that the user can have on the predictive model given an industry.
 
## 6.2. Managerial Implications
- An important managerial implication is in identifying and improving awareness of a firm’s competitors for human capital [Chen et al. 2007].
- Competitor identification and awareness are important because they are necessary precursors to competitor analysis and strategy [Pant and Sheng 2015].
- Previous literature has well documented that managers may have "myopia" or "blind spots" when it comes to identifying a firm’s product market competitors [Zajac and Bazerman 1991, Walker et al. 2005, Pant and Sheng 2015].
- Such myopia may be worse for identifying a firms’ competitors in the labor market.
- Because the labor market competitor prediction includes a large number of firms across industries, it can help improve managers’ awareness of their firm’s labor market competitors, especially from distant product markets.
- After a pair of firms is identified as future labor market competitors, the 2D competitor analysis framework can help managers make strategic decisions beyond HR, such as product development and customer relationship management [Peteraf and Bergen 2003, Somaya and Williamson 2008, Markman et al. 2009].
- Proposed skill-based labor overlap metrics serve as the measurement for the horizontal axis.
- Proposed HCF prediction and 2D competitor analysis can provide a global view of employee mobility across firms.
- The knowledge of which quadrant (see Figure 1) the future labor market competitors fall into allows a manager or analyst to apply different strategies for different labor market competitors.
- The methodology uses only publicly available data and can be implemented with relative ease.
 
## 6.3. Academic Implications
- This study contributes to the academic literature.
- Theoretical/qualitative studies exist regarding how, why, and the conditions under which firms compete for human capital and its relationship with firm product market competition [Chen 1996, Peteraf and Bergen 2003, Markman et al. 2009].
- There have been few empirical or predictive analytics studies on firm labor market competition, especially for a large number of firms.
- New metrics of firm human capital endowment and overlap are proposed and validated by evaluating their effectiveness in predicting the firm’s future labor market competitors.
- Proposed human capital endowment distributions (e.g., skill vectors, upstream and downstream vectors) and corresponding overlap measures provide a rich set of quantitative representations to test implications of the resource-based view of firms.
- With the proposed human capital overlap metrics and the prediction of HCF, the 2D competition analysis described by previous qualitative studies can be operationalized with significant potential for practical utility [Markman et al. 2009].
- The operationalization of the 2D competition analysis makes possible future empirical studies that use the relative positioning of firm pairs in different quadrants.
- Such investigations can advance understanding of firm competitive dynamics with a multimarket perspective that goes beyond existing qualitative notions [Markman et al. 2009].

# 7. Conclusion
- The study focuses on the prediction of future labor market competition.
- A paucity of empirical work exists even on the contemporaneous identification of labor market competitors in previous literature.
- The study is not just the first in terms of addressing the problem of predicting future labor market competitors, but it is also a first in terms of suggesting metrics of human capital overlap that can help in even contemporaneous identification of labor market competition.
- The notions of (human) resource bundles are operationalized, as suggested by RBV, by viewing them as distributions over explicit and tacit knowledge.
- Such operationalization of firm-level human capital and consequent overlap metrics has implications for the general area of strategic human capital and HR management literature beyond the prediction problem addressed here.
- The study uses the public profiles of more than 89,000 employees and constructs various metrics for the human capital overlap between firms.
- The granular individual-level skill terms allow skill-based labor overlap measures to be created that indicate the interfirm similarity in their explicit knowledge base.
- Because the HCF between firms reflects the flow of both explicit and tacit knowledge, the resulting network structure is leveraged to measure more global metrics of human capital overlap.
- Proposed labor- (skill) and network-based human capital overlap metrics are critical to good predictive performance for the prediction of future labor market competitors.
- By proposing human capital overlap metrics and using them for predicting future labor market competitors, the paper fulfills the long-existing need for a comprehensive firm competitor analysis with both the product and labor dimensions of interfirm overlap [Chen 1996, Peteraf and Bergen 2003].
- The nuanced analysis that the two dimensions allow is discussed while focusing on labor market competition.
- Applying data analytics for talent acquisition and retention has been identified as one of the most urgent challenges facing HR leaders around the world.
- The HCF prediction task in the study can help narrow a capability gap between the urgency and readiness of data-driven HR management.
- The prediction framework can be used to form the basis of a targeted recruitment strategy.
- The predictive models can provide a list of firms as targets for future hiring, increasing the rate of successful hires and reducing the cost of hiring.
- Predictive models can also identify a set of firms that may target a particular firm for hiring its employees.
- Such information can be of strategic value for a firm for designing a more effective talent retention program and tracking where employees may be leaving for can be crucial for a company’s future strategic development [Somaya and Williamson 2008].
- The knowledge of which quadrant (see Figure 1) the future labor market competitors fall into allows a manager or analyst to apply different strategies for different labor market competitors.
- The methodology uses only publicly available data and can be implemented with relative ease.
 
## 7.1. Future Directions
- The data is seeded with employees of S&P 100 firms in 2015 and then extended to encompass other firms (3,467 firms in total) in which they were previously employed.
- The ranking of Yahoo BOSS may be biased toward more popular and potentially higher-valued employees within a firm.
- The analysis focuses on the movement of more-valued human capital than employees of a random set of firms.
- In the future, the analysis can be extended by seeding it with the employees of an even larger set of firms so as to have a greater diversity in terms of the quality of human capital.
- Another future direction is to predict labor market competition at a more granular skill level.
- Proposed human capital overlap metrics may be used for future product market competitor prediction.
- As a result of the relationship between firms’ product market and labor market interactions [Markman et al. 2009, Younge et al. 2015], it is valuable to identify long-run competitors that may manifest many years after talent acquisition.
- The network analysis presented here is at the level of firms, but it can be applied to a context where the nodes are industries (at various levels of granularity) or states (as well as nations) instead of firms.
- The proposed framework can be extended to predicting interindustry, interstate, or even international competition for human capital.
- Web footprints of employees are providing rich information on labor market competition that can provide predictive utility and insights across the economy.

---

# Executive summary of 1. Introduction
- This section introduces the growing intensity of interfirm competition, emphasizing the competition for human capital in the labor market, which is crucial for firm success.
- It discusses the resource-based view (RBV) of the firm, highlighting human capital as a key resource for competitive advantage, particularly in the knowledge economy.
- The section outlines the 2D competitor analysis framework, which considers both product and resource similarities, and positions the study's contribution in addressing the gap in empirically measuring interfirm overlap along the human capital dimension.
- It concludes by explaining the study's aim to propose human capital overlap metrics derived from employee skills and career mobility patterns to predict future labor market competition.

# Executive summary of 2. Data
- This section describes the data used in the study, which is seeded from employees of S&P 100 companies and their LinkedIn profiles.
- It explains the process of obtaining and compiling a longitudinal employer-employee matched dataset from 2000 to 2014, including information on job experiences, education, and skills.
- The section outlines the robustness checks performed to verify the representativeness of the data, such as comparing firm-level statistics with Compustat data and computing Kendall rank correlation between firm sizes.
- It also discusses the characteristics of the data, including the high education level of the workers and their active participation in the labor market.

# Executive summary of 3. Human Capital Overlap Metrics
- This section details the proposed metrics for measuring human capital overlap between firms, which include labor overlap and HCF network overlap.
- It describes the construction of skill-based labor overlap metrics, such as skill term similarity (SkillTermSim) and skill topic similarity (SkillTopicSim), based on the skills possessed by employees.
- It explains how the HCF network is constructed from employee migrations between firms and how it is used to capture broader cues to human capital overlap beyond explicit skills alone.
- The section also discusses the properties of the HCF network, including its small-world connectivity and weak industrial homophily.

# Executive summary of 4. Control Metrics
- This section outlines the control metrics used in the study, which include basic economic metrics and product overlap metrics.
- It describes the economic metrics, such as revenue, revenue growth rate, number of employees, and growth rate in the number of employees, and explains their relevance in predicting labor market competition.
- The section also details the product overlap metrics, including SIC Code similarity (SICSim), business description term similarity (BusdescTermSim), and business description topic similarity (BusdescTopicSim), and explains how they are calculated.
- It concludes by summarizing the three sets of features used in the predictive analysis: basic economic metrics, product overlap metrics, and human capital overlap metrics.

# Executive summary of 5. Predictive Analysis
- This section presents the predictive analysis conducted in the study, which aims to predict future labor market competition using the proposed metrics.
- It describes the data set construction process, including the unit of analysis (source-target-year firm pair) and the time span considered (2000 to 2014).
- The section explains the outcome variable, which is a binary indicator of interfirm labor market competition based on a threshold value (δ) of HCF.
- It outlines the various machine learning methods used for prediction, including KNNs, regularized logistic regression, SVMs, CART, bagging of LR, bagging of SVMs, random forest, MLP, and CNN.
- The section also presents the prediction results, including the AUC values for different models with different sets of predictors, and discusses the key observations from the results.

# Executive summary of 6. Discussion
- This section discusses the implications of the study's findings, focusing on the incorporation of the proposed labor market overlap metrics into the 2D competitor analysis framework.
- It explains how the product and labor overlap metrics are measured and visualized in the 2D competition plot, and discusses the varying levels of AUC for different quadrants.
- The section provides an example of a firm pair (HP and GM) in the lower-left quadrant of the plot, highlighting the strategic insights that can be derived from the analysis.
- It also discusses the managerial implications of the study, such as improving awareness of a firm's competitors for human capital and making strategic decisions beyond HR.
- The section concludes by outlining the academic implications of the study, such as operationalizing the 2D competition analysis and providing a rich set of quantitative representations to test implications of the resource-based view of firms.

# Executive summary of 7. Conclusion
- This section summarizes the key findings and contributions of the study, emphasizing the novelty of the proposed human capital overlap metrics and their effectiveness in predicting future labor market competitors.
- It highlights the operationalization of firm-level human capital and its implications for the general area of strategic human capital and HR management literature.
- The section also discusses the practical applications of the prediction framework, such as forming the basis of a targeted recruitment strategy and designing a more effective talent retention program.
- It concludes by suggesting future directions for research, such as extending the analysis to a more granular skill level, predicting long-run competitors, and predicting interindustry, interstate, or even international competition for human capital.

</details>

# Carlos Torelli
- Department Head 
- Anthony J Petullo Professor of Business Administration
### education
- Ph.D., Business Administration, University of Illinois at Urbana-Champaign, 2007
- M.B.A., Marquette University, 1997
- M.B.E., Simon Bolivar University, 1993
- B.E., Civil Engineering, Andres Bello Catholic University, 1986
### research interest (chatgpt says...)
- global branding, cross-cultural consumer behavior, self-regulation, and persuasion. 
- identifying the key cultural factors that drive consumer behavior in globalized economy
- uncovering the underlying socio-cognitive processes for such culturally-driven behaviors.
### teaching
- Global Marketing, Brand Management Marketing Management

```
title: Value instantiation: how to overcome the value conflict in promoting luxury brands with CSR initiatives
authors: Ji Kyung Park, Carlos J. Torelli, Alokparna (Sonia) Basu Monga, Deborah Roedder John
journal: Marketing Letters
published: 2019
```

# Executive Summary
- **Research Problem:** Negative consumer responses arise when luxury brands (associated with self-enhancement values) incorporate corporate social responsibility (CSR) initiatives (associated with self-transcendence values).
- **Proposed Solution:** "Value Instantiation" - encouraging individuals to generate reasons for a value and pursue it through concrete examples.
- **Methodology:** Two studies using undergraduate students were conducted.
    - **Study 1: Celebrity Philanthropy**
        - *Design:* Three conditions: No CSR, CSR, Value Instantiation + CSR.
        - *Value Instantiation:* Participants read a story about Brad Pitt and Angelina Jolie (successful, wealthy celebrities) engaging in philanthropic activities.
        - *Measurement:* Evaluation of a fictitious luxury brand's product (Bellavista sunglasses) after viewing an ad reflecting their assigned condition.
        - *Key Finding:* The negative impact of CSR on product evaluation among those high in self-enhancement was mitigated by exposure to the celebrity philanthropy example.
    - **Study 2: Self-Visualization**
        - *Design:* 2x2 between-subjects design: Pursuit of Self-Enhancement (High vs. Low) x Value Instantiation (Yes vs. No).
        - *Self-Enhancement Manipulation:* Participants viewed fictitious average value rankings of their peers.
        - *Value Instantiation:* CSR ad for "Cavalli" sunglasses (standard vs. self-referencing - encouraging participants to imagine *themselves* engaging in the philanthropic action).
        - *Measurement:* Product evaluation of Cavalli sunglasses.
        - *Key Finding:* High self-enhancement participants evaluated the luxury brand more favorably when exposed to the self-referencing value instantiation CSR ad.
- **Theoretical Framework:**
    - **Schwartz's Value Theory:** Values are organized in a circular structure; opposing values (self-enhancement and self-transcendence) are incompatible, leading to unease when brands promote both.
    - **Value Instantiation:** Providing concrete examples of value-expressive behavior strengthens the perceived importance and feasibility of the value. This paper extends the concept to instantiating *incompatible* values.
- **Key Findings:**
    - Value instantiation reduces unfavorable responses to luxury brands using CSR appeals.
    - This effect is strongest among core luxury brand consumers driven by self-enhancement values.
    - Both celebrity endorsement and self-visualization were effective methods for value instantiation.
- **Contributions:**
    - Strategic guidance for luxury brands using CSR.
    - Extended understanding of how values influence consumer judgments.

#luxury_brands #csr #value_incompatibility #value_instantiation #self_enhancement #self_transcendence #schwartz_value_theory #consumer_behavior #marketing_strategy #celebrity_endorsement #visualization #integrative_thinking

<details>
    
  <summary>Click to expand sections</summary>


# 1. Introduction

-  Value-based marketing, focusing on consumers’ high-level values such as improving communities and society, has become a notable trend [Kotler et al., 2010].
    -   Luxury brands are increasingly incorporating **Corporate Social Responsibility (CSR)** activities, such as philanthropy, environmental sustainability, and ethical business practices.
    -   A 2018 report indicated that 85% of the top global luxury brands are involved in socially responsible activities.
    -   For example, Giorgio Armani's Acqua for Life campaign supports UNICEF by providing clean drinking water [since 2010].
    -   Salvatore Ferragamo launched a limited-edition shoe addressing sustainability.
    -   Tiffany & Co commits to the responsible sourcing of diamonds.
-  When luxury brands promote CSR, they blend opposing values into their marketing strategies.
    -   Luxury brands embody **self-enhancement values** (self-interest, power, achievement), while CSR initiatives activate **self-transcendence values** (interests of others, benevolence, welfare).
    -   These values are considered incompatible [Schwartz, 1992].
    -   This incompatibility can lead to consumer unease and unfavorable responses to the brand [Torelli et al., 2012].
-  I propose a solution to offset the **value incompatibility** that luxury brands face: **value instantiation**.
    -   *Value instantiation* encourages people to **generate reasons for a value** and **pursue it through value-expressive behavior** [Maio et al., 2001].
    -   I propose that exposure to examples of **engaging in philanthropic activities while pursuing self-enhancement** will encourage consumers to consider that the two values can be pursued simultaneously.
    -   This **value integration** will reduce negative reactions to a luxury brand's CSR, especially among core consumers.
- My findings provide strategic guidance for luxury brands wanting to incorporate CSR, addressing a gap in research on overcoming negative effects of value incompatibility.
    -   I identify **value instantiation** as a solution.
    -   I introduce two approaches: (1) **exposing consumers to the philanthropic activities of self-enhancement-driven celebrities** and (2) **encouraging consumers to visualize themselves engaging in philanthropic activities while pursuing self-enhancement values**.
-  This research extends the understanding of how values are applied to consumer judgments.
    -   Prior work showed that generating reasons for a single value fosters value-expressive attitudes but suppresses incompatible values [Maio et al., 2009].
    -   I show that instantiating an incompatible value (self-transcendence) in the context of a focal value (self-enhancement) can promote the simultaneous pursuit of both values *without conflict*.

# 2. Conceptual background

## 2.1. Luxury–CSR value incompatibility

-   Values are abstract goals that serve as guiding principles [Schwartz, 1992].
    -   Values often conflict with one another.
-   Schwartz's model of values proposes four higher-order value types: **self-enhancement, openness to change, self-transcendence, and conservation**.
    -   These values are arranged in a circular structure.
    -   Adjacent values are compatible, while opposite values are incompatible [Schwartz, 1992].
    -   The model was supported in samples from 67 nations [Schwartz and Rubel, 2005].
    -   Values sharing the same row/column are compatible (e.g., self-transcendence and conservation), while values in diagonals are incompatible (e.g., self-enhancement and self-transcendence).
-   Luxury brands embody **self-enhancement values**, while CSR elicits **self-transcendence values**.
    -   Promoting these incompatible values simultaneously leads to consumer unease and unfavorable brand evaluations [Torelli et al., 2012].
    -   Despite prior research showing CSR appeals are detrimental to luxury brands, no research addresses how to overcome these effects.

## 2.2. Countering negative reactions to luxury–CSR value incompatibility

- I propose that instantiating self-transcendence values in the context of self-enhancement via a concrete example would offset the harmful effects of value conflict.
    -  I suggest that examples of engaging in philanthropic activities while pursuing self-enhancement encourage people to integrate these values.
    -  This encourages consideration that self-transcendence values can be pursued simultaneously without inhibiting self-enhancement.
    -  This reduces negative reactions to luxury brands promoting CSR.
-  Research by Maio et al. supports this.
    -   Giving individuals an opportunity to generate reasons for a value via examples of value-expressive behavior fosters value-expressive attitudes and behaviors [Maio and Olson, 1998, 2001].
    -   For instance, contemplating examples of equality leads to more egalitarian behaviors [Maio et al., 2001].
    -   These concrete examples provide cognitive support for the value, making it a more rational guide for behavior [Maio et al., 2001].
-  I investigate whether instantiating an incompatible value in the context of a focal value promotes the simultaneous pursuit of both values.
-   Although values have varying compatibility, all are intrinsically desirable [Schwartz, 1992].
    -   People perceive themselves as multifaceted, possessing opposite characteristics [Sande et al., 1988].
    -   Pursuing a focal value inhibits the pursuit of an incompatible value [Maio et al., 2009].
    -   The tension can be resolved through integrative thinking [Tetlock, 1986].
    -   Exposure to multiple cultures leads to developing integrative schemas [Tadmor and Tetlock, 2006].
-   Value instantiation encourages effortful integration of incompatible values, promoting their simultaneous pursuit.

## 2.3. Consumers’ self-enhancement value priority as a moderating factor

-   Luxury brands signal status and are sought by consumers pursuing self-enhancement [Mandel et al., 2006; Rucker and Galinsky, 2008].
    -   These consumers evaluate luxury brands based on their ability to fulfill self-enhancement values [Rucker and Galinsky, 2008; Torelli and Shavitt, 2010].
-   I predict that CSR activities of luxury brands (promoting self-transcendence) will be negatively received by those prioritizing self-enhancement.
    -   However, value instantiation, achieved through concrete examples, will attenuate these negative responses.
    -   *H1*: This instantiation encourages consumers to consider that self-transcendence values can be pursued simultaneously with self-enhancement.

## 2.4. Overview of empirical studies

-   I used two different approaches for **value instantiation** to test my predictions:
    -   Study 1: exposing participants to **philanthropic activities of self-enhancement-driven celebrities**.
    -   Study 2: encouraging participants to **visualize themselves engaging in philanthropic activities while pursuing self-enhancement values**.
-   Across the studies, I found that instantiating self-transcendence values in the context of self-enhancement value pursuit is effective in offsetting harmful effects of incorporating CSR appeals into luxury brands.

# 3. Study 1: Value instantiation through philanthropic activities of self-enhancement-driven celebrities

## 3.1. Sample and procedure

-   Two hundred and three undergraduate students were randomly assigned to one of three conditions: (1) no CSR, (2) CSR, and (3) value instantiation + CSR.
-   Participants rated the personal importance of self-enhancement values (eight items from Schwartz’s value survey [Schwartz, 1992]).
-   In the **value instantiation + CSR** condition, participants read a story about Brad Pitt and Angelina Jolie engaging in philanthropic activities while pursuing self-enhancement values.
    -   The story emphasized their success and wealth and highlighted their socially responsible actions.
-   Participants in the no value instantiation conditions read a neutral story.
-   All participants were presented with an ad for a fictitious luxury brand (Bellavista sunglasses).
    -   The ad described the luxurious image of the sunglasses.
    -   In the no CSR condition, the ad further promoted a luxurious image.
    -   In the CSR condition and the value instantiation + CSR condition, the ad promoted a socially responsible image.
-   Participants evaluated the Bellavista sunglasses on a seven-point scale.

## 3.2. Results

### 3.2.1. Product evaluations

-   I conducted a multiple regression analysis with product evaluation as the dependent variable, the pursuit of self-enhancement values, the experimental condition, and the interaction between the pursuit of self-enhancement values and the experimental condition as the independent variables.
-   Significant interactions emerged.
    -  There was a significant interaction between the pursuit of self-enhancement values and the first dummy variable (no CSR = 1, CSR = 0), β = 0.76, t(197) = 3.15, p < 0.01.
    -   There was a significant interaction between the pursuit of self-enhancement values and the second dummy variable (value instantiation + CSR = 1, CSR = 0), β = 0.48, t(197) = 1.99, p = 0.05.
-   Simple slope tests were conducted in each condition [West et al., 1996].
    -   In the no CSR condition, participants with a high pursuit of self-enhancement values evaluated Bellavista sunglasses more favorably, β = 0.35, t(197) = 2.21, p = 0.03.
        -   Thus, the luxury brand’s sunglasses were more appealing to core consumers.
    -   In the CSR condition, participants with a high pursuit of self-enhancement values evaluated Bellavista sunglasses with CSR appeals less favorably, β = − 0.41, t(197) = 2.26, p = 0.03.
    -   In the value instantiation + CSR condition, high and low pursuit participants responded equally favorably, β = 0.06, t(197) < 1, p > 0.25.
-   Participants with a high pursuit of self-enhancement values evaluated the Bellavista sunglasses less favorably in the CSR condition than in the no CSR condition.
    -   This was confirmed by a positive significant relationship between the first dummy variable (no CSR = 1, CSR = 0) and product evaluations, β = 0.91, t(197) = 3.74, p < 0.001.
-   However, they evaluated the Bellavista sunglasses more favorably in the value instantiation + CSR condition than in the CSR condition.
    -   This was confirmed by a positive significant relationship between the second dummy variable (value instantiation + CSR = 1, CSR = 0) and product evaluations, β = 0.51, t(197) = 2.03, p = 0.04.
-   Participants with a low pursuit of self-enhancement values were not influenced by the experimental condition.

## 3.3. Discussion

-   Study 1 confirmed that negative reactions to a CSR message from a luxury brand dissipated when self-transcendence values were instantiated in the context of self-enhancement value pursuit through a concrete example.
    -   Participants with a high pursuit of self-enhancement values evaluated a luxury product with a CSR appeal less favorably.
    -   Their unfavorable response was reduced upon reading about the philanthropic activities of self-enhancement-driven celebrities.
-   I extended the findings in study 2:
    -  I manipulated the pursuit of self-enhancement values to rule out extraneous factors.
    -  I explored a different approach for value instantiation.
        -   I encouraged consumers to visualize themselves engaging in philanthropic activities while pursuing self-enhancement values.
-   Self-referencing facilitates consumers to imagine and visualize their consumption of the brand [Krishnamurthy and Sujan, 1999].
    -  Self-referencing incorporated into a CSR ad urges consumers to visualize themselves engaging in socially responsible actions, encouraging integrative thinking.
    -  *H2*: This would reduce unfavorable evaluations of the luxury brand's product, especially among those motivated to pursue self-enhancement values.

# 4. Study 2: Value instantiation through self-visualization of pursuing two incompatible values

## 4.1. Sample and procedure

-   One hundred and twenty-four undergraduate students were randomly assigned to conditions in a 2 (pursuit of self-enhancement values manipulation: high, low) × 2 (value instantiation: yes, no) between-subjects design.
-   Participants received the pursuit of self-enhancement value manipulation [Maio et al., 2009].
    -   Participants were shown 16 values and asked to carefully read fictitious average value rankings of people aged 18 to 24.
    -   In the high pursuit condition, the four self-enhancement values were ranked from 1 to 4 (M = 2.5).
    -   In the low pursuit condition, the four self-enhancement values were ranked from 13 to 16 (M = 14.5).
    -   Participants identified the four most highly ranked (least important) values and wrote the names of the values.
    -   Participants read an explanation about the four preferred (four least preferred) self-enhancement values of the reference group.
    -   Participants were asked to write their own explanation about why people emphasized the four self-enhancement values more (less).
-   Participants were presented with the same CSR ad used in study 1, but referring to an actual luxury brand “Cavalli.”
    -   In the no value instantiation condition, the message was written in the third person.
    -   In the value instantiation condition, the same message was addressed directly to encourage participants to imagine and visualize themselves engaging in a philanthropic action.
-   Participants evaluated the Cavalli sunglasses on the same 7-point scale.

## 4.2. Results

### 4.2.1. Manipulation check

-   In a pretest, 63 participants were exposed to the pursuit of self-enhancement values manipulation.
-   The importance of self-enhancement values was higher in the high pursuit of self-enhancement value condition than in the low pursuit of self-enhancement value condition (MHigh = 5.40, MLow = 4.47; t(61) = 2.73, p = 0.01).

### 4.2.2. Product evaluations

-   An ANOVA yielded a significant pursuit of self-enhancement value manipulation × value instantiation interaction, F(1, 120) = 4.61, p = 0.03.
    -   In the no value instantiation condition, participants in the high pursuit condition evaluated Cavalli sunglasses less favorably than those in the low pursuit condition (M = 4.19, SD = 1.12 versus M = 4.83, SD = 1.02), F(1, 120) = 5.17, p = 0.03.
    -   No differences emerged in the value instantiation condition (M = 4.77, SD = 1.17 versus M = 4.55, SD = 1.15), F(1, 120) < 1, p > 0.25.
-   Participants in the high pursuit condition evaluated Cavalli sunglasses more favorably in the value instantiation condition than in the no value instantiation condition (M = 4.77, SD = 1.17 versus M = 4.19, SD = 1.12), F(1, 120) = 4.37, p = 0.04.
-   This was not the case among participants in the low pursuit condition.

## 4.3. Discussion

-   I replicated the results from study 1 by manipulating the pursuit of self-enhancement values and using a different value instantiation approach.
    -   After reading the CSR ad of a luxury brand, participants primed with high pursuit of self-enhancement values evaluated the luxury brand’s product less favorably.
    -   This negative response dissipated when these consumers processed the CSR ad in relation to the self.
-   Instantiating self-transcendence values in the context of self-enhancement values through an example (whether consumers externally observe another person’s philanthropic behavior as in study 1 or internally visualize one’s own philanthropic behavior as in study 2) is effective in offsetting the detrimental effects of incorporating the CSR appeals into the luxury brands.

# 5. General discussion

-   My research provides solutions for luxury brands that promote CSR appeals in their advertising and marketing programs.
    -   I identify “value instantiation” via an example of engaging in philanthropic activities while pursuing self-enhancement values as a solution.
    -   Exposure to such an example (which encourages people to engage in integrative thinking in which self-transcendence values can be pursued simultaneously without inhibiting their pursuit of self-enhancement values) is effective in reducing unfavorable responses to a luxury brand’s product promoted with CSR appeals, particularly among the core consumer segment of luxury brands, who strongly pursue self-enhancement values.

## 5.1. Contributions to luxury brand management

-   Luxury brands are increasingly engaging in CSR, but such initiatives have been found to backfire due to value incompatibility [Achabou and Dekhili, 2013; Janssen et al., 2017; Torelli et al., 2012].
-   My research identifies value instantiation as a solution and introduces two different approaches for value instantiation that luxury brand managers can employ:
    -   (1) exposing consumers to celebrities pursuing the two values together (study 1) and (2) encouraging consumers to visualize themselves pursuing the two values simultaneously (study 2).
-   Luxury brand managers could promote CSR activities of their brands after value instantiation by demonstrating the philanthropic life of successful celebrities.
-   Luxury brand managers could employ creative approaches to encourage consumers to visualize themselves engaging in philanthropic activities while pursuing luxury brands by using self-referencing in their TV or print ads or using VR.

## 5.2. Contributions to value research

-   I extend prior value research by demonstrating that “value instantiation” is a particularly effective approach in diminishing negative reactions to incompatible values.
-   Prior research has shown that instantiating a focal value provides cognitive support for the value, suppressing the pursuit of an incompatible value [Maio et al., 2001, 2009].
-   I show that instantiating an incompatible value in the context of focal value pursuit via a concrete example encourages people to integrate the two incompatible values and to apply them simultaneously to subsequent value-expressive judgment.
-   The consequences of value instantiation can be extended to two values, even when the two values are incompatible with each other.

## 5.3. Future research directions

-   Future research could examine a wide range of luxury brands to assess the level of perceived value incompatibility between CSR and luxury brands.
-   It would be interesting to examine whether the effect of value instantiation would vary, depending on consumers’ inferences about another person’s prosocial behavior.
-   We might ask whether adding value complexity to luxury brands would always increase value incompatibility.
-   Another important direction would be to investigate other contexts of value incompatibility.
-   My findings highlight the importance of bringing value instantiation into consumer research.
-   I provide a new conceptual framework for understanding how consumers’ value pursuit is shaped and how to overcome value incompatibility often experienced by consumers.

---

# Executive summary of 1. Introduction
- The introduction highlights the increasing trend of luxury brands incorporating CSR activities.
-  It identifies the conflict between self-enhancement values (associated with luxury brands) and self-transcendence values (associated with CSR).
- It introduces the concept of value instantiation as a potential solution to overcome this value incompatibility and outlines the research's novel contributions.

# Executive summary of 2. Conceptual background
- This section explains the theoretical framework for understanding value incompatibility, drawing on Schwartz's model of values.
- It argues that luxury brands embodying self-enhancement values clash with CSR initiatives that promote self-transcendence.
- It introduces the concept of value instantiation and explains the rationale for how it can counter the negative effects of value incompatibility by encouraging consumers to integrate seemingly opposing values.

# Executive summary of 3. Study 1: value instantiation through philanthropic activities of self-enhancement-driven celebrities
- This section details the methodology and results of Study 1.
-  It describes how participants were exposed to examples of celebrities engaging in philanthropic activities while maintaining a self-enhancement-driven lifestyle.
- The study found that this exposure reduced the negative responses to CSR appeals among consumers with a high pursuit of self-enhancement values, supporting the hypothesis that value instantiation can mitigate the adverse effects of value incompatibility.

# Executive summary of 4. Study 2: value instantiation through self-visualization of pursuing two incompatible values
- This section presents Study 2, which uses a different approach to value instantiation.
- It encourages participants to visualize themselves engaging in philanthropic activities while pursuing self-enhancement values.
- The results reinforce the findings of Study 1, demonstrating that self-visualization as a form of value instantiation can effectively reduce negative reactions to CSR initiatives in luxury branding.

# Executive summary of 5. General discussion
- The general discussion summarizes the key findings of the research.
- It highlights the importance of value instantiation as a solution for luxury brands seeking to incorporate CSR.
-  It outlines the contributions of the research to luxury brand management and value research, and suggests directions for future investigations.

</details>

```
title: Building Brands for the Emerging Bicultural Market: The Appeal of Paradox Brands
authors: Maria A. Rodas, Deborah Roedder John, Carlos J. Torelli
journal: Journal of Consumer Research
published: 2021
```
 
# Executive Summary
- **Paradox brands** are brands that incorporate seemingly contradictory associations in their **brand identity**. These contradictions can manifest in different ways:
    - **Brand Personality**: Combining traits like "rugged" and "sophisticated" that are not typically associated with one another.
    - **Brand Values**: Featuring values that are in motivational conflict, such as "traditional" (emphasizing conservation) and "trendy" (emphasizing openness to change).
- This research demonstrates that paradox brands are particularly appealing to **bicultural consumers**, a rapidly growing segment representing over a third of the US population. This appeal stems from the **cognitive flexibility** that bicultural individuals develop through navigating multiple cultural contexts.
    - *Cognitive Flexibility*: The ability to simultaneously consider multiple conflicting representations of a single object or event.
- Across seven studies, we consistently find that bicultural consumers evaluate paradox brands more favorably and choose them more often compared to **non-paradox brands**. Moreover, bicultural consumers exhibit more favorable evaluations and greater choice of paradox brands than **monocultural consumers**.
- A key mediator of this effect is **brand engagement**: Bicultural consumers' greater cognitive flexibility allows them to engage more deeply with the contradictory elements of paradox brands, leading to more positive attitudes.
    - *Brand Engagement*: The level of interaction and activity a consumer initiates with a brand, with a focus on cognitive engagement (being occupied, fully absorbed, or engrossed with the brand).
- Furthermore, the **acculturation strategy** adopted by bicultural individuals moderates their response to paradox brands. Those who adopt an **integration strategy**, maintaining identification with both their home and host cultures, show the strongest preference for paradox brands.
    - *Integration Acculturation Strategy*: Maintaining identification with one’s cultural heritage while also identifying with the new cultural identity.
- **Specific findings from the studies**:
    - **Study 1 (Field Study)**: At a farmers' market (monocultural) and a Latino vendor's market (bicultural), participants chose between a tote bag featuring the "Rugged Sophistication" (paradox) brand or the "Rugged Outdoors" (non-paradox) brand. **60.7% of bicultural consumers chose the paradox brand, significantly more than the 45.3% of monocultural consumers who did so.**
    - **Study 2 (Online Experiment)**: Participants evaluated a "Modern Heritage" (paradox) or "Modern Brand" (non-paradox) clothing brand. **Bicultural consumers rated the paradox brand more favorably than both the non-paradox brand and monocultural consumers**. This effect was **mediated by cognitive flexibility**: Bicultural participants reported higher cognitive flexibility, which explained their more positive evaluations of the paradox brand.
    - **Study 3 (Cognitive Flexibility Prime)**: Monocultural participants were primed to think flexibly or not before evaluating the "Rugged Sophistication" paradox brand. **Those primed with cognitive flexibility rated the brand more favorably**, supporting the causal role of this cognitive style.
    - **Studies 4 & 5 (Brand Engagement Mediation)**: Hispanic/Asian American participants evaluated an "Exciting & Peaceful" (paradox) brand versus either "Exciting" or "Peaceful" (non-paradox) brands. **Participants rated paradox brand more favorably**. This effect was **mediated by their level of brand engagement**. In study 5, an analysis ruled out the possibility that the positive response to paradox brands was a result of processing fluency.
    - **Study 6 (Full Serial Mediation)**: This study confirmed the entire proposed model: **Cultural background (bicultural vs. monocultural) -> Cognitive Flexibility -> Brand Engagement -> Paradox Brand Evaluation.**
    - **Study 7 (Acculturation Strategy Moderation)**: Hispanic participants with either an integrated, separated or assimilated acculturation strategy. Those in the **integration acculturation strategy** condition rated the paradox brand significantly more favorably than those in other conditions.
- These findings suggest that marketers should consider developing paradox brand identities, particularly when targeting bicultural consumers who embrace integration acculturation strategies.

#bicultural_consumers #paradox_brands #cognitive_flexibility #brand_engagement #acculturation_strategy #brand_identity #consumer_behavior


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The growth of bicultural consumers, representing over a third of the US population with $4.1 trillion in buying power, presents a significant challenge and opportunity for marketers [Nielsen, 2015].
    - Marketers are increasing budgets and tailoring campaigns to appeal to this demographic [Filippelli, 2013; Swaminathan, 2015].
- Academic research on brand strategies for bicultural consumers is limited, primarily focusing on advertising (bilingual or promoting cultural values) [Kubat and Swaminathan, 2015; Luna and Peracchio, 2005; Noriega and Blair, 2008; Lau-Gesk, 2003], or shifting responses upon assimilation [Chen, Ng, and Rao, 2005; Ng, 2010]. Research on general brand strategies is virtually non-existent [Monga and Lau-Gesk, 2007].
- This article addresses the need for research on brand strategies designed specifically for bicultural consumers by focusing on brand identities.
    - We address the question of whether certain types of brands are more appealing to bicultural consumers.
    - Brand identities are focused because they represent the firm's definition of the brand, providing a foundation for positioning, communication, and market success [Aaker, 1996b; John and Torelli, 2017; Keller, 2013].
- *Paradox brands*, which incorporate contradictory brand associations, represent a novel strategy for appealing to bicultural consumers.
    - We find that bicultural consumers evaluate paradox brands more favorably and choose them more than non-paradox brands.
    - We also find that bicultural consumers respond more favorably than monocultural consumers to paradox brands.
    - Acculturation strategy within the bicultural community affects receptiveness to paradox brands.
- Our findings contribute to theory and practice.
    - Conceptually, we identify **cognitive flexibility** as a key factor in understanding how biculturals respond to different types of brands [Aytug et al., 2018; Tadmor, Galinsky, and Maddux, 2012].
    - We show that biculturals gravitate toward and actively engage with contradictions, with active engagement in processing paradox brands leading to more favorable attitudes.
    - Our findings link cognitive flexibility with brand engagement, revealing implications for attitude change and affective outcomes for the first time.
- Our findings provide guidance for marketers facing dilemmas in building brands for the bicultural market, which is not homogeneous.
    - One dilemma involves incorporating meanings that resonate with different subgroups without creating contradictions.
    - Another dilemma concerns tweaking existing brand identities for bicultural consumers without diluting the brand's focus.
    - We address concerns about contradicting traditional brand positioning, which emphasizes clarity and simplicity [Trout and Ries, 1986]. We indicate brands incorporating contradiction can be successful, and even more appealing for certain segments, like biculturals.

# 2. Defining Paradox Brands
- The term "paradox" often refers to contradictions in consumption situations [Brown, Kozinets, and Sherry, 2003; Mick and Fournier, 1998]. We define a *paradox brand* as a **brand identity that includes brand associations that appear to be contradictory in nature**.
- Several aspects of this definition are noteworthy.
    - First, **brand identity** refers to a set of brand associations selected by marketers to represent the brand, which can be product-related or symbolic [Aaker, 1996a].
    - Our research focuses on abstract associations like brand personality and brand values.
    - Second, brand identity defines the brand from the firm’s viewpoint, unlike a brand image, which reflects consumer perceptions.
    - Brand identities are defined in terms of positive associations, whereas brand images can include negative associations.
    - Our research focuses on brand identities and contradictory positive associations, instead of contradictions among negative and positive associations.
- We focus on contradictions in brand associations related to brand personality [Aaker, 1997] and brand values [Torelli et al., 2012].
    - These contradictions emerge from the understanding that certain brand associations are unlikely to co-occur.

## 2.1. Contradiction in Brand Values
- Values are universal abstract representations of desired end-states that serve as guiding principles [Schwartz, 1992].
    - Schwartz's framework includes 11 distinct values categorized into four higher-order value dimensions: openness to change, self-enhancement, conservation, and self-transcendence.
    - These values are placed on a circular continuum, where compatible values are adjacent and incompatible values are opposite.
    - Openness to change (being open to change) conflicts with conservation (preserving the status quo).
- Marketers imbue brands with human values to make them meaningful beyond utilitarian benefits [Durgee, O’Connor, and Veryzer, 1996].
    - Research shows that consumers perceive brands as representations of human values [Allen et al., 2008; Torelli and Kaikati, 2009; Torelli et al., 2012].
    - Schwartz's value structure is used to measure values associated with brands [John and Torelli, 2017; Torelli et al., 2012], e.g., Apple (openness), IBM (conservation), Rolex (self-enhancement), Red Cross (self-transcendence).
- A brand that combines contradictory values (i.e., values in motivational conflict) is considered a paradox brand.
    - We examine paradox brands that contradict openness to change (modern/trendy) and conservation (traditional/classic).

## 2.2. Contradiction in Brand Personality
- Brands are often defined by product attributes and benefits [Aaker, 1996b; Keller, 1993]. There is a growing trend to incorporate human-like meanings into brands [MacInnis and Folkes, 2017].
    - Brand personality is a widely used framework [Aaker, 1996b; Batra, Lehmann, and Singh, 1993; John and Torelli, 2017; Keller, 2013].
- Brand personality is the set of human personality characteristics associated with a brand.
    - Aaker [1997] identifies five personality dimensions: sincerity, competence, excitement, sophistication, and ruggedness.
    - Examples: Hallmark (sincerity), Intel (competence), Absolut Vodka (excitement), Louis Vuitton (sophistication), and Eddie Bauer (ruggedness). A sixth dimension is peacefulness/calmness [Aaker et al., 2001].
- Some brand personality dimensions appear contradictory.
    - Ruggedness (tough, masculine, outdoorsy) contradicts sophistication (glamorous, feminine, smooth).
    - Excitement (daring, spirited) contradicts peacefulness (shy, calmness).
- A brand that combines contradictory personalities is considered a paradox brand.
    - We examine paradox brands based on brand personality contradictions between (1) ruggedness and sophistication and (2) exciting and peaceful, in addition to values-based contradiction (3) openness to change and conservation.

# 3. Consumer Response to Paradox Brands
- We make two overarching predictions regarding bicultural consumers' response to paradox brands.
    - *H1*: Bicultural consumers will respond more favorably to paradox brands than non-paradox brands.
    - *H2*: Bicultural consumers will respond more favorably to paradox brands than will monocultural consumers.
- The conceptual rationale underlying these predictions is illustrated in figure 1.
    - Bicultural consumers exhibit greater cognitive flexibility due to navigating dual cultures with contrasting elements.
    - Higher cognitive flexibility promotes greater engagement with paradox brands.
    - Greater engagement leads to more favorable brand evaluations.

## 3.1. Cultural Background and Cognitive Flexibility
- Bicultural individuals have been exposed to and internalized two cultures [Benet-Martınez and Haritatos, 2005].
    - They may be immigrants, refugees, indigenous people, ethnic minorities, or multi-ethnic individuals [Berry, 2003].
    - Biculturalism entails synthesizing norms from two groups [Rotheram-Borus, 1993] and switching between cultural schemas [Hong et al., 2000].
- People exposed to both home and host cultures recognize different ways to arrange customs and lives [Rogoff, 2003].
    - They challenge culture-specific assumptions, integrate new ideas, make connections, and have new insights [Maddux, Adam, and Galinsky, 2010; Tadmor, Galinsky, and Maddux, 2012].
    - Comparing and switching between perspectives enhances cognitive flexibility [Benet-Martınez, Lee, and Leu, 2006; Tadmor and Tetlock, 2006; Tadmor, Tetlock, and Peng, 2009; Crisp and Turner, 2011].
- *Cognitive flexibility* is the ability to simultaneously consider multiple conflicting representations of a single object or event.
    - It allows making uncommon associations, linking categories, and holding multiple perspectives [De Dreu, Baas, and Nijstad, 2008; Isen, 2001; Murray et al., 1990].
    - Biculturals develop greater cognitive flexibility, leading to psychological and performance advantages.
    - Aytug et al. [2018] show that interaction with multiple cultures results in creative thinking through increased cognitive flexibility.
    - Creativity involves flexibly framing problems and finding connections [Ward, Smith, and Finke, 1999].

## 3.2. Cognitive Flexibility and Engagement with Paradox Brands
- *Brand engagement*, the level of consumer interaction with a brand, is important for understanding consumer response.
    - Engagement can be cognitive, emotional, or behavioral [Hollebeek, 2011; Hollebeek, Glynn, and Brodie, 2014; Hughes, Swaminathan, and Brooks, 2019].
    - We focus on cognitive engagement, defined as being occupied, fully absorbed, or engrossed with the brand [Higgins, 2006; Hollebeek, 2011].
- Paradox brands invite engagement from bicultural consumers due to the opportunity for divergent thinking.
    - Divergent thinking, or switching between cognitive categories, drives engagement with tasks that promote such thinking [De Dreu, Nijstad, and Baas, 2011].
    - Contradictory elements prompt biculturals to connect with categories, think flexibly, and engage with brand information [De Dreu et al., 2011; Ou et al., 2018].
    - This should not be the case for monocultural consumers with lower cognitive flexibility.
- Paradox brands elicit a higher degree of engagement among bicultural consumers, enabled by their cognitive flexibility.
    - Biculturals are more likely to engage with paradox brands than non-paradox brands because non-paradox brands do not prompt divergent thinking.

## 3.3. Brand Engagement and Evaluation of Paradox Brands
- Higher engagement is associated with positive outcomes for brands.
    - Consumers more engaged with brands respond more favorably in terms of attitudes, preferences, intentions, and consumption [Calder, Isaac, and Malthouse, 2016; Calder, Malthouse, and Schaedel, 2009; Carvalho and Fernandes, 2018; Hollebeek et al., 2014].
- *H3*: Bicultural consumers' higher engagement with paradox brands will result in more positive responses to these brands.
    - We expect biculturals to evaluate paradox brands more favorably than non-paradox brands and more favorably than monocultural consumers.
    - This is consistent with research in marketing and psychology [Higgins, 2006; Higgins and Scholer, 2009].
- As people become more engaged, this intensifies their attraction to generally positive stimuli.
    - Because paradox brands include positive brand associations, stronger engagement intensifies attraction, leading biculturals to evaluate them more favorably.

# 4. Overview of Empirical Studies
- We test our predictions in seven studies.
- Study 1: Conducted in a field setting, shows that bicultural consumers respond more positively to paradox brands, choosing them over non-paradox brands.
- Studies 2 and 3: Examine the process responsible, providing evidence for the causal links.
    - Study 2: Shows that biculturals possess higher levels of cognitive flexibility, resulting in more favorable paradox brand evaluations.
    - Study 3: Shows that even monocultural consumers respond more positively to paradox brands if they are primed to be more cognitively flexible.
- Studies 4 and 5: Provide evidence for the link between cognitive flexibility, brand engagement, and brand evaluation.
    - Bicultural consumers, with higher cognitive flexibility, engage more with paradox brands, leading to more favorable evaluations.
- Study 6: Examines the full causal model. Bicultural consumers evaluate paradox brands more favorably because of higher cognitive flexibility, enabling stronger engagement, leading to positive evaluations.
- Study 7: Examines differences in bicultural consumers that moderate more positive responses to paradox brands, comparing integrated versus separated bicultural consumers. Integrated consumers, with greater cognitive flexibility, respond more positively.

# 5. Study 1: Consumer Response to Paradox Brands
- Study 1 tested our predictions that bicultural consumers respond more favorably to paradox brands than monocultural consumers and that bicultural consumers respond more favorably to paradox versus non-paradox brands.
- The study was conducted in a field setting where consumers made a choice between a paradox versus non-paradox branded product.

## 5.1. Method
- *Sample*: The study was conducted at two different venues on the same weekend.
    - Monocultural consumers were recruited at a farmers’ market in a predominantly white neighborhood (N = 128).
    - Bicultural consumers were recruited at a vendors’ market targeting Latinos in the same city (N = 117).
    - Participants took the survey in exchange for a tote bag.
- *Procedure*: We told participants we were interested in their opinions about two different concepts for a new clothing brand.
    - Participants viewed information about both a paradox brand (Rugged Sophistication) and a non-paradox brand (Rugged Outdoors).
    - The paradox brand (Rugged Sophistication) included two brand personality descriptors (rugged, outdoorsy) that were contradictory to two other brand personality descriptors (glamorous, charming).
    - The non-paradox brand (Rugged Outdoors) included four personality descriptors that were consistent with each other (rugged, tough, outdoorsy, hardy).
    - We asked them to choose the brand they would like on their tote, either Rugged Sophistication or Rugged Outdoors.
- *Stimuli Selection and Pretest*: We designed descriptions of paradox brands based on a pilot study.
    - We examined a large set of consumer brands to determine what types of paradoxes (contradictory meanings) exist for brands in the US market.
    - Three of the most common paradoxes were used as a basis for our paradox brand descriptions in our studies—rugged sophistication, openness tradition, and exciting peaceful.
    - We selected “Rugged Sophistication” as the paradox brand for study 1.
    - A pretest with biculturals (N = 101) and monocultural consumers (N = 108) ensured that Rugged Sophistication was perceived as intended.
    - The paradox brand was perceived as more contradictory than the non-paradox brand.

## 5.2. Results and Discussion
- We coded participants’ choice of tote bag as 0 (non-paradox brand) or 1 (paradox brand).
    - As predicted, the majority of bicultural consumers (60.7%) chose the paradox brand (Rugged Sophistication) over the non-paradox brand (Rugged Outdoors).
    - Furthermore, as expected, bicultural consumers chose the paradox brand more than monocultural consumers did (60.7% vs. 45.3%).
- Our results support the prediction that bicultural consumers favor paradox brands over non-paradox brands and favor paradox brands more than monocultural consumers.

# 6. Study 2: Cognitive Flexibility as a Mediator
- The study aims to replicate the effects of Study 1 using a paradox brand with contradictory brand values (openness to change and conservation) and to test the prediction that biculturals possess greater cognitive flexibility that results in more favorable responses to paradox brands.

## 6.1. Method
- *Participants and Procedure*:
    - 90 monocultural members of Amazon’s Mechanical Turk and 93 Hispanic/Latinos were recruited.
    - Participants were randomly assigned to view a description of one of these brands: Modern Heritage (paradox brand) or Modern Brand (non-paradox brand).
    - Participants rated their assigned brand and completed the Cognitive Flexibility Scale.
- *Stimuli Pretest*:
    - We recruited bicultural (N = 100) and monocultural (N = 107) consumers.
    - After reading the same information about either the paradox or non-paradox brand as in the main study, participants rated the brand using the measure of contradiction described in study 1.
    - They also rated the brand on several additional dimensions (novelty, uniqueness, excitement, creativity) to rule out alternative explanations.

## 6.2. Results and Discussion
- *Brand Evaluation*:
    - Bicultural consumers evaluated the paradox brand more favorably than the non-paradox brand.
    - Bicultural consumers evaluated the paradox brand more favorably than monocultural consumers.
- *Cognitive Flexibility*:
    - Bicultural participants indicated a higher level of cognitive flexibility than monocultural participants.
- *Moderated Mediation Analysis*:
    - Higher levels of cognitive flexibility mediated the positive effect of biculturalism on paradox brand evaluation.
- *Discussion*:
    - The study replicates findings from study 1, with cognitive flexibility mediating cultural differences in paradox brand evaluations.
- *Follow-up Study*:
    - To address the issue of whether biculturals are responding positively to the greater complexity or number of meanings in the paradox brand rather than the contradictory nature of these meanings, we conducted a follow-up study with 300 Hispanic biculturals, which included three brand conditions: (1) paradox brand (rugged, sophisticated); (2) non-paradox single meaning brand (rugged, tough); and (3) non-paradox dual meaning brand (rugged, sincere).
    - Found that brand evaluations were more favorable in the paradox brand condition than for either of the two non-paradox conditions, with no difference in evaluations for the non-paradox single meaning versus the non-paradox dual meaning brands.
    - Findings confirm that the more favorable response to paradox brands among biculturals is due to the contradictory nature of the meanings in these brands, as opposed to their greater complexity versus non-paradox brands.

# 7. Study 3: Priming Cognitive Flexibility
- Manipulated cognitive flexibility in a sample of monocultural consumers to show that monocultural consumers primed with cognitive flexibility respond more favorably to paradox brands, similar to bicultural consumers.

## 7.1. Method
- 200 monocultural members of Amazon’s Mechanical Turk took the survey for a small monetary payment.
- Participants were assigned to one of two conditions: control or cognitive flexibility prime.
- To prime neutral versus high cognitive flexibility, we asked participants to think of either one explanation (control) or multiple possible explanations (cognitive flexibility prime) for an ambiguous situation.
- Participants read that we were interested in their input on a new brand of outdoor clothing and were then shown the description of the paradox brand (Rugged Sophistication).
- To ensure that the priming task worked as intended, participants completed the cognitive flexibility measure described in study 2, answered several demographic questions, and were debriefed and dismissed.

## 7.2. Results and Discussion
- *Manipulation Check*:
    - Participants in the cognitive flexibility prime condition reported higher levels of cognitive flexibility than those in the control condition.
- *Brand Evaluation*:
    - Participants in the cognitive flexibility prime condition had more positive evaluations of the paradox brand than participants in the control condition.
- This result conceptually replicates the difference between bicultural and monocultural consumers in their evaluation of a paradox brand found in study 2 and, thus, provides further evidence of the role of cognitive flexibility as a driver of more positive evaluations for paradox brands.

# 8. Study 4: The Mediating Role of Brand Engagement
- This study tests the prediction that a higher level of cognitive flexibility prompts biculturals to engage more strongly with a paradox brand, resulting in more positive evaluations.
- It also considers an alternative mediator: biculturals may be more likely to resolve the contradictions inherent in paradox brands, which could contribute to more positive evaluations.

## 8.1. Method
- We recruited 267 English-speaking Hispanics currently living in the United States.
- Participants were randomly assigned to one of three brand conditions: (1) paradox brand: exciting and peaceful; (2) non-paradox brand: exciting, or (3) non-paradox brand: peaceful.
- Participants responded to three statements measuring their level of engagement with the brand.
- Following filler questions, participants responded to several statements about resolving the paradox brand contradiction.

## 8.2. Results and Discussion
- *Manipulation Check*:
    - Participants in the paradox brand condition perceived the brand to be more contradictory than did participants in the non-paradox exciting condition, as well as those in the non-paradox peaceful brand condition.
- *Brand Evaluation*:
    - Participants in the paradox brand condition evaluated the brand more favorably than did participants in the non-paradox exciting condition, as well as those in the non-paradox peaceful brand condition.
- *Brand Engagement*:
    - Participants in the paradox brand condition had stronger engagement with the brand than did participants in the non-paradox exciting condition, as well as those in the non-paradox peaceful brand condition.
- *Mediation Analysis*:
    - Higher levels of brand engagement mediated the positive effect of the paradox brand on brand evaluation.
- *Contradiction Resolution as an Alternative Mediator*:
    - Results failed to show evidence of mediation
- *Discussion*:
    - Our results provide support for the proposed process, namely that bicultural consumers, who tend to have higher levels of cognitive flexibility, engage more strongly with the paradox brand, which leads to a more favorable evaluation (vs. non-paradox brand).
    - We also ruled out the resolution of contradiction as an alternative mediator.
- *Follow-up Study*:
    - Conducted a follow-up study with a sample of 300 English-speaking Asian Americans living in the United States, who participated in a study with the same procedure as study 4.
    - The results replicated our findings with Hispanic Americans. Specifically, Asian Americans evaluated the paradox brand more favorably than the non-paradox brand, and this effect was mediated by brand engagement.
    - This expands our findings to a new segment of biculturals

# 9. Study 5: A Further Look at Brand Engagement
- This study aims to provide further evidence for our proposed process by incorporating a more comprehensive measure of brand engagement.
- Furthermore, we examined whether fluency might be an alternative explanation for biculturals’ more positive evaluations of paradox brands.

## 9.1. Method
- 194 English-speaking Hispanics living in the United States from a Qualtrics panel
- Participants were randomly assigned to either the non-paradox or paradox brand condition.
- After completing the brand evaluation measure, participants completed a more extensive measure of processing engagement.
- We asked participants to rate the ease with which they processed the brand information to measure fluency as an alternative mechanism for the effects.

## 9.2. Results and Discussion
- *Brand Evaluation*:
    - Participants in the paradox brand condition evaluated the brand more favorably than did participants in the non-paradox condition.
- *Brand Engagement*:
    - Participants in the paradox brand condition engaged more strongly with the brand than did participants in the non-paradox condition.
- *Mediation Analysis*:
    - Higher levels of brand engagement mediated the positive effect of the paradox brand on brand evaluation.
- *Fluency as an Alternative Mediator*:
    - Results failing to show evidence of mediation
- *Discussion*:
    - Results consistent with study 4, showing that bicultural consumers evaluate paradox (vs. non-paradox) brands more positively due to stronger engagement with the brand. We were also able to rule out an alternative explanation based on fluency effects.

# 10. Study 6: Testing the Full Serial Mediation Model
- This study aims to provide evidence of our proposed process by testing the full serial mediation model: cultural background -> cognitive flexibility -> brand engagement -> paradox brand evaluation.

## 10.1. Method
- 84 monocultural members of Prolific’s online pool and 81 Hispanic/Latinos living in the United States from Qualtrics’ panel
- Participants completed the brand evaluation measure, the brand engagement scale, and a cognitive flexibility measure.

## 10.2. Results and Discussion
- *Stimulus Pretest*:
    - Confirmed that both biculturals and monoculturals perceived the same level of contradiction in the paradox brand.
- *Brand Evaluation*:
    - Bicultural participants had more positive evaluations of the paradox brand than did monocultural participants.
- *Cognitive Flexibility*:
    - Bicultural participants had higher levels of cognitive flexibility than monocultural participants.
- *Brand Engagement*:
    - Bicultural participants had stronger engagement with the paradox brand than did monocultural participants
- *Serial Mediation Analysis*:
    - Cognitive flexibility and thus brand engagement mediated the effects of cultural background (bicultural vs. monocultural) on brand evaluation.
- As predicted, these results show that bicultural consumers evaluate paradox brands more favorably due to their higher levels of cognitive flexibility, which prompts biculturals to be more strongly engaged with the paradox brand, resulting in more favorable evaluations.

# 11. Study 7: The Moderating Role of Acculturation Strategy
- This study examines the moderating role that acculturation strategy plays in bicultural consumers’ response to paradox brands.

## 11.1. Method
- Two hundred five Hispanics currently living in the United States who were members of CloudResearch’s panel.
- Participants were randomly assigned to either the paradox or non-paradox brand condition and evaluated their assigned brand.
- We then measured acculturation strategy using Ward and Kennedy’s (1994) Acculturation Index (AI), which assesses identification with home culture (i.e., Hispanic) and identification with host culture (i.e., American).

## 11.2. Results and Discussion
- *Acculturation Strategy*:
    - Participants were classified into acculturation strategies based on median splits of American and Hispanic cultural identification.
    - We then created a dummy variable to designate participants who highly identify with both the Hispanic and American cultures (i.e., integration acculturation strategy) and those who identify with only one of the cultures (i.e., separation or assimilation acculturation strategy).
- *Brand Evaluation*:
    - The interaction between brand condition and acculturation strategy was significant.
    - Bicultural consumers with an integration acculturation strategy evaluated the paradox brand more favorably than the non-paradox brand and more favorably than biculturals with a separation or assimilation acculturation strategy.
- In sum, our results show that integrated bicultural consumers (i.e., those who highly identify with both their home and host cultures) evaluate paradox brands more favorably than those who adopt a separation or assimilation acculturation strategy.
- This result underscores the important insight that bicultural consumers are not homogenous with regard to their response to paradox brands, which needs to be taken into consideration when marketers develop paradox brands for the bicultural consumer segment.

# 12. General Discussion
- Consumer markets are changing due to globalization and the emergence of new demographic groups.
- Our research demonstrates that building a paradox brand can be a successful strategy to win bicultural consumers.
    - Bicultural consumers evaluate more favorably and choose more often paradox brands than non-paradox brands.
    - In addition, bicultural consumers respond more favorably to paradox brands than monocultural consumers.
    - This effect is mediated by the greater cognitive flexibility exhibited by bicultural consumers, which facilitates engaging with the contradictions in a paradox brand, and in turn enhances brand evaluations.
    - Furthermore, our findings identify bicultural consumers who embrace an integration acculturation strategy as a segment particularly prone to respond favorably to paradox brands.
- Next, we discuss the contributions and implications of our findings for research on cultural differences in consumer behavior, research on biculturalism and cognitive flexibility, as well as on value creation from engaging with brands. Finally, we conclude with a discussion of managerial implications and opportunities for future research.

## 12.1. Cultural Differences in Consumer Behavior
- Our findings contribute to the broader conversation about the effects of culture on brand strategies.
- To our knowledge, we are the first to examine strategies for building brands with bicultural consumers.
- Paradox brands, which incorporate contradictory meanings, are particularly appealing to bicultural consumers, and importantly, are more appealing than traditional non-paradox brands, which provide a more focused meaning for the brand without contradiction.

## 12.2. Biculturals, Cognitive Flexibility, Engagement, and Value Creation
- We demonstrate that the greater cognitive flexibility of biculturals not only enhances their creative thinking but also has evaluative consequences when engaging with contradictory brand elements.
- We demonstrate that bicultural consumers can derive value from engaging with the contradictions inherent in a paradox brand.
- It is noteworthy that the effects uncovered here are driven by the strength of engagement, and not by the valence (e.g., pleasant vs. unpleasant) or outcome (e.g., resolving contradictions) of the engagement.

## 12.3. Managerial Implications
- Our research offers another option, which could result in a more efficient and successful way to reach bicultural consumers. Specifically, marketers could build a single paradox brand that includes brand associations that appeal to more than one segment of bicultural consumers (e.g., Hispanic Americans and Asian Americans), even though these associations may be somewhat contradictory.
- there may be an opportunity to employ paradox brands successfully with monocultural consumers. For example, brand messages could be developed to induce cognitive flexibility to make the paradox brand more appealing to all consumers. This notion is supported by findings in study 3, where we demonstrated that priming cognitive flexibility (by prompting monocultural consumers to consider multiple perspectives) resulted in more favorable evaluations of paradox brands.

## 12.4. Future Research on Branding Strategies for Bicultural Consumers
- there is reason to believe that the effects uncovered here about biculturals’ preference for paradox (vs. non-paradox) brands could extend to monocultural consumers with extensive multicultural experiences. This prediction would be consistent with findings in study 3 demonstrating that monoculturals primed with cognitive flexibility favor paradox brands.
- In our studies, we operationalized paradox brands in terms of contradictions of abstract brand associations, such as brand values and brand personality. It seems plausible that bicultural consumers might be drawn to value brands that combine these contradictory brand elements.
- Considering a culture-mixed product as another type of paradox brand (i.e., involving a contradiction between two abstract cultural frames), our research would predict that biculturals would be more accepting of such products than their monocultural counterparts.

---

# Executive summary of 1. Introduction
- The introduction highlights the growing importance of bicultural consumers as a market segment and the lack of research on effective brand strategies for this group. It introduces the concept of paradox brands, which incorporate contradictory brand associations, as a potential strategy for appealing to bicultural consumers. The introduction also outlines the theoretical framework linking cognitive flexibility, brand engagement, and brand evaluation, and it emphasizes the implications for marketers.

# Executive summary of 2. Defining Paradox Brands
- This section provides a clear definition of paradox brands as brand identities that include contradictory brand associations. It distinguishes brand identity from brand image and focuses on contradictions in brand personality and brand values, supported by established frameworks in marketing research.

# Executive summary of 3. Consumer Response to Paradox Brands
- This section presents the central hypotheses of the research: Bicultural consumers will respond more favorably to paradox brands than both non-paradox brands and monocultural consumers. It explains the theoretical rationale behind these predictions, emphasizing the role of cognitive flexibility in enabling bicultural consumers to engage with paradox brands and develop more positive evaluations.

# Executive summary of 4. Overview of Empirical Studies
- This section offers a concise overview of the seven empirical studies conducted to test the hypotheses. It summarizes the purpose, methodology, and key findings of each study, providing a roadmap for understanding the research process and the evidence supporting the proposed theoretical framework.

# Executive summary of 5. Study 1: Consumer Response to Paradox Brands
- This study demonstrates that bicultural consumers prefer paradox brands (Rugged Sophistication) over non-paradox brands (Rugged Outdoors) in a real-world setting. The findings support the hypothesis that bicultural consumers are more drawn to brands with contradictory associations.

# Executive summary of 6. Study 2: Cognitive Flexibility as a Mediator
- This study supports that bicultural consumers evaluated paradox brands more favorably due to their higher cognitive flexibility. A subsequent study found the favorable response to paradox brands is due to the contradictory nature of the meanings.

# Executive summary of 7. Study 3: Priming Cognitive Flexibility
- This study confirms that when primed to be cognitively flexible, monocultural consumers show more favorable responses to paradox brands. This provides further evidence of the crucial role that cognitive flexibility plays in the paradox brand effect.

# Executive summary of 8. Study 4: The Mediating Role of Brand Engagement
- This study examines brand engagement. The study found consumers engaged more strongly with the paradox brand, leading to a more favorable evaluation. Furthermore, the study ruled out resolving contradiction as an alternative mediator.

# Executive summary of 9. Study 5: A Further Look at Brand Engagement
- With a more comprehensive measure of brand engagement, this study supports the results of study 4. Findings are that consumers evaluate paradox brands more positively because of stronger brand engagement. Moreover, fluency effects are ruled out as an alternative explanation.

# Executive summary of 10. Study 6: Testing the Full Serial Mediation Model
- Bicultural consumers evaluate paradox brands more favorably due to their higher levels of cognitive flexibility, which prompts biculturals to be more strongly engaged with the paradox brand, resulting in more favorable evaluations.

# Executive summary of 11. Study 7: The Moderating Role of Acculturation Strategy
- The final study finds that an integration acculturation strategy, which identifies with both home and host cultures, creates a stronger preference toward paradox brands.

# Executive summary of 12. General Discussion
- The discussion highlights the contributions of the research to understanding cultural differences in consumer behavior, the role of cognitive flexibility and engagement in value creation, and the managerial implications for building brands that appeal to bicultural consumers. It also suggests avenues for future research, such as exploring the effects of paradox brands on monocultural consumers with multicultural experiences and examining contradictions in concrete brand elements.

</details>

```
title: Redefining Home: How Cultural Distinctiveness Affects the Malleability of In-Group Boundaries and Brand Preferences
authors: Carlos J. Torelli, Rohini Ahluwalia, Shirley Y. Y. Cheng, Nicholas J. Olson, Jennifer L. Stoner
journal: Journal of Consumer Research
published: 2017
```

# Executive Summary

- This research investigates how **cultural distinctiveness** (feeling different from the surrounding cultural environment) influences consumer preferences for brands associated with a *related cultural group* (geographically close and/or sharing sociohistorical roots).
- Seven studies demonstrate that consumers experiencing *cultural distinctiveness* tend to favorably evaluate and prefer brands associated with a *related cultural group*, even if those brands aren't typically the top choice.
- **Key theoretical framework**:
    - The **pro-in-group bias** for culturally related brands is driven by a heightened desire to connect with "home". This desire prompts consumers to expand their *in-group boundaries* to include the related cultural group, broadening their definition of "home".
- **Key findings** / arguments including
    -  This **pro-in-group bias** is attenuated when the salience of *intergroup rivalries* is high. In such cases, experiencing *cultural distinctiveness* can backfire, leading to less favorable evaluations of brands associated with a related cultural group.
    - The research manipulates *cultural distinctiveness* using videos of everyday situations in foreign cultures (e.g., streets in India, cafeterias in Asian colleges). Control groups watched videos about insects.
    - **Study 1a** (US participants):  Participants watched videos (cultural distinctiveness vs. neutral). Measured desire to connect with home and inclusion of Canada in "home".  *Result*: Cultural distinctiveness increased desire to connect with home, which in turn led to a greater likelihood of including Canada within a broadened definition of home.
    - **Study 1b** (US participants): Participants watched videos (cultural distinctiveness vs. neutral). Measured the size of the circle they drew on a map to define "home." *Result*: Participants experiencing cultural distinctiveness drew wider circles, implying an expansion of the boundaries defining "home".
    - **Study 2a** (Minnesota residents): Participants watched videos (cultural distinctiveness vs. neutral). Measured preference for brands associated with Wisconsin (neighboring state) vs. Texas (distant state). *Result*: Cultural distinctiveness led to greater preference for Wisconsin-associated brands.
    - **Study 2b** (Mexican nationals in US vs. Mexico):  Compared product choices (Argentine vs. Turkish song) between Mexican immigrants in the US (high cultural distinctiveness) and Mexicans in Mexico. *Result*: Mexican immigrants preferred the Argentine song more than Mexicans living in Mexico.
    - **Study 3** (US participants, during Sochi Olympics): Participants watched videos (cultural distinctiveness vs. neutral). Measured support for the Canadian curling team.  Rivalry salience between US and Canada was high due to the Olympics. *Result*: Cultural distinctiveness *decreased* support for the Canadian team, reversing the pro-in-group bias.
    - **Study 4** (Minnesota residents, college football context): Participants watched videos (cultural distinctiveness vs. neutral) and were exposed to high or low rivalry manipulation (Minnesota vs. Wisconsin). Measured support for Wisconsin Badgers (vs. Texas Longhorns) and identification with the Upper Midwest. *Result*: Under low rivalry, cultural distinctiveness increased support for Wisconsin and identification with the Upper Midwest. Under high rivalry, this effect was attenuated.
    - **Study 5** (Hong Kong Chinese): Participants watched videos (cultural distinctiveness vs. neutral). Measured brand evaluation of Singapore Airlines (culturally proximal) and Air France (culturally distant), as well as perceived rivalry between Singapore Airlines and Hong Kong airlines. *Result*:  When rivalry salience was low, cultural distinctiveness increased positive evaluation of Singapore Airlines. When rivalry salience was high, this effect was reversed.

#cultural_distinctiveness #in_group_bias #brand_preference #cultural_symbolism #intergroup_rivalry #malleability_of_in_group_boundaries #cultural_consumption


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction

- The paper starts with scenarios of a Chinese student in the US and an American tourist in India, both experiencing *cultural distinctiveness*, which refers to the perception of significant differences between one's current cultural environment (host culture) and home culture [Oberg, 1960]. This leads to a feeling of *cultural separation* and a desire to connect with "home."
- The paper argues that *cultural distinctiveness* has significant implications for the marketplace and offers new insights into the expansion of *in-group boundaries*.
-  **Relevance**: With increasing globalization, consumers frequently encounter diverse cultures through international travel, business, study abroad, and migration (e.g., over 1.1 billion international tourists in 2014).
- Previous research focused on consumption from one's established home culture [Pe~naloza, 1994; Stayman and Deshpande´, 1989]. However, the authors emphasize the need to examine how consumption related to *other cultural groups* is impacted, especially when home culture brands aren't readily available.
- For example, would a Hong Kong student in Canada be more likely to choose Singapore Airlines (a related but not home culture) compared to buying the ticket in Hong Kong? The authors aim to expand the understanding of *intercultural consumption* in today's global environments.
- The research develops a conceptual framework for *cultural distinctiveness* and experiments to manipulate it.
- The paper investigates whether *cultural distinctiveness* induces consumers to expand their definition of "home" by broadening their *in-group boundaries*. This contrasts with past research that views cultural home as static [Triandis, 2007].
- **Adaptive decision makers**: The authors propose that consumers adaptively change their cultural home definition based on available options.
- Previous culture research primarily focused on when home culture influences behavior [Briley, Wyer, and Li 2014]; the authors explore factors determining how "home" is defined.
- The paper addresses the desire to *reject specific cultures*, incorporating salient rivalries into the framework, unlike past research that mainly focused on the desirability of certain cultures [Leclerc, Schmitt, and Dube´ 1994].
- A series of seven studies support the conceptual framework and its impact on broadening *in-group boundaries*, with implications for acculturation, ethnic and group-based consumption, and brand management.

# 2. Theoretical Background

## 2.1. Cultural Distinctiveness

- People frequently encounter cultures different from their own, highlighting incongruities in daily life, food, social norms, and values [Oberg, 1960].
- Individuals experience situational fluctuations in their sense of distinctiveness from the new culture [Berry, 1997; Church, 1982], influenced by the salience of cultural differences [Berry, 1997; Church, 1982].
- *Cultural distinctiveness* is formally defined as experiencing significant cultural differences with the immediate environment and feeling culturally distinct.
- **H1**: Experiencing high levels of *cultural distinctiveness* will lead to an increased desire to connect with the home culture.
    - **Rationale**:  Past research shows that immersion in unfamiliar environments leads to thoughts about home [Oberg, 1960].
    - Research on acculturation indicates that higher perceived dissimilarity between home and host cultures strengthens association with home culture [Nesdale and Mak 2003].
    - Optimal distinctiveness theory [Brewer, 1991] posits that people seek a balance of similarity and distinctiveness, motivating them to associate with home culture to counter strong feelings of intergroup distinctiveness.
- Consumers can satisfy their need to connect with home by interacting with people from their home culture or consuming related products [Mehta and Belk, 1991; Pe~naloza, 1994].
- These consumption decisions reinforce commitment to the home culture [Saran, 1985].
- The central question is how "home culture" is determined.

## 2.2. Cultural “Home” as a Dynamic Concept

- Home provides a sense of belonging and affiliation, having both geographical and psychological connotations.
- Home culture has often been conceptualized as the consumer’s *cultural in-group* at a geographical level (e.g., home country) [Pe~naloza, 1994; Stayman and Deshpande´, 1989], also including psychological bases of affiliation.
- The existing literature views home as relatively static.
- Early cultural work characterized culture as a fixed set of shared values [Hofstede, 1980; Triandis, 2007].
- More recent perspectives focus on *when* a culture will guide judgments, considering factors like cultural cues [Briley, Morris, and Simonson 2005]. Biculturalism research examines shifting cultural lenses based on situational primes [Morris, Chiu, and Liu 2015].
- The paper addresses the questions of *what* is home culture and *how* is it determined.
- The research proposes a malleable notion of home culture, conceptualized as an *in-group* with stretchable boundaries, expanded to meet the consumer's needs through adaptive consumption.
- Consumers may broaden their definition of home to include related cultural groups based on perceived similarity, even if this expanded in-group isn't chronically accessible.
- The authors' notion of malleable in-group boundaries doesn't require pre-existing categories; it's a more fluid and ad hoc process. This research explores *in-group boundary malleability* in the absence of explicit group definitions.
- Optimal distinctiveness theory suggests that perceptions of explicit *in-groups* can change in response to motivations [Brewer, 1991].  Pickett et al. [2002] found that university students wanting to assimilate with their in-group overestimated the university's enrollment.
- Social identity complexity research suggests that people simultaneously hold numerous social identities, and perceived overlap between these identities increases the potential to construe them as a single in-group [Roccas and Brewer 2002].
- Consumers may expand their home identity to include other groups if they perceive sufficient overlap and if inclusion helps fulfill salient needs.

## 2.3. Cultural Distinctiveness and Consumption

- Could the desire to connect with home, associated with experiencing *cultural distinctiveness*, be strong enough to motivate boundary expansion of the consumer’s *cultural home in-group*?
- Boundary expansion can enhance the likelihood of connecting with something or someone associated with home culture.
- This is more likely if products related to the inherent home culture are not available and an available option offers some similarity in terms of geographical proximity or cultural roots.
- When consumers experiencing *cultural distinctiveness* are exposed to products associated with a potentially construed "home" culture, they will broaden their definition of home to include the related group. This triggers positive responses (pro-in-group bias) to brands associated with the related cultural group, even if these aren't typically preferred.
- **H2**: Experiencing cultural distinctiveness will prompt consumers to expand their *in-group boundaries* to incorporate related cultural groups into a broadened definition of home.
- **H3**: Expansion of *in-group boundaries* will lead to a pro-in-group bias toward brands and products associated with related cultural groups.

## 2.4. Role of Rivalry Salience

- Similar cultural groups are often geographically close, leading to intergroup competition and rivalry.
- Some geographically close cultural groups may not have a salient affiliation with the home culture [Mervis and Rosch 1981].
- The social identity complexity literature asserts that overlap perceptions between groups are contingent on situational, cognitive, and motivational factors [Roccas and Brewer 2002].
- Expansion of *in-group boundaries* is unlikely in the face of enduring antagonistic relations [Hewstone 1996]. A history of hostility can render intergroup rivalries chronically salient and trigger negative attitudes toward brands associated with the antagonistic group [Klein et al. 1998].
- Less antagonistic rivalries can hinder inclusion, as they are a threat to positivity and distinctiveness [Crisp, Stone, and Hall 2006].
- Increased awareness of a related cultural group could strengthen negative feelings and out-group bias [Crisp et al. 2006].
- Situationally induced salience of *intergroup rivalry* can prompt negative attitudes toward brands associated with a rival cultural group [Cheng et al. 2011].
- High salience of rivalry between one’s established home culture and a geographically/culturally proximal group disrupts inclusion within broadened *in-group boundaries*.
- The consumer will be motivated to identify more strongly with the established home identity.
- Connecting with the home identity can be achieved by denigrating products associated with the rival cultural group [Oyserman, Fryberg, and Yoder 2007].
- Consumers experiencing *cultural distinctiveness* may have lowered evaluations of brands associated with a related cultural group if rivalry salience is high.
- **H4a**: When the salience of the rivalry between one’s established home culture and a related cultural group is high, consumers experiencing cultural distinctiveness will not exhibit an expansion of *in-group boundaries*.
- **H4b**: When the salience of the rivalry between one’s established home culture and a related cultural group is high, consumers experiencing cultural distinctiveness (vs. baseline condition) will exhibit an attenuation of the *pro-in-group bias*, or potentially a reversal, with an out-group bias toward products and brands associated with the related cultural group.

# 3. Overview of Studies

- The authors developed a *cultural distinctiveness* manipulation using three pilots.
- Hypotheses were tested in seven studies (see appendix A for summary).
    - Studies 1a and 1b tested whether *cultural distinctiveness* leads to a desire to connect with home (H1) and motivates *in-group boundary* expansion (H2).
    - Studies 2a and 2b assessed the impact of *cultural distinctiveness* on preference for products associated with related cultural groups due to the expected *pro-in-group bias* (H3).
    - Study 3 examined the role of *intergroup rivalry* salience during the Sochi Olympics, supporting the reversal predicted in high-rivalry settings (H4a and H4b).
    - Study 4 tested the complete theoretical framework (high and low rivalry) using a different method for assessing boundary expansion.
    - Study 5 examined the role of *intergroup rivalry* salience in an economic context in Asia.

# 4. Pilot Studies: The Cultural Distinctiveness Manipulation

- A manipulation of *cultural distinctiveness* was developed in the lab, using videos with sights and sounds of everyday situations in a foreign culture.
- Videos displayed streets in an Indian town, a cafeteria in an Asian college, and a street in an African town. Participants imagined being in the situation and thinking about differences and what they might miss from home.
- Control condition: Videos about insects.

## 4.1. Pilot 1

- Assessed the extent to which videos induced *cultural distinctiveness*.
- Participants: American undergraduate students (N=60).
- Method: Watched cultural distinctiveness or baseline videos and rated the extent to which the videos prompted feelings of *cultural distinctiveness* and provided open-ended responses.
- Results: Participants in the *cultural distinctiveness* condition reported significantly higher *cultural distinctiveness* compared to the baseline condition (p < .001). Open-ended thoughts were more indicative of *cultural distinctiveness* in the experimental condition (p < .001).

## 4.2. Pilot 2 (Mood Effects)

- Assessed the extent to which videos impacted affective states (PANAS scale).
- Participants: 39 American students.
- Results: No significant differences in positive or negative mood between conditions.

## 4.3. Pilot 3 (Need for Closure and Robustness across Cultures)

- Examined whether manipulation influenced need for closure, and assessed robustness with Asian participants using the African town video.
- Participants: University students in Hong Kong (N=80).
- Method: Watched cultural distinctiveness or baseline videos, completed a *cultural distinctiveness* scale, the Need for Closure Scale, and the PANAS scale.
- Results: Participants in the cultural distinctiveness condition reported higher *cultural distinctiveness* (p < .001). No significant differences in need for closure, positive mood, or negative mood.

# 5. Studies 1A and 1B: Cultural Distinctiveness and Boundary Expansion

- Studies examined whether *cultural distinctiveness* leads to a desire to connect with home (H1) and motivates *in-group boundary* expansion (H2). Participants were US citizens, and studies assessed the extent to which they would include neighboring regions in their home culture.

## 5.1. Study 1a

- Participants: 115 American students.
- Design: Between-subjects (cultural distinctiveness vs. baseline).
- Method: Rated desire to connect with home. Were presented with an exercise to define home and rated overlap between Canada and their notion of home.
- Results: Desire to connect with home was higher in the *cultural distinctiveness* condition (p < .03). Participants were more likely to perceive Canada as part of their in-group in the *cultural distinctiveness* condition (p < .001). A mediation analysis showed that the heightened desire to connect with home mediated the greater likelihood to include Canada in a broadened definition of home.

## 5.2. Study 1b

- Participants: 61 American students.
- Method: Watched cultural distinctiveness or baseline videos. Drew a circle on a map of the Americas to identify their "home."
- Results: Participants drew wider circles (implying expansion) in the *cultural distinctiveness* condition (p < .035).
- These studies suggest that *cultural distinctiveness* induces a desire to connect with home, motivating people to expand *in-group boundaries*. The authors expect this boundary expansion to be guided by available consumption alternatives.

# 6. Studies 2A and 2B: Brand and Product Preferences

- When consumers experiencing cultural distinctiveness are exposed to a product symbolizing a cultural group with the potential to be included within a broadened definition of home, they are likely to increase identification with this cultural group and expand their in-group boundaries accordingly, enhancing preferences for brands associated with this cultural group.

## 6.1. Study 2a

- Designed to test H3. Recruited participants from Minnesota and assessed their preferences for brands associated with Wisconsin and Texas.
- Participants: 89 undergraduate students in Minnesota.
- Design: Between-subjects (cultural distinctiveness vs. baseline). Randomly assigned to target choice scenarios (online seminar from University of Wisconsin vs. University of Texas or breakfast from Johnsonville (Wisconsin) vs. Bob Evans (Ohio)).
- Results: Participants in the cultural distinctiveness condition preferred the brands associated with the neighboring state (p < .035). In the neutral baseline condition, participants were indifferent between the two brands. Those in the cultural distinctiveness condition preferred the neighboring state brands significantly more than the distant state brands (p < .001).

## 6.2. Study 2b

- Designed to test the generalizability of the *pro-in-group bias* effect in a real choice setting.
- Participants: Mexican immigrants in the US and Mexicans living in Mexico.
- Design: Compared product choices of Mexican immigrants in the US (chronically high in cultural distinctiveness) with Mexicans living in Mexico.
- Method: Participants were presented with songs from two new rock bands (Argentine and Turkish) and asked to choose one.
- Results: Mexican immigrants in the US chose the Argentine song significantly more than random chance (p < .001). Only 58% of Mexicans living in Mexico chose the Argentine song, no different than a random choice.
- Results demonstrate the robustness of the *pro-in-group bias* toward related cultural groups, triggered by chronic *cultural distinctiveness*, using actual choices in a real-world setting.

# 7. Study 3: Rivalry Salience in Olympic Games

- The Olympic Games provides a pertinent context for studying rivalries.
- Significant rivalries exist between the United States and Canada in winter sports.
- Study 3 used this rivalry as a backdrop in a naturalistic study run during the Sochi Olympics, testing the reversal in preferences predicted by the framework in the context of salient intergroup rivalries (hypotheses 4a and 4b).

## 7.1. Method

- Participants: 130 American participants recruited using MTurk.
- Design: Randomly assigned to the *cultural distinctiveness* versus neutral video condition.
- Procedure:  Informed of the upcoming final for women’s curling (Canada and Sweden). Asked to imagine they were in Sochi and answered questions assessing support for the Canadian team.
- Rivalry Pretest: Confirmed high rivalry between Canada and the US during the Olympics (p < .001).

## 7.2. Results and Discussion

- An ANOVA on the support for the Canadian team revealed a significant reversal of *pro-in-group bias* (p < .03).
- Participants primed with *cultural distinctiveness* were significantly less likely to support the Canadian curling team. *Cultural distinctiveness* backfired when *intergroup rivalries* were salient.
- In high-rivalry contexts, a heightened need to connect to the home culture cannot be channeled via increased identification with a related cultural group, given its salient rival status. Individuals adopt an alternate route: rooting against or denigrating the rival.

# 8. Study 4

- Study 4 was designed to build on the findings of study 3 by examining the interplay between the expansion of *in-group boundaries* (triggered by *cultural distinctiveness*) and *intergroup rivalry* salience, and their subsequent impact on individuals’ support for brands associated with a related cultural group.

## 8.1. Method

- Participants: 126 undergraduate students from the University of Minnesota.
- Procedure: *Cultural distinctiveness* (video) vs. Neutral Baseline, Rivalry (text excerpt on MN vs. WI rivalry) or No Rivalry, then asked support for Wisconsin Badgers (vs. Texas Longhorns)
- Measures:  Support for Wisconsin Badgers, Identification with Upper Midwest (3 items), Salience of rivalry

## 8.2. Results and Discussion

- Manipulation Checks: Rivalry manipulation successful.
- Support for Wisconsin Badgers: Main effect of rivalry, and significant cultural distinctiveness * rivalry condition interaction (p < .05)
    - Low Rivalry: cultural distinctiveness significantly increased support (p < .02)
    - High Rivalry: directionally lowered support, n.s.
- Identification with the Upper Midwest:  Significant cultural distinctiveness * rivalry condition interaction (p < .025)
    - Low Rivalry: cultural distinctiveness significantly increased identification (p < .04)
    - High Rivalry: directionally lowered identification, p=.07
- Mediating Role of Identification with the Upper Midwest: Under low rivalry salience, this mediated the higher support for Wisconsin Badgers (95% CI = .01 to .54). Not significant in high-rivalry condition.

# 9. Study 5: Intergroup Rivalry in an Economic Context

-  Designed to examine the role of salient intergroup rivalries in the context of economic rivalries relating to brands. Singapore and Hong Kong, Asian cities that share sociohistorical roots but are also economic rivals, were chosen for the study.

## 9.1. Method

- Participants: 87 Hong Kong Chinese undergraduate students.
- Method: Randomly assigned to cultural distinctiveness (African town video) versus a neutral baseline condition (insects video). Participants evaluated Singapore Airlines and Air France on an eight-item scale. Also rated the extent to which they perceived a salient rivalry between Singapore Airlines and the airlines from Hong Kong.

## 9.2. Results and Discussion

- Repeated measures ANOVA with brand attitude as DV, cultural distinctiveness as between-subjects factor, rivalry salience as continuous predictor, and familiarity as covariates, yielded significant interaction effects.
- Significant main effect of cultural distinctiveness, F(1,81) ¼ 4.92, p < .05, g2 ¼ .06; a significant cultural distinctiveness  rivalry salience interaction, F(1, 81) ¼ 5.09, p < .05, g2 ¼ .06.
- When rivalry salience was low (1 SD below the mean), participants primed with cultural distinctiveness evaluated Singapore Airlines more favorably than those in the neutral baseline condition (p < .05).
- However, when rivalry salience was high (1 SD above the mean), this effect was reversed, with participants primed with cultural distinctiveness evaluating Singapore Airlines less favorably than those in the neutral baseline condition (p < .05).
- Participants’ attitudes toward Air France were not influenced by cultural distinctiveness or the levels of rivalry salience.

# 10. General Discussion

- With rising globalization, consumers increasingly come into contact with diverse cultures, leading to cultural distinctiveness and a desire to connect with home.
- The study investigates how cultural distinctiveness influences consumers’ preferences for brands that symbolize a related cultural group.
- Results show that consumers experiencing cultural distinctiveness are likely to prefer brands associated with a related cultural group, even if they are not the favored option. This pro-in-group bias is driven by a heightened desire to connect with “home,” prompting consumers to expand their in-group boundaries.
- However, this pro-in-group bias is attenuated when the salience of intergroup rivalries is high, resulting in less favorable evaluations of brands associated with a related cultural group.
- The hypothesized effects emerged across different cultural groups, preference assessments, and dependent variables.
- The research offers contributions to theories of ethnic-based consumption, consumer acculturation, social identity, and brand symbolism.
- The findings have several implications for brand managers, ethnic-oriented products, and expansion of target markets.
- The paper acknowledges limitations (focus on a single related cultural group) and highlights future research directions (triggers of mental states experienced by international migrants).

---

# Executive summary of 1. Introduction

- Introduction is that in globalization, *cultural distinctiveness* (perception of differences between host and home culture) affects consumption; aims to study whether feelings of cultural distinctiveness leads to broaden definition of home or not
- The research develops a conceptual framework for *cultural distinctiveness* and experiments to manipulate it.
- The paper investigates whether *cultural distinctiveness* induces consumers to expand their definition of "home" by broadening their *in-group boundaries*. This contrasts with past research that views cultural home as static [Triandis, 2007].
- The authors propose that consumers adaptively change their cultural home definition based on available options.

# Executive summary of 2. Theoretical Background

- *Cultural distinctiveness* leads to heightened desire to connect with home culture [Oberg, 1960; Nesdale and Mak 2003; Brewer, 1991].
- *Home* can be seen as a *dynamic concept* and as an *in-group*, and it will stretch if needed.
- Desire to connect is strong enough to motivate boundary expansion.
- When *cultural distinctiveness* high, *pro-in-group bias* occurs, unless *rivalry salience* is high.
- *H1*: Experiencing high levels of *cultural distinctiveness* will lead to an increased desire to connect with the home culture.
- *H2*: Experiencing cultural distinctiveness will prompt consumers to expand their *in-group boundaries* to incorporate related cultural groups into a broadened definition of home.
- *H3*: Expansion of *in-group boundaries* will lead to a *pro-in-group bias* toward brands and products associated with related cultural groups.
- *H4a*: When the salience of the rivalry between one’s established home culture and a related cultural group is high, consumers experiencing cultural distinctiveness will not exhibit an expansion of *in-group boundaries*.
- *H4b*: When the salience of the rivalry between one’s established home culture and a related cultural group is high, consumers experiencing cultural distinctiveness (vs. baseline condition) will exhibit an attenuation of the *pro-in-group bias*, or potentially a reversal, with an out-group bias toward products and brands associated with the related cultural group.

# Executive summary of 3. Overview of Studies

- A summary of all studies, including manipulated distinctiveness by video sight and sounds of different cultures (Indian town, Asian cafeteria, African town, insects)
- Studies 1a and 1b: test *cultural distinctiveness* by priming desire to connect home by *in-group boundary* expansion
- Studies 2a and 2b: tested preferences of product from other cultures and the relationship
- Study 3: test rivalry salience through the Olympics
- Study 4: the complete theoretical framework (with high and low rivalry)
- Study 5: test the role of economic context in Asia

# Executive summary of 4. Pilot Studies: The Cultural Distinctiveness Manipulation

- **Pilot 1** assesses the extent to which videos induced *cultural distinctiveness* among US undergraduate students, reporting significantly higher *cultural distinctiveness* compared to the baseline condition
- **Pilot 2** assesses mood effects, no significant differences in positive or negative mood between conditions
- **Pilot 3** (Conducted in HongKong) examines whether manipulation influenced need for closure, and assesses robustness with Asian participants using the African town video, showed that high *cultural distinctiveness* with no significant differences in need for closure, positive mood, or negative mood.

# Executive summary of 5. Studies 1A and 1B: Cultural Distinctiveness and Boundary Expansion

- Focuses on *cultural distinctiveness* leads to desire to connect to home (H1) which motivate *in-group boundary* expansion (H2).
- **Study 1a**: measured the desire to connect to home, then measure inclusion of Canda. Showed high desire to connect to home will include Canada in the definition of home
- **Study 1b**: participants draw circle on map, and with high cultural distintiveness they will draw bigger (to expand) the in-group boudaries
- These shows that *cultural distinctiveness* induces a desire to connect with home, motivating people to expand *in-group boundaries*

# Executive summary of 6. Studies 2A and 2B: Brand and Product Preferences

- When experiencing *cultural distinctiveness*, exposed product or brand symbolizing, they are likely to increase identification for greater brands.
- **Study 2a**: Test for H3 in participants from Minnesota, and measure their preferences for Wisconsin and Texas. Found that participants in *cultural distinctiveness* prefer the brands associated with the neighboring state (to expand in-group),
- **Study 2b**: test how *pro-in-group bias* affect a setting in the real world (among Mexican and the US vs Mexico). Measured the song preference (Argentine and Turkish). Mexican in the US choose Argentine significantly
- These show the robustness by triggered *cultural distinctiveness* using real-world setting

# Executive summary of 7. Study 3: Rivalry Salience in Olympic Games

- Studies rivalry salience and it impacts bias on the related cultural groups.
- High *cultural distinctiveness* are less likely to support
- The Olympic is the perfect setting, with competition that can heighten perceptions to *intergroup rivalry*

# Executive summary of 8. Study 4

- Build from study 3 and examines the interplay between the expansion of *in-group boundaries* triggered by *cultural distinctiveness* and *intergroup rivalry salience*.

# Executive summary of 9. Study 5: Intergroup Rivalry in an Economic Context

- Showed economic rivalries related to brands
- Found the model applied
- Participants were exposed to *cultural distinctiveness* and favored brand.

# Executive summary of 10. General Discussion

- With a lot of global contact to more and more diverse cultures, *cultural distinctiveness* plays a major factor when wanting to connect with home
- Preferences go towards brands to a related culture and is driven to connect “home.”
- Pro-in-group bias is only attenuated when rivalry is high and can result in not favorable brands,
- Results show across a lot of things
- Show implications for brand managers, ethnic-oriented products, and expansion of markets.

</details>

# Sridhar Seshadri
- Alan J. and Joyce D. Baltz Endowed Professor of Business Administration
### education
- Ph.D., Management Science, University of California at Berkeley, 1993
- Postgrad Diploma, Management, Indian Institute of Management at Ahmedabad, 1980
- Bachelor of Technology, Mechanical Engineering (w/distinction), Indian Institute of Technology at Madras, 1978
### research interest
- Stochastic modeling and applications in queueing systems, supply chain management and revenue management
### teaching
- Management Decision Models (BADM 374)
- Supply Chain Management (BADM 566) 
- Healthcare Process Mgmt (BADM 590)

```
title: Variations of the Bullwhip Effect Across Foreign Subsidiaries
authors: Seungrae Lee, Seung Jae Park, Sridhar Seshadri
journal: Manufacturing & Service Operations Management
published: 2023
```

# Executive Summary
- This study investigates **variations** in the **bullwhip effect** across **foreign subsidiaries** and examines how various factors specific to both countries and subsidiaries influence it.
- I use a **balanced panel data set** of Korean-owned subsidiaries to estimate the bullwhip effect, measured by the **ratio of purchase volatility to demand volatility (PAR)**, comparing it with the traditional **ratio of production volatility to demand volatility (BAR)**.
- I find that PAR significantly differs from the BAR, better reflecting the bullwhip effect's prevalence in foreign subsidiaries.
- Key **theoretical/conceptual framework** discussions:
    - Bullwhip effect: Defined as the phenomenon where the variance of orders exceeds that of sales, increasing upstream in the supply chain [Lee et al., 1997].
    - Country-specific factors:
        - Wage level: Higher production costs are expected to decrease the bullwhip effect [Bray and Mendelson, 2012].
        - Political stability: High policy uncertainty due to political instability can increase the bullwhip effect [Henisz and Delios, 2001; Zhong et al., 2019].
        - Import penetration: Increased market competition is expected to improve firm efficiency and reduce the bullwhip effect [Nickell, 1996; Berger and Hannan, 1998; Fabrizio et al., 2007].
    - Import transaction factors:
        - Import distance and international transportation costs: Longer replenishment lead times and higher ordering costs are expected to increase the bullwhip effect [Chen et al., 2000; Lee et al., 1997; Cachon, 1999; Chen and Lee, 2012].
    - Parent relational factors:
        - Expatriate managers: Facilitate information sharing between the parent firm and subsidiary, expected to reduce the bullwhip effect [Kobrin, 1988; Tan and Mahoney, 2004].
        - Ownership structure: Strong ownership (wholly owned subsidiaries) enables better operational coordination [Yu et al., 2009; Madhok, 2006].
- Key **findings**:
    - The bullwhip effect is significantly influenced by both country-specific and subsidiary-specific factors.
    - *H1a*: Higher wage levels in the foreign country decrease the subsidiary's bullwhip effect.
    - *H1b*: Greater political stability in the foreign country reduces the subsidiary's bullwhip effect.
    - *H1c*: Higher import penetration in the foreign country lowers the subsidiary's bullwhip effect.
    - *H2a*: With a high import ratio, longer import distances increase the subsidiary's bullwhip effect.
    - *H2b*: With a high import ratio, higher international transportation costs increase the subsidiary's bullwhip effect.
    - *H3a*: With a high intrafirm sales ratio, more expatriate managers decrease the subsidiary's bullwhip effect.
    - *H3b*: With a high intrafirm sales ratio, wholly-owned subsidiaries have a lower bullwhip effect than joint ventures.
    - Longer inventory days and weaker corporate governance increase the bullwhip effect.
- Implications: Managers should consider political stability and import penetration when locating subsidiaries in developing countries to mitigate the bullwhip effect. Relocating subsidiaries closer to suppliers or to countries with lower transportation costs can reduce the bullwhip effect. Deploying expatriate managers and wholly owning subsidiaries are recommended strategies.

#bullwhip_effect #foreign_subsidiary #multinational_firm #country_specific_factors #subsidiary_specific_factors #purchase_volatility #demand_volatility

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Over the past few decades, multinational firms have expanded production and services globally by locating subsidiaries in many foreign countries.
- Foreign subsidiaries have become influential on not only global supply chains but also the global economy.
- It is important to identify the success factors of foreign subsidiaries and understand the conditions under which such factors are more effective in terms of various performance metrics.
- The main goals of foreign subsidiaries are to exploit **low production costs (efficiency-seeking)** and **expand sales markets (market-seeking)** [Hill and Hult, 2019].
- A growing number of multinational firms operating in China reallocate production plants to Southeast Asian countries to seek lower labor costs, whereas other multinational firms expand production in China to gain more access to its enormous growing market [Cohen et al., 2018; Economist, 2019].
- Firm decisions on constructing global supply chain structures are not only affected by labor costs but also by international transportation (or logistics) costs and market conditions [Cohen et al., 2018].
- Previous studies investigate the effects of country-specific factors on various aspects of firm performance [Jain et al., 2014; Boute and Van Mieghem, 2015].
- Differently, I focus on the bullwhip effect of foreign subsidiaries and explore how various country-specific and foreign subsidiary-specific factors affect the variations of the bullwhip effect across foreign subsidiaries.
- In the seminal work of Lee et al. [1997], the **bullwhip effect** is defined as a phenomenon that occurs when the variance of orders is larger than that of sales, and the distortion tends to increase as one moves to the upstream.
    - They identify four contributing factors of the bullwhip effect (i.e., demand signaling, order batching, price fluctuations, and shortage gaming) and provide corresponding countermeasures.
- One of the well-known strategies for reducing the bullwhip effect is **information sharing** among firms in the supply chain that can improve supply chain coordination [Lee et al. 1997; 2004].
    - Chen et al. [2000] show that sharing demand information reduces the bullwhip effect, but sharing does not completely eliminate the effect.
    - Studies that use various model settings also demonstrate that information sharing improves supply chain performance [Chen 2003].

## 1.1. Empirical Studies on the Bullwhip Effect
- The seminal work of Cachon et al. [2007] measures the bullwhip effect using U.S. industry-level data and observes that the bullwhip effect is more prevalent in the wholesale industry than the retail and manufacturing industries.
    - In addition, they find that the bullwhip effect is affected by seasonality, price volatility, and demand shocks and that it did not decrease over time.
- Shan et al. [2014] measure the bullwhip effect using Chinese public firm-level data and observe its presence in about 67% of firms and all 14 industries.
    - They find a significant relationship between the bullwhip effect and firm-specific operational factors, such as inventory days, demand shock persistence, and demand seasonality.
    - In contrast to Cachon et al. [2007], they observe that the bullwhip effect decreased over time between 2002 and 2009.
- Bray and Mendelson [2012] measure the bullwhip effect using U.S. firm-level data and find that about 65% of firms exhibit the bullwhip effect.
    - Specifically, they consider the information transmission lead time on demand and decompose the bullwhip effect accordingly.
- Bray et al. [2019] use data of Chinese supermarket retailers and find empirical evidence that ration gaming triggers the bullwhip effect.
- Osadchiy et al. [2021] consider a supply network instead of a supply chain to investigate intrafirm bullwhip effect and demand amplification.
    - They find that although the intrafirm bullwhip effect exists, the demand variance of a downstream firm is not always less than that of an upstream firm.
- Behavioral studies also analyze the bullwhip effect using the “Beer Game” [Sterman 1992].
    - Croson and Donohue [2003] show that sharing point of sales data reduces the bullwhip effect and that this reduction is more beneficial to upstream firms than downstream firms.
    - Croson and Donohue [2006] show that underweighting the supply line (i.e., orders that have been placed but not yet received) on ordering decisions increases the bullwhip effect.
    - Croson et al. [2014] find another contributing factor of the bullwhip effect: coordination risk, that is, the risk that a firm manager is uncertain whether another firm manager deviates from coordinated decisions.

## 1.2. Research gap and Contribution
- Contrary to the previously mentioned studies, I estimate the variation of the bullwhip effect using data of Korean-owned foreign subsidiaries in more than 70 countries.
- First, my data set provides detailed information on a subsidiary’s purchase costs for a given period, enabling me to construct an alternative measure of the bullwhip effect by estimating the ratio of purchase volatility to demand volatility.
    - I control for time variations between foreign subsidiaries by using a balanced panel data set of subsidiaries between 2006 and 2013 and compute the bullwhip effect ratio for each subsidiary.
    - Comparing my ratio with the traditional ratio of production volatility to demand volatility widely used in existing literature, I find that my ratio better reflects the bullwhip effect and that the two ratios differ significantly, especially for manufacturing subsidiaries.
- Second, my cross-sectional estimation results suggest that various factors of foreign subsidiaries have a significant impact on their bullwhip effect.
    - Specifically, I show that the foreign subsidiary’s bullwhip effect is lower when the level of wages, degree of political stability, or degree of import penetration of a foreign country where the subsidiary is located is high.
    - In addition, when a subsidiary relies heavily on import purchases, its bullwhip effect is higher with a longer import distance or a higher international transportation cost.
    - Moreover, when the subsidiary’s primary customers are related entities (i.e., a parent firm or other subsidiaries that belong to the same parent firm), the bullwhip effect can be decreased by the parent firm’s deployment of more expatriate managers or sole ownership over the subsidiary.
- Third, these findings provide empirical evidence regarding analytical findings from the literature on the bullwhip effect and suggest new strategies for the bullwhip effect reduction.
    - For example, my findings on the effects of import distance and international transportation costs support that longer replenishment lead time and higher ordering costs contribute to the bullwhip effect [Lee et al. 1997, Chen et al. 2000], whereas the effect of foreign wage level confirms the analytical implication of a higher bullwhip effect under low production costs [Bray and Mendelson 2012].
    - Moreover, my empirical findings of a foreign subsidiary’s lower bullwhip effect under high political stability and high import penetration provide new remedies for the bullwhip effect reduction.
    - Multinational firms can also deploy expatriate managers or wholly own subsidiaries as new strategies to reduce the bullwhip effect of foreign subsidiaries.

# 2. Data and Amplification Ratios
- This section describes the data set and explains its pros and cons with respect to the data sets used in other studies.
- I define and compare two amplification ratios of the bullwhip effect.
- I refer to the home country as the country in which a parent firm is located and the foreign country as any country other than the home country.
- A foreign subsidiary indicates a foreign entity owned and controlled by the parent firm.

## 2.1. Data
- My empirical analysis is based on a data set of foreign subsidiaries owned by Korean multinational firms, obtained from the Export–Import Bank of Korea.
- It contains financial and operational information on individual subsidiaries operating businesses worldwide between 2006 and 2013, including subsidiary location, assets and liabilities, revenues and costs, employment, and industry of operation.
- The data also contain information on a subsidiary’s cost of goods sold (COGS), inventory, and purchase and sales values.
- An individual foreign subsidiary reports its purchase and sales values in six items every fiscal year:
    - (1) related subsidiaries located in the focal foreign country;
    - (2) other independent firms located in the focal foreign country;
    - (3) its parent firm in Korea (i.e., home country);
    - (4) other independent firms in Korea;
    - (5) related subsidiaries located in third countries;
    - (6) other independent firms located in third countries.
- This unique and critical information allows us to identify each foreign subsidiary’s operations, such as the total purchases (by adding all six items in purchase) or the total imports (by adding the value of purchases from Korea and third countries).
- Another novel feature of the data is that it contains details regarding employee information, distinguished by origin: local employees and Korean employees, further categorized by job positions.
- Compared with data sets used in previous empirical studies, my data set has pros and cons for analyzing the bullwhip effect:
    - First, my data set provides information on the value of purchases for each foreign subsidiary, enabling me to evaluate the bullwhip effect using a more appropriate methodology.
        - Previous studies [Cachon et al. 2007, Shan et al. 2014] use the ratio of the variance of production to that of the COGS to measure the bullwhip effect, wherein the COGS is a proxy for demand and production is estimated by the sum of the COGS and an inventory increase, which captures the amount of order to the supplier.
        - By contrast, I propose a ratio of the variance of purchases to that of the COGS to measure the bullwhip effect.
    - Second, my data set includes foreign subsidiaries that operate businesses in more than 70 countries.
        - These data allow me to confirm contributing factors to the bullwhip effect that previous studies have not empirically validated and shed new insight into other crucial factors of the bullwhip effect that have not yet been considered.
    - Third, compared with previous studies that use industry- or firm-level data and monthly or quarterly data [Cachon et al. 2007, Bray and Mendelson 2012, Shan et al. 2014], my annual subsidiary-level data exhibit less product aggregation but more time aggregation.
        - However, previous empirical evidence shows that aggregation, especially time aggregation, does not significantly change the value of bullwhip effect metrics [Bray and Mendelson 2012, Shan et al. 2014].
- Table 1 presents the descriptive statistics of Korean-owned foreign subsidiaries and their parent firms.
    - Most subsidiaries and their parent firms appear to operate in the retail, wholesale, and manufacturing industries.
    - A subsidiary’s intrafirm transactions account for more than 40% and 30% of total purchases and sales transactions, respectively.

## 2.2. Definition of Amplification Ratios
- Following the definition of Lee et al. [1997] of the bullwhip effect, I measure how an entity’s order is amplified relative to its customer demand by computing the order amplification ratio (OAR) as follows:
    - Order Amplification Ratio (OAR) : V[Order] / V[Demand]
- To estimate the OAR, I define two types of amplification ratios:
    - Benchmark Amplification Ratio (BAR): V[Production] / V[COGS]
        - Previous empirical studies [Cachon et al. 2007, Shan et al. 2014] use COGS as a proxy for demand and production, calculated as the sum of COGS and an inventory increase, as a proxy for order.
    - Purchase Amplification Ratio (PAR): V[Purchase] / V[COGS]
        - I propose an alternative estimate for the OAR by using the COGS as a proxy for demand and the number of purchases as a proxy for order.
- To remove long-run trends in the data, I adjust the series of production, COGS, and purchases by taking the log and the first difference [Cachon et al. 2007, Shan et al. 2014, Osadchiy et al. 2021].

## 2.3. Comparison of Amplification Ratios
- Cachon et al. [2007] show the similarity between the order and production series.
- However, they admit that “the production measure reflects inflow of material to an industry, which includes inbound deliveries as well internal production processes.”
- Chen and Lee [2012] also state that “in some cases, the order receipt information, if not available, is inferred from the sales and inventory data”.
- Thus, the variance of production can overestimate or underestimate the variance of orders for a manufacturer.
- We find that Productionit is not necessarily a good proxy for orders in the manufacturing industry because it includes not only the purchase cost but also internal production cost.
- Furthermore, if the purchase cost and internal production cost are highly and negatively correlated, then PARi will be greater than BARi. Otherwise, PARi will be less than BARi.

## 2.4. Existence of the Bullwhip Effect
- Table 2 reports the mean values for amplification measures of PAR and BAR for Korean-owned foreign subsidiaries between 2006 and 2013.
- I divide foreign subsidiaries by their major locations and industry sectors to explore variations in bullwhip effect intensities across countries and industries.
- A ratio greater than one in the PAR (BAR) suggests that a foreign subsidiary’s purchase (production) variance is greater than its demand variance.
- Several interesting features are observed among foreign subsidiaries in different locations and industries:
    - First, the bullwhip effect is prevalent all over the globe and tends to amplify as one moves to the upstream.
        - Specifically, the mean values of the PAR of the aggregate series in all major locations are significantly greater than one, and the mean values of the PAR for the manufacturing industry are greater than those for the retail and wholesale industries.
    - Second, although I observe the prevalence of the bullwhip effect worldwide, its strength is heterogeneous between countries.
        - These findings suggest that the heterogeneity of the bullwhip effect is associated with country-specific factors, which will be explored in the following section.
    - Third, comparing the PAR and BAR, I find that the magnitude of the difference between the two amplification ratios in the retail and wholesale industries is relatively small.
        - However, across manufacturing sectors, the mean values of the PAR are significantly greater than those of the BAR in all major locations.
        - This is because the correlation coefficients between the purchase costs and internal production costs are mostly negative.
        - I also find that, in most cases, the PAR tends to be greater than the IAR because the existence of the ordering cost results in a higher variance of purchase costs than that of internal production costs.
- These findings suggest that if we use the BAR as the bullwhip effect measure, different conclusions can be drawn on the prevalence of the bullwhip effect compared with that under the PAR.

## 2.5. Comparison with Prior Studies
- My findings from the BAR are aligned with the results from previous related studies.
- Shan et al. [2014] show that BAR values tend to be greater than one when using Chinese firm data but show a decreasing trend with the bullwhip effect.
- Conversely, Bray and Mendelson [2015] show strong production smoothing and bullwhip effects from the U.S. automobile industry.
- Overall, the bullwhip effect appears to be prevalent globally, wherein the PAR values of Korean-owned foreign subsidiaries are mostly greater than one, and they tend to intensify as one moves to the upstream.
- Meanwhile, different PAR values based on a foreign subsidiary’s location further suggest that country-specific factors can determine the strength of the bullwhip effect.
- Between alternative amplification ratios, I find that the PAR is significantly different from the BAR used in existing literature, such that a foreign subsidiary’s purchase variance is much greater than its production variance, and their difference is mostly statistically significant.
- In following sections, I explore how country-specific and foreign subsidiary-specific factors affect the degree of the bullwhip effect by using the PAR as the bullwhip effect measure.

# 3. Hypotheses and Estimation Strategy
- This section constructs three sets of hypotheses that reflect key features influencing the bullwhip effect of foreign subsidiaries.
- I then explain the estimation strategy.

## 3.1. Hypotheses and Main Variables
- I speculate that the bullwhip effect of foreign subsidiaries is related to three types of country-specific factors: wage level, political stability, and import penetration.

### 3.1.1. Country-Specific Factors
- First, I expect that the foreign subsidiary’s bullwhip effect is lower with higher production costs in the country where the subsidiary is located.
    - Bray and Mendelson [2012] show that a higher (lower) production cost decreases (increases) the bullwhip effect because when the production cost is high (low), the firm adjusts its order quantity and production levels less (more) aggressively.
    - For the empirical analysis, I use the level of wages to measure the production cost incurred in the foreign country, denoted by *Wage*.
    - *H1a (Wage)*: A foreign subsidiary’s bullwhip effect decreases with a higher wage level of the foreign country where the subsidiary is located.
- Second, I expect the foreign subsidiary’s bullwhip effect to decrease with a higher degree of political stability of the country where the subsidiary is located.
    - Under politically unstable conditions, foreign subsidiaries may find it challenging to follow coordinated decisions in their global supply chains [Yu et al. 2009].
    - For the empirical analysis, I set the variable denoted by *PoStab* to estimate the degree of political stability of the foreign country.
    - *H1b (Political Stability)*: A foreign subsidiary’s bullwhip effect decreases with a higher degree of political stability of the foreign country where the subsidiary is located.
- Third, I speculate that the foreign subsidiary’s bullwhip effect decreases with a higher degree of market competition because competition is widely known to improve firm efficiency [Nickell 1996, Berger and Hannan 1998, Fabrizio et al. 2007].
    - For the empirical analysis, I use the import penetration ratio denoted by *ImpPen* to measure the degree of foreign market competition where a subsidiary is located.
    - *H1c (Import Penetration)*: A foreign subsidiary’s bullwhip effect decreases with a higher degree of import penetration of the foreign country where the subsidiary is located.

### 3.1.2. Import Transaction Factors
- I speculate that when a foreign subsidiary relies more heavily on imports than local sourcing, its bullwhip effect is related to import transaction factors: import distance and international transportation costs.
- Reducing the replenishment lead time from a supplier is widely known as a strategy for reducing the bullwhip effect [Chen et al. 2000, Lee et al. 1997, Bray and Mendelson 2012].
- Order batching increases the bullwhip effect, and a high ordering cost, which includes transportation cost, is one of the main factors causing a firm to order in batch [Lee et al. 1997, Cachon 1999, Chen and Lee 2012].
- Considering these implications on import transactions, I speculate that a foreign subsidiary’s bullwhip effect is higher with longer import distances and higher international transportation costs when the subsidiary relies more heavily on imports than local sourcing.
- For the empirical analysis, I measure import distance using the physical distance between a focal foreign country and an exporting country, denoted by *Dist*.
- To measure international transportation costs between two countries, I use a matched partner technique to construct the ratio of Cost, Insurance, and Freight (CIF) to Free on Board (FOB), denoted by *CIF/FOB*.
- Given two types of purchase transactions, that is, local sourcing and imports, the impact of import distance and international transportation costs on the foreign subsidiary’s bullwhip effect is expected to be effective when the subsidiary’s amount of import transactions is large enough compared with that of local sourcing.
- Thus, I calculate the import ratio of the foreign subsidiary and then construct a dummy variable *Import* that is equal to one if a foreign subsidiary’s import purchases account for more than 75% of its total purchases and zero otherwise.
    - This variable indicates foreign subsidiaries with a high import ratio or import-intensive subsidiaries.
- *H2a (Import Distance)*: Under a high import ratio of a foreign subsidiary, a longer import distance increases the subsidiary’s bullwhip effect.
- *H2b (International Transportation Costs)*: Under a high import ratio of a foreign subsidiary, a greater international transportation cost increases the subsidiary’s bullwhip effect.

### 3.1.3. Parent Relational Factors
- I consider two unique features of foreign subsidiaries related to their parent firms, that is, expatriate managers from the parent firm and the parent firm’s ownership over the foreign subsidiary, which are expected to affect the foreign subsidiary’s bullwhip effect.
- Information sharing between the parent firm and its foreign subsidiaries can be promoted by deploying expatriate managers from the parent firm [Kobrin 1988, Tan and Mahoney 2004].
- Strong ownership over the foreign subsidiary enables a higher level of coordination between parent and subsidiary firms [Yu et al. 2009].
- Thus, under a parent firm’s strong (weak) ownership over a foreign subsidiary, the parent firm has greater (less) motivation to facilitate information sharing between related entities.
- Considering that information sharing with customers is a key remedy for reducing the bullwhip effect [Lee et al. 1997, Chen et al. 2000], I speculate that a foreign subsidiary’s bullwhip effect related to intrafirm sales (i.e., sales to the parent firm and other related subsidiaries) is small when expatriate managers are prevalent within the foreign subsidiary or when the parent firm exercises strong ownership over the foreign subsidiary.
- For the empirical analysis, using the information on job positions employees hold within the foreign subsidiary from my data, I classify Korean employees in the executive and managerial positions as expatriate managers.
    - I then construct the expatriate manager ratio denoted by *Expat* as the total number of expatriate managers over the total number of employees in managerial positions.
- For ownership structure, I construct an indicator variable denoted by *Whole* that equals one if the parent firm is the only investor in the foreign subsidiary (i.e., a wholly owned subsidiary) and zero if the foreign subsidiary is a joint venture.
- Given two types of sales transactions—intrafirm sales and interfirm sales (i.e., sales to unrelated entities)—the impact of expatriate managers and ownership on the foreign subsidiary’s bullwhip effect is expected to be effective when the number of intrafirm sales is large enough compared with that of interfirm sales.
- Thus, I calculate the intrafirm sales ratio of the foreign subsidiary and then construct the dummy variable *IntraSales*, which equals one if a foreign subsidiary’s average intrafirm sales account for more than 75% of its total sales and zero otherwise.
    - This variable indicates foreign subsidiaries with a high intrafirm sales ratio or intrafirm sales-intensive subsidiaries.
- *H3a (Expatriate Managers)*: Under a high intrafirm sales ratio of a foreign subsidiary, a higher number of expatriate managers from the parent firm decreases the subsidiary’s bullwhip effect.
- *H3b (Whole Ownership)*: Under a high intrafirm sales ratio of a foreign subsidiary, wholly owned subsidiaries have a lower bullwhip effect than joint-venture subsidiaries.

## 3.2. Control Variables
- To control other variables that may affect the bullwhip effect, I include several foreign subsidiary-specific characteristics by considering the existing literature.
- Following the analyses of Cachon et al. [2007] and Shan et al. [2014], I add a foreign subsidiary’s autoregressive coefficient (ρ) as a proxy for the persistence of demand shocks.
- Additionally, I include inventory days, denoted by *InvDays*, and a foreign subsidiary’s sales, denoted by *Sales*.
- The gross profit margin of a foreign subsidiary, denoted by *GM*, is also included.
- Furthermore, I adjust for inflation by using the producer price index of the corresponding foreign countries to deflate subsidiary-specific values.
- In addition to the previously mentioned control variables used as the main determinants of the degree of the bullwhip effect in the existing literature, I control the subsidiary’s experience in the foreign country because the subsidiary can be more efficient as it adapts to the new environment [Gong 2003].
    - I set the variable *Age* by computing the number of years since the establishment year of a foreign subsidiary to 2006.
- I also consider corporate governance measures because the parent firm can distort the foreign subsidiary’s operations under weak corporate governance [Shleifer and Vishny 1997].
    - To control this, I compute the foreign institutional ownership ratio of the parent firm denoted by *Inst* as a proxy for the corporate governance level [Chen et al. 2012].

## 3.3. Sample Description of Hypothesis Tests
- To test my hypotheses, I include foreign subsidiaries that only conduct transactions with firms in the focal foreign country or home country, or both.
- I remove the influence of the length of the time horizon on the variances of production, purchases, and COGS by including foreign subsidiaries observed every year during my sample period.
- After considering the industry of operation, I use 1,490 foreign subsidiaries that operate in the retail, wholesale, and manufacturing industries for my empirical analysis.
- Table 3 reports the summary statistics of the variables used in testing my hypotheses.

## 3.4. Model Specifications
- To test my hypotheses on a foreign subsidiary’s bullwhip effect relative to country-specific characteristics (i.e., Hypothesis 1), I regress the following baseline equation:
    - ARijkl  β0 + β1Wage k + β2PoStab k + β3ImpPenk + α′Cil + γl + γj + γk + εijkl
- I then run two separate regressions to examine the import transaction factors and parent relational factors on the foreign subsidiary’s bullwhip effect.
- I consider the influence of import transaction factors (i.e., Hypothesis 2) by adding import transaction variables to the baseline equation, as follows:
    - ARijkl  β0 + β1Wage k + β2PoStab k + β3ImpPenk + β4Distk + β5CIF=FOBk + β6Import i + β7Distk × Import i + β8CIF=FOBk × Import i + α′Cil + γl + γj + γk + εijkl
- To consider the influence of parent relational factors on the foreign subsidiary’s bullwhip effect (i.e., Hypothesis 3), I estimate the following equation by adding the Expat, Whole, and IntraSales variables to the baseline equation:
    - ARijkl  β0 + β1Wagek + β2PoStabk + β3ImpPen k + β4Expat i + β5Wholei + β6IntraSales i + β7Expat i × IntraSalesi + β8Wholei × IntraSales i + α′Cil + γl + γj + γk + εijkl

# 4. Estimation Results
- This section first provides the estimation results, using the PAR as the measure of the bullwhip effect and the BAR as the benchmark measure in Section 4.1.
- Then, I compare my findings with previous studies in Section 4.2.

## 4.1. Main Results
- Tables 4, 5, and 6 present the cross-sectional regression results for Hypotheses 1, 2, and 3, respectively, where heteroskedasticity-consistent standard errors that correct for clustering at the foreign country level are reported in all specifications.
- First, examining the effects of country-specific factors on a foreign subsidiary’s PAR, the coefficients of Wage, PoStab, and ImpPen are all negative and statistically significant in Column (1) of Table 4.
    - These results imply that a higher wage level, a higher degree of political stability, or a higher degree of import penetration in the foreign country decreases the subsidiary’s bullwhip effect, supporting Hypothesis 1.
- Considering the influence of import transaction factors, Column (1) in Table 5 shows that the coefficients of the interaction terms of Dist and CIF/FOB with Import are positive and statistically significant.
    - These results suggest that the foreign subsidiary’s import distance and international transportation costs are effective for increasing the bullwhip effect of the subsidiary with a high import ratio, all of which support Hypothesis 2.
- When the influence of parent relational factors is considered, Column (1) in Table 6 shows that the coefficients of the interaction terms of Expat and Whole with IntraSales are negative and statistically significant.
    - These results imply that the parent firm’s deployment of expatriate managers or sole ownership over the foreign subsidiary (i.e., wholly owned subsidiary) significantly lowers the bullwhip effect of the subsidiary with a high intrafirm sales ratio, supporting Hypothesis 3.
- For the control variables, the coefficients of InvDays and Inst are statistically significant.
    - Their signs suggest that longer inventory days of a foreign subsidiary or weak corporate governance of a parent firm significantly increase the subsidiary’s bullwhip effect.
- I note that to test Hypothesis 2 (3), I use the dummy variable of Import (IntraSales) that represents foreign subsidiaries with a high import ratio (intrafirm sales ratio).
    - For the robustness check, I use the raw values of the two ratios. The estimation results using the ratio variables confirm my hypotheses.

## 4.2. Comparison with Prior Studies
- Examining the estimation results of using the BAR as a benchmark, it is notable to find that most of my hypotheses are not supported by the BAR, although the empirical findings of Cachon et al. [2007] and Shan et al. [2014] tend to be aligned with both the PAR and the BAR.
- Specifically, Column (2) of Table 4 shows that while the coefficient of ImpPen is statistically significant as in Column (1), the coefficients of Wage and PoStab are consistently insignificant.
- Thus, I find that the BAR is not an effective measure to support my hypotheses.
- For the control variables in this study, insignificant effects of the AR(1) coefficient (AR1rho) in both bullwhip amplification ratios support the negligible impact of persistent demand shocks on the bullwhip effect, as shown by Cachon et al. [2007].
- Conversely, positive and significant coefficients of a foreign subsidiary’s inventory days (InvDays) in both bullwhip ratios provide supportive evidence on the effect of inventory days on the bullwhip effect, as shown by Shan et al. [2014].
- Regarding the sales variable (Sales), whereas Cachon et al. [2007] show the positive effect of sales on the bullwhip effect, both amplification ratios of this study and the results of Shan et al. [2014] do not show its significance.
- Finally, the estimation results of my study using both amplification ratios and those of Shan et al. [2014] provide the same evidence that the gross margin (GM) is not closely related to the bullwhip effect.
- All in all, these findings suggest that my estimation results, using both PAR and BAR, are well-aligned with the main determinants of the bullwhip effect found by the two previous studies.
- I emphasize that most of my hypotheses are not supported by the BAR because the BAR includes the internal production cost and the purchase cost, the PAR only considers the purchase cost as a proxy for orders.
- Thus, the PAR is, indeed, a better proxy for the order amplification ratio (provided in Equation (1)) compared with the BAR, and using a less precise measure of the bullwhip effect, BAR may impede validating the contributing factors in the bullwhip effect.
- Notably, I further examine the strength of the bullwhip effect for individual industries by identifying its determinants.
- My hypotheses are mostly well supported in the manufacturing industry, whereas they are only partially supported in the retail and wholesale industries.

# 5. Discussion
- This study contributes to the existing literature on the bullwhip effect by empirically exploring variations of the bullwhip effect across foreign subsidiaries.
- Although a multinational firm operates its foreign subsidiaries by considering multiple factors, such as low foreign production costs or proximity to the destination market [Levy 1995, Cohen et al. 2018], my study examines the extent to which these decision-driving factors affect a foreign subsidiary’s bullwhip effect.
- Foreign labor cost is one of the most important determinants of firm decisions about constructing a global supply chain structure [Levy 1995, Cohen et al. 2018].
- Although a lower cost of labor is obviously a critical factor for attracting more multinational firms to transfer production bases to developing countries, I show that lower labor costs could instead increase the bullwhip effect because of the excessive production level changes.
- My findings also suggest that the political stability of a foreign country, which is not widely considered in supply chain literature, is critical to reducing the foreign subsidiary’s bullwhip effect because political instability increases uncertainty in the business environment.
- Furthermore, highly competitive foreign market conditions can reduce a foreign subsidiary’s bullwhip effect by creating an environment that improves subsidiary production efficiency.
- Thus, based on my results, managers of multinational firms are advised that, although locating foreign subsidiaries in developing countries tends to reduce production costs, doing so could instead increase the bullwhip effect if these countries are politically unstable or have a lower import penetration.
- The global supply chain tends to have long transportation distances with high ordering costs, and global logistic factors have huge variations between countries [Levy 1995, Hausman et al. 2005, Jain et al. 2014].
- This study confirms that import distance and international transportation costs increase a foreign subsidiary’s bullwhip effect when it imports a large number of products.
- To the best of my knowledge, my study is the first to provide empirical evidence that the bullwhip effect is lower under short distances or low ordering costs from suppliers.
- My findings on the import transaction factors along with the country-specific factors provide critical implications for the reshoring operations of multinational firms in developed countries.
- When a foreign subsidiary relies heavily on the sales to its related entities, such as its parent firm or other related subsidiaries, an effective strategy to reduce the bullwhip effect is having more expatriate managers from the parent firm or being wholly owned by the parent firm.
- These findings would apply uniquely to the foreign subsidiary-level bullwhip effect and have not been previously explored in the literature.
- Thus, these strategies can provide new remedies for multinational firms to reduce the bullwhip effect of their foreign subsidiaries.
- Although I limit the scope of this study to the variations of the bullwhip effect across foreign subsidiaries, this study also provides implications on the traditional determinants of the bullwhip effect.
- In this regard, the study empirically supports the analytical findings on the bullwhip effect that have not yet been empirically validated (i.e., production costs, replenishment lead time, and ordering costs) and suggests new critical factors for the bullwhip effect (i.e., political stability and market competition).
- In practice, a firm often deploys its staff to the partner firms or occasionally acquires the partner firm’s equity shares mostly to have sustainable demand/supply of products or to promote knowledge transfer.
- My empirical findings further imply that these kinds of strong relationships between independent firms through either labor mobility or equity acquisition may reduce the bullwhip effect in their supply chain.
- This study has a few limitations:
    - First, although my results point out that purchase is a more accurate and appropriate measure for the order amount than production (i.e., the sum of sales and inventory change), purchase is still not the same as order.
    - Second, I did not explore the impact of the bullwhip effect on a foreign subsidiary’s financial performance.
    - Third, the results may still be limited to Korean multinational firms.
    - Fourth, although my instruments come close to satisfying the exclusion restriction in the IV analysis for Hypothesis 3, it is possible that I may miss any possible direct pathways from the IVs to the bullwhip effect.
    - Finally, it would be interesting to consider the supply network of foreign subsidiaries and explore other unique features of the bullwhip effect across foreign subsidiaries.

---

# Executive summary of 1. Introduction
- Multinational firms increasingly use foreign subsidiaries for global production and service delivery, making these subsidiaries key players in global supply chains and the economy.
- Identifying factors contributing to the success of foreign subsidiaries is crucial.
- While subsidiaries aim to exploit low production costs and expand markets, factors like labor, transportation costs, and market conditions influence global supply chain structure.
- This study focuses on the bullwhip effect in foreign subsidiaries, examining how country-specific and subsidiary-specific factors affect its variations.
- The bullwhip effect, defined as increased order variance relative to sales variance upstream in the supply chain, is influenced by demand signaling, order batching, price fluctuations, and shortage gaming.
- Information sharing is a key strategy for reducing the bullwhip effect, although it doesn't eliminate it entirely.

# Executive summary of 2. Data and Amplification Ratios
- This section describes the data set of Korean-owned foreign subsidiaries, highlighting its strengths and weaknesses compared to data sets used in prior research.
- Two measures of bullwhip effect amplification are defined: the benchmark amplification ratio (BAR) based on production variance and the purchase amplification ratio (PAR) based on purchase variance.
- The data set includes detailed information on subsidiary purchases, sales, COGS, inventory, and employee information, facilitating a comprehensive analysis of the bullwhip effect.
- The PAR is proposed as an alternative measure of the bullwhip effect, calculated as the ratio of purchase variance to COGS variance.
- The study also explores the existence of the bullwhip effect by examining the amplification ratios for Korean-owned foreign subsidiaries in different locations and industries, and comparing with prior studies.
- A key finding is that the PAR is significantly different from the BAR, indicating that a foreign subsidiary's purchase variance is often much greater than its production variance.

# Executive summary of 3. Hypotheses and Estimation Strategy
- This section outlines the hypotheses regarding factors affecting the bullwhip effect in foreign subsidiaries, focusing on country-specific factors, import transaction factors, and parent relational factors.
- Key country-specific factors include wage level, political stability, and import penetration, which are expected to negatively impact the bullwhip effect.
- Import transaction factors, such as import distance and international transportation costs, are expected to increase the bullwhip effect, especially when the import ratio is high.
- Parent relational factors, including expatriate managers and ownership structure, are hypothesized to reduce the bullwhip effect, particularly for subsidiaries with high intrafirm sales ratios.
- Control variables, such as the autoregressive coefficient, inventory days, sales, and gross margin, are included to account for other factors that may influence the bullwhip effect.
- The sample includes foreign subsidiaries that conduct transactions primarily with firms in the focal foreign country or home country, with industry of operations controlled.

# Executive summary of 4. Estimation Results
- This section presents the results of cross-sectional regression analyses for the hypotheses, using the PAR as the primary measure of the bullwhip effect and the BAR as a benchmark.
- The results show that higher wage levels, greater political stability, and higher import penetration in the foreign country decrease the subsidiary's bullwhip effect, supporting the first set of hypotheses.
- Import distance and international transportation costs are found to increase the bullwhip effect for subsidiaries with a high import ratio, supporting the second set of hypotheses.
- The deployment of expatriate managers and sole ownership by the parent firm are found to significantly lower the bullwhip effect for subsidiaries with a high intrafirm sales ratio, supporting the third set of hypotheses.
- Control variables such as inventory days and corporate governance also have significant effects on the bullwhip effect.
- In contrast, the BAR is not found to be an effective measure for supporting the hypotheses, indicating that the PAR is a more accurate reflection of the bullwhip effect.

# Executive summary of 5. Discussion
- This study contributes to the literature by empirically exploring the variations of the bullwhip effect across foreign subsidiaries.
- It is shown that decision-driving factors, such as foreign labor costs, affect the bullwhip effect.
- It is recommended that managers of multinational firms consider political stability and import penetration when locating foreign subsidiaries in developing countries.
- When a foreign subsidiary relies heavily on sales to its related entities, an effective strategy to reduce the bullwhip effect is having more expatriate managers from the parent firm or being wholly owned by the parent firm.
- The limitations of the study include data limitations in measuring "order", the impact of the bullwhip effect on a foreign subsidiary's financial performance, and its possible limitation to Korean multinational firms.

</details>

```
title: Dual Sourcing Inventory Systems: On Optimal Policies and the Value of Costless Returns
authors: Ganesh Janakiraman, Sridhar Seshadri
journal: Production and Operations Management
published: 2017
```
# Executive Summary
- We study **dual sourcing inventory systems** with backordering and stationary stochastic demands.
- There are two supply sources, differing in unit prices and lead times. One offers a lower unit price but has a longer lead time.
- We investigate the value of **costless returns** to the cheaper, longer lead-time supplier (returning inventory without incurring any cost beyond the holding cost).
    - We prove that this option has zero value under stationary, stochastic demand environments, regardless of demand uncertainty. This means the **optimal long-run average cost** remains the same whether the return option is available or not.
- Key **theoretical findings**:
    - Optimal policies for dual sourcing systems (with and without returns) exhibit complex structures.
    - Optimal expedited ordering policy follows an order-up-to structure.
    - Sensitivity properties of the optimal policy are derived, showing how order quantities from the two suppliers change with respect to changes in the state vector.
- Important **findings/arguments** include:
    - The **optimal expedited order quantity** in any period follows an order-up-to rule, raising the expedited inventory position to a certain level, denoted as *S E* (q), if possible, but never exceeding *y* (the minimizer of the expected holding and shortage cost function).
    - When the system starts with an **expedited inventory position** of *y* and no outstanding orders from the regular supplier, any optimal policy orders a non-negative quantity from the regular supplier.
    - The function prescribing the order quantities from R (regular supplier) is non-increasing, and a positive change to the quantity ordered from R in the most recent period results in a smaller negative change in the optimal quantity ordered from R in the current period.
    - "Shifting the pipeline vector" away from E (expedited supplier) results in a decrease in the optimal order quantity from R if the pipeline vector is non-negative.
- **Practical implication**: The optimal cost of a dual sourcing system depends exclusively on the distribution of the "noise" in demand and is independent of the size of the predictable stream.

#dualsourcing #inventorysystems #costlessreturns #optimalpolicies #samplepathanalysis #multiplesuppliers #backordering #stochasticdemand #orderupto #sensitivityanalysis

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- We study an inventory system with stationary stochastic demands, backordering, and two supply sources (R and E). R has a lower unit price but a longer lead time.
- The key question: What is the value of the option to return/sell inventory to R at cost (i.e., costless returns)?
    - We show that the value of this option is zero, regardless of the demand distribution's uncertainty, as long as it's stationary.
- With a single supply source, costless returns have zero value because the optimal policy brings the inventory position to a constant level [Sheopuri et al., 2010].
    - The dual sourcing system requires significant analysis to show the same result because its optimal policy doesn't have a simple structure.
- Our analysis produces new results on optimal policies in dual sourcing systems with and without the return option.
- Janakiraman et al. [2015] use our main result to show that the optimal cost in dual sourcing systems (without returns) is insensitive to a deterministic, constant shift in the demand distribution.
- Several proofs use sample path arguments instead of induction-based arguments.
    - The analysis in section 4 is illuminating because the result is an infinite horizon result without a finite horizon counterpart, not following from standard convergence arguments.
- Our work joins the growing stream of analysis of dual sourcing inventory systems [Hua et al., 2015; Janakiraman et al., 2015; Li and Yu, 2014; Allon and Van Mieghem, 2010; Sheopuri et al., 2010; Song and Zipkin, 2009; Veeraraghavan and Scheller-Wolf, 2008].
    - See Minner [2003] for a review of prior work.

# 2. Model and Notation
- The model and notation are borrowed from Janakiraman et al. [2015].
- *D* refers to the dual sourcing inventory system without the return option.
    - R is the "regular supplier" and E is the "expedited supplier."
    - Lead time from R is *lR* periods, from E is *lE* periods, and *lR* > *lE*.
    - The lead time difference is *d* = *lR* - *lE*.
- Let *h* denote any feasible ordering policy and Θ denote the set of all feasible policies.
- Sequence of events in every period *t* under policy *h*:
    - (1) Order placed from R in *t* - *lR* for *qh;R t-lR* units is delivered, and order placed from E in *t* - *lE* for *qh;E t-lE* units is delivered (if *lE* > 0). Net-inventory is *zh t*.
    - (2) Ordering decisions are made: *qh;R t* >= 0 units are ordered from R and *qh;E t* >= 0 units are ordered from E. If *lE* = 0, these *qh;E t* >= 0 units are immediately delivered.
    - (3) Demand, *dt*, is realized.
    - (4) Cost *cht* for the period is charged:
        - If *lE* > 0: *cht* = *h* \* (*zh t* - *dt*)+ + *b* \* (*dt* - *zh t*)+ + *c* \* *qh;E t*
        - If *lE* = 0: *cht* = *h* \* (*zh t* + *qh;E t* - *dt*)+ + *b* \* (*dt* - *zh t* - *qh;E t*)+ + *c* \* *qh;E t*
    - *h* and *b* are per-unit holding and backorder costs, respectively.
    - *c* > 0 is the unit-cost premium charged by E over R. (We do not include a unit procurement cost for R because the difference in the unit costs of E and R exclusively determines the infinite horizon, average-cost optimal policy.)
    - We assume *c* < *b* \* (*lR* - *lE*), otherwise single-sourcing from R is optimal.
- Demands in different periods are i.i.d. *D* is the random demand in any period; *F* is its distribution function.
- ^*D* is identical to *D* except that negative orders (costless returns) on R are allowed.
    - The set of feasible policies in ^*D* is a larger set ^*H* ⊇ *H*. (Unit procurement cost from R is zero, so there is no revenue or cost for returning a unit to R in ^*D*.)

# 3. Results on Finite- and Infinite-Horizon Optimal Policies
- We present properties of the optimal policy in ^*D* (dual sourcing with negative orders from R allowed).
    - Corresponding properties can also be verified in *D*.
- Objective: minimize the expected discounted sum of costs over a finite planning horizon (periods 1, 2, ..., *T*) with discount factor *a* ∈ (0, 1).
- Demands in periods 1, 2, ..., *T* are i.i.d. Random variable representing demand over *k* periods is *D*[1, *k*].
- Let *G*(*y*) = *h* \* E[(*y* - *D*[1, *lE* + 1])+ ] + *b* \* E[(*D*[1, *lE* + 1] - *y*)+ ] denote the expected holding and shortage cost function *lE* + 1 periods ahead.
    - *G* is a convex function and assumed to have a unique minimizer, *y*.
    - The existence of an optimal policy is assumed as granted.

## 3.1. Optimal Policy Properties
- We consider the case where *lE* > 0. (The case of *lE* = 0 can be handled with minor changes.)
- Let *It* denote the expedited inventory position at the beginning of period *t* before orders are placed but after deliveries.
- From Lemma 2.1 of Sheopuri et al. [2010], an optimal policy *h* can be described as follows:
    - For every period *t*, there exists a pair *q*R*t*(.) and *q*E*t*(.) prescribing the ordering quantities from R and E, respectively, in that period as functions of the state vector in that period only through *It* and the vector *qt* = (*q*R*t+1-d*, *q*R*t+2-d*, ..., *q*R*t-1*).
    - *qt* denotes the vector of orders placed on R before period *t* which are yet to “enter the expedited pipeline”.

### 3.1.1 Claims on Optimal Policies
- **Claim 1: (Order-Up-To Structure of Optimal Expedited Ordering Policy in ^D)**
    - Every optimal policy is characterized by a set of functions {*S*E*t*: *t* = 1, 2, ..., *T*}, mapping *Rd-1* to *R1* such that the expedited order quantity in period *t* follows the rule of raising the expedited inventory position to the level *S*E*t*(*qt*), if possible (i.e., an order-up-to rule).
        - That is, *q*E*t*(*It*, *qt*) = (*S*E*t*(*qt*) - *It*)+.
    - Moreover, *S*E*t*(*q*) <= *y* for all *q*.
- **Claim 2: (Bound on Regular Order Quantity in ^D)**
    - Assume that at the beginning of period *t* (before orders are placed), the system starts with an expedited inventory position *It* = *y* and no other orders from the regular supplier in the pipeline (i.e., the regular inventory position is also *y*; *qt* = 0).
    - Then, any optimal policy orders a non-negative quantity from R in period *t*.
        - Mathematically, *q*R*t*(*y*, 0) >= 0 for all *t*.

## 3.2. Sensitivity Properties of Optimal Policy
- We want to understand how the optimal order quantities from the two suppliers change with respect to changes in the state vector.
- Parts (a) and (b) of Claim 3 have been established for System D recently in the papers by Li and Yu [2014] and Hua et al. [2015].
- Let *ei* denote the unit vector (0, 0, ..., 0, 1, 0, 0, ..., 0) with the value 1 in its *i*th component, for any *i* ∈ {1, 2, ..., *d*}.
- **Claim 3: (Sensitivity Properties of Optimal Policy in ^D)**
    - Consider any period *t* ∈ {1, 2, ..., *T*}. There exist optimal ordering functions *q*R*t* and *q*E*t* that are non-increasing and satisfy the following statements.
        - (a) The function *q*R*t* satisfies the chain of inequalities:
            - - *x* <= *q*R*t*(*It*, *qt* + *xed*) - *q*R*t*(*It*, *qt*) <= *q*R*t*(*It*, *qt* + *xed-1*) - *q*R*t*(*It*, *qt*) <= ... <= *q*R*t*(*It*, *qt* + *xe1*) - *q*R*t*(*It*, *qt*) <= 0 for any *x* >= 0.
        - (b) The function *q*E*t* satisfies the chain of inequalities:
            - - *x* <= *q*E*t*(*It*, *qt* + *xe1*) - *q*E*t*(*It*, *qt*) <= *q*E*t*(*It*, *qt* + *xe2*) - *q*E*t*(*It*, *qt*) <= ... <= *q*E*t*(*It*, *qt* + *xed*) - *q*E*t*(*It*, *qt*) <= 0 for any *x* >= 0.
        - (c) Statement (b), along with Claim 1, implies that the corresponding function *S*E*t* satisfies the chain of inequalities:
            - -*x* <= *S*E*t*(*qt* + *xe1*) - *S*E*t*(*qt*) <= *S*E*t*(*qt* + *xe2*) - *S*E*t*(*qt*) <= ... <= *S*E*t*(*qt* + *xed-1*) - *S*E*t*(*qt*) <= 0 for any *x* >= 0.
- Optimal order quantities from R and E are both decreasing with respect to the state vector, but the rates of decrease are bounded above by 1 along every component.
- Optimal quantity ordered from R is more sensitive to orders placed more recently on R and least sensitive to the current expedited inventory position.
- Optimal quantity ordered from E is more sensitive to orders placed on R farther in the past and most sensitive to the current expedited inventory position.

## 3.3. Infinite Horizon Results
- While the results of this section (Claims 1–3) were derived under the assumption of a finite planning horizon (*T* < ∞), standard dynamic programming, convergence arguments [Huh et al., 2011; Sch€al, 1993] can be used to show their validity for the infinite horizon case (*T* = ∞) for any discount factor *a* ∈ (0, 1).
- **Claim 4: (Infinite Horizon Results in ^D)**
    - Consider the infinite horizon, discounted-cost problem with any discount factor *a* ∈ (0, 1). Let *q*R(*I*; *q*) and *q*E(*I*; *q*) denote the functions prescribing the order quantities from R and E, respectively, in any period under an optimal policy when the expedited inventory position at the beginning of that period is *I* and the vector of outstanding orders from R is *q*.
    - All the results in Claims 1–3 hold with the modification that the time subscript be dropped from the appropriate functions.
        - (a) Monotonicity: The function *q*R is non-increasing and satisfies *q*R(*y*, 0) >= 0.
        - (b) Sensitivity: The inequality *q*R((*I*, *q*) + (0, 0, *x*)) >= *q*R(*I*; *q*) - *x* holds for any (*I*, *q*) and any *x* > 0. That is, a positive change to the quantity ordered from R in the most recent period results in a smaller negative change in the optimal quantity ordered from R in the current period.
        - (c) Shift-Monotonicity: The inequality *q*R(*I* + *x*; *q1*, *q2*, ..., *qd-1*) >= *q*R(*I*; *x*, *q1*, *q2*, ..., *qd-3*, *qd-2* + *qd-1*) holds for any *I* ∈ *R* and any non-negative vector (*q1*, *q2*, ..., *qd-1*) ∈ *Rd-1+* and any *x* >= 0. That is, “shifting the pipeline vector” away from E results in a decrease in the optimal order quantity from R if the pipeline vector is non-negative.
        - (d) Base-Stock Optimality: There exists a function *S*E mapping *Rd-1* to *R1* such that *q*E(*I*; *q*) = (*S*E(*q*) - *I*)+. Moreover, *S*E(*q*) <= *y* for all *q*, which implies that *q*E(*y*, 0) = 0.
- Claims 1–4 hold in D, except Claim 2, which is vacuous or redundant in D.

# 4. Value of the Costless Return Option
- We show that the option of placing negative orders (costless returns) on the regular supplier does not provide any cost advantage. This result is true when the performance measure used is the infinite horizon long run average cost.
- We first need a preliminary result which establishes an identical statement for the infinite horizon discounted cost model but under a specific starting state.

## 4.1. Claims on Costless Returns

- **Claim 5: (Consider the Infinite Horizon, Discounted Cost Problems in D and ^D)**
    - Assume that at the beginning of period 1 (before orders are placed), the system (in either case) starts with an expedited inventory position of *y* and no other orders from the regular supplier in the pipeline; that is, the regular inventory position is also *y*.
    - Then, the optimal infinite horizon discounted costs in D and ^D are equal.
    - In the case in which negative orders from R are allowed, there exists an optimal policy which, when starting from the assumed starting state in period 1, orders only a non-negative quantity from R in every period. Thus, the option of placing a negative order from R is never exercised, which immediately implies the desired result.

## 4.2. Theorem on Costless Returns
- Following Huh et al. [2011] and the related discussion in Sheopuri et al. [2010], the vanishing discount approach of Sch€al [1993] can be used to verify that the optimal infinite horizon discounted costs in D and ^D converge to the optimal infinite horizon average costs in D and ^D, respectively, as the discount factor a converges to 1.
    - These optimal infinite horizon average costs are independent of the starting state. Thus, Claim 5 implies the following result.
- **Theorem 1**: The optimal long run average costs in D and ^D are identical. In other words, the ability to order negative amounts from R does not provide a cost advantage. Thus, the value of the option to make costless returns to R is zero.

## 4.3. Comments on Assumptions
- Observe that in our mathematical formulation of the problem in ^D, we allow orders for arbitrarily large negative quantities. The more realistic assumption is that the quantity we can return in a period is bounded from above by the inventory on hand at the beginning of that period. Let us call such a system as ^D 0.
    - Clearly, ^D 0 is a more constrained system than ^D. However, it does have the return option which D does not have. Thus, the optimal long run average cost in ^D 0 is sandwiched between those for D and ^D. Then, Theorem 1 implies that the optimal long run average costs in D and ^D 0 are also identical; so, the value of the return option is zero under the more realistic assumption above.
- Next, we comment on the assumption that the lead time for negative orders (i.e., returns) is the same as the lead time for regular orders.
    - In other words, we assume that the shipment time is lR periods in both the forward and return directions from/to supplier R. In the return direction, the firm bears the holding cost for a returned unit until the end of the return lead time of lR periods.
- Our final comment pertains to an implicit assumption that also results from the assumption that the return lead time is l R periods. This implies that if x units are returned in a period, a reduction of x units from the net-inventory level is registered (in our model) only lR periods later; thus, in the interim periods, these x units are allowed to satisfy customer demand. This is contradictory to the physical reality that once the x units are returned, they are not available to satisfy customer demand from that instant onwards even though the returned units are incurring holding costs during the return lead time.
    - Similar to Comment 1 above, we could define a new system ^D00 with the constraint that returned units are not available to meet customer demand during the return lead time, in addition to the constraint in ^D 0 that only inventory on hand can be returned.
    - Then, it can be seen that ^D00 is a more constrained system than ^D 0 since ^D 0 has the ability to reduce backorder costs using returned inventory (which also simultaneously reduces the holding cost in ^D0) during the return lead time whereas ^D 00 does not.
    - On the other hand, ^D00 is a less constrained system than D since ^D00 enjoys the return option. Thus, the optimal cost in ^D 00 is sandwiched between that of D and ^D 0. Then, as a corollary to Comment 1 above, we see that the optimal long run average cost in ^D 00 is also equal to that in D.

## 4.4. Implication of Theorem 1
- As mentioned in section 1, Theorem 1 has been used by Janakiraman et al. [2015] to show that the optimal cost in a dual sourcing system is insensitive to a deterministic, constant shift in demand in every period.
- We now present a generalization of their result and its proof. Let D denote a dual sourcing system with i.i.d. demands and let D t denote the random variable representing demand in period t. Similarly, consider an identical dual sourcing system ~D whose demand in period t is represented by the random variable D t + At, where At >= 0 is deterministically known in period t - l R (i.e., A t is known sufficiently in advance).
    - Then, CD; = C~D; where CD; and C~D; denote the optimal long run average costs in D and ~D, respectively. In words, when demand is composed of a predictable stream plus an uncertain but stationary noise, the optimal cost of a dual sourcing system depends exclusively on the distribution of the noise and is independent of the size of the predictable stream.
- The proof is the following: The optimal policy in D can be “mimicked” in ~D by adding A t units to the regular order in every period t - lR; so, there is always a feasible policy in ~D which attains the same cost as the optimal cost in D. Thus, C~D; <= CD;.
- To prove the opposite inequality, we consider a system, ^D, identical to D except that costless returns to the regular supplier are allowed in ^D. By Theorem 1, we know that CD; = C^D;, where C^D; is the optimal long run average cost in ^D. Now, we can take the optimal policy in ~D and construct a feasible policy in ^D by subtracting A t units from the regular order in every period t - l R; this ensures that the cost under this policy in ^D equals the optimal cost in ~D. Thus, we have C^D; <= C~D;. Combining the cost comparisons derived thus far, the equality CD; = C~D; is obtained.

# Executive summary of 1. Introduction
- The introduction introduces the concept of dual sourcing inventory systems and focuses on the value of costless returns.
- Costless returns refer to the option of returning inventory to the cheaper, long-lead time supplier without incurring any cost beyond the holding cost.
- The key argument is that this option has zero value under stationary stochastic demand, despite the complexity of optimal policies in dual sourcing systems.
- The introduction also highlights the paper's contribution to understanding optimal policies in dual sourcing systems and its application in showing the insensitivity of optimal cost to deterministic shifts in demand.

# Executive summary of 2. Model and Notation
- This section defines the model and notation used in the paper, primarily borrowed from Janakiraman et al. [2015].
- It describes the dual sourcing inventory system with two suppliers (R and E) with different lead times and costs.
- The section outlines the sequence of events in each period, including deliveries, ordering decisions, demand realization, and cost calculation.
- The core of the section is establishing the mathematical framework to represent the inventory system and its dynamics.

# Executive summary of 3. Results on Finite- and Infinite-Horizon Optimal Policies
- This section presents the properties of optimal policies in dual sourcing systems, first under a finite horizon and then extended to an infinite horizon.
- It introduces Claims 1, 2, and 3, which describe the order-up-to structure of expedited ordering policies, bounds on regular order quantities, and sensitivity properties of optimal policies.
- Claim 4 summarizes the infinite horizon results derived from the finite horizon analysis.
- Key concepts include expedited inventory position, expected holding and shortage cost function, and the sensitivity of order quantities to changes in the state vector.

# Executive summary of 4. Value of the Costless Return Option
- This section provides the central result of the paper, showing that the option of costless returns has no cost advantage in the long run.
- It presents Claim 5, which establishes the equality of optimal discounted costs in systems with and without costless returns under specific starting conditions.
- Theorem 1 formally states that the optimal long-run average costs are identical in both systems, implying zero value for the costless return option.
- The section also discusses the assumptions and limitations of the analysis and provides a generalization of a previous result regarding the insensitivity of optimal cost to predictable demand streams.


</details>

# Ramanath Subramanyam
- Professor of Business Administration
- William N. Scheffel Faculty Scholar 
- Academic Director of Business
### education
- Ph.D., Business Administration, University of Michigan, 2004
- B.E., Electronics and Communication Engineering, Regional Engineering College, 1996
### research interest
- Machine Learning, Management of IS Design Processes and IS Project Management, IT Sourcing Governance, Digital Business Strategies, Economics of Information Systems, Software Complexity and Project Management, New Product Development, and Business Value of Information Technology
### teaching
- Business Intelligence, Business Analytics, Information Systems Development and Management, Business Value of IT, IT Strategy and Innovations, Enterprise Software Management, and IT for Networked Organizations

```
title: EXTRACTING ACTIONABLE INSIGHTS FROM TEXT DATA: A STABLE TOPIC MODEL APPROACH
authors: Yi Yang, Ramanath Subramanyam
journal: MIS Quarterly
published: 2023
```
 
# Executive Summary
- **Problem**: This paper addresses the **instability problem** of **topic models** like **Latent Dirichlet Allocation (LDA)**, where results are inconsistent and irreproducible, leading to potentially unreliable research outcomes and hindering actionable business insights. This instability stems from **local optima** during model training, affecting both **extrinsic** (top words, document labels) and **intrinsic** (topic/word, document/topic distributions) model properties.
- **Stability Mechanism**:  Stable LDA achieves improved stability by **guiding** the model inference process.  Instead of a purely random exploration of the "likelihood landscape", Stable LDA "steers" the model towards more consistent solutions by encoding prior knowledge about word relationships.
- **Kernel Theory Design**: We propose **Stable LDA**, a new method incorporating topical word clusters into the topic model to "steer" the model inference toward consistent results. The design leverages:
    - **Topical word clusters**: Words that often co-occur in the same context belong to the same cluster and have similar topic distributions. We obtain topical word clusters by clustering **word embeddings** (learned semantic representations of words).
    - We convert topical word clusters into a tree in which each cluster forms a subtree, and then incorporate this tree into the standard LDA model under a **Dirichlet tree prior**. This creates "soft constraints" guiding the model.
    - **Dirichlet tree prior**: treating the topical word clusters as soft priors, meaning that we can still rely on the Bayesian learning framework to estimate the posterior distributions. It's a generalization of the standard Dirichlet prior allowing for flexible control on the topic-word multinomial distribution.
    - **Gibbs Sampling Initialization:**  Gibbs sampling is initialized with words in the same cluster having the same topic assignment to further "guide" the sampling process and reduce the variance in initial topic-word distributions.
- **Variables**: The model utilizes the following variables:
    - **Input**: Unstructured textual data (documents).
    - **Latent Variables**: Topics (distributions over words), Topic distributions per document.
    - **Derived Variables**: Document topic probabilities, measures of similarity between documents based on topic distributions, Topic label variables (binary indicators of whether a document discusses a given topic).
- **Findings**:
    - Stable LDA significantly improves **model stability** across four real world textual datasets (Amazon reviews, Yelp reviews, StackExchange Q&A, company descriptions), while maintaining or improving **topic model quality** (as measured by **model fit** (Perplexity) and **topic coherence** (Cv, Cuci)).
    - The **intrinsic stability** metrics (𝑆𝑡𝑜𝑝𝑖𝑐 prob and 𝑆𝑑𝑜𝑐 prob) directly quantify the changes of probability distributions in two topic models.
    - Variables generated from Stable LDA lead to more consistent estimations in **econometric analyses**, demonstrated through case studies (online Q&A usefulness, review ratings).
    - The empirical studies validate that the model derived from Stable LDA provide consistently more accurate insights from the perspective of using variables derived from them.

#topic_modeling #stability #stable_lda #text_analysis #empirical_analysis #latent_dirichlet_allocation #word_embeddings #dirichlet_tree_prior #topic_coherence #model_fit #local_optima #kernel_theory #gibbs_sampling


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Firms have valuable opportunities to gain business insights from unstructured textual data via machine learning techniques such as **topic modeling** [Rai, 2016].
- **Topic modeling techniques** (e.g., **LDA** [Blei et al., 2003]) extract latent topics from documents by assuming each document is a mixture of topics and each topic is a mixture of words (e.g., a hotel review may talk about the “price” topic and “location” topic).
- LDA is used in two ways: as a data exploration tool to quickly identify the distribution of topics in a document or text corpus and synergistically combined with qualitative or econometric methods to generate theory [Rai, 2016].
    - For example, Mankad et al. [2016] used LDA to discover topics in consumer reviews from a hotel platform.
    - For example, Khern-am-nuai et al. [2018] applied LDA to consumer reviews and analyzed differences in review topics when the online platform began offering monetary incentives for writing reviews. They set up a regression model, where the dependent variable is the probability that a review is about video game topics. 
    - LDA is used to derive independent or dependent variables of interest, which adds both theoretical and practical value to IS research [Shmueli & Koppius, 2011].
- A problem with topic models is that they are not stable and the results are often challenging to replicate, even on the same dataset at different times due to multiple possible local optima and the approximate inference techniques employed in model fitting.
- LDA may be unstable from two perspectives:
    - **Extrinsic instability**: The top representative words for the same topic and the document’s topic label assignment may change across different runs.
    - **Intrinsic instability**: The underlying topic-word and document-topic probability mixtures are also likely to be different, which can propagate to the dependent or independent variables, leading to unreliable analyses.
- The instability problem of LDA can undermine their usability and potentially decrease scholars’ trust in modeling outcomes [Nosek et al., 2015; Stodden et al., 2016].
    - The presence of model instability implies that the generated variables may be unstable and result in measurement errors [Yang et al., 2018], introducing systematic biases into coefficient estimates and resulting in spurious or incorrect estimates.
- Stakeholders expect reproducible and consistent model results for actionable insights. For example, a service management scenario where a hospitality manager applies topic modeling on consumer reviews to examine the relationship between consumer satisfaction and various topics mentioned in the reviews.
- To address the gaps in the literature:
    - First, none of the prior work has demonstrated how the potential concern of unstable LDA can adversely affect the reproducibility and robustness of subsequent econometric analyses.
    - Second, we demonstrate through our study that prior mitigation approaches might not be very effective due to their focus on extrinsic stability at the expense of intrinsic stability.
- We propose the concept of **topical word clusters**: words that often co-occur in the same context belong to the same cluster and have similar topic distributions.
    - We obtain the topical word clusters by clustering word embeddings [Mikolov et al., 2013].
    - We convert topical word clusters into a tree in which each cluster forms a subtree, and then incorporate this tree into the standard LDA model under a Dirichlet tree prior.
    - The Dirichlet tree prior allows us to treat the topical word clusters as soft priors, meaning that we can still rely on the Bayesian learning framework to estimate the posterior distributions.
    - We name our proposed approach **Stable LDA**.
- We evaluate the proposed Stable LDA using several real-world textual datasets and find that it can significantly improve stability, as captured by different stability metrics. This improved stability does not come at the cost of model performance, as measured by model fit and topic coherence.
- We validate the effectiveness of Stable LDA in generating variables that result in more consistent econometric estimations than those generated from a standard LDA model.
- Our main contributions are twofold:
    - A methodological contribution by proposing a new method, Stable LDA, to mitigate the topic model instability concern.
    - Raise awareness that variables derived from unstable topic model outcomes may result in inconsistent estimations in subsequent econometric analyses.
 
# 2. Literature Review
- We review existing research in IS, marketing, and management science that employs LDA topic modeling techniques.
- We review prior efforts related to the assessment and mitigation of topic model instability.
- We review how the stability problem is studied in machine learning and econometric modeling literature.
 
## 2.1. Topic Modeling in IS And Management Research
- Topic modeling techniques such as LDA are increasingly used to mine textual data across different areas of management research [Rai, 2016].
- In IS and marketing literature, the LDA model has been extensively used in the analysis of consumer reviews to extract topics and generate variables of interest [Abrahams et al., 2015; Büschken & Allenby, 2016; Khern-am-nuai et al., 2018; Lappas et al., 2016; Mankad et al., 2016; Puranam et al., 2017; Tirunillai & Tellis, 2014; Wang & Chaudhry, 2018].
- LDA-based topic models have also been adopted in management research and practice to analyze other forms of unstructured textual data, such as online forum discussions [Yue et al., 2019], search queries [Gong et al., 2018], information on mobile apps [He et al., 2019; Wang et al., 2018], literature reviews [Larsen & Bong, 2016], research papers [Hankammer et al., 2016], patents [Kaplan & Vakili, 2015], tweets [Geva et al., 2019; Lee et al., 2018; Liu et al., 2016], company descriptions [Shi et al., 2016], organizational blogs [Guo et al., 2017], corporate reports [Bao & Datta, 2014; Dyer et al., 2017], corporate tax disclosures [Bozanic et al., 2017], and analyst reports [Huang et al., 2017], etc.
 
## 2.2. Topic Model Stability
- The statistical inference process of a probabilistic topic model LDA can introduce instability.
- Prior work has studied the instability issue of topic models, which manifests at the topic level and at the document level.
    - Mantyla et al. [2018] studied topic level instability, Waal and Barnard [2008] studied document level instability respectively, and Chuang et al. [2015] developed a visualization tool to diagnose topic model instability.
    - Miller and McCoy [2017] studied the alignment of unstable hierarchical topic models, while Su et al. [2015] examined how noise in the text corpus can affect the stability of topic models.
    - However, studies in this line of work have not proposed strategies to mitigate instability.
- These studies have primarily focused on **extrinsic instability** (i.e., stability with respect to document topic labels or the topic’s most representative words).
- They have not considered **intrinsic instability** (i.e., the document-topic distributions and topic-word distributions), which are essential properties that drive extrinsic instability.
- Several recent studies have aimed to alleviate the instability issues via innovative LDA extensions.
    - Yang et al. [2015] and Yang et al. [2016] proposed improving document label stability using document labels, wherein documents with similar probability distributions would be assigned with the same labels.
    - Koltcov et al. [2016] proposed a regularization procedure to encourage words in a local context window to have similar probabilities.
    - Belford et al. [2018] proposed an ensemble approach to aggregate topic models and then applied matrix factorization to obtain the final model.
- In a departure from prior studies, we propose incorporating topical word clusters generated from a word-embedding clustering procedure with a Dirichlet tree prior to stabilize the inference process of topic modeling.
- There are related studies that incorporate pairwise word associations into LDA with the intent to improve topic-word coherence [Gao et al., 2019; Xie et al., 2015], but these studies do not address the topic model instability issue.
- These studies incorporate pairwise word associations, which can be problematic for model stability because it may introduce undesirable word associations due to pairwise transitivity (e.g., “cellphone” is correlated with “battery,” “battery” is correlated with “crime,” but “cellphone” may not necessarily be correlated with “crime”).
 
## 2.3. Model Stability in Machine Learning and Econometrics
- In machine learning literature, the stability of a predictive model (e.g., a classification model such as a decision tree) is a measure of how robust the model is in detecting underlying patterns from different subsamples of the original dataset.
    - According to computational learning theory [Angluin, 1992], it is generally expected that a stable learning algorithm can produce consistent outcomes and reliably generalize to unobserved data.
    - Prior work has studied the stability of supervised models such as support vector machines (SVM) [Bousquet & Elisseeff, 2002] and unsupervised models such as K-means clustering [Rakhlin & Caponnetto, 2007].
- In econometrics, model stability is of crucial importance for inference and the interpretation of regression results [Hansen, 1992], based on the extent to which estimated parameters remain stable across samples.
    - Statistical techniques (e.g., Brown et al., 1975; Garbade, 1977; McCabe & Harrison, 1980; Page, 1955) have been developed to detect shifts (or change points) in a regression relationship.
- In contrast, in machine learning and in our work, stability is studied with the assumption that the underlying data generation process remains constant, and it is the intrinsic property of learning algorithms that causes the model outcomes to be inconsistent.
- IS researchers have taken a rigorous approach toward potential problems in machine learning methods.
    - In the domain of recommender systems, recent work [Adomavicius & Zhang, 2012, 2016] has focused on evaluating the stability of recommendation algorithms.
    - Instability could decrease users’ trust in recommender systems and reduce users’ acceptance of their recommendations [Benbasat & Wang, 2005; Komiak & Benbasat, 2006].
    - Adomavicius and Zhang [2012] have empirically demonstrated the increased stability of model-based methods over alternatives such as neighborhood-based methods.
- Recent IS work has also aimed to enhance our collective understanding of methodological tools and increase the credibility of certain research methods.
    - For example, Lin et al. [2013] studied the relationship between large samples and the p-value problem.
    - Similarly, Yang et al. [2018] and Qiao and Huang [2021] discussed the potential estimation bias caused by inaccurate measures from machine learning models.
 
# 3. Empirical Evidence on Topic Model Instability
- We empirically evaluate model stability using four textual datasets.
 
## 3.1. Topic Model and Stability Basics
- Let us consider a textual corpus consisting of a set of documents 𝑑 ∈ 𝒟. Each document is a sequence of n words 𝑤𝑛 drawn from a vocabulary 𝒱 of size 𝑉.
- Statistical topic models such as LDA can derive latent topics from textual corpora in an unsupervised fashion, and serve as dimension-reduction techniques to represent each document as a mixture of different topics.
- Specifically, LDA assumes that there is a set 𝒯 of size 𝑇 latent topics in a document collection and topics are modeled as latent variables 𝑧𝑛 ∈ 𝒯for each word 𝑤𝑛.
    - The generative process of LDA defines 𝜃𝑑 , the topic distribution for document d, as being drawn from a Dirichlet distribution, 𝜃𝑑 ∼ 𝐷𝑖𝑟(𝛼).
    - In addition, 𝜙𝑡 is the topic-specific word distribution, which is also drawn from a Dirichlet distribution, 𝜙𝑡 ∼ 𝐷𝑖𝑟(𝛽).
    - 𝛼 and 𝛽 are the hyperparameters for the two Dirichlet distributions.
    - Model parameters in LDA are estimated using Bayesian inference Markov chain Monte Carlo (Gibbs sampling) or variational inference.
- When employing LDA to extract document topics, the topic modeling results may vary over different runs.
    - For a textual corpus D, let us suppose that a researcher runs topic model LDA with the same parameter setting twice and obtains two models 𝑀1 and 𝑀2.
    - Due to the exchangeability property of LDA, we need to perform topic model alignment to make sure that, for two different topic models 𝑀1 and 𝑀2, the topic indices are matched.
    - We employ the Hungarian algorithm [Kuhn, 1955] to efficiently solve the alignment problem.
- Prior work examining topic model stability primarily focuses on **extrinsic instability** (i.e., stability with respect to the topic’s most representative words or document topic labels).
    - For topic-level stability, the literature has proposed metrics to assess whether the top representative words change between two runs [Chuang et al., 2015; Koltcov et al., 2016; Mantyla et al., 2018].
        - In particular, the **Top10 word stability** is calculated as the percentage of shared words in the top representative words list of each topic: 𝑆𝑡𝑜𝑝𝑖𝑐 𝑡𝑜𝑝𝑤𝑜𝑟𝑑𝑠 = ∑ |𝑡 𝑇𝑜𝑝𝑡 1∩𝑇𝑜𝑝𝑡 2|/10 , where 𝑇𝑜𝑝𝑡 is the set of most representative 10 words, i.e., with the highest probability, of topic t.
    - For document-level stability, it is often a common practice that users assign the topic with maximum probability to be the document label.
        - Accordingly, prior literature has examined changes in the document label to quantify topic model extrinsic document stability [Belford et al., 2018; Waal & Barnard, 2008; Yang et al., 2016].
        - Label stability is defined as the percentage of documents that share the same labels in two models: 𝑆𝑑𝑜𝑐 𝑙𝑎𝑏𝑒𝑙 = ∑ 𝕀𝑑∈𝐷 (arg 𝑚𝑎𝑥 𝑡 𝜃𝑑𝑡 1 =arg 𝑚𝑎𝑥 𝑡 𝜃𝑑𝑡 2 )/𝐷 , where 𝕀(. ) is the indication function.
 
## 3.2. Topic Model Intrinsic Stability
- Prior studies have examined the stability issue of LDA, discussion has mostly centered around extrinsic stability, which is closely related to the data exploration outcomes of LDA. However, they do not consider intrinsic instability, i.e., document-topic distributions and topic-word distributions, which are essential properties that drive extrinsic instability.
- Intrinsic stability directly measures the change of probability distributions and needs to be thoroughly examined.
- Intrinsic stability is more closely related to the robustness and reproducibility of subsequent analysis because many IS and management studies rely on LDA probability distributions, rather than the top 10 words or document labels, to construct variables of interest.
- To better evaluate topic model stability and connect it with the topic-modeling stream of work in IS and management research, we propose explicitly measuring the intrinsic stability, as follows.
- For topic-level stability, we propose an **intrinsic topic-word stability metric**, which is defined as 1 minus the L1-norm Manhattan distance between 𝜙𝑡𝑤 1 and 𝜙𝑡𝑤 2 in 𝑀1 and 𝑀2: 𝑆𝑡𝑜𝑝𝑖𝑐 𝑝𝑟𝑜𝑏 = 1 − 1/2 ∑ ∥𝜙𝑡 1−𝜙𝑡 2∥1𝑡 𝑉 = 1 − 1/2 ∑ ∑ ∣𝜙𝑡𝑤 1 −𝜙𝑡𝑤 2 ∣𝑤𝑡 𝑉 , where V is the size of vocabulary in the dataset, and ∥ 𝜙𝑡 1 − 𝜙𝑡 2 ∥1is the L1-norm.
- For document-level stability, we propose a metric to directly measure the change of document-topic distribution θ in the topic model.
    - **Document stability** is defined as 1 minus the L1-norm Manhattan distance between 𝜃𝑑𝑡 1 and 𝜃𝑑𝑡 2 : 𝑆𝑑𝑜𝑐 𝑝𝑟𝑜𝑏 = 1 − 1/2 ∑ ∥𝜃𝑑 1−𝜃𝑑 2∥1𝑑∈𝐷 /𝐷 = 1 − 1/2 ∑ ∑ ∣𝜃𝑑𝑡 1 −𝜃𝑑𝑡 2 ∣𝑡𝑑∈𝐷 /𝐷 , where D is the number of documents in the dataset, and ∥ 𝜃𝑑 1 − 𝜃𝑑 2 ∥1 is the L1-norm.
- Our proposed intrinsic stability metrics directly quantify the changes of probability distributions in two topic models. 𝑆𝑡𝑜𝑝𝑖𝑐 𝑝𝑟𝑜𝑏 and 𝑆𝑑𝑜𝑐 𝑝𝑟𝑜𝑏 can complement previously proposed extrinsic stability measures 𝑆𝑡𝑜𝑝𝑖𝑐 𝑡𝑜𝑝𝑤𝑜𝑟𝑑𝑠 and 𝑆𝑑𝑜𝑐 𝑙𝑎𝑏𝑒𝑙 and help researchers t better diagnose the topic model before they construct variables for subsequent analysis.
 
## 3.3. Model Stability Experiments
- We empirically evaluate model stability using four textual datasets.
- We collected two consumer review datasets: online product reviews collected from Amazon.com and online restaurant reviews collected from Yelp.com [Ni et al., 2019].
- We collected a dataset pertaining to the online knowledge Q&A platform StackExchange.com [Liu et al., 2020; Peng et al., 2020].
- We collected a text dataset that contained company descriptions [Qader et al., 2018].
- All four datasets were preprocessed using a standard tokenizer and stopword removal tool. We limited the vocabulary to 5,000 unique words because the choice of a very large vocabulary is likely to result in slow inference and poor topic generation performance [Boyd-Graber et al., 2014].
- In the experiment, we used Gensim’s LDA open-source implementation [Rehurek & Sojka, 2010]. We used a uniform symmetric 𝛼 (document-topic density) with a value of 1.0/𝑇 and a uniform 𝛽 (topic-word density) with a value of 0.01. For each dataset, we set epochs to 10 with a default iteration of 50.
- We set the number of topics at 20 for each dataset. We ran each model setting 10 times and present the average results of four stability measures in Figure 1.
 
## 3.4. Discussion
- The results of our experiments reveal the severity of the instability problem in traditional LDA models.
- The two extrinsic measures, 𝑆𝑡𝑜𝑝𝑖𝑐 𝑡𝑜𝑝𝑤𝑜𝑟𝑑𝑠 and 𝑆𝑑𝑜𝑐 𝑙𝑎𝑏𝑒𝑙 , exhibit troubling findings: we found that, on average, nearly half of high-probability words changed under each topic, as did the topic labels of documents.
- The instability issues similarly manifest themselves in the intrinsic stability metrics, 𝑆𝑡𝑜𝑝𝑖𝑐 𝑝𝑟𝑜𝑏 and 𝑆𝑑𝑜𝑐 𝑝𝑟𝑜𝑏.
- We surmise that local optima may be one important and likely reason behind this instability problem. In LDA, since posterior distributions of model parameters may be computationally intractable, approximate inference methods such as Gibbs sampling and variational inference are used.
- We empirically observe that topic model LDA, driven by the intrinsic instability in probability distributions, is not stable in producing consistent model results, which may lead to reproducibility issues and inconsistent estimation in data exploration and empirical research.
 
# 4. Data-Driven Approach for Enhancing Topic Model Stability
- In order to improve the stability of LDA-based models, we theorize that it will be helpful to provide some guidance about where to optimize the model.
- We define the concept of the **topical word cluster**: In a topical word cluster, words should be semantically and syntactically similar and should conceptually be understood as being about a topic.
- Our data-driven approach for enhancing topic model stability is implemented in two steps:
    - First, we propose a word-embedding-based data-driven technique that automatically generates sets of topical word clusters from the dataset.
    - Second, topical word clusters are then used to initialize model inference and incorporated into the LDA model as must-link and cannot-link constraints so that the model optimization process is more controlled.
 
## 4.1. Topical Word Cluster Generation
- Word embeddings have increasingly become an important building block in many text analysis tasks and capture both semantic and syntactic meanings among words [Mikolov et al., 2013].
- We train word embeddings on the text dataset and treat words with similar representations in the embedding space as topical word clusters.
- We used a K-means algorithm to find natural clusters among words.
    - If K is much larger than T (number of topics), then we will have numerous topical clusters, with each cluster representing a small number of words.
        - This choice will result in a large number of cannot-link pairs between topical clusters, imposing overly strong constraints on model estimation and eventually resulting in poor model fit and possibly unstable learning.
    - If K is equal to T, then all words will be associated with a topical cluster and used to build the Dirichlet prior.
        - This choice will result in a large number of must-link pairs within each topical cluster, which could also be considered to be a strong constraint on model estimation, resulting in poor model fit.
    - Through our experiments across different datasets, we found that a setting of K that exceeds T by a number ranging from 5-10 generates fairly consistent topic stability and good model fit.
- We outline our topical word generation procedure as follows:
    - First, we train a word-embedding model, such as word2vec’s CBOW (continuous bag-of-words) [Mikolov et al., 2013] on the collection of documents.
    - Next, we find topical word clusters among words using a K-means algorithm.
 
## 4.2. Incorporating Topical Word Clusters in LDA for Stability
- We denote the K topical word clusters as 𝐶𝑖, 𝑖 ∈ {1, . . . , 𝐾}, where each cluster 𝐶𝑖 contains 𝑛𝑖words, i.e., 𝑤𝑖𝑗 ∈ 𝐶𝑖, 𝑗 ∈ {1, . . . , 𝑛𝑖}.
- Our goal is to allow topical word clusters, to influence the values of the hidden topics z, and then indirectly influence and stabilize the topic-word distribution 𝜙 and document-topic distribution 𝜃.
- The generation of topical word clusters implies that those words would have similar probabilities under the same topic.
- Thus, we want to replace the default LDA Dirichlet prior with a specifically designed prior that captures such word clustering information.
- We adopted a **Dirichlet tree prior** approach [Dennis III, 1991] to model topical clusters.
- The Dirichlet tree prior is a generalization of the Dirichlet prior that allows flexible control on the topic-word multinomial.
- In standard LDA, the Dirichlet prior is a flat tree with only one internal node (root node) with V leaf nodes, where V is the number of vocabulary (words).
- For a Dirichlet tree prior, given topical word clusters, we can construct a tree structure and refine the generative process for 𝜙𝑡.
- In the tree, words that share topical similarity are connected to an intermediate node before they connect to the root node.
- Each subbranch represents a topical cluster and words would have similar probabilities in the subbranch.
- A word that does not belong to any topical cluster is directly connected to the root node.
- The generative process of the Dirichlet tree, LDA’s topic-word distribution consists of two steps:
    - In the first step, starting from the root node, we draw a multinomial distribution for all the child nodes of the root node 𝜙𝐼 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛽), where 𝑖 ∈ 𝐼 denotes all child nodes of the root node.
    - In the second step, for all the internal nodes, we draw another multinomial distribution for all the leaf node connections to the internal nodes 𝜙𝐿 ∼ 𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝜂), where 𝑙 ∈ 𝐿 denotes all leaf nodes of the internal node, and 𝜂 is a Dirichlet prior.
- The probability 𝜙𝑡 of a word w is then the product of the multinomial parameters from the leaf to the root.
- We choose Dirichlet parameters 𝛽 and 𝜂 to encode word clusters.
    - Since Dirichlet distributions produce sparse multinomials when their parameters are small (less than one), a small 𝛽 would encourage the root node to prefer sparse distribution of probability among its children.
    - When the Dirichlet parameters are large (much greater than one), it favors more uniform distributions so that the words have similar probabilities. Hence, we choose a large 𝜂.
- The joint probability of a Dirichlet Tree topic model parameterized on 𝛼, 𝛽, 𝜂 is:
    - 𝑝(𝐰, 𝐳, 𝐥, 𝜙, 𝜃; 𝛼, 𝛽, 𝜂) = ∏ ∏ 𝑝(𝜙𝑡,𝑖|𝜂) 𝑖𝑡 [∏ 𝑝(𝜃𝑑|𝛼)[∏ 𝑝(𝑧𝑑,𝑛|𝜃𝑑 )𝑝(𝑙𝑑,𝑛|𝜙, 𝑧𝑑,𝑛)𝑝(𝑤𝑑,𝑛|𝑙𝑑,𝑛) 𝑛 ] 𝑑 ],
- We use Gibbs sampling for posterior inference, initialized so that words in the same topical word clusters have the same initial topics.
- In summary, we propose a data-driven technique to stabilize the topic model LDA. We theorize that word clusters can stabilize topic-word distributions and, in turn, improve document-topic and overall model stability.
 
# 5. Experiments on Textual Datasets
- We report on the experiments we conducted and evaluate the effectiveness of our proposed Stable LDA method in the following manner.
    - First, we examine whether the proposed method indeed mitigates the instability problem.
    - Second, we examine if the improvement of stability comes at the cost of model quality.
    - Third, we examine how the choice of model parameters affects the performance of Stable LDA.
- We used the four datasets—Amazon Product Review, Yelp Restaurant Review, StackExchange Q&A, and Company Description.
- In addition to the standard LDA baseline, we include several recent approaches designed to alleviate the instability issue in our benchmarks: Doc LDA [Yang et al., 2016], Ensemble LDA [Belford et al., 2018], and Granulated LDA [Koltcov et al., 2016].
 
## 5.1. Model stability
- For all datasets, we chose to set the number of topics as 𝑇 = 50 and the Dirichlet tree prior as 𝜂 = 1000.
- We can see that the proposed data-driven word-clustering method can effectively group words that are semantically similar.
- We expect to see that the conditional probability p(“mcaffee”|topic) ≈ p(“kaspersky”|topic) ≈ p(“virus”|topic) is consistent in different models, which should result in improved stability.
- Across all datasets, Stable LDA significantly improved model stability across four stability metrics.
- For example, on the Amazon dataset, the LDA model had a word stability of 𝑆𝑡𝑜𝑝𝑖𝑐 𝑡𝑜𝑝𝑤𝑜𝑟𝑑𝑠 0.432, while Stable LDA improved the word stability metrics to 0.863.
 
## 5.2. Model quality
- We further evaluate model quality dimensions such as model fit and topic coherence.
- We measure the fit of the models on the text data, using a standard metric: **perplexity**.
    - 𝑝𝑒𝑟𝑝𝑙𝑒𝑥𝑖𝑡𝑦 = exp{− 1/𝑁𝐷 ∑ log 𝑝(𝐝|𝜽)}, where 𝑁𝐷 is the number of tokens of text dataset.
    - A lower perplexity number indicates a higher data likelihood.
- **Topic coherence** measures the degree of semantic similarity between the high-probability words in the topic, using the 𝐶𝑣 coherence measure.
- Stable LDA achieved the lowest (best) model perplexity on three out of four text datasets, and the highest topic coherence on all four datasets.
- The improvement in model fit and topic coherence over LDA and the benchmark models can be attributed to two reasons:
    - The Gibbs sampling of Stable LDA is partially initialized with word embedding clusters.
    - The Dirichlet tree prior encourages words within the same word embedding cluster to have similar topic distributions and therefore improves the words coherence of each topic.
 
## 5.3. Sensitivity analysis
- We present Stable LDA’s performance when different topics are used in Table 6.
- We examine the effect of 𝜂, which is the strength of the Dirichlet prior in the Dirichlet tree distribution, on the model performance.
- We varied the value of 𝜂 from 1 to 5000 and present its effects on model stability using the Company Description dataset in Table 7.
- The results show that while initializing Gibbs sampling with topical word clusters did improve LDA’s stability, the stability can be further improved by imposing a Dirichlet tree prior on the model inference.
- Continually increasing the strength of the Dirichlet tree prior does not further improve the model stability beyond a certain point, and imposing an overly strong association may even reduce the model fit and topic coherence.
 
# 6. Practical Value in Empirical Study
- In such contexts, researchers often use LDA outcomes to generate variables of interest as independent or dependent variables.
- We report on two empirical case studies to demonstrate how Stable LDA facilitates more robust empirical analysis.
 
## 6.1. Empirical Study: Online Knowledge Community
- We examined the relationship between the textual similarity of questions and answers and the usefulness of the answer in an online Q&A knowledge community.
- We employed the StackExchange Q&A dataset for the empirical study.
- The independent variable, **QASimilarity**, was measured as the cosine similarity between the topic features of the question and the topic features of the answer.
- The dependent variable, **AnswerHelpfulness**, was the number of votes an answer receives.
- We estimated two models: an ordinal least square regression model (OLS) and a logistic regression model (Logit).
- For the regression model based on LDA-derived measures, some coefficient estimates of the QASimilarity variable switched directions from positive to negative over the course of 10 runs.
- In contrast, the coefficient estimate of the QASimilarity variable in the Stable LDA regression (OLS) model was consistently in the same direction.
- The QASimilarity variable in StableLDA/OLS and StableLDA/Logit was consistently and statistically significant in all 10 runs (p-value < 0.001).
- The p-values of the QASimilarity variable in the LDA/OLS and LDA/Logit model were not stable.
- QASimilarity correlations in different LDA runs averaged 0.550, while those in Stable LDA runs averaged 0.870.
 
## 6.2. Empirical Study: Online Consumer Reviews
- We examined the relationship between the numerical rating of an online review and its discussed topics in an online review context.
- We used the Amazon review dataset with 20 topics.
- The dependent variable was the numerical rating score of the review.
- The independent variable 𝑇𝑜𝑝𝑖𝑐𝑖𝑘 was derived from LDA or StableLDA topic labels, and is equal to 1 if the topic probability 𝜃𝑖𝑡 was greater than a threshold, and 0 otherwise.
- We estimated the following ordinary least square regression model (OLS): OLS: 𝑅𝑎𝑡𝑖𝑛𝑔𝑆𝑐𝑜𝑟𝑒 = 𝛽0 + ∑ 𝛼𝑡𝑇𝑜𝑝𝑖𝑐𝑡 𝑇 𝑡=1 + 𝛽2 log(𝑊𝑜𝑟𝑑𝑠) + 𝛽3𝑆𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡+ + 𝛽4𝑆𝑒𝑛𝑡𝑖𝑚𝑒𝑛𝑡− + 𝜖.
- We calculated Pos2Neg as the percentage of positive and significant topic estimations in the first topic model that became negative and significant estimations in the second topic model, and calculated Neg2Pos as the opposite.
- There was a noticeable number of LDA topics for which the coefficient estimates changed directions over two different runs.
- The topics derived by Stable LDA were consistent, and none of these topics’ coefficient estimations changed in direction.
 
# 7. Discussion and Conclusion
- In this work, we first empirically validate the instability problem using four real-world textual collections. To alleviate the problem, we propose a Stable LDA method that leverages word-association relationships to enhance the stability of the topic model.
- We evaluated the performance of Stable LDA using four textual datasets and found promising improvements in model stability, while maintaining model quality.
- Further, we present two empirical case studies and demonstrate that the variables derived from Stable LDA can achieve more consistent and robust estimations in comparison to variables derived from LDA.

---

# Executive summary of 1. Introduction
- This section introduces the use of topic modeling for extracting insights from textual data, highlighting the instability issues of LDA and their impact on research outcomes. It also outlines the proposed solution, Stable LDA, and its contributions.

# Executive summary of 2. Literature Review
- This section reviews existing research on topic modeling, focusing on its applications in IS and management, the challenges of model instability, and related research in machine learning and econometrics.

# Executive summary of 3. Empirical Evidence on Topic Model Instability
- This section empirically demonstrates the instability of traditional LDA models using four real-world datasets, emphasizing the changes in both extrinsic and intrinsic stability measures across different runs. It also discusses potential reasons for the instability, such as local optima.

# Executive summary of 4. Data-Driven Approach for Enhancing Topic Model Stability
- This section introduces the concept of topical word clusters and the proposed Stable LDA method, which incorporates these clusters into the LDA model using a Dirichlet tree prior. It details the two-step approach: generating topical word clusters using word embeddings and incorporating them into LDA for improved stability.

# Executive summary of 5. Experiments on Textual Datasets
- This section reports the experiments conducted to evaluate Stable LDA's effectiveness in mitigating instability and maintaining model quality. It compares Stable LDA with baseline methods and analyzes the impact of model parameters on performance.

# Executive summary of 6. Practical Value in Empirical Study
- This section presents two empirical case studies demonstrating how Stable LDA facilitates more robust empirical analysis. The case studies focus on online knowledge communities and online consumer reviews, illustrating the consistency and reliability of variables derived from Stable LDA in econometric analyses.

# Executive summary of 7. Discussion and Conclusion
- This section summarizes the findings of the study, highlighting the improved stability and model quality achieved by Stable LDA. It also discusses practical implications, limitations, and future research directions, emphasizing the importance of awareness and robust implementation of topic modeling in various disciplines.

</details>

```
title: Patterns in Information Systems Portfolio Prioritization: Evidence from Decision Tree Induction
authors: Prasanna Karhade, Michael J. Shaw and Ramanath Subramanyam
journal: MIS Quarterly
published: 2015
```
 
# Executive Summary
- The study investigates the **decision rationale (DR)** used for **information systems portfolio prioritization (ISPP)**, focusing on *why* certain initiatives are approved and others rejected, a critical yet understudied aspect of **IS governance**.
- We argue that a firm's **IS strategy** significantly influences its ISPP DR, which should align with the firm's strategy to ensure effective IS governance.
- Key **theoretical** / **conceptual framework** discussions
    - **IS Strategy Types:** We extend prior work on IS conservative and IS innovator strategies [Chen et al., 2010]. IS conservatives prioritize efficiency improvements, while IS innovators focus on exploring new business opportunities.
    - **Decision Rationale Attributes:** The DR is characterized by three attributes: **communicability**, **consistency**, and **risk appropriateness**, all vital for effective IS governance.
        - Communicability: How easily the DR can be understood and shared.
        - Consistency: The degree to which the DR is applied uniformly across similar initiatives.
        - Risk Appropriateness: Whether the DR aligns with the firm's risk appetite, focusing on risk assessment/mitigation for conservatives and opportunity exploration for innovators.
    - **Decision Tree Induction Methodology (DTIM):** We employ DTIM to uncover the tacit DR used for ISPP, which is often unknown even to decision-makers [Markus et al., 2002]. DTIM identifies the most informative attributes (decision attributes) and generates decision rules to predict outcomes.
- Key **findings** / arguments including
    - **IS Conservatives:**
        - *H1*: IS conservatives use a DR that is easy to communicate, highly consistent, and focused on risk assessment/mitigation.
        - Our results show that IS conservatives prioritize initiatives with clear risk mitigation mechanisms. The best representative decision tree uses only five attributes, and 63% of decisions follow the same decision rule. This rule emphasizes medium internal maturity, further highlighting risk aversion.
    - **IS Innovators:**
        - *H2*: IS innovators use a DR that is focused on exploring opportunities, not easy to communicate, and not applied with high consistency.
        - The IS innovator’s DR is more complex, involving six attributes, and has lower consistency, with the primary rule explaining only 40% of decisions. Innovators focus on initiatives that offer marketing benefits, signifying an emphasis on opportunity exploration.
    - These findings highlight the crucial role of IS strategy in shaping the DR used for ISPP and provide valuable implications for IS governance practices.

#is_strategy #is_portfolio_prioritization #it_portfolio_management #is_governance #it_governance #decision_making #decision_tree_induction

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The paper addresses the growing importance of information systems (IS) investments in Fortune 500 firms and the need for chief information officers (CIOs) to align the decision rationale (DR) used for IS portfolio prioritization (ISPP) with their IS strategies [Kohli, 2007; Piccoli and Ives, 2005].
- Existing research has primarily focused on approved IS investments, neglecting the understanding of why certain investments are rejected [Xue et al., 2008]. It is critical to understand which investments do not make it through the decision process and why.
- One of the key goals of IS governance is to encourage desirable behavior in the prioritization and use of IS [Weill and Ross, 2004].
    - Incongruence between DR and a firm’s IS strategy can lead to unsuitable IS initiatives and missed strategic opportunities.
- While prior research has examined who makes IS governance decisions and at what stages [Brown, 1997; Sambamurthy and Zmud, 1999; Weill and Ross, 2004; Xue et al., 2008], there's a need to focus on *why* certain initiatives are approved or rejected.

## 1.1. Research Gap and Objectives
- I extend prior theoretical work on IS conservative–innovator strategy types [Chen et al., 2010] to develop theoretical profiles of the DR used for ISPP.
- These profiles are based on three attributes:
    - communicability of DR [Segars and Grover, 1999],
    - consistency in applying DR [Chen et al., 2010],
    - risk appropriateness of DR [Boynton and Zmud, 1987].
- *Research Questions*: 
    - Address the gap in research by asking Why are certain initiatives approved and simultaneously why are certain others rejected?
    - Broaden research on IS governance by maintaining that the IS strategy adopted by firms is likely to explain the DR it uses for ISPP.
- I propose that IS conservatives, with stable environments and risk-averse tendencies, will use a different DR than IS innovators, who operate in dynamic environments and take risks.
- I analyze ISPP decisions from a Fortune 50 firm with two business units adopting distinct IS strategies.
- Data includes detailed information on IS initiatives, characterized by benefit, risk-assessment, and risk-mitigation attributes, as well as the final prioritization decisions.

## 1.2. Contributions
- My contributions to the literature are threefold.
    - First, I broaden prior theoretical work on IS strategy types by developing theoretical profiles for the DR used for ISPP in congruence with these IS strategies.
        - This contingency view of IS strategy considered in the context of ISPP is consistent with the view that firms adopting different IS strategies follow correspondingly different DR for ISPP [Chen et al., 2010; Weill and Ross, 2004].
    - Second, I address a key question linking IS governance and ISPP: Why are certain initiatives approved and simultaneously why are certain others rejected?
        - I broaden research on IS governance by maintaining that the IS strategy adopted by firms is likely to explain the DR it uses for ISPP.
    - Third, I make key methodological contributions. DR used for ISPP is often tacit, making it difficult to capture or share [Markus et al., 2002].
        - Decision tree induction methodology (DTIM) enables us to codify these tacit interconnections and link these emergent decision rules to final decision outcomes.
        - While prior research on IS governance has exclusively focused on approved decisions, I submit that a juxtaposition of the DR used for approving initiatives with the DR used for rejecting other initiatives is likely to deepen our understanding of IS governance.

# 2. Theoretical Background
- While assessing the business application needs of IS and ISPP has been suggested to be a critical component of IS governance [Weill and Ross, 2004], research that examines the DR firms use for ISPP is much needed [Xue et al., 2008].
- Our framework examines DR used for ISPP.

## 2.1. IS Strategy Types
- Recent research that has developed an IS strategy typology by developing profiles of IS conservatives and IS innovators [Chen et al., 2010] serves as our theoretical foundation.
- IS conservatives adopt a strategy that enables them to reap the benefits of IS by reducing operational inefficiencies.
- IS innovators adopt a strategy to apply IS in innovative ways to explore new business opportunities.
- Looking ahead, although profiles of IS strategy types have been developed, theoretical linkages between IS strategy and corresponding profiles of DR used for ISPP need further investigation.

## 2.2. Decision Rationale
- Although prior research has identified information attributes crucial for ISPP, a school of thought maintains that decisions often emerge based on tacit interconnections among information attributes during decision making [Markus et al., 2002].
- Interconnections among multiple information attributes converge into patterns as decisions emerge [Mintzberg, 1994].
- Since decision making often involves tacit interconnections among multiple information attributes, a suitable methodology is needed for opening up the black box of decision making [Langley et al., 1995].
- DTIM is suitable for discovering these tacit interconnections as it does not impose ex ante biases [Quinlan, 1990, 1993] on the manner in which information attributes influence decisions.
- DTIM identifies the most informative attributes for explaining decisions (namely, decision attributes) and excludes all noninformative attributes from the decision tree.
- Decision trees, outcomes of DTIM, are collections of decision rules generated based on the informative attributes.
- I employ three heuristics (high prediction accuracy, parsimony, and reliability) to help us select the most credible approximation (decision tree) of the tacit DR [Huber, 1981].

## 2.3. IS Portfolio Prioritization and IS Governance
- Although prior research has examined the locus of IS governance decisions and the valuation of IS portfolios, theoretical profiles of DR used for ISPP have not yet been examined.
- Understanding the DR that firms apply for ISPP is critical as ineffective prioritization is not only likely to be associated with investment in unsuitable IS initiatives, but also associated with firms missing out on IS-enabled strategic business opportunities [Weill and Ross, 2004].
- One of the key goals of IS governance is to encourage desirable behavior in the prioritization of IS portfolios.
- Decision rights are central to IS governance and, thus, ensuring that ISPP decisions (approval and rejection decisions on IS initiatives to pursue in the future) are made effectively is critical.
- Prior research on IS governance has focused on the locus of decision making by examining the extent to which decision makers share responsibility [Sambamurthy and Zmud, 1999].
- Recently, Xue et al. [2008] incorporated an additional dimension of the timing of IS governance decisions by examining who the key decision makers are and when, at which stages in the IS investment process, are they involved in making decisions?
- I examine the link between IS strategy and IS governance by submitting that a firm’s IS strategy is likely to influence the DR it uses for ISPP.
- I develop theoretical profiles for the DR firms used for ISPP by relying on three key attributes that have strong implications for IS governance.
    - First, senior management’s commitment, in terms of their time and attention, is critical for effective prioritization [Rajegopal, 2012].
        - Thus, communicability of the DR has strong implications for IS governance and effective resource utilization, especially the scarce resource of senior management’s time and attention [Segars and Grover, 1999].
    - Next, consistency in applying the DR is important to IS governance as proposals with similar characteristics warrant similar decisions [Chen et al., 2010].
    - Finally, for effective IS governance, the DR used for ISPP must ensure that an appropriate risk posture is embodied in the firm’s future IS portfolios [Boynton and Zmud, 1987].
- In summary, I broaden the literature on IS governance by addressing a key question on ISPP: Why are certain initiatives approved and why are certain others rejected?
- Next, I develop theoretical profiles of DR used for ISPP in congruence with a firm’s IS strategy.

# 3. Research Propositions
- The IS strategy adopted by a firm is likely to exert a strong influence on the DR it uses for ISPP.
- I identify characteristics that determine the IS strategy of a firm [Chen et al., 2010] and collectively influence its DR.
- I maintain that DR applied for ISPP is likely to be effective, from an IS governance perspective, when it is easy to communicate, is applied consistently, and is risk-appropriate with respect to the IS strategy of the firm.
- I rely on three attributes to define theoretical profiles of the DR: communicability of the DR [Segars and Grover, 1999], consistency in applying the DR [Chen et al., 2010], and risk appropriateness of the DR [Boynton and Zmud, 1987].
- The three attributes discussed below have been noted to effectively characterize decision processes [Boonstra, 2003; Markus et al., 2002] and have strong implications for IS governance.

## 3.1. Decision Rationale Profiles
- Communicability: As the complexity of DR increases, its communicability (the ease with which it can be articulated and shared with others) is likely to suffer.
    - Simple, communicable DR uses scarce resources (senior management’s time and attention) judiciously [Segars and Grover, 1999].
- Consistency: Since the tacit DR used for ISPP is typically difficult to capture [Markus et al., 2002], this DR is best represented in the form of if–then decision rules [Huber, 1981].
    - Each decision rule explicitly codifies the tacit DR.
    - The frequency with which decision rules are applied (the number of decisions made using the same decision rule) represents the consistency with which the tacit DR is applied for ISPP.
- Risk Appropriateness: Given differences in the risk appetites of firms, I propose that a correspondingly different DR is likely to be considered risk appropriate [March, 1994], as firms differing in risk appetites are likely to raise/answer different kinds of questions before making ISPP decisions.

## 3.2. Decision Rationale for IS Conservatives
- IS conservatives focus on improving the efficiency of their internal operations [Chen et al., 2010].
- IS conservative strategy is associated with three characteristics: a relatively stable external environment, formal decision-making structure, and risk-averse tendencies in the quest for efficiency improvements.
- I propose that these characteristics are likely to collectively exert a similar influence on DR used by IS conservatives for ISPP.
- A stable external environment fosters simplicity in the DR; thus, complexity of the DR used by IS conservatives is likely to be low.
- IS conservatives adopt a formalized decision-making structure [Jansen et al., 2006] to maintain stability [Chen et al., 2010] by applying a simple, highly consistent DR.
- Acting risk appropriately, decision makers with a low-risk appetite are likely to approve high-risk initiatives only after ensuring that risk-mitigation mechanisms have been designed to control risks [Boynton and Zmud, 1987; Straub and Welke, 1998].
- In summary, the easily communicable, highly consistent DR used by IS conservatives for ISPP is likely to focus on the assessment/mitigation of risks.
- *H1*: The decision rationale used by IS conservatives for IS portfolio prioritization is likely to be easy to communicate, applied with high consistency, and focused on risk assessment/mitigation.

## 3.3. Decision Rationale for IS Innovators
- IS innovators monitor an eclectic array of new opportunities in their dynamic external environment [Chen et al., 2010].
- IS innovator strategy is associated with three characteristics: a dynamic external environment, an organic decision-making structure, and risk-taking tendencies.
- IS innovators are expected to enter new markets; therefore, they must quickly adapt to changes in their dynamic external environment since ignoring these changes imposes prohibitively high opportunity costs [Chen et al., 2010].
    - Thus, dynamism in the external environment is likely to exert a strong influence on the DR used by IS innovators for ISPP.
- IS innovators are expected to continually analyze dynamic information flows in their external environment, which is likely to necessitate a complex DR.
- To exploit new opportunities, IS innovators are compelled to experiment, and are encouraged to act before engaging in extensive debate and dialogue.
- Formalized/rigid rules stifle experimentation [Jansen et al., 2006], which is important to IS innovators.
- To take risks intelligently, IS innovators often develop tacit rules, representing well-designed experiments [March and Shapira, 1987], to increase the likelihood of success in exploring new opportunities.
- However, these tacit rules are likely to be applied less consistently owing to the high dynamism in the external environment.
- IS innovators thrive on change in their environment and often create this change by intelligently taking risks.
- They explore new opportunities by focusing on the potential business value associated with their proposed initiatives, notwithstanding the riskiness of these initiatives.
- Thus, IS innovators are likely to de-emphasize risk mitigation during ISPP.
- In summary, DR used by IS innovators is not likely to be easy to communicate, is not likely to be applied with a high consistency, and is likely to be focused on exploring opportunities.
- *H2*: The decision rationale used by IS innovators for IS portfolio prioritization is likely to be focused on exploring opportunities, not easy to communicate, and not applied with high consistency.

# 4. An Empirical Analysis of IS Portfolio Prioritization
- I choose a multibusiness firm as our research setting.
- Business units within our empirical setting pursued different IS strategies along the IS conservative–innovator continuum in congruence with their differing business strategies.
- IS portfolios of business units at the ends of the IS conservative–innovator continuum are selected for further investigation.
- Front-line managers characterized their proposals, business initiatives they submitted for funding, with a rich set of information attributes.
- DTIM was applied across data gathered from both business units to discover the DR tacitly applied for ISPP by CIOs and other key decision makers.
- Since DTIM discovers approximations of the tacitly applied DR, I rely on the best representative DT, a credible approximation of the tacit DR, for each portfolio to examine support for our propositions.

## 4.1. Ascertaining the IS Strategy
- To characterize differences in IS strategies adopted by business units at our research site, I adopt recent theoretical research on IS strategy types [Chen et al., 2010].
- Our research site affords us a naturally controlled empirical setting that enables us to closely examine differences in DR tacitly applied for ISPP across business units, within one Fortune 50 firm, that are pursuing different IS strategies.
- Data are gathered via various mechanisms to ascertain the IS strategies adopted by these business units.
- For effective triangulation [Miles and Huberman, 1984], data are collected by the following methods: face-to-face, semi-structured, open-ended interviews with key informants, confidential documents obtained from key informants, observation of ISPP sessions, content analysis of annual reports.
- The IS strategy of one business unit was classified as an IS conservative whereas the IS strategy of another business unit was classified as an IS Innovator.

## 4.2. Input Portfolio Data
- Front-line managers can be a source of new ideas and firms can miss out on numerous IS-enabled business opportunities if they do not listen to their front-line managers [Kohli, 2007].
- On an annual basis, front-line managers at our research site are encouraged to identify new IS-enabled business application areas to pursue in the future.
- These funding proposals are collected across the two business units and each proposal is described using multiple information attributes.
- I refer to these data as the input portfolio data.
- I distinguish between information attributes and decision attributes as follows: Information attributes are inputs to DTIM, and decision attributes, a subset of information attributes, are outputs identified by DTIM.
- Decision attributes are the most pertinent information attributes for explaining decisions, as identified by DTIM.
- Data on all information attributes are provided by front-line managers.
- Input portfolio data were collected by collaborating with the metrics group within the firm.
- By focusing on a controlled sample of business applications of IS, I attribute differences in DR used for ISPP to differences in IS strategies of these two business units.

## 4.3. Information Attributes

- I analyze three types of information attributes [Boynton and Zmud, 1987; McFarlan, 1981; Sabherwal and Chan, 2001] used to richly describe business initiatives in this study.
    - First, the CIO and other key business leaders seek a rich description of initiatives in terms of potential benefits they can offer.
        - Such a description helps these decision makers to gauge the business value associated with these initiatives and enables them to identify promising initiatives to pursue [Aral and Weill, 2007].
    - Second, since ISPP involves the selection of initiatives to pursue in the future, ISPP requires managing risks.
        - Thus, assessment of risks associated with proposed initiatives in the portfolio is an integral component of ISPP.
    - Third, risk mitigation is a critical component of IS governance [Nolan and McFarlan, 2005] and thus decision makers at our research site demanded data on risk-mitigation mechanisms designed by front-line managers to control risks associated with their proposals [March and Shapira, 1987].
- Descriptions of the 15 information attributes used by front-line managers to characterize their proposed initiatives are presented next.
    - **Benefit information attributes:** I create a total of five variables to capture the different types of benefits, these business applications of IS can offer. Insights from prior research [Aral and Weill, 2007; Broadbent et al., 1999; Sabherwal and Chan, 2001] guided these transformations.
        - For all benefit-related variables, the inter-rater reliability was over 95 percent.
    - **Risk-assessment information attributes:** Risk assessment is critical for ISPP [Nolan and McFarlan, 2005]. Based on suggestions from prior work [Lyytinen et al., 1995], measures from McFarlan [1981] are used to assess risks associated with initiatives in our data.
    - **Risk-mitigation information attributes:** Risk-mitigation mechanisms are critical for the successful implementation of IS-enabled business initiatives [Broadbent et al., 1999; Piccoli and Ives, 2005; Ramasubbu et al., 2008; Straub and Welke, 1998].
        - I rely on three categories of risk-mitigation mechanisms [Iversen et al., 2004; Nolan and McFarlan, 2005; Sherer and Alter, 2004], namely internal, external, and process risk-mitigation mechanisms.

## 4.4. IS Portfolio Prioritization Decisions

- For each proposed initiative, ISPP decisions are made by a steering committee comprising of the VP, the CIO, and senior business executives responsible for IS governance.
- Decisions for an initiative could either be a rejection or an approval.
- At the end of ISPP, I gather these decision data from these decision makers.
- This independence limits the extent to which our study suffers from the common methods bias.

## 4.5. Decision Tree Induction Methodology

- DTIM [Quinlan, 1993] enables us to open up the black box of decision making [Langley et al 1995] and empowers us to discover the underlying, tacit DR applied during ISPP, which can often be unknown to the decision makers themselves.
- This is particularly true when decisions emerge based on interactions between groups of decision makers.
- DTIM iteratively groups together observations (i.e., initiatives) such that they are similar not only in certain information attributes but also in their final decision outcomes.
- The objective of DTIM is to discover tacit combinations of information attributes associated with similar final decisions [Quinlan, 1990, 1993].
- The output of DTIM is a DT which only retains the most pertinent information attributes (i.e., decision attributes) for explaining decisions.
- DTIM organizes attributes in a context-dependent manner; certain questions are only raised depending on answers obtained to questions answered previously [Quinlan, 1990].
- I would like to clarify that DTs discovered by DTIM are not reflective of the exact rules or the “script” used by the decision makers during ISPP, but rather, are approximations of the tacit underlying DR.

### 4.5.1. Credible Approximations of Decision Rationale
- To ensure that DR is comprehensively discovered from all the initiatives in the input portfolio, a process of drawing bootstrapped, mutually exclusive, training and testing subsamples is repeated multiple times.
- An iteration of DTIM is described next. In each iteration, I draw two random, mutually exclusive subsamples of initiatives from the original portfolio; one set, known as the training set, from which the tacit DR is discovered by the DTI algorithm [Quinlan, 1986], and another disjoint set of initiatives, known as the testing set, which is used to test the predictive accuracy of this discovered DR.
- Specifically, a randomly drawn sample of 80 percent of the portfolio was used for training and the prediction accuracy of the discovered DR was tested on a disjoint randomly drawn sample of 20 percent of the total portfolio.
- Parsimonious DTs are preferable as they compactly articulate the discovered tacit DR.
- Further, for robustness, a similar analysis is conducted using other training/testing subsample splits.
- These multiple iterations help us judge the robustness of the discovered tacit DR.

### 4.5.2. Discovering the Most Informative Attributes

- The theoretical basis for inducing DTs on a portfolio of decisions, each of them described by a set of information attributes and final decisions, is summarized as follows [Quinlan 1986, 1990; Tessmer et al. 1993].
- This procedure recursively partitions the sample into smaller subsets, in step with the growth of the DT.
- DTIM chooses the most pertinent information attribute on which to split the training sample and thus which attributes will be included in the DT (i.e., decision attributes) and is driven by the information-theoretic justification presented below.
- The concept of information entropy [Shannon and Weaver, 1963] serves as the theoretical basis for determining which attributes to include in the DT.
- The amount of information (or the reduction in uncertainty) provided by the information attribute is the primary justification for classifying the examples and only the most informative attributes are included in the DT.
- The entropy or uncertainty is equal to 0 if and only if all the pi’s but one are equal to 0.
- All the DTs are induced by using the most widely used C4.5 algorithm [Quinlan, 1986, 1990].
- This algorithm builds DTs from a dataset using information entropy and information gain ratio for choosing which information attributes are included in the DT [Quinlan, 1993].
- DTs are induced using the open source, software DTIM platform called Weka [Hall et al., 2009].

### 4.5.3. Three Heuristics for Selecting the Best Representative Decision Tree

- Multiple approximations of the underlying, tacit DR are derived by repeating the process of drawing two mutually disjoint training/testing subsamples of differing sizes.
- This repetition is integral to DTIM to ensure that multiple approximations of the underlying DR are available to the researchers.
- Given these multiple approximations, I systematically rely on three heuristics to select the best representative approximation, a credible approximation, of the underlying DR.
    - 1. High predictive accuracy: Prediction accuracy of DTs induced on a subset of training data is tested on a mutually disjoint testing data set.
    - 2. High parsimony: The induced DT is expected to be a compact, parsimonious approximation of the underlying, tacit DR so that it can serve as an effective decision-making aid.
    - 3. High reliability: Since the process of drawing training samples to induce DTs and testing the predictive accuracy of induced DTs on mutually disjoint testing samples is repeated several times, I am able to assess the robustness of the induced DT.

### 4.5.4. Interpreting a Decision Tree

- I would like to reiterate that the DTs presented here were discovered by DTIM as approximations of the tacit DR applied for ISPP.
- Since I employ three heuristics (high prediction accuracy, parsimony, and reliability) to choose a best representative DT, I am fairly certain the DTs that I present here represent credible approximations of the tacit DR.
- All 15 information attributes characterizing initiatives in conjunction with the final decision, are inputs to DTIM.
- All information attributes discovered by DTIM to be most informative for explaining decisions are included in the DTs as decision attributes and DTIM excludes all the non-informative attributes from the DT.
- The most informative decision attribute is the top-most attribute in the DT.
- Importance of attributes decreases as we move away from the top of the DT to the leaves, namely the endpoints of the DT.
- Next, I summarize the operationalization details for three attributes that define the DR profile: communicability of the DR, consistency in applying the DR, and risk appropriateness of the DR.
    - Communicability of the DR is inversely related to the complexity of the DT.
        - Number of decision attributes included in the DT is an effective proxy of the complexity of the DR.
    - Highest proportion of decisions made by tacitly applying the same decision rule represents the consistency in applying the DR.
    - Finally, I assess the risk appropriateness of the DR by counting the number and kinds of decision attributes included in the DT across the three categories identified in Table 5: benefits, risk assessment, and risk mitigation attributes.

# 5. Discussion of Results and Implications

## 5.1. IS Strategy Types and Best Representative Decision Trees

### 5.1.1. IS Conservative’s Decision Tree

- The number of decision attributes included in the DT, as a proportion of the total number of 15 information attributes provided as inputs to DTIM, serves as an effective proxy for the complexity, which is inversely related to the communicability, of the tacitly applied DR.
- Of the 15 information attributes provided as inputs, the DR tacitly applied by the IS conservative for ISPP is best represented using only five (5/15 = 33%) attributes. This simple DT easily communicates the DR tacitly applied for ISPP.
- The IS conservative’s DR contains a decision rule consistently applied with a very high frequency.
- In all, 63 percent of the decisions for the IS conservative’s portfolio are explained by the application of just this one main decision rule.
- In other words, we observe that the IS conservative’s DR comprises of a simple, highly consistent line of reasoning.
- IS conservative’s DT includes only 1 benefit attribute and 2 decision attributes each for assessing and mitigating risks.
- When deciding on the IS conservative’s portfolio, decision makers seem to rely on a higher proportion (4/5 = 80%) of risk assessment/mitigation decision attributes.
- Thus, the IS conservative’s DR is highly consistent, easily communicable, and focuses on assessment/mitigation of risks.
- Our findings support Proposition 1.

### 5.1.2. IS Innovator’s Decision Tree
- Of the 15 information attributes provided as inputs, DTIM discovered that the DR tacitly applied by the IS innovator for ISPP was best represented by six decision attributes.
- A total of 40 percent (6/15 = 40%) of the information attributes are needed to communicate the IS innovator’s DR, implying that this rationale is not very simple.
- Only 40 percent of the decisions in the IS innovator portfolio are explained by the use of the most consistent decision rule.
- The proportion of decisions made most consistently using just one decision rule was lower than the proportion that was observed in the IS conservative’s portfolio.
- The IS innovator’s DT includes three benefits attributes, one attribute for risk assessment, and two decision attributes for risk mitigation.
- When prioritizing the IS innovator portfolio, decision makers seem to rely on a higher proportion (that is, 50%) of benefit-related decision attributes implying a focus on exploring opportunities.
- In summary, the IS innovator’s DR is not necessarily easy to communicate, does not apply a decision rule with high consistency, and focuses on exploring opportunities.
- Our findings support Proposition 2.

## 5.2. Theoretical Implications
- One of the key goals of IS governance is to encourage desirable behavior in the prioritization of IS portfolios.
- Thus, our research, which examines DR used for ISPP, has strong implications for IS governance.
- Next, I present these implications from our research for IS governance.
- First, although prior research has provided extensive insights on the locus (centralization and/or decentralization) of the decision making for IS governance, questions pertaining to why firms make ISPP decisions have been left unanswered.
- Our study is one of the first to empirically examine the relationship between ISPP decisions (namely, approval and rejection decisions on IS-enabled business initiatives to pursue for the future) and the IS strategy of the firm, which is critical for IS governance.
- Second, our research proposes that the IS strategy of a firm is likely to be a key antecedent in explaining the DR it applies for prioritizing its IS portfolios.
- I submit that congruence between a firm’s IS strategy and its ISPP DR is likely to be associated with effectively planned IS portfolios as it not only incentivizes managers to develop the right kinds of initiatives but also guides decision makers to approve the right kinds of initiatives for future implementation.
- Third, while existing research has examined IS strategies by analyzing variations in the composition of IS application portfolios [Sabherwal and Chan, 2001], I broaden this important body of work by investigating the theoretical properties of DR applied during ISPP.
- I contribute to the literature on IS governance by addressing a key question on ISPP: Why are certain initiatives approved and simultaneously why are certain others rejected?
- I do so by developing theoretical profiles of the ISPP DR by encompassing three dimensions: communicability of DR, consistency in applying DR, and risk appropriateness of DR.
- I submit that understanding theoretical DR profiles across differing IS strategies strongly complements extant IS governance research by explicitly examining if the right kinds of questions are raised during ISPP.
- Thus, I present theoretical properties of the DR which can encourage rule-following in congruence with a firm’s IS strategy. I adopt a portfolio-level unit of analysis to contribute insights on the means firms adopt to effectively prioritize their IS portfolios.

## 5.3. Extensions

- I extended research on IS conservative–innovator strategy by proposing DR profiles for ISPP, in congruence with these IS strategies.
- Analysis of our data yielded support for our propositions.
- Building on this foundation, I identify two extensions for future research.

### 5.3.1. Egregious Deviations from Theoretical Decision Rationale Profiles

- Deviations from the theoretical DR profile applied by firms who have adopted strategies along the IS conservative–innovator continuum is likely to be associated with ineffective ISPP [Segars and Grover, 1999] for at least three reasons.
- I submit that deviations from the theoretical DR profile could be politically motivated.
- Favoritism on behalf of decision makers would result in deviations from the theoretical DR along the dimension of consistency.
- *Proposition 1 for Future Research*: Deviation between the decision rationale applied by firms adopting strategies along the IS conservative–innovator continuum and the theoretical decision rationale profile, is likely to be associated with ineffective IS portfolio prioritization.

### 5.3.2. Mediating Role of the Decision Rationale Profile

- Prior research maintains that a chosen IS strategic posture or digital business strategy is an antecedent of firm-level outcomes [Mithas et al., 2013].
- Our results highlight an intervening theoretical mechanism, namely DR applied by firms for ISPP, which is likely to have a strong impact on firm-level performance outcomes.
- This revised articulation sheds light on the mediating influence of the DR applied for ISPP for two reasons, which I elaborate next.
- First, congruence between a firm’s IS strategy and the DR it uses for prioritizing its IS portfolio fosters a rule-based approach to portfolio prioritization which provides front-line managers with the right incentives to design suitable IS-enabled initiatives and disincentivizes (rule-defiant) behavior [Prendergast, 1999].
- Second, congruence between DR employed by firms during ISPP and their IS strategies is likely to ensure the suitable initiatives are approved for future implementation and the unsuitable initiatives are weeded out earlier in the prioritization stages.
- *Proposition 2 for Future Research*: Decision rationale profile is likely to mediate the relationship between planned IS strategies along the IS conservative–innovator continuum and the business value realized from these investments.

## 5.4. Managerial Implications of Our Results
- DTs open up the black box of IS decision making by codifying the tacit DR applied during ISPP in a manner that can easily be scrutinized by decision makers.
- Our findings enable us to develop recommendations for assisting CIOs in managing their IS portfolios [Maizlish and Handler, 2005].

- The first three steps I outline next follow from our research design whereas the latter three follow from our findings.
    - Identify initiatives in distinct portfolios: Although DR for prioritizing distinct portfolios (IT hardware/infrastructure portfolios versus IS portfolios) is likely to be different, I propose that to govern these portfolios effectively, it is critical for CIOs to catalogue all initiatives in these portfolios [Kumar et al., 2008].
    - Gather portfolio data: Information attributes used to describe initiatives must be consistently used across all initiatives within a portfolio.
    - Define the decision scheme: For some portfolios, a binary (yes/no) decision scheme might be suitable whereas other instances might necessitate an additional partial funding decision outcome.
    - Develop a repository of decision rules: CIOs need to articulate their DR in the form of decision rules.
    - Apply decision rules: Having reached a consensus on the repository of nonnegotiable decision rules to apply, interactions between CIOs and front-line managers proposing initiatives during ISPP are likely to be more efficient and fair.
    - Manage portfolio lifecycles: CIOs should strive to improve the consistency of their decision making, to the extent possible, and educate front-line managers to develop better initiatives.

# 6. Concluding Remarks

## 6.1. Limitations
- Our study has certain limitations. First, our data are obtained from two business units within one large Fortune 50 firm.
- This could imply that our work suffers from limited generalizability.
- To address this limitation, I integrate three attributes (communicability of DR, consistency in applying DR, and risk appropriateness of D) for developing theoretical profiles of DR tacitly applied during ISPP.
- Our research enables us to theorize the linkages between the IS strategy adopted by firms and the corresponding DR applied during ISPP.
- Second, while the categories and specific anchors used as information attributes in this study were validated for their comprehensiveness at our research site, I acknowledge that investigations at other firms might require refinements to these information attributes.

## 6.2. Conclusion
- In this paper, I submit that the IS strategy of a firm is likely to explain its ISPP DR.
- I examine differences in the ISPP DR across different IS strategy types.
- I develop theoretical profiles of DR used for ISPP by relying on three attributes: communicability of DR, consistency in applying DR, and the risk appropriateness of DR.
- Since DR applied during ISPP is often tacit, unknown even to the decision makers themselves, I adopt DTIM, which is appropriate for discovering this tacit DR.
- By analyzing over 150 actual ISPP decisions, I find support for our theoretical DR profiles.
- Implications for IS governance practices are developed.

---

# Executive summary of 1. Introduction
- The introduction highlights the **critical importance** of **aligning IS portfolio prioritization (ISPP) with IS strategies** to improve IS governance.
- It identifies the **gap** in existing research, which **primarily focuses on approved investments**, neglecting the **reasons behind rejected initiatives**.
- The introduction emphasizes the need for research to address the **"why" behind ISPP decisions** and introduces the **study's objective** of **developing theoretical profiles** for the **decision rationale (DR)** used by IS conservatives and innovators.
# Executive summary of 2. Theoretical Background
- The theoretical background **reviews existing literature** on IS strategy types, decision rationale, and IS governance.
- It establishes the **theoretical foundation** by highlighting **IS conservatives' focus on efficiency** and **IS innovators' focus on new opportunities**.
- It emphasizes the **importance of understanding the tacit interconnections** among information attributes during decision-making and **introduces decision tree induction methodology (DTIM) as a suitable tool** for uncovering these connections.
# Executive summary of 3. Research Propositions
- This section presents the **core propositions** of the research.
- We argue that a firm's IS strategy influences the DR used for ISPP and that effective DR should be easy to communicate, consistent, and risk-appropriate.
- It details **specific hypotheses for IS conservatives** (high communicability, consistency, risk assessment/mitigation) and **IS innovators** (opportunity exploration, lower communicability/consistency).
# Executive summary of 4. An Empirical Analysis of IS Portfolio Prioritization
- This section **describes the empirical setting, data collection methods, and the application of DTIM**.
- It highlights the **use of a multi-business Fortune 50 firm** with business units adopting different IS strategies.
- It outlines the **data gathered from front-line managers** (information attributes) and **key decision-makers** (final decisions).
- We emphasize **the use of DTIM to discover tacit decision rules** and the **selection of the best representative decision tree** using specific heuristics.
# Executive summary of 5. Discussion of Results and Implications
- This section **presents the findings**, demonstrating the **distinct decision trees and DRs used by IS conservatives and innovators**.
- We find that IS conservatives prioritize risk mitigation, while IS innovators focus on opportunity exploration, supporting the proposed hypotheses.
- The section discusses the **theoretical implications for IS governance** and proposes extensions for future research, such as examining deviations from theoretical profiles and the mediating role of the DR profile.
# Executive summary of 6. Concluding Remarks
- The concluding remarks acknowledge **the study's limitations**, including the **limited generalizability of the findings** due to the use of data from a single firm.
- We reiterate the **contribution of the study** in **linking IS strategy to ISPP DR** and **developing theoretical profiles** for different IS strategy types.
- It emphasizes the **practical implications for IS governance** and provides valuable insights for CIOs and other decision-makers involved in IS portfolio prioritization.

</details>

```
title: Candidate diversity and granularity in IT portfolio construction
authors: Yu‑Ju Tu, Yu‑Hsiang Huang, Troy J. Strader, Ramanath Subramanyam, Michael J. Shaw
journal: Information Technology and Management
published: 2020
```

# Executive Summary

This study investigates how **attribute diversity** (variety of benefits, risks, costs) and **investment granularity** (divisibility of investment units) in candidate IT projects impact IT portfolio performance, building on **Modern Portfolio Theory (MPT)**.  Using **optimization modeling**, **simulation**, and **experiments**, we demonstrate that **higher diversity and granularity lead to superior IT portfolios**, even with **budget constraints**, by enhancing selection and allocation flexibility.

**Key Frameworks:**

*   **MPT:**  IT portfolios should maximize return (benefit) while minimizing risk.
*   **Diversity as Variety:** Attribute differences among candidate projects increase selection options.
*   **Granularity:** Divisible investment units allow for flexible resource allocation.

**Key Findings (Propositions):**

*   ***P1: Higher Diversity = Better Portfolios:*** Variety in project attributes leads to superior portfolios.
*   ***P2: Higher Granularity = Better Portfolios:*** Divisible investments enable more efficient resource allocation.
*   ***P3: Low Diversity Limits Granularity's Impact:*** Granularity is ineffective without diversity.
*   ***P4: Diversity & Granularity Can Outperform Larger Budgets:*** Well-chosen projects can overcome budget limitations.

**Experiment Results:**

*   Simulations showed higher diversity and granularity consistently improved portfolio performance.
*   Low diversity limited the impact of granularity.
*   Portfolios with high diversity/granularity and tight budgets sometimes outperformed those with low diversity/granularity and larger budgets.

**Conclusion:**

Diversity and granularity are crucial for IT portfolio success, potentially outweighing budget size. Focusing on project characteristics optimizes IT investments and maximizes business value.
#it_portfolio #diversity #granularity #budget_constraints #simulation #computational_experiment #modern_portfolio_theory


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- IT portfolios are sets of IT investments, and firms invest in numerous IT projects to meet various goals [1].
- Firms must govern IT investments at the portfolio level due to government policies and industry practices [2, 3].
- IT investments should be managed as a portfolio to maximize overall business value [4–11].
- IT portfolio construction differs from financial portfolio construction because IT projects are not always equivalent to financial assets.
- Prior studies propose categorical frameworks to classify IT portfolios [8, 12, 13] and computational approaches for portfolio prioritization [4, 6, 7, 14, 15].
- Research gaps remain in understanding the relevant characteristics that determine IT portfolio construction performance, such as synergy [6, 7].
- I investigate the characteristics of candidate IT investment projects that influence the likelihood of constructing a superior IT portfolio.
- I observe that attribute differences, decision-making units, and budget limits are associated with variations in performance.
- I develop propositions and a computational model to examine these associations and variations.
    - I define the aggregate attribute difference in candidate IT investment projects as the degree of **diversity** [16]. The attributes considered include benefit, risk, and cost.
    - I define the composition of divisible decision-making units in candidate IT investment projects as the degree of **granularity** [17].
- The main finding is that greater diversity and granularity in candidate IT investment projects increase the likelihood of constructing a superior IT portfolio because of the enhanced freedom for selection and allocation.
    - These characteristics can also mitigate the impact of budget limits on IT portfolio construction performance.
    - I derive these findings through computational experiment, Monte Carlo simulation, and real-world IT investment business cases.
- The study's major contributions are highlighting the positive effect of diversity and granularity on portfolio performance and translating the notions of diversity and granularity into ITPM.
- This study also enhances IT portfolio research methodology richness by using computational simulation and experiment to examine theoretical propositions.

# 2. A superior IT portfolio
- A superior asset portfolio generates the greatest investment benefits without exceeding a planned level of investment risk.
    - This concept is based on the return-risk criterion, and the performance of IT portfolio construction can be evaluated through similar rationale [6, 7].
    - The portfolio construction alternative with higher return but lower risk is superior, while the portfolio construction alternative with lower return but higher risk is inferior.
- A superior IT portfolio can be constructed by selecting a subset from a set of candidate IT investment projects where aggregate benefit is maximized, according to the risk tolerance level (risk appetite) that a firm plans to take and align with the firm’s IT strategy [1].

## 2.1. Attribute diversity in candidate IT investment projects
- Diversity has various meanings across research disciplines.
    - In financial studies, diversity is mostly equivalent to separation [21].
    - In this study, I draw on the concept of **diversity as variety** in organizational management [22] and software engineering [16].
    - Diversity (variety) refers to the degree of attribute difference in a set of candidate IT investment projects.
- Higher attribute diversity is a positive factor in constructing a superior IT portfolio.
    - Portfolio diversity as variety is the opposite of similarity.
    - An important intrinsic value of diversity is based on the **freedom of selection** [23, 24].
    - A set of candidate IT investment projects with higher attribute diversity can be more flexible for portfolio construction.
- *P1*: All else being equal, a set of candidate IT investment projects that involve higher diversity is more likely to generate a superior IT portfolio.

## 2.2. Investment granularity in candidate IT investment projects
- The granularity of investment decision-making units varies in different IT portfolio construction contexts.
    - Some IT portfolios are constructed with the divisible investment decision-making unit [7], while others are constructed with the binary investment decision-making unit [6].
    - In financial studies, the investment decision-making unit generally does not vary across different portfolio construction contexts.
- Different enterprise software products present different degrees of structural granularity [17].
    - Some involve modular components, and thus their development scopes are easily adjustable to meet different needs.
- Some IT investment projects with higher structural granularity can provide a proportional investment decision-making unit, while others can provide an all-or-nothing investment decision-making unit [25].
- Investment **granularity** refers to the degree of divisible investment decision-making units within a set of candidate IT investment projects.
- Higher granularity is a favorable factor in constructing a superior IT portfolio.
    - Constructing an IT portfolio based on a set of IT investment projects with higher granularity has the greater potential for allocating more funds to the more beneficial projects.
- *P2*: All else being equal, a set of candidate IT investment projects that involve higher investment granularity is more likely to generate a superior IT portfolio.

## 2.3. The joint impact of diversity and granularity
- *P3*: All else being equal, a set of candidate IT investment projects that involve very low diversity is less likely to generate a superior IT portfolio, even though this candidate set involves very high granularity.
- The granularity-induced allocation advantage in IT portfolio construction is essentially a type of comparative advantage.
    - According to MPT, only the more beneficial asset (e.g., more return and less risk) is worth the allocation of more funds.
    - When assets are very similar to each other (i.e., very low diversity), there will be no "more beneficial asset" that should be worth the allocation of more funds.

## 2.4. Budget constraints in IT portfolio construction
- IT portfolio construction is subject to budget limits [15, 26–28].
- A budget limit refers to a limited amount of capital or financial support for constructing an IT portfolio.
- The degree of the decrease in IT portfolio construction performance due to budget limits may vary with different degrees of attribute diversity and investment granularity in different sets of candidate IT investment projects.
- Greater diversity and granularity can enhance freedom of selection and freedom of allocation.
    - This means that the impacts of such favorable characteristics can mitigate or even offset the unfavorable constraining impact of a budget limit in IT portfolio construction, depending on which side’s impact is stronger.
- *P4*: All else being equal, it is possible that a set of candidate IT investment projects that involve both higher diversity and granularity, but share a tight budget limit (less budget), can generate a more superior IT portfolio than a set of candidate IT investment projects that involve both lower diversity and granularity, but share a loose budget limit (more budget).

# 3. Methodology
- I employ a methodology based on computational simulation and experiments to examine the propositions [29, 30].
- Simulation is suitable for addressing research with analytical intractability or observational biases.
- The approach combines **optimization modeling**, **real-world data**, **numerical simulation (Monte Carlo)**, and **computational experiments**.
- The optimization model aims to construct an IT portfolio based on a set of candidate IT investment projects.
- The model objective is to maximize IT investment project portfolio benefits while minimizing risk within the budget constraint.

## 3.1. Optimization Model
- Model (1) maximizes the IT portfolio benefit while minimizing risk within the budget constraint. The notation is as follows:
    -  *n*: The quantity of a set of candidate IT investment projects
    -  *xi*: The ith IT investment project decision variable
    -  *vi*: The investment benefit associated with the ith IT investment project
    -  *ri*: The investment risk associated with the ith IT investment project
    -  *ci*: The investment cost associated with the ith IT investment project
    -  *vij*: The interdependent investment benefit associated with the ith and jth IT investment projects
    -  *rij*: The interdependent investment risk associated with the ith and jth IT investment projects
    -  *Cb*: Budget constraint
    -  *λ*: Risk tolerance coefficient
- The candidate IT investment project needs to provide benefit (*vi*), risk (*ri*), and cost (*ci*). Several pairs of IT investment projects may co-create interdependent benefits (*vij*) and risks (*rij*) [6].
- *Cb* denotes a predetermined IT budget threshold. *λ* denotes a weight coefficient for balancing IT portfolio benefit and risk.
- We operationalize key concepts with computational metrics (numerical values).
    - I use the ratio of benefit and risk to compare the performance of IT portfolio constructions.
    - I use statistical distribution variance to compare degrees of attribute diversity.
    - For granularity, I use a count-based method.
    - For each candidate set of IT investment projects, I use an overall cost percentage threshold to measure its budget constraint.
- I design eight experiment scenarios with higher or lower degrees of diversity (*Hd* vs. *Ld*), granularity (*Hg* vs. *Lg*), and budget constraint (*Hb* vs. *Lb*).
    - The scenarios range from higher-diversity–higher-granularity–higher-budget (*HdHgHb*) to lower-diversity–lower-granularity–lower-budget (*LdLgLb*).
    - The simulated inputs for these scenarios are generated by a Monte Carlo simulation.
- For higher diversity-related attributes, the attributes are simulated based on the normal distribution (*vi* ~ *N*(𝜇v, 𝜎2
v), *ci* ~ *N*(𝜇c, 𝜎2
c), *ri* ~ *N*(𝜇r, 𝜎2
r)).
    - The values of 𝜇v, 𝜇c, 𝜇r, and 𝜎v, 𝜎c, 𝜎r are referred to a set of real-world IT investment business cases in a financial service division within a large US enterprise.
    - Benefit is estimated by NPV, and risk estimation is based on the scoring approach.
- Each lower diversity-related attribute is simulated based on a uniform distribution.
- Each higher granularity-related variable (*xi*) is simulated as a divisible decision-making unit. Each lower granularity-related variable is simulated as a binary decision-making unit.
- The loose budget constraint and tight budget constraint are simulated by two randomly predetermined ranges for setting budget thresholds. They are 20–50% and 50–80%.
- The remaining settings in the experiment are controlled to be constant or vary randomly across all the scenarios.
    - The quantity of each set of candidate IT investment projects is 100 (*n*), and there will be 10 different degrees of risk tolerance levels (*λ*) considered.
    - The occurrences of interdependent benefit (*vij*) and risk (*rij*) are simulated with an asymmetric random binary array.
    - The interdependency occurrence probability is configured as 10%.
- All these computational experiment scenarios and simulations are executed by an optimization modeling and simulation package (LINGO).

# 4. Experiment results
- The overall IT portfolio construction performance results are the averaged benefit of optimal IT portfolios under each experimental scenario and risk tolerance level. Their standard deviations are also presented.
- These results are generated through more than 8000 successful computational iterations.
- The averaged results of higher diversity scenarios show greater benefit at each risk tolerance level than those with lower diversity scenarios.
    - This result supports Proposition 1.
- The averaged results of higher granularity scenarios generally dominate those with lower granularity scenarios.
    - This suggests that the set of candidate IT investment projects with higher degrees of investment granularity is more likely than the set of candidate IT investment projects with lower degree of investment granularity to generate superior IT portfolio construction choices, which supports Proposition 2.
- The averaged results of the lower diversity and higher granularity scenarios and those with lower diversity and lower granularity show a very similar level of benefit at each tolerance level.
    - This result denotes that the sets of candidate IT investment projects with lower degrees of attribute diversity are likely to generate almost indistinguishable IT portfolio construction choices, which supports Proposition 3.
- The averaged results of all of the scenarios with tighter budget constraints show less benefit at each risk tolerance level compared with those with looser budget constraints.
- There is an overlap between the lines representing portfolio construction performance under the condition of more budget with lower diversity and granularity, and the performance under the condition of lesser budget with greater diversity and granularity.
    - This signifies that, even given a tighter budget, it is not impossible to generate superior IT portfolio construction choices, which supports Proposition 4.
- The statistical significance of those performance differences between each two experiment scenarios are computed by pair-wise t tests.
- In general, the results above support the aforementioned propositions.

# 5. Summary of findings and discussion
- The likelihood of constructing a superior IT portfolio can increase when the portfolio construction targets involve higher diversity and granularity.
    - This is because increased diversity and granularity enhance the freedom of selection and allocation in portfolio construction. They also mitigate the constraining impact of budget limits.
- Although some findings seem similar to some implications of MPT, they are essentially different from their contextual focuses.
    - The main findings regarding diversity are not based on removing interdependencies but improving the freedom of selection in portfolio construction.
    - MPT does not consider the freedom of allocation.
    - MPT does not consider the impact of budget limits.
- The table contrasts MPT focuses with this study focuses:
    - MPT: Diversity (diversification) as separation is focused on removing asset interdependency (correlation) in financial portfolio construction. This Study: Diversity as variety is focused on enhancing the freedom of selection in IT portfolio construction.
    - MPT: Decision making unit in financial portfolio construction always has the highest granularity (the perfect divisibility). This Study: Decision making unit in IT portfolio construction can have low or high granularity. When the degree of granularity as the freedom of allocation increases, the construction performance increases.
    - MPT: The joint impact of diversity and granularity (divisibility) on financial portfolio construction performance is not concerned. This Study: There is a joint impact of diversity and divisibility on IT portfolio construction performance.
    - MPT: The impact of budget limit on financial portfolio construction performance is not concerned. This Study: Budget limit, diversity, granularity and their interactions together influence IT portfolio construction performance.

# 6. Concluding remarks
- IT portfolio construction has many more subtle considerations than financial portfolio construction, although both of them share the similar objectives of maximizing return, minimizing risk, and balancing them.
- Other than that, IT portfolio construction needs to consider that different sets of candidate IT investment projects have different degrees of diversity and granularity.
    - These characteristics, along with different degrees of IT portfolio budget limits, can significantly affect IT portfolio construction performance.
- ITPM is analogous to organizational management.
    - IT portfolio construction is essentially a management of multiple investments in IT related resources for achieving organizational goals.
    - Resource selection and allocation are the bottlenecks to improving organizational performance.
- What I establish in this study is one step in the direction of addressing key determinants or factors for improving IT portfolio construction performance.
- Future research should focus on addressing the limitations of this study.
    - One obvious assumption is that any candidate IT investment project can be compared with another project rationally in portfolio construction.
    - I have not considered the flexibility in terms of managing time uncertainty in IT portfolio.
    - One implicit assumption is that IT investment project size proportionally determines its expected benefit/risk.

---

# Executive summary of 1. Introduction
- IT portfolios are collections of IT investments, with firms investing in numerous projects to achieve diverse objectives. Effective IT investment governance at the portfolio level is crucial.
- This study addresses gaps in prior research by investigating characteristics of IT candidate projects that can improve IT portfolio construction.
- I examine how attribute differences, decision-making units, and budget constraints influence portfolio performance, defining diversity and granularity.
- The study demonstrates that greater diversity and granularity in candidate projects enhance portfolio performance by providing more selection and allocation options.

# Executive summary of 2. A superior IT portfolio
- A superior IT portfolio maximizes investment benefits while managing risk, aligning with return-risk criteria from modern portfolio theory (MPT).
- Attribute diversity is viewed as variety rather than separation, enhancing the freedom of project selection, while granularity refers to the divisibility of investment decision-making units, increasing allocation flexibility.
- Propositions are developed, highlighting how higher diversity and granularity lead to better portfolio outcomes, even when considering budget constraints and the joint impact of diversity and granularity.

# Executive summary of 3. Methodology
- A computational approach combining optimization modeling, real-world data, Monte Carlo simulation, and computational experiments is employed to examine the propositions.
- The optimization model maximizes IT investment benefits and minimizes risk within budgetary limits, considering factors like project benefits, risks, costs, interdependencies, and risk tolerance.
- The experiment uses varying levels of diversity, granularity, and budget constraints to simulate and evaluate IT portfolio performance across different scenarios.

# Executive summary of 4. Experiment results
- The experiment results support the propositions, showing that higher diversity and granularity in candidate IT investment projects generally lead to superior portfolio construction choices.
- The results highlight the importance of these factors in enhancing portfolio performance, even when budget constraints are present.
- Pair-wise t tests were used to assess the statistical significance of the performance differences between experiment scenarios.

# Executive summary of 5. Summary of findings and discussion
- Key findings indicate that greater diversity and granularity improve IT portfolio construction performance, by enhancing selection and allocation.
- This study has distinctions from Modern Portfolio Theory (MPT), including its focus on diversity as variety, its consideration of investment granularity, and its inclusion of budget constraints in the analysis.

# Executive summary of 6. Concluding remarks
- IT portfolio construction involves more nuanced considerations compared to financial portfolio construction, including factors like project diversity, granularity, and budget limits.
- This study's approach extends financial MPT by incorporating unique IT investment decision-making contexts.
- Future research should address the study's limitations, such as assuming rational comparison of projects and not accounting for time uncertainty or non-linear relationships between project size and benefits/risks.

</details>

# Vanitha Virudachalam
- Assistant Professor of Business Administration
### education
- Ph.D., Wharton School of Business, University of Pennsylvania, 2020
- M.P.P., Honors, Public Policy, University of Chicago, 2012
- B.A. in Applied Mathematics, High Honors, Operations Research, University of California at Berkeley, 2006
### research interest
- Service Operations, Public Sector Operations, Education Operations, Healthcare Operations
### teaching
- Business Analytics II (BADM 211) statistics in generating basic inferences to predictive modeling Identify opportunities for improving business decisions using data

```
title: Going the Distance: The Impact of Commute on Gender Diversity in Public Service
authors: Dawson Kaaua, Vanitha Virudachalam
journal: Manufacturing & Service Operations Management
published: 2025
```
 
# Executive Summary
- **Problem**: There is a gender disparity in the political sphere, with women underrepresented in political positions.
- **Research Questions**: We ask three main questions: 1) To what extent does commute distance increase women’s barrier to entry for political positions? 2) Is this more or less of an obstacle if the political position is full-time, part-time, or a hybrid of these two types of structures? 3) What can be done to lower this barrier if work travel poses a significant obstacle to female politicians?
- **Theoretical Frameworks**: 
    - **Workplace flexibility**: We consider the impact of flexible work policies on the likelihood of females to run for office.
    - **Impact of distance on performance**: We examine the implications of a service provider having to contend with distance, from the perspective of how it impacts the pool of service providers.
    - **Diversity and bias in operations**: We investigate how to get women into more leadership positions.
    - **Public service operations**: We study the behavior of the candidate given the constraints of the system.
- **Methodology**:
    - Observational study: We use data on state-level political candidates to study the relationship between distance from a state house district to the state capitol and the proportion of female political candidates in that district, considering the varying degrees of commute difficulty and work hours expected.
    - Conjoint survey experiments: We collect data from college students and past political candidates to investigate whether policies like work-from-home, daycare benefits, and paid parental leave could help to close the gender gap in politics.
- **Key Findings**:
    - In states with **full-time legislatures**, districts farther from the state capitol tend to have a **smaller percentage of female candidates**.
    - In states with **part-time legislatures**, districts farther from the state capitol tend to have a **larger percentage of female candidates**.
    - In **hybrid states**, a longer commute distance leads to a **smaller percentage of female candidates**, although the effect is muted.
    - **Paid parental leave** might motivate women at the beginning of their political careers with longer commutes to run for office.
    - A **remote work/proxy voting policy** could help to sustain the political careers of these types of women.
- **Managerial Implications**: For organizers and policymakers seeking to encourage more women to run for office, commute distance is a barrier for would-be candidates in states with full-time legislatures, and making paid parental leave and remote work/proxy voting available may help.
 
#labor_force_participation #politics #gender_parity #diversity #us_elections #flexibility #service_operations #commute_distance #work_life_balance #state_legislatures #conjoint_analysis #paid_parental_leave #remote_work #proxy_voting

 <details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- We focus on the gender disparity in the political sphere, where women are underrepresented [Center for American Women and Politics 2021a, 2021b; U.S. Census Bureau 2021].
    - Female political leadership has material impacts on the well-being of women and children [Bhalotra and Clots-Figueras 2014; Chattopadhyay and Duflo 2004; Iyer et al. 2012; Karimi and Roy 2024; Wowak et al. 2021].
- Supply-side forces, linked to the historical role of women in the home, contribute to gender disparities in the workforce [Coltrane and Shih 2010].
    - Women tend to seek out jobs with better work-life balance, prioritizing shorter commute times [Barbulescu and Bidwell 2013; Crane 2007; Hanson and Johnston 1985; Le Barbanchon et al. 2021].
    - Marriage and motherhood heighten the reversion to these historical roles [Booth and Van Ours 2013; Khazan 2021; Lim and Rasdi 2019; Lundborg et al. 2017; Wang et al. 2013].
- Politics offers a unique vantage point to study flexible work and commute distance in women's labor force participation.
    - Many political positions are part-time and poorly-paid [Hattery and Lindstrom 2009; National Conference of State Legislatures 2021b; Zale 2019].
    - Commute distance is often heightened for political work.
    - The expected amount of work travel may limit the pool of women who seek political office.
- We seek to answer three questions:
    - To what extent does commute distance increase women’s barrier to entry for political positions?
    - Is this more or less of an obstacle if the political position is full-time, part-time, or a hybrid?
    - What can be done to lower this barrier if work travel poses a significant obstacle to female politicians?
- To answer these questions, we use data on state-level political candidates and conduct conjoint survey experiments.
    - We study the relationship between the distance from a state house district to the state capitol and the proportion of female political candidates in that district.
    - We take advantage of the variation in the number of hours state-level representatives are expected to work [National Conference of State Legislatures 2021b].
    - We investigate whether a variety of policies, including work-from-home policies such as voting by proxy; daycare benefits; and paid parental leave, could help to close the gender gap in politics.
- Our findings highlight a significant obstacle to achieving a truly representative government—the differential impact of commute distance on women considering entering politics.
    - In states with full-time legislatures, legislative districts farther from the state capitol tend to have a smaller percentage of female candidates.
    - In states with part-time legislatures, legislative districts farther from the state capitol tend to have a larger percentage of female candidates.
    - For states with hybrid legislatures, a longer commute distance leads to a smaller percentage of female candidates, although the effect is muted.
    - Paid parental leave might motivate women at the beginning of their political careers with longer commutes to run for office while a remote work/proxy voting policy could help to sustain the political careers of these types of women.
 
# 2. Literature
- We integrate research streams in operations management: workplace flexibility; the impact of distance on operations; gender diversity and bias; and public service operations.
- **Workplace flexibility**: This encompasses when to work (schedule flexibility), where to work (location flexibility), and how much workers work (full-time versus part-time and temporary workers).
    - Employees tend to value schedule flexibility [Ibanez 2022; Kamalahmadi et al. 2021; Mas and Pallais 2017; Katsnelson and Oberholzer-Gee 2021].
    - Location flexibility has also become increasingly popular with workers [Barrero et al. 2021; Choudhury et al. 2021; Mas and Pallais 2017].
    - We contribute to this work by studying the impact of flexible work policies on the likelihood of females to run for office.
- The literature on flexible workers focuses on how employers can optimally utilize such workers [Ata et al. 2019; Berenguer et al. 2023; Dong and Ibrahim 2020; Kesavan et al. 2014].
    - We extend this literature to consider the employee perspective, that is, we study the impact of the availability of flexible work on who joins the pool of workers.
    - Our work is related to Lu and Lu [2017], who study this in the context of nurse staffing for nursing homes.
- **Impact of distance on performance**: This can take many forms.
    - Businesses benefit from being close to company headquarters [Kalnins and Lafontaine 2013].
    - Reduced transportation and communication costs increased the ability of the government to effectively monitor state workers [Mastrorocco and Teso 2024].
    - "Multisiting" in healthcare has negative effects [KC and Tushe 2021].
    - Like KC and Tushe [2021], we are interested in understanding the implications of a service provider having to contend with distance, albeit from the perspective of how it impacts the pool of service providers.
- **Diversity and bias in operations**:
    - Organizational benefits of female leadership and diversity more broadly [Chen et al. 2024; Ranganathan and Shivaram 2021; Wowak et al. 2021].
    - Provider bias affects service provision, underscoring the need for diversity in the pool of service providers [Mejia and Parker 2021; Wang et al. 2023].
    - We seek to answer the question implicitly raised by this body of work, which is how to get women into more leadership positions.
- **Public service operations**:
    - Work in education [Keppler et al. 2022; Virudachalam et al. 2023], public health operations [Bastani et al. 2021; Kaplan 2020; Levi et al. 2020; Long et al. 2018; Yang et al. 2013], government services [Buell et al. 2021; Lorca et al. 2015], elections [Cachon and Kaaua 2022; Mondschein et al. 2022], and campaigns under different voting systems [Morales and Thraves 2021].
    - We study the behavior of the service provider (i.e., the candidate) given the constraints of the system [Morales and Thraves 2021], but in our work, the candidate may choose not to participate in the system at all.
 
# 3. Data and Estimation
- We investigate the relationship between commute distance and female participation in state house and senate elections.
    - State elections are chosen because the commute distance is easily measurable.
    - State positions are less likely to suffer from confounding factors versus U.S. Congress positions.
- Data is collected from:
    - KnowWho: home addresses and demographic information of U.S. political candidates.
    - Google Maps, Bing Maps, and the R package tidygeocoder: latitude and longitude coordinates of candidate and state capitol addresses.
    - U.S. Census Bureau: demographic information by geographic region.
    - MIT Election Data and Science Laboratory: various statistics on U.S. elections.
- The 2020 election cycle is unlikely to have been impacted by the COVID-19 pandemic because:
    - Candidates must meet multiple milestones in the months leading up to the general election [National Conference of State Legislatures 2023a, 2023b].
    - Candidates likely decided to run for office before pandemic-related restrictions began to be implemented in March 2020.
    - Even if candidates registered but ultimately dropped out due to reasons related to the pandemic, they would still be included in the KnowWho data set.
- The number and percentage of women who were elected to the state legislature in 2020 was consistent with other election years [Center for American Women and Politics 2023].
- We examine whether senate and house districts that are farther from the state capitol have lower female candidate participation in elections for public service positions.
- **Identification Strategy**: We consider multiple possible sources of endogeneity.
    - One potential issue is reverse causality, where the level of distance is driven by the percentage of women running for office.
        - We use the percentage of female candidates in a district within a particular race (PctFemale) as the dependent variable.
        - The independent variable of interest is the commute distance (Distance) for a given district, which we calculate as the haversine distance between the state capitol and the average of all candidates’ mailing addresses.
    - Another potential source of endogeneity is district-level omitted variables, that is, factors that impact the percentage of women that run for office which are not observed.
        - We control for omitted factors by including district-level factors that might influence the percentage of female candidates: average age (Age) and income (Income) of residents, population density (PopDensity), voter support for Hillary Clinton in the 2016 presidential election (PctClinton), the percentage of residents who identify as white (PctWhite), the percentage of households with minor children (PctChildren), the percentage of residents who are married (PctMarried), and candidate demographics for a given senate or house district, i.e., the percentage of Democrat (PctDemocratCandidates) and minority candidates (PctMinorityCandidates).
    - An additional factor that may influence the proportion of female participation in state government is the level of time commitment required by state legislatures [National Conference of State Legislatures 2021b].
        - We hypothesize that part-time legislatures may actually be an attractive career option for women, and the lower expectations for hours worked may diminish concerns about commute distance.
    - To further guard against potential endogeneity, we take advantage of the contiguous state geography of the United States.
        - State senate or house districts close in proximity but on opposite sides of a state border can have similar demographics, yet differing commute distances to the state capitol.
- We develop a regression technique (“district pair analysis”) to estimate the effect of a longer commute distance on female participation in state politics.
    - For this analysis, we first match cross-state pairs of districts that are in adjacent states; within 100 miles; of the same chamber (state senate or state house) and legislature type (full-time, part-time, or hybrid); and in the same election year (2018 or 2020).
- Prior to doing so, we first visualize these districts to investigate whether any trends in female representation are apparent.
- Determining whether clear trends exist broadly requires a rigorous causal inference methodology, which we formulate in our district pair analysis.
- Before performing that analysis, we first conduct a preliminary OLS regression (i.e., a “district-level analysis”), in which each district-year is a unit of observation.
    - We first consider the following OLS specification:
        - PctFemalei, t, s, c = β0 + β1Distancei, t, s, c + αs + γc + ρt + μXi, t, s, c + ɛi, t, s, c, (1)
    - We include three types of fixed effects: state (αs), chamber (γc), and year (ρt).
    - To examine heterogeneity in the impact of commute distance on female participation among the different types of legislatures, we include an additional specification that adds interaction terms for the expected level of commitment.
        - PctFemalei, t, s, c = β0 + β1Distancei, t, s, c + β2Distancei, t, s, c × HybridLegs + β3Distancei, t, s, c × FTLegs + γc + ηs + ρt + μXi, t, s, c + ɛi, t, s, c: (2)
- The impact of commute distance on female participation appears heterogeneous with the most detrimental effect in full-time legislatures; a less detrimental effect in hybrid legislatures; and no significant effect in part-time legislatures.
- We look for a consistent but causal result using the more rigorous district pair analysis which further mitigates endogeneity concerns.
    - With this setup, the 3,330 districts with elections in 2018 and/or 2020 generate 100,068 cross-state pairs.
    - For each unique cross-state pair of districts, one district will have a longer commute distance than the other.
    - For each unique district pair, without loss of generality, let index i (j) denote the district with the longer (shorter) commute.
    - Then, for each pair, we can calculate the difference in the percentage of female candidates, the commute distance, and the covariates, where ∆Variablei, j = Variablei − Variablej.
- In order for our pairing process to be effective, state house districts that are located close to each other but on opposite sides of the state borders should look similar with respect to other factors that might impact PctFemale.
    - To assess the similarity of the district pairs, we first look at the average values of each of the district-level covariates across all districts with a longer (shorter) commute distance, that is, those indexed by i (j).
    - The averages across most covariates appear very similar in magnitude.
    - To further verify the comparability of districts with longer and shorter commutes, we conduct placebo tests for each of the covariates.
        - For each district pair i, j, we regress the difference in the covariate value (∆Covariatei, j) on the difference in commute distance (∆Distancei, j).
- Then, to quantify the impact of commute distance on female participation in state elections, we run the following regression on the pair of districts i, j:
    - ∆PctFemalei, j t, p, c = δ1∆Distancei, j t, p, c + ζp + γc + ρt + μ∆Xi, j t, p, c + ∆χi, j t, p, c, (3)
    - In the specification given in (3), ζp is the state pair fixed effect, γc is the legislative chamber fixed effect, ρt is the year fixed effect, and ∆χi, j t, p, c is the error term.
- Our initial district pair specification does not take into account the number of hours state legislators work.
    - We include this expected level of commitment in the next specification; that is, the NCSL legislature categories are interacted with the difference in commute distance.
        - ∆PctFemalei, j t, p, c = δ1∆Distancei, j t, p, c + δ2∆Distancei, j t, p, c × HybridLegp + δ3∆Distancei, j t, p, c × FTLegp + ζp + γc + ρt + μ∆Xi, j t, p, c + ∆χi, j t, p, c, (4)
- The coefficient on ∆Distancei, j t, p, c is significant and positive, suggesting that for the base category of part-time legislatures, as commute distance increases, the percentage of female candidates also increases.
    - The coefficient for the hybrid legislature interaction term is negative and significant, suggesting that the effect of commute distance for hybrid legislatures may be neutral or slightly negative.
    - The coefficient for the full-time legislature interaction term is negative and significant, suggesting that as distance increases for districts in states with full-time legislatures, the percentage of female candidates decreases.
- Our results indicate that the effect of commute distance on female participation in state government is nuanced, driven by the anticipated workload from serving as a state senator or representative.
    - As one might expect, in full-time legislatures, the percentage of female candidates decreases as commute distance increases.
    - Somewhat surprisingly, there is an equal and opposite effect for part-time legislatures: in districts in such states, the percentage of female candidates increases as commute distance increases.
    - We posit that there are three possible mechanisms underpinning these results:
        - In two-parent households with children, men are more likely to be the breadwinner.
        - For districts located further from the capitol, flexible work opportunities, which women value, may be limited, thus increasing the attractiveness of work in a part-time state legislature.
        - Our data set does not capture the transportation options in a given district.
 
# 4. Conjoint Analysis
- We investigate other policies that might impact the likelihood of women to run for office.
    - We perform two choice-based conjoint analysis experiments to examine how different dimensions of workplace flexibility options may incentivize potential political candidates, who vary in gender and commute length to the capitol, to run for office.
    - The conjoint analyses focus on two distinct populations: (i) college students who represent potential political candidates who have no experience running for office; (ii) 2020 state house candidates (identified in the KnowWho data set) who represent potential political candidates who have some experience with politics.
- In analyzing the preferences of potential political candidates who have no experience with politics, we recruited 486 students from Georgetown University.
    - The students were presented with 12 sets of three hypothetical job descriptions for a political position and were instructed to choose the description that would make them most likely to run for political office.
    - Each description included four attributes with levels that varied for each set presented:
        - Paid Parental Leave (ParentalLeave): Whether the position comes with 12 weeks of paid parental leave for birth or adoption of a child.
        - Work from Home % (RemoteWork): Percent of time allowed to work from home.
        - Child Daycare (DayCare): Whether the position comes with free onsite daycare for children under five years old (i.e., non-school-aged children) during working hours.
        - Work Hours (WorkHrs): Expected hours worked per business day (Monday thru Friday) for the position, not including commute time.
    - Additionally, students were assigned a daily round trip commute time and a hypothetical age.
    - Demographic information was collected on the participants including their gender.
- To perform the analysis of the conjoint results, we used multinomial logit hierarchical Bayesian estimation.
    - To examine whether the students responded truthfully, we calculated the average zero-centered utilities for each attribute.
    - Student preferences were consistent with our expectations, which suggests the subjects responded to the survey truthfully.
- We then analyzed the variation in conjoint importance scores for each of the four attributes.
    - For a given student, the importance scores across the four attributes sum to 1 (i.e., 100%), thus the regression results for one attribute should be interpreted in conjunction with the regression results for the other attributes.
- Focusing specifically on the policies that are important to female potential candidates who have no past experience with political office, we find that all females place high importance on paid parental leave, and this preference is especially strong for females with longer commutes.
- To understand the preferences of potential political candidates who have some experience running for office, we conducted another conjoint study on 358 candidates for state house in 2020.
    - Demographic information was collected on each of the candidates.
    - The candidates were then presented with 12 sets of three hypothetical employee benefits packages that would accompany the state house position they ran for in 2020.
    - Each package included three possible attributes with levels that varied across each set presented:
        - Work from Home % (RemoteWork): Percent of time allowed to work from home.
        - Paid Parental Leave (ParentalLeave): Whether the position comes with 12 weeks of paid parental leave for birth or adoption of a child.
        - Child Daycare (DayCare): Whether the position comes with free onsite daycare for children under five years old (i.e., non-school-aged children) during working hours.
    - For their participation, candidates were compensated according to an incentive compatible conjoint design motivated by Agrawal et al. [2015].
- To analyze the conjoint results for candidates, we again used multinomial logit hierarchical Bayesian estimation.
    - We then analyzed the variation in conjoint importance scores for each of the three attributes.
- The results suggest that for potential political candidates who have some experience with politics, females with longer commutes value remote work/proxy voting more than females with shorter commutes.
    - This is in contrast to the other conjoint study where the group of female students with longer commutes placed a higher value on paid parental leave (versus females with shorter commutes).
 
# 5. Discussion
- Women face unique challenges when entering the workforce, including work-life balance issues and discrimination.
    - Many of these challenges are heightened for aspiring female politicians due to the unique nature of political work.
- We investigate the impact of required travel on female participation in government.
- We take a two-pronged approach when investigating this issue, conducting both observational and conjoint studies.
    - First, using data about the home addresses and demographic information about political candidates, we show that the impact of commute distance on the percentage of female candidates in state house and senate legislative races varies based on the type of state legislature vis-`a-vis workload time intensity.
        - In particular, there is a significant negative relationship between commute distance and the percentage of female candidates for full-time legislatures, but there is a significant positive relationship between commute distance and the percentage of female candidates for part-time legislatures.
        - For hybrid legislatures, the effect appears to be neutral or slightly negative.
        - Our results suggest that, consistent with previous literature, the combination of a long commute distance and full-time work can be a deterrent to women running for political office.
        - Interestingly, however, the combination of a greater commute distance and part-time legislatures appears to be more attractive to women.
    - Finally, using two conjoint studies, one on college students and another on state house candidates in 2020, we gauge the value of different workplace policies to different groups of potential political candidates: one with no political experience and another with some political experience.
        - Our results suggest that paid parental leave is the policy most likely to temper the potentially negative effect of a longer commute on women who have less experience with politics while remote work/proxy voting is most likely to temper the effect for women who have more experience.
- Paid parental leave policies end up being less effective for politicians than other types of careers, since it’s not always possible to have someone fill in for them to represent their region.
    - More work from home flexibility (such as remote work or proxy voting) can make a meaningful difference for politicians, both for new parents and others.
    - Overall, policies that provide paid parental leave and remote work/proxy voting opportunities to politicians could lead to a more diverse and inclusive legislature in the United States.
- Our work highlights the importance of family-friendly policies in shaping the pool of public sector workers.
 
---
 
# Executive summary of 1. Introduction
- We investigate the gender disparity in the political sphere, where women are underrepresented. Female political leadership has material impacts on the well-being of women and children.
- Supply-side forces, linked to the historical role of women in the home, contribute to gender disparities in the workforce. Women tend to seek out jobs with better work-life balance, prioritizing shorter commute times.
- Politics offers a unique vantage point to study flexible work and commute distance in women's labor force participation.
- We aim to determine the extent to which commute distance impacts women's entry into politics, how this varies with the type of political position (full-time, part-time, or hybrid), and what can be done to lower this barrier.
- We use data on state-level political candidates and conduct conjoint survey experiments.
- Our findings show that in states with full-time legislatures, districts farther from the state capitol tend to have a smaller percentage of female candidates, while in part-time legislatures, the opposite is true. Paid parental leave and remote work/proxy voting policies may help mitigate the detrimental effects of commute distance on female representation.
 
# Executive summary of 2. Literature
- We integrate research on workplace flexibility, the impact of distance on operations, gender diversity and bias, and public service operations.
- Workplace flexibility encompasses schedule and location flexibility, as well as the amount of work (full-time, part-time). We contribute by studying the impact of flexible work policies on females' likelihood to run for office.
- The literature on flexible workers focuses on how employers can optimally utilize such workers, while we extend it to consider the employee perspective.
- The impact of distance on performance can take many forms, and we are interested in understanding the implications of a service provider having to contend with distance, albeit from the perspective of how it impacts the pool of service providers.
- Diversity and bias in operations highlights the organizational benefits of female leadership and the need for diversity in service providers. We seek to answer the question of how to get women into more leadership positions.
- Public service operations research includes work in various areas, and we study the behavior of the candidate given the constraints of the system.
 
# Executive summary of 3. Data and Estimation
- We investigate the relationship between commute distance and female participation in state house and senate elections, using state elections to measure commute distance and avoid confounding factors.
- Data is collected from KnowWho, Google Maps, Bing Maps, the U.S. Census Bureau, and the MIT Election Data and Science Laboratory.
- The 2020 election cycle is unlikely to have been impacted by the COVID-19 pandemic due to candidates meeting milestones and deciding to run before restrictions.
- We examine whether districts farther from the state capitol have lower female candidate participation.
- **Identification Strategy**: We address potential endogeneity through reverse causality and district-level omitted variables, using percentage of female candidates and commute distance as variables.
- We develop a regression technique (“district pair analysis”) to estimate the effect of a longer commute distance on female participation in state politics.
- Prior to doing so, we visualize these districts to investigate whether any trends in female representation are apparent.
- Before performing that analysis, we first conduct a preliminary OLS regression (i.e., a “district-level analysis”), in which each district-year is a unit of observation.
- The impact of commute distance on female participation appears heterogeneous with the most detrimental effect in full-time legislatures; a less detrimental effect in hybrid legislatures; and no significant effect in part-time legislatures.
- Our results indicate that the effect of commute distance on female participation in state government is nuanced, driven by the anticipated workload from serving as a state senator or representative.
 
# Executive summary of 4. Conjoint Analysis
- We investigate other policies that might impact the likelihood of women to run for office.
- We perform two choice-based conjoint analysis experiments to examine how different dimensions of workplace flexibility options may incentivize potential political candidates, who vary in gender and commute length to the capitol, to run for office.
- In analyzing the preferences of potential political candidates who have no experience with politics, we recruited 486 students from Georgetown University and presented with hypothetical job descriptions including attributes like paid parental leave, remote work, daycare, and work hours.
- To perform the analysis of the conjoint results, we used multinomial logit hierarchical Bayesian estimation.
- We then analyzed the variation in conjoint importance scores for each of the four attributes.
- Focusing specifically on the policies that are important to female potential candidates who have no past experience with political office, we find that all females place high importance on paid parental leave, and this preference is especially strong for females with longer commutes.
- To understand the preferences of potential political candidates who have some experience running for office, we conducted another conjoint study on 358 candidates for state house in 2020.
- We then analyzed the variation in conjoint importance scores for each of the three attributes.
- The results suggest that for potential political candidates who have some experience with politics, females with longer commutes value remote work/proxy voting more than females with shorter commutes.
 
# Executive summary of 5. Discussion
- Women face unique challenges when entering the workforce, including work-life balance issues and discrimination, heightened for aspiring female politicians.
- We investigate the impact of required travel on female participation in government using observational and conjoint studies.
- We find that the impact of commute distance on female representation varies based on the type of state legislature (full-time, part-time, hybrid).
- Using conjoint studies, we gauge the value of different workplace policies for potential political candidates with varying political experience.
- Paid parental leave is most likely to temper the negative effect of commute for women with less political experience, while remote work/proxy voting is more likely to temper the effect for women with more experience.
- Paid parental leave policies end up being less effective for politicians than other types of careers.
- Overall, policies that provide paid parental leave and remote work/proxy voting opportunities to politicians could lead to a more diverse and inclusive legislature in the United States.

</details>

```
title: Too Much Information: When Does Additional Testing Benefit Schools?
authors: Vanitha Virudachalam, Sergei Savin, Matthew P. Steinberg
journal: Management Science
published: 2024
```

# Executive Summary
- This research explores the impact of **interim assessments** and **merit-based teacher bonuses** in U.S. K-12 school districts using a two-period principal-agent model.
- The study examines when these assessments are **beneficial** versus **detrimental** to student achievement.
- **Theoretical Framework**:
    - **Two-period dynamic principal-agent model**: The model captures the school district's (**principal**) decision to implement interim assessments and offer merit pay to teachers (**agents**), who then choose their effort levels.
    - **Markovian dynamics**: Two-state (**proficient/not proficient**) Markovian dynamics describe student test readiness evolution, influenced by teacher effort and the starting state.
- **Key Findings**:
    - Interim assessments' usefulness is **not guaranteed**.
    - The **accuracy** of interim assessments can be a **double-edged sword**.
        - Positive results can incentivize second-period teacher effort.
        - Negative results can demotivate teachers.
    - Even if an interim assessment leads to a higher probability of proficiency, the increased expected costs of merit-based bonuses may exceed the district's budget.
        - Even free interim assessments might be too expensive.
    - For schools starting behind (**not proficient**), interim assessments are valuable only in limited cases.
        - If the proficiency retention probability's **baseline is zero**, merit-based bonuses are generally better.
        - If the baseline is positive, interim assessments can be worthwhile:
            - With a low budget and possibly low **similarity ratio**, incentivizing second-period effort after a positive result.
            - With a moderate budget and higher similarity ratio, incentivizing effort after any midyear result, but the probability of proficiency is higher with interim assessments.
    - For schools starting on track (**proficient**) with a **zero baseline proficiency retention probability**, interim assessments are optimal only with a low similarity ratio and a moderate budget that supports incentivizing effort after a positive midyear result.

#assessment #teacher_incentives #principal_agent_model #dynamic_programming #education_policy #merit_pay #interim_assessments #student_achievement


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- The student assessment landscape in U.S. education has changed dramatically in the last two decades [Olson, 2005].
    - The *No Child Left Behind Act of 2001 (NCLB)* spurred frequent, annual testing.
    - School districts sought ways to gauge midyear progress.
    - High-stakes annual tests impacted teacher evaluation and compensation [Doherty and Jacobs 2013, Steinberg and Donaldson 2016].
- The study focuses on a school district that implements midyear assessments and offers merit-based bonuses.
    - The study aims to determine when midyear assessments are **beneficial** or **detrimental** to student achievement.
    - This work is applicable to any system combining monetary incentives with additional information for employees.
- Student assessments have long been a source of information on education quality [Linn 2000], but the form and extent of testing have varied.
    -  NCLB required states to ensure student proficiency on state tests and demonstrate "adequate yearly progress" [Dillon and Rotherham 2007, Klein 2015].
    - This led school districts to rely on costly third-party assessments to evaluate midyear progress [Young and Kim 2010].
    - Spending on classroom assessments increased significantly from 2001-2002 to 2018-2019 [Cavanagh 2015, Raugust et al. 2019].
    - The *Every Student Succeeds Act (ESSA)* granted states greater authority, but annual state tests remain a federal requirement [Klein 2016].
- This shift has given rise to a new category of assessments: *interim assessments*.
    - Traditionally, schools relied on *formative* and *summative assessments* [Bloom et al. 1971].
    - **Summative assessments**: are administered at the end of an instructional period to check student knowledge against broad standards [Perie et al. 2007, 2009].
    - **Formative assessments**: are used to obtain ongoing feedback from students [Perie et al. 2007].
    - Interim assessments lie between formative and summative assessments in frequency and scope [Perie et al. 2007].
    - Interim assessments evaluate students against specific achievement goals, guiding teaching and decision-making at school and district levels.
    - Unlike formative assessments, interim assessments can be aggregated across classrooms and schools [Li et al. 2010].
- Interim assessments are viewed as critical investments, even with shrinking budgets [Sawchuk 2009].
    - They are used in most of the largest school districts in the United States [Burch 2010].
    - However, there remains a limited understanding of their effectiveness [Bulkley et al. 2010, Burch 2010], due to a lack of clear standards.
    - Assessments in this category are marketed using various terms, despite their different character from traditional formative assessments [Li et al. 2010].
    - These assessments can serve instructional, evaluative, or predictive purposes [Perie et al. 2007], but their use is highly variable.

## 1.1. Performance-Based Pay
- School districts have also employed performance-based pay for teachers and administrators [Podgursky and Springer 2007, Pham et al. 2020].
    - The $4.35 billion Race to the Top Fund encouraged districts to experiment with pay-for-performance contracts [U.S. Department of Education 2009].
    - School districts were also encouraged to develop formative or interim assessments [Goertz et al. 2009].
    - Approximately 20% of school districts include some merit-based pay incentives based on measured teacher performance [Steinberg and Donaldson 2016].
- Over the last two decades, some school districts have invested in midyear interim assessments and explored merit-based rewards for teachers.
    - However, there are no studies of their combined effect.
    - This study addresses this gap and provides insight into the interaction between these two approaches.
- The research analyzes the effect of implementing an interim assessment that serves a predictive purpose.
    - Summative assessments are used as part of the school’s accountability system, specifically through merit-based rewards.
    - **Research Question**: Under what conditions should a school district invest in a midyear interim assessment?
- The district is modeled as a principal that provides financial incentives and improves the information available to teachers (agents).
    - A two-period dynamic principal-agent framework is used.
    - The probability of transitioning to the proficient state depends on:
        - The baseline probability of achieving proficiency.
        - The sensitivity to teacher effort.
        - Whether the school starts in the proficient or not proficient state.
    - The **similarity ratio** captures the degree of similarity between the two states.

## 1.2. Model Assumptions
- If the district invests in an interim assessment, both the district and teachers know the state of student proficiency.
    - Otherwise, teachers rely on less accurate information from formative assessments.
- The district offers teachers a merit-based bonus if a sufficient portion of their students show proficiency on the year-end test.
- Teachers respond to the bonus and information by choosing dynamic effort levels.
- A scalar represents the multiple levers teachers can use to influence educational outcomes.
- The usefulness of midyear interim assessments is far from guaranteed, due to the complexity of incentivizing teacher effort across two periods.
    - Incentivizing first-period effort can be difficult if the similarity ratio is too high or too low.
    - Accuracy is a double-edged sword.
        - Accurate assessments can make it easier to incentivize second-period effort when the school is on track.
        - Accurate assessments can be harder if the school is behind midyear.
    - A higher probability of ending the year proficient may push the district’s expected cost beyond the available budget.
        - Even a free interim assessment might be too expensive.
- For schools starting the year behind (**not proficient**), investing in an interim assessment is valuable only in limited cases.
    - If the proficiency retention probability's baseline is zero, merit-based bonuses are usually better.
    - If the baseline is positive, an interim assessment can be worthwhile in two cases:
        - If the district has a low budget and possibly low similarity ratio.
        - If the district has a moderate budget and higher similarity ratio.
- For schools starting the year on track to achieve proficiency and have a **baseline proficiency retention probability of zero**, investing in an interim assessment is only optimal in the case of a low similarity ratio and a moderate budget.

# 2. Literature Review
- The analysis draws on the principal-agent model literature in economics and operations management [Baron and Myerson 1982; Lewis and Sappington 1991, 1993, 1997; Cr´emer et al. 1998].
    - It also draws on the literature on performance pay in K–12 education.
- The study focuses on the role of incomplete information on an incentive contract.
    - Similar to early work in this area, the study determines the optimal contract the principal should offer for completing a task [Baron and Myerson 1982].
    - The agent receives a transfer payment as compensation, corresponding to a merit-based bonus.
    - These papers consider static games in which the information considered corresponds to the cost of production [Baron and Myerson 1982].
    - This study considers a dynamic game in which costs are known but there is uncertainty about the intermediate state.
- Multi-period dynamic models account for a small fraction of the principal-agent literature [Fudenberg et al. 1990, Plambeck and Zenios 2000, Fuloria and Zenios 2001, Shumsky and Pinker 2003, Zhang and Zenios 2008, Chu and Sappington 2009].
    - The principal is inducing effort based on an observable outcome that is stochastically determined by the agent’s effort choice.
    - The model includes hidden information about the intermediate state [Zhang and Zenios 2008], which may be revealed by the principal.
    - A key difference is that the transfer comes from a fixed budget that does not depend on the principal’s utility.
- Many papers assume information acquisition is a motivating factor for the agent [Lewis and Sappington 1997].
    - This paper identifies settings in which additional information may demotivate the agent [Gneezy et al. 2011].
- The work also adds to a rich stream of papers focused on contracting and performance pay in K–12 education.
    - Researchers point out theoretical difficulties, such as justifying why only some teachers receive bonuses [Murnane and Cohen 1986].
    - There is also the potential for harmful competition and low morale [Johnson 1984].
    - Some concerns can be ameliorated through school-level incentives [Clotfelter and Ladd 1996].
    - In this work, teachers within a school form a homogeneous group that can earn a school-level reward.
    - The payment of the reward at a single point in time is consistent with current practice [Fryer 2013, Chiang et al. 2017].
- The empirical evidence on the effects of merit pay remains mixed [Pham et al. 2020].
    - Short-term studies find that teacher-level merit pay improves student retention but negatively impacts student attendance [Eberts et al. 2002].
    - It is also correlated with higher test scores [Figlio and Kenny 2007].
    - Increased collaboration among teachers and alignment between instruction and test preparation even in the absence of a significant, lasting effect on student test scores [Springer et al. 2011].
    - Fryer [2013] finds no evidence that school-level incentive programs lead to improvements in student performance outcomes or in teacher or student behavior.
- Longer-term merit-pay experiments are more promising if modest [Dee and Wyckoff 2015].
    - Chiang et al. [2017] find that merit pay led to a slight increase in student achievement by the second year of the program.
- These empirical studies focus on the impact of a performance-based financial incentive.
    - They do not consider the influence of midyear assessments on teaching practices and student performance.
    - This analysis focuses on identifying school districts that may benefit from additional information brought in by interim assessments versus those that are better off using formative assessments alone.

# 3. Model: Combining Assessments and Merit-Based Pay to Achieve Proficiency
- This section presents a dynamic principal-agent model that captures the interaction between the school district and the group of teachers at a school.
    - The district explores the option of investing in additional information on the state of student performance.
    - Merit-based bonuses are provided to maintain or achieve standards of performance.
    - Teachers respond by selecting a dynamic policy defining their effort levels.
- Consider a discrete-time, two-period model, in which each period is half of the school year and time indices correspond to the beginning, middle, and end of the school year, respectively.
    - The school proficiency state indicates whether that school is **proficient** or **not proficient**.
    - **Proficient** means that a sufficient fraction of the school’s students are on track to satisfy state-imposed learning standards as measured by a state-administered standardized assessment.
- At the beginning of each period, teachers decide how much effort to allocate toward activities to improve student performance, decisions unobserved by the school district.
    - The school’s teachers are homogeneous and act as a group with teachers’ average school-level effort.
    - Modeling the average effort level allows us to focus on the first-order effect of the incentives.
    - The multidimensional nature of efforts that teachers make is approximated by a single aggregate measure.
    - Teacher effort represents additional effort that can be elicited through merit-based pay, so there is a positive probability of reaching proficiency even when there is no additional effort.
- The student proficiency state in each period evolves based on the state in the beginning of the period and the teachers’ effort decision in that period.
    - One term represents the effort-dependent probability of transitioning from "**not proficient**" in the beginning to "**proficient**" in the end of the time period.
    - Another term represents the respective probability of transitioning from "**proficient**" to "**not proficient**".
- The functional form of the transition probabilities reflects standard assumptions of monotonicity and nonincreasing return on effort.
    -  **Assumption**: There's a formula (which we are ignoring) governing student transition to **proficient** state or remaining in that state.
    -  The proficiency retention probability’s sensitivity to effort, representing the **increase** in probability due to increase in teacher effort.
    -  The proficiency retention probability’s baseline, the probability that a school remains in (or moves to) the proficient state **without additional effort**.
    - When baseline is zero, teachers’ effort decisions are made entirely based on balancing their expected monetary reward with the cost of exerting effort.
- The ratio of probabilities (transitioning to proficiency or remaining not proficient) equals a constant term, which characterizes the ease with which a school starting the period in the **not proficient** state can improve, relative to a school that must simply maintain proficiency.
    - This term is the **similarity ratio**, in which a higher value indicates greater similarity between starting in the **proficient** and **not proficient** state in a given period.
    - The similarity ratio is bounded above because it is more difficult for schools to attain proficiency than to maintain it [Davison et al. 2004, Neal and Schanzenbach 2010].

## 3.1. Assessments: Cost and Information Structure
- The initial and final states are known to both the teachers and the district based on the previous and current years’ state standardized tests.
- The school district chooses to evaluate students’ midyear progress using either formative assessments exclusively or by administering an additional, interim assessment.
    - For either choice, both the teachers and the school district learn the results.
- **Formative assessments** do not require an additional expense, but they less accurately assess students’ preparedness for year-end state tests.
- **Interim assessments** have a fixed cost but perfectly reveal the midyear state.
    - In this analysis, we assume a zero cost for interim assessments.
- The proficiency level indicated by the formative assessments is represented by the term.
- The probability that the formative assessment result takes a particular value given the true intermediate state is captured by the **true** and **false positive rates**.
- It is assumed that the formative assessments never perfectly assess the midyear state and that the false positive rate never exceeds the true positive rate.
    - **Assumption**: The **true positive rate** (correctly assessing a **proficient** school) is higher than the **false positive rate** (incorrectly assessing a **not proficient** school as **proficient**).

## 3.2. Timeline of Events
- At the beginning of the year, based on the initial state, the district decides whether to implement an interim assessment and offers teachers a compensation contract comprising base-pay and merit-pay components.
    - Teachers always receive the base pay, but the merit pay depends on the state of proficiency at the end of the year.
    - If the school ends the year in the proficient state, teachers earn a bonus.
    - Otherwise, the merit-pay component is zero.
- In response, the teachers determine the policy they use to select their effort levels.
    - Given the initial state and the teachers’ effort level in the first period, the system transitions to the midyear state.
    - If the school district administers an interim assessment, both the teachers and district learn the true midyear proficiency state.
    - If not, the teachers and district see the result derived from formative assessments, and the teachers use this to estimate the probability that the school is on track to achieve proficiency.
    - Teachers then select the effort level to be applied in the second period.
    - At the end of the school year, a standardized assessment is administered, and the final proficiency state is revealed.
    - The teachers are then paid according to the compensation contract.
- It is assumed that both the district and the teachers are risk neutral.
- The problems faced by the teachers (**agent**) and the district (**principal**) are described.

## 3.3. Teachers' Problem: Dynamic Response
- Given the district’s interim assessment decision and proposed merit-based contract, teachers choose the effort levels that maximize their expected merit-based compensation net of the cost of effort they incur.
    - In the absence of a merit-based bonus, teachers will not exert any additional effort.
- The interim assessment serves a predictive, not instructional, purpose.
    - The interim assessment does not provide information to teachers that increases the effectiveness of their effort or decreases their cost of effort.
- The teachers’ cost is modeled using a simple linear functional form representing stationary and constant marginal cost of effort.
    - **Assumption**: There's a direct cost (effort) on teachers involved when they exert effort.
- The teachers’ decision problem is represented by a two-period dynamic program.
    - Their effort decision in the second period depends on the information they receive after the first period.
    - The decision in the first period is influenced by the policy they adopt for the second period.

## 3.4. District's Problem: Optimal Assessment-Incentive
- For each school, the district wants to select the assessment type and matching merit-pay compensation to incentivize teachers to choose effort levels that maximize the school’s probability of ending the year in the proficient state.
- The total amount of investment in the information provided by the interim assessment and the incentive payments is limited by the school-level budget.
- It is acceptable for payments to a particular school to exceed the allocated budget as long as the budget constraint is satisfied in expectation (averaged across all schools).
- We look at the the probability that the school is in the proficient state at the end of the year under the optimal teacher effort, given the school starts in a particular state.
- For a given initial state, the district’s decision can be expressed as an **optimization problem**: maximize the probability of being proficient by choosing how big of a bonus to give and whether or not to give interim assessment.
- The above optimization problem comes with a constrait, that the expected payout from the bonus should be less than the school level budget.
- Those (previously defined) expressions describe a principal–agent problem in which a principal selects the combination of information set and incentives for the agent and the agent’s response is represented by a dynamic programming policy.
- It is assumed that the district only invests in an interim assessment when the probability that the school is proficient at the end of the year is strictly greater under an interim assessment than in its absence.
    - **Assumption**: The district only invests in interim assessemnt if and only if interim assesment will bring higher probability of being proficient.
- The analysis of this principal–agent problem is provided.

# 4. Analysis
- This section begins by determining the teachers’ optimal effort decision in each period for any given testing and merit-based bonus scheme.
- Using these results, the optimal level of merit-based bonus under both assessment decisions and, as a consequence, the districts’ optimal assessment decision is determined.

## 4.1. Optimal Teachers’ Effort Policy
- The teachers’ problem is a two-period dynamic program.
- For given bonus, the teachers’ effort decision for the second period is characterized.
    - Both the first and second period effort decisions depend on the magnitude of the merit-based bonus.
- second period bonus threshold: this is a value that determines wheter teachers will exert effort or not.
- Under the interim assessment, that is, when the teacher know about the status in middle of year, we calculate the probability of proficient based on known status.
- **Lemma**: In the second period, teachers will only exert maximum effort if bonus is at or above the **bonus threshold**.
- The optimal effort level follows a **threshold policy**: a bonus above the threshold results in maximum effort, and a bonus below the threshold results in no effort.
- A stronger belief that the true state is proficient leads to a **lower threshold level**.
- In schools that fall behind by the middle of the year or start the school year in the not proficient state, higher rewards are necessary to incentivize teachers to exert effort.
- A high similarity ratio makes it easier for teachers to move the school from the not proficient state to the proficient state, which results in a lower second period bonus threshold.
- When teachers do not know the true intermediate state, they rely on both the formative assessments and the first period teacher effort level to estimate the probability that the school is in the proficient state and select their second period effort level.
    - Teachers take into account that exerting high effort levels in the first period leads to a greater likelihood that the school will be in the proficient state by midyear; therefore, a lower monetary incentive is necessary to motivate second period effort following high effort in the first period.
- The accuracy of the midyear assessment result also factors into teachers’ effort decisions.
    - More accurate assessments make teachers easier to incentivize if the midyear assessment result is proficient but harder to incentivize if it is not proficient.
    - Inaccurate assessments allow teachers to preserve their belief that earning a bonus is possible and, thus, prevents them from becoming demotivated.
    - A false positive result can be beneficial for the district.
- The teachers’ effort level decision in first period depends on first period bonus threshold.
- **Lemma**: There's exists bonus threshold such that, in the first period, teachers either exert no effort if the bonus is below this threshold or maximum effort if the bonus is at or above this threshold.
- It all comes down to wheter or not the formative assessemnt result is accurate.
- As with the teachers’ second period effort level, optimal effort in the first period is positive if and only if the bonus offered by the school district exceeds a threshold.
    - However, the influence of the various parameters on the first-period bonus threshold are more complex because of teachers’ anticipation of students proficiency levels at the midyear and their own second period effort levels.
- A higher probability of being in the proficient state at the end of the first period does not necessarily result in a lower first period threshold.
- For schools that begin the year in the not proficient state, the first period bonus threshold always exceeds the second period bonus threshold.
    - It is always more expensive to incentivize effort in the first period than the second period regardless of the midyear assessment result.
- The first period bonus threshold does not depend on the accuracy of the midyear assessment.
- The bonus threshold is a U-shaped function of the similarity ratio.
    - When the not proficient state is dissimilar to the proficient state, teachers are not motivated to put in effort, and therefore, the bonus to induce them to exert effort needs to be high.
    - When it is easy to reach proficiency, this discourages effort.
    - The cost of incentivizing positive effort in the first period is lowest for moderate levels of the similarity ratio: in this region, teachers are motivated by both the large negative consequence from remaining in the not proficient state after the first period as well as the feasibility of moving to the proficient state.
- For schools that begin the year in the proficient state, the effort-inducing threshold does depend on the accuracy of the midyear assessment as well as the other parameters.
    - The threshold is increasing in the similarity ratio for the ratio above a lower bound.
    - Under an interim assessment, the effort-inducing threshold is always increasing in the similarity ratio, that is, the lower bound is equal to zero.
    - When the district does not invest in interim assessments, the potential inaccuracy of the formative assessment result leads to a nonzero lower bound, and the threshold is nonmonotonic in the ratio below this bound.

## 4.2. District’s Merit-Based Incentive and Assessment Decisions
- The district’s decision on the optimal bonus level is analyzed, using the characterization of the optimal teachers’ response policy in the presence of merit-based incentives and information about midyear student performance.
    - The district’s optimization problem is solved, in which the interim assessment decision is held fixed.
- The district’s estimate of the probability of achieving proficiency in the final state is a nondecreasing function of the merit-based bonus: the better the bonus, the higher change of getting proficient.
- **Lemma**: The optimal for the school district to offer a bonus if and only if the budget exceeds some limit.
- The optimal bonus level is a stepwise, monotone increasing function of the available budget.
    - For small budget levels, the district cannot afford to offer a bonus.
    - As the budget increases, the optimal bonus at each step corresponds to a bonus threshold value, either the first period bonus threshold or a value of the second period bonus threshold.
- The district’s optimal interim assessment decision for a particular school is now turned to.
    - Teachers use formative assessments throughout the school year, and for a school-specific budget, the district must determine whether it is advantageous to supplement this information with an interim assessment.
- It is assumed that the district only invests in an interim assessment when the probability that the school is proficient at the end of the year is strictly greater under an interim assessment than under formative assessments alone.
- For extreme budget levels, the interim assessment decision does not impact the school’s performance on the end-of-the-year standardized test.
    - When the budget is too small, the school district cannot afford to offer a reward that is high enough to induce teachers to exert any additional effort throughout the school year regardless of the testing decision.
    - For a large enough budget, the school district can afford a reward that incentivizes maximum effort levels throughout the school year regardless of the school’s midyear performance.
    - The school district can, thus, maximize the probability of the school achieving proficiency no matter the interim assessment decision.
- For nontrivial budget values, the introduction of the interim assessment is beneficial in a limited number of scenarios, which are driven by the following factors.
    - A high value of the similarity ratio means that the starting and midyear states have only a negligible impact on the probability that the school achieves proficiency at the end of the school year.
    - Schools in the not proficient state in any period, a low value of the ratio makes the possibility of transitioning to the proficient state less likely.
    - When teachers are motivated to exert effort by the prospect of a monetary reward, accurate information can make it easier to incentivize effort levels in the case of a positive midyear result, but it can also suppress effort levels in the case of a negative midyear result.
    - For school districts with a limited budget, even free assessments may be unaffordable because of a high expected cost of merit-based rewards.
- **Proposition**: Suppose that the school starts the year in the **not proficient** state and the interim assessment is free. Then, the following results hold:
a. If the **proficiency retention probability’s baseline** is zero, then it is optimal for the school district to forego investing in an interim assessment regardless of the accuracy of the formative assessments.
b. If the **proficiency retention probability’s baseline** is greater than zero, investing in an interim assessment depends on a threshold similarity ratio. Under a threshold, interim assessment is a good investment only when budget is low, otherwise interim assement is good investement when budget is moderate.
- For schools that start the year in the not proficient state, an interim assessment is not necessarily a worthwhile investment.
    - This outcome is strongest in the case in which there is no possibility of moving to the proficient state without additional effort.
- Next, consider the case in which there is a nonzero probability of moving to the proficient state in the absence of additional effort.
    - It is harder to incentivize effort in the first period than in the second, and second period effort is harder to incentivize after a worse midyear assessment result.
    - The interim assessment is a valuable investment in two cases.
        - In both cases, the budget is too small to support a monetary incentive large enough to induce first period effort.

# Executive summary of 1. Introduction
- The introduction provides context for the study, highlighting the changes in student assessment over the last two decades due to the No Child Left Behind Act (NCLB) and the Every Student Succeeds Act (ESSA).
- School districts have increased their reliance on interim assessments to gauge midyear progress, but the effectiveness of these assessments is not fully understood.
- The introduction also discusses the rise of performance-based pay for teachers and the need to understand the combined effect of interim assessments and merit-based rewards.
- The research question focuses on determining the conditions under which a school district should invest in midyear interim assessments.
# Executive summary of 2. Literature Review
- The literature review draws on the principal-agent model literature and the literature on performance pay in K–12 education.
- The study focuses on the role of incomplete information on incentive contracts and considers a dynamic game in which costs are known but there is uncertainty about the intermediate state.
- The literature review also discusses the potential for additional information to demotivate agents and the difficulties of implementing merit pay in an education setting.
- The analysis focuses on identifying school districts that may benefit from interim assessments versus those that are better off using formative assessments alone.
# Executive summary of 3. Model: Combining Assessments and Merit-Based Pay to Achieve Proficiency
- The model section presents a dynamic principal-agent model that captures the interaction between the school district and the group of teachers.
- The district explores the option of investing in additional information on student performance and providing merit-based bonuses to maintain or achieve standards of performance.
- Teachers respond by selecting a dynamic policy defining their effort levels.
- The model includes assumptions about the transition probabilities between proficiency states, the cost and information structure of interim and formative assessments, and the timeline of events.
- The teachers’ problem and the district’s problem are described, including the optimization problem for the district to select the assessment type and matching merit-pay compensation.
# Executive summary of 4. Analysis
- This section presents the analysis of the model, starting with determining the teachers’ optimal effort decision in each period for any given testing and merit-based bonus scheme.
- The optimal level of merit-based bonus under both assessment decisions is determined, and the district’s optimal assessment decision is analyzed.
- Key findings include that the usefulness of interim assessments is not guaranteed and that the accuracy of interim assessments can be a double-edged sword.
- The analysis also identifies conditions under which interim assessments are valuable for schools starting the year in the not proficient or proficient state.

</details>

I apologize for the oversight in the previous response. I missed the instruction to avoid mathematical notations. I will correct that in this revised summary.

```
title: Crosscutting Areas Surgical Case-Mix and Discharge Decisions: Does Within-Hospital Coordination Matter?
authors: Hessam Bavafa, Lerzan ¨Ormeci, Sergei Savin, Vanitha Virudachalam
journal: OPERATIONS RESEARCH
published: 2022
```
# Executive Summary
- The study addresses the problem of a **profit-driven**, **resource-constrained** hospital managing **patient flow**. This involves strategically choosing the right mix of **elective surgeries** (what kind and how many) and deciding on optimal **discharge policies** (when patients are sent home).
- We develop a model to evaluate how hospital profit is affected by the specific combination of **elective procedures** offered and how **patient discharges** are managed.
- The model analyzes the impact of these decisions on the utilization of two main classes of hospital resources: **front-end resources** (e.g., operating rooms) and **backroom resources** (e.g., recovery beds).
- We propose a new approach to model the **patient recovery process**, linking the **length of stay** to the **probability of readmission**. This captures the trade-off between shorter stays (freeing up beds) and potential complications leading to readmissions.
- We use a **simplified method** to estimate how much front-end and backroom resources are used for different surgery types and discharge policies.
- We evaluate the benefits of a **coordinated decision-making process** where both the **types of surgeries** offered and **discharge decisions** are made together, recognizing that in practice these decisions are often made by different hospital units.
- We compare the profits under **coordination** with two **decentralized approaches**:
    - **Front-end (FE) policy**: Decisions are made solely based on front-end costs, reflecting a situation where operating room efficiency is the primary concern.
    - **Siloed (SI) policy**: Discharge decisions are based on backroom costs, and the case-mix is optimized accordingly, which mirrors scenarios where bed availability is prioritized.
- **Key Findings**:
    - Hospitals using the **FE policy** can benefit from coordination when **backroom costs** are high, even if they don't exceed surgical costs. This implies that neglecting backroom considerations can lead to inefficiencies when those costs are significant.
    - Hospitals using the **SI policy** benefit from coordination only when **surgical costs** are high and dominate the cost structure. This suggests that focusing solely on minimizing backroom costs is only optimal when surgical costs are the primary driver of hospital expenses.
    - Specifically, for the five procedure types considered, DRG 743 is the least preferred and DRG 470 the most preferred with respect to front-end capacity under FE policy. On the other hand, under SI policy, DRG 330 is the lowest and DRG 247 is the highest in backroom preference. This highlights how different priorities lead to different "preferred" surgery types.

#hospital_capacity_management #stochastic_models #revenue_management #inpatient_flow_management #case_mix #discharge_policy #coordination #front_end_resources #backroom_resources #patient_recovery #readmission_probability

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Hospitals are complex organizations, requiring high levels of patient care coordination [Georgopoulos and Mann 1962].
- Coordination can take many forms, impacting healthcare provision, including emergency preparedness [Mills et al. 2018, 2020].
- Coordinated operational decision-making among hospital department heads [Shortell et al. 1976] and physician–nurse collaboration [Mitchell and Shortell 1997, Nair et al. 2012] are associated with positive patient outcomes.
- We focus on the coordination between **case-mix** (the types of patients and procedures a hospital handles) and **patient discharge policies** for a **profit-maximizing**, **resource-constrained** hospital.
- The hospital has two resource types: **front-end** (operating rooms) and **backroom** (recovery beds).
- The hospital manages profitability by controlling the **size and composition of its elective patient portfolio** and using **discharges** to control patients’ **length of stay (LOS)**.
- These decisions are often made by different parties [Centers for Medicare and Medicaid Services 2008]:
    - Case-mix: Hospital administrators and surgeons.
    - Discharge: Attending physicians.
- There's a complex interdependence between these decisions:
    - Case-mix requires the availability of both resource types.
    - Discharge decisions must balance backroom congestion and readmission probability.
- Patient discharge decisions are impacted by hospital occupancy levels [Forster et al. 2003, Kc and Terwiesch 2012, Long and Mathews 2017] and readmission rates [Fisher et al. 1994].
- Hospitals could benefit from a process where **case-mix** and **discharge policies** are determined together.
- We characterize the benefits of coordinated decision-making by comparing it to two decentralized approaches likely in real-world hospitals.
    - **Front-end (FE) policy**: Decisions dictated exclusively by front-end costs, approximating surgeons' influence.
    - **Siloed (SI) policy**: Discharge decisions based on backroom costs, and case-mix is set in response to the discharge policy.
- Operating rooms are often viewed as the major bottleneck and revenue generator [Cardoen et al. 2010, Hof et al. 2017].
- Some literature focuses on the operating room as the sole resource [Blake and Donald 2002], leading to the front-end policy.
- Another stream optimizes case-mix, taking patient length of stay distributions as given [Beli¨en and Demeulemeester 2007], similar to the siloed policy.
- Our model analyzes surgical procedures and patient discharge policies, introducing a model of patient recovery using a notion of health state.
- We tie patient health state at discharge to patient readmission probability and length of stay.
- Inclusion of the patient discharge policies in the hospital decision framework is novel.
- We focus on the following research questions:
    1. How does the patient discharge policy influence the portfolio of elective surgical procedures?
    2. When is it justified to use limiting patient discharge policies (e.g., discharging patients as early as feasible or minimizing avoidable readmissions)?
    3. What are the effects of limiting patient discharge policies outside of their domain of optimality?
- **Proposition 2** addresses the first question by describing the best portfolio for a given discharge policy.
    - The "**dominance**" index introduced provides an intuitive measure of the attractiveness of a surgical procedure.
    - We establish conditions for the unimodality of the dominance index (**Lemma 2**).
    - We use single- and two-procedure settings to illustrate the impact of the dominance index on hospital decisions.
- The second question is addressed by deriving optimal portfolios and discharge decisions under the **FE (Proposition 3)** and **SI (Proposition 4)** policies.
    - Limiting patient discharge policies arise naturally in decentralized resource management settings.
    - Results show that surgical procedures follow an intuitive preference ordering.
- The third question is addressed in our numerical study, which focuses on the effectiveness of the **FE** and **SI** policies when optimal discharge policies require coordination.

# 2. Literature Review
- The **case-mix planning problem** involves choosing the optimal size and composition of a hospital's portfolio of procedures.
- This is a key strategic consideration for a hospital managing its profitability in the face of resource constraints.
- It has become increasingly important due to reimbursement schemes centered on diagnostic related groups (DRGs) [Roth and Van Dierdonck 1995, Hof et al. 2017].
- We study this question considering the capacity of operating rooms (OR) and recovery beds, fitting into the literature on quantity-based revenue management for multiple resources.
- Some authors have explicitly characterized the optimal case-mix (patient mix).
    - Adan and Vissers [2002] developed an integer linear programming model to determine the optimal patient mix for elective procedures based on multiple resource constraints with deterministic LOS.
    - Ma and Demeulemeester [2013] take a multi-level approach, first determining the optimal case-mix and then using these results to build the master surgical schedule assuming that the mean LOS is fixed.
    - Freeman et al. [2018] develop a multiphase approach that utilizes mathematical programming to determine a set of solutions for the case-mix and master surgery schedule, assuming stochastic LOS, and then use simulation to assess the quality of each candidate solution.
- Our work is closely related to Bavafa et al. [2019], who characterize the optimal number of elective procedures for a single-specialty hospital using a two-moment approximation of OR and recovery bed usage, extending this work by jointly determining the optimal discharge decision for each procedure.
- The consideration of the discharge decision, determining the distribution of LOS and readmission probability, is the key element that distinguishes our work.
- We model the impact of readmissions on hospital resource utilization since some readmitted patients need additional surgeries.
- Unlike some prior work, we do not explicitly characterize the optimal number of beds or operating rooms or consider a hard capacity constraint.
- There is also a body of work focusing on short-term decisions determining the case-mix, like the master surgery schedule.
    - Beli¨en and Demeulemeester [2007] considered optimal bed usage when building cyclic surgery schedules by building a master schedule that levels bed usage, using stochastic patients per operating room block and stochastic LOS.
    - Adan et al. [2009] extended their previous work by considering a stochastic, rather than deterministic, patient LOS.
    - Rath et al. [2017] use a two-stage mixed-integer stochastic dynamic programming model to determine the optimal allocation of operating rooms and anesthesiologists and the sequencing of surgeries.
- This discussion focuses on quantity-based revenue management for multiple resources in healthcare.
    - Bavafa et al. [2019] provide a broader discussion of the quantity-based revenue management literature, including the single-resource setting.
    - Hof et al. [2017] provide a recent review of the case-mix planning problem, including a discussion of how it relates to other hospital planning problems.
- Additional research exists for hospital discharge decisions:
    - Studying the LOS relationship with readmission probabilities.
        - Carey and Lin [2014] found an inverse correlation between LOS and readmission probability for heart attack and heart failure patients.
        - We assume this inverse correlation to be true.
        - Carey [2015] states that expected cost savings from avoiding readmission is 15% to 65% of one more day of stay.
    - There is evidence that physicians respond to congestion when determining discharge levels, which influences patient readmission likelihood.
        - Kc and Terwiesch [2012] show that ICUs ration bed capacity during busy periods through a more aggressive discharge policy, increasing the likelihood of a revisit to the ICU with a longer LOS.
        - Long and Mathews [2017] found that ICUs have a discretionary boarding time that increases significantly when ward occupancy levels are high.
        - This supports that discharge policies must be considered at the strategic level.
    - Given the tradeoff between longer LOS with resource limits, some focus on the ideal discharge policy.
        - Chan et al. [2012] devised an ICU discharge policy considering readmission risks, leading to throughput increases without compromising patient outcomes.
        - Shi et al. [2019] developed a decision-support tool assessing patient readmission risk to aid discharge decisions.
        - Mills et al. [2020] compared surge capacity results based on discharge coordination between the emergency department and inpatient units versus smoothing the workload in inpatient units.
- Our focus is on the strategic, long-term policy necessary to profitably manage resources.
- Our modeling approach reflects our focus on the portfolio and discharge decisions at a strategic level complementing studies focusing on tactical management of hospital resources.
    - Dynamic models require approaches tracking the state of the system [Ayvaz and Huh 2010, Helm et al. 2011, Shi et al. 2019, Liu et al. 2019].
- Our analysis leaves out optimization of intraday scheduling of procedures [Patrick et al. 2008, Begen and Queyranne 2011, Helm and Van Oyen 2014, Zacharias and Pinedo 2017, Diamant et al. 2018].

# 3. Managing Elective Procedures and Patient Discharges: A Model
- Here, we show a model to describe the tradeoffs of a hospital controlling elective procedure portfolios and discharge policies to manage profitability.
- Assume a hospital offers different kinds of elective surgeries. Each surgery type has a range for how many surgeries are performed daily, where the low end ensures sufficient skill and quality are maintained.
- Hospitals also complete urgent procedures, where the total procedure load is elective plus urgent.
- The goal is to manage resource usage and minimize cost.

## 3.1. Modeling Hospital Resource Utilization
- Presenting a model linking hospital-chosen elective procedures with utilization of front-end (surgery time) and backroom (recovery beds) resources.

### 3.1.1. Duration of Surgical Procedures, Health State, and Recovery Dynamics
- Focus is on patient length of stay and how and when they are discharged.
- Patient discharge happens when physicians perceive sufficient recovery to continue follow-up outside the hospital.
- Physicians have non-zero probability of incomplete recovery and readmission decisions.
- Let a number represent a patient's health after a procedure, where physicians combine factors to determine if a patient has sufficiently recovered for discharge.
- Physicians have freedom determining discharge thresholds.

### 3.1.2. Discharge Threshold and Patient Length of Stay
- Doctors pick a discharge threshold - a health level - for discharging patients recovering from a procedure.
- The patient length of stay depends on this threshold, varying between minimum and maximum stay with a distribution.

### 3.1.3. Readmission Dynamics
- Payers utilize hospital readmissions to indicate hospital care quality.
- Hospitals get no payment after patient readmission under some systems.
- Therefore, care after readmission is "rework" caused by deficits in original care/patient recovery.
- High readmission rates relate to hospital discharge policy.
- Dynamic assumptions:
    - Patients who are discharged have a readmission probability that lowers as their health increases.
    - If patients are readmitted, they may have to repeat the initial procedure.
    - Recovery rates for readmission correlate with the original admission rate.

## 3.2. Utilization of Hospital Resources: Central-Limit Approximation
- We want to analyze the impact of chosen procedures and thresholds for the patients on hospital operations.
- Some math-based formulas are used to describe the urgent procedures.
- The idea is that the effect of the discharge thresholds on the hospital is described by some key measures, which are described using math.

## 3.3. Hospital Compensation, Operating Costs, and Optimization Problem
- Hospitals receive a payment for procedures to offset daily expenses.
- The overall optimization problem to reduce costs and maximize profit is stated.

# 4. Optimal Elective Portfolio and Discharge Policies
- The goal is to analyze the maximization problem and explain the optimal policies.
- For a given discharge policy, we define the sets of least and most preferable indices for the procedures.
    - The least-preferred procedures are performed only at the minimum level, and the most-preferred are done at the maximum.
- Next is the set of procedures that are in play, or not yet preassigned.

# Executive summary of 1. Introduction
- Hospitals must coordinate patient care due to their complexity.
- We study coordination between **case-mix** and **discharge policies** for a **profit-maximizing** hospital with **limited resources**.
- A hospital has **front-end** (operating rooms) and **backroom** (recovery beds) resources, needing balance between resource use and patient readmission risk.
- This leads to the research questions regarding operations of a hospital.

# Executive summary of 2. Literature Review
- The **case-mix planning problem** focuses on choosing optimal procedure types/numbers to maximize profit and is central for hospitals that manage profit and resources.
- Our research fits with revenue management work, looking at capacity for beds/operating rooms, also emphasizing the importance of discharge decisions.
- Prior research looks at optimal bed/procedure usage, but this research differs by considering discharge decisions, impacting LOS and readmission probability.

# Executive summary of 3. Managing Elective Procedures and Patient Discharges: A Model
- To study the tradeoffs in hospitals managing profitability, a model is presented with elective procedures and discharge policies.
- The model links elective procedure portfolios with front-end/backroom resource utilization.
- Assumptions about procedure duration, health state, recovery, and readmission are made to characterize the patient LOS and discharge process.

# Executive summary of 4. Optimal Elective Portfolio and Discharge Policies
- Optimization and characterizing the optimal policies where thresholds determine procedures to perform at upper/lower limits with a dominance index.
- Focus on front-end and backroom hospital resources to determine structure of optimal portfolio.

</details>

# Yixin Iris Wang
- Assistant Professor of Business Administration
### education
- Ph.D., Technology and Operations, University of Michigan, 2018
- M.A., Applied Economics, University of Michigan, 2014
- B.S., Statistics, Fudan University, 2012
### teaching
- Operations Management (BADM 275) Operations Management is about developing, producing, and delivering goods and services that meet and exceed customer expectations. decision making frameworks and techniques

```
title: Manufacturing and Regulatory Barriers to Generic Drug Competition: A Structural Model Approach
authors: Yixin (Iris) Wang, Jun Li, Ravi Anupindi
journal: Management Science
published: 2023
```

# Executive Summary
- The study investigates the **determinants** of market concentration in the generic pharmaceutical industry, focusing on how **manufacturing complexity** and the **regulatory environment** (specifically, ANDA review time) affect manufacturers' entry decisions.
- We develop a **structural model** to capture firms' entry decisions in a simultaneous game.
    - A firm's **payoff** from entering a market is modeled as a linear function of:
        - Market characteristics (e.g., market size, chronic vs. acute disease, drug complexity).
        - Firm-specific characteristics (e.g., specialty, prior experience, manufacturing quality).
        - Regulatory environment (ANDA review time).
        - Competitive effects (impact of other firms' entry).
        - Market-specific and firm-market specific unobserved factors.
    - **Dimension reduction:**  Instead of modeling each firm's individual decision, the model focuses on the number of entrants from two types of firms: "leaders" and "non-leaders."  This simplifies the analysis while allowing for heterogeneous competitive effects (the impact of a leader firm entering may be different than a non-leader). The leader firms were based on their highest total number of generic approvals and their number of approvals in our study period. We consider all the other firms as nonleader firms.
    - Addresses the challenge of multiple equilibria using a **Nash product maximizer selection rule.**  When multiple entry configurations are possible, the model selects the one that maximizes the product of each firm's profit from entering.
    - The entry decision is whether to enter when the **postentry payoff is nonnegative**.
- The model is estimated using a comprehensive dataset from six sources covering 820 generic small-molecule markets from 2003 to 2017.
- Key **findings**:
    - Manufacturing complexity, indicated by the number of active ingredients in a drug, significantly reduces the likelihood of generic entry (e.g., increasing the number of active ingredients reduces entry probability from 8.93% to 5.64%).
    - Delays in the ANDA review process significantly decrease the number of firms entering a market (e.g., a six-month increase in review time can negate the benefit of entering a double-sized market).
    - The impact of review time is heterogeneous across markets with different sales potentials: larger branded market size is required to generate the same level of generic competition when review time is long.
- **Policy simulations** indicate that a shortened ANDA review time increases the average number of entrants per market and reduces the fraction of markets without generic entry.
    - Zero-entry markets reduce by 10 percentage points (from 45% to 35%) for low potential markets.
    - However, a notable portion of markets, particularly those with low potential, would still lack generic competition.
    - Shorter review time has negligible impact on probability of having more than two entrants in low sales potential markets.
- We suggest that the FDA should consider alternate interventions, such as granting extended exclusivity for the first entrant in zero-entry markets, and prioritizing review of applications in low potential markets.

#regulatory_barriers #generic_drugs #market_competition #structural_model #manufacturing_complexity #ANDA_review #entry_decision #policy_simulation


<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Generic drugs account for a large portion of prescriptions and a smaller portion of total expenditures, providing significant savings to the healthcare system [Association for Accessible Medicines 2017; Gottlieb 2017].
    - The cost savings are only realized when the drug market is competitive.
- Generics competition drives down drug prices significantly [FDA 2019].
    - The first entrant brings the price down to 61% of the branded price.
    - The price falls to 46% with two entrants and 21% with four entrants.
- Many drug markets do not attract a large number of generics manufacturers, with a significant portion having only one to three generic manufacturers, and some branded drugs having no generic competition [Department of Health and Human Services 2017].
- One key factor influencing generics manufacturers' entry decisions is **manufacturing complexity**, as generics manufacturers need to develop efficient production processes, especially when handling potentially hazardous substances [e.g., penicillin].
    - Generics manufacturers often run production lines of different chemical substances in parallel to keep the production costs low, which elevates the risk of cross-contamination.
- Another important factor is the **FDA’s regulatory actions**, particularly the ANDA approval process, which has seen a substantial increase in applications, leading to review delays [nearly 2,300 applications awaiting a decision as of October 1, 2012].
- The FDA initiated the GDUFA in 2012 to address this problem and speed up the review process, leading to a reduction in the queue length since 2013 [Woodcock 2016; Brennan 2016].
    - A faster FDA review enables manufacturers to enter new markets earlier and generate revenue sooner.
    - It also increases the likelihood of entry by competitors, potentially deterring firms from entering due to anticipated price competition.
- These countervailing effects make the relationship between ANDA review time and entry not obvious.
- We examine the following questions:
    - *RQ1:* What are the key determinants of a generics manufacturer’s entry decision?
    - *RQ2:* Would a shorter review time necessarily lead to more competitive drug markets?
- We develop a structural model that captures a firm’s decision in a simultaneous entry game.
- To address the identification challenge of multiple equilibria, we use a framework that allows for a general form of heterogeneity and a selection rule that chooses the equilibrium that maximizes the Nash product [Berry 1992; Bajari et al. 2010].
- We overcome the challenge of data scarcity by collating data from six disparate sources to create a dataset for generic drug entries from 2003 to 2017, characterizing 820 generic small-molecule markets.
- Our reduced-form evidence and structural estimates indicate that manufacturing complexity and review time delays are associated with fewer generics entries.
    - We find that the impact of review time is heterogeneous across markets with different sales potential.
- We conduct a policy experiment based on the estimates from our structural model to investigate the effect of regulatory barriers on competition in drug markets.
    - The result shows that a reduction in the ANDA review time significantly raises the level of market competition.
    - The shortened review time, however, has a negligible impact on the probability of having more than two entrants in markets with low sales potential.
    - We suggest the FDA prioritize review of applications in low potential markets to achieve more entry in those markets.
- In summary, our research demonstrates that manufacturing complexity and delays in the ANDA review process significantly affect the level of competition in drug markets.
    - Policy simulation results reveal that reduced delays increase entry in general, but the effect is limited for low potential markets.

# 2. Literature Review
- Previous research has looked at the efficiency of generics entry on drug price reductions [Grabowski and Vernon 1992; Frank and Salkever 1997; Olson and Wendling 2013].
- A stream of literature has studied the impact of several supply and demand characteristics on a manufacturer’s entry decision [Morton 1999; Gallant et al. 2010; Ching 2010].
    - Morton [1999] shows that larger branded markets, markets to address chronic diseases, and markets in which firms have prior experience attract more entries.
    - Subsequent studies incorporate competitor effects into the entry decision and formulate an oligopoly game among manufacturers using dynamic structural models [Gallant et al. 2010; Ching 2010; Ellison and Ellison 2011; Kong 2016].
    - We contribute to the literature by modeling market and firm heterogeneity and the heterogeneous competitor effects.
- Ching [2010] is the only other study that looks at the role of the application review process in generics entry decisions, but the author does not include any metric for review time in the model.
    - In our study, we consider many more markets than Ching [2010].
    - We retrieve the size of ANDA review backlog from a FDA report and use it as a proxy for the speed of the ANDA review process.
- The effect of delays on the decision to join a queue has been studied in the operations literature [Veeraraghavan and Debo 2009; Yang et al. 2016; Deacon and Sonstelie 1985; Png and Reitman 1994; Allon et al. 2011; Lu et al. 2013; Yu et al. 2016; Aks¸in et al. 2013].
    - We contribute to this line of work by studying the effect of waiting in a competitive setting.
    - A recent paper by Rao [2019] studies the effect of the expedited review process on firm’s investment decisions under competition in the innovator drug industry.
- Several recent empirical studies investigate the pharmaceutical supply chain [Xu et al. 2021; Gray et al. 2015; Ball et al. 2017; Karimi et al. 2021].
- Our work also contributes to the growing literature on the use of structural estimation in the operations management literature to identify underlying drivers of operational efficiency and conduct policy simulations [Li et al. 2014; Moon et al. 2018; Guajardo et al. 2015; Mani et al. 2015; Colak and Bray 2016; Zheng 2016; Olivares et al. 2012; Kim et al. 2014; Jiang et al. 2022; Bray and Mendelson 2012; Bray and Mendelson 2015; Olivares et al. 2008].

# 3. Data
## 3.1. Background
- Prescription medications make up a significant portion of the United States' healthcare spending [CMS 2015].
- Over the last decade, there has been a declining trend in the price of generic drugs and an increasing trend in their use [GAO 2016].
- Savings from prescribing generics has increased [Association for Accessible Medicines 2017].
- The generics industry started its boom period after the Drug Price Competition and Patent Term Restoration Act of 1984, often referred to as the Waxman-Hatch Act.
- Generics manufacturers need to fulfill requirements in the ANDA’s six areas: chemistry, manufacturing, testing, labeling, inspections, and bioequivalence.
- With an increasing number of generics applications submitted to the FDA since 2005, the agency was unable to keep up with the pace of demand.
    - In 2003, the average approval time for a generic drug was 20 months, which jumped to 31 months in 2011 and continued to rise to 42 months in 2014 [Meadows 2003; Ebert 2016].
- In response, to reduce the backlog and the long review time, the FDA initiated the five-year GDUFA program in 2012 to speed up the review of ANDAs [Brennan 2017].

## 3.2. Data Sources
- To study the effect of each of the market entry determinants, we obtain data from the following sources:
    - Annual Editions of the Orange Book (2000-2019)
    - Clinformatics Data Mart
    - NDC Directory
    - DrugBank database (Version 5.0)
    - FDA Inspections database
    - Implementation of the Generic Drug User Fee Amendments of 2012 [Woodcock 2016]
- We obtain patent data, exclusivity data, and approved drug products data from annual editions of the Orange Book.
- To measure market size, we obtain the annual prescription charge and quantity for the reference branded drugs from Clinformatics Data Mart, provided by Optum, Inc.
    - We cross-validate it with data from the Centers for Medicare & Medicaid Services (CMS) and Drugs.com.
- To better assess market profitability and production difficulty of each drug product, we obtain additional characteristics of the drugs from the DrugBank database [Law et al. 2013; Ayvaz et al. 2015].
- To develop a proxy for manufacturing quality, we obtain facility inspection records since October 1, 1999, from the FDA Inspections database through a Freedom of Information Act (FOIA) request.
- To study the impact of an ANDA backlog on a firm’s entry decision, we obtain the annual number of pending generics applications since 2000 from the FDA report on Implementation of the GDUFA [Woodcock 2016].

# 4. Sample and Variable Construction
## 4.1. Definition of Drug Markets
- We define a drug market as a potential drug market to enter when the patent of the branded version expires, focusing on patents with expiration dates during the period from 2003 to 2017.
- We exclude markets with patent expiration dates before 2003 or after 2017 because of lack of data on market and firm characteristics.
- We follow Conti and Berndt [2020] to define a potential drug market at the level of a combination of active ingredient and route of administration.
    - We choose to include the route of administration in the definition because of the variations in production cost and a requirement that manufacturers submit an ANDA for each route of administration separately.

## 4.2. Competition Among Initial Entrants
- We identify the entry time of a manufacturer by the FDA approval date, also available in the Orange Book.
- For each potential drug market, we choose to focus on the initial entries, that is, entries with ANDA approval dates within the first two years after the patent expiry date.
    - During this time, prices generally drop significantly as manufacturers compete for market share [Emanuel 2011].
- We model the manufacturers’ entry decisions during this initial stage as a simultaneous game.

## 4.3. Market and Firm Characteristics
- We include several attributes that affect market profitability and production cost for all manufacturers.
    - We use total prescription charges of branded products one year before patent expiry as a proxy for the revenue of the branded drug market [Morton 1999].
    - We also include the number of strength levels, whether the drug is used to treat chronic diseases, and the number of substitutes based on the structured indication of the drug products.
    - We include indicator variables for the 20 largest branded pharmaceutical firms to control for the influence from a branded producer [Ellison and Ellison 2011].
- We focus on the number of APIs and the route of administration to characterize the manufacturing complexity of drug products.
    - We thus include an indicator for drug products that are administered via the parenteral route.
- We also consider market-firm specific attributes that affect entry or production costs of a firm in a drug market.
    - We include three count variables for each market-firm pair to indicate the firm’s experience in markets with the same API, the same route, and the same therapeutic class, respectively [Morton 1999; Lee et al. 2016].
    - We measure firm’s concentration over therapeutic classes via the Herfindahl-Hirschman Index (HHI over therapeutic classes) and include it as an additional control variable.
    - We infer the manufacturing quality measure by using the number of citations per inspection in the FDA inspection records; a higher citation count implies a lower quality of the firm’s manufacturing facilities.
    - Last, we control for the count of a firm’s recent ANDA approvals to account for potential serial correlation in a firm’s manufacturing and financial conditions.

## 4.4. Regulatory Environment Measure
- Because a firm’s entry decision can be affected by the anticipated review time, we apply a three-year lag and use the lagged count of pending applications in the queue as a proxy of the anticipated review time.
- To ensure robustness of our results, we adopt an instrumental variable strategy when estimating the effect of queue length.
    - We consider two potential instruments: the GDUFA 2012 program and the number of drugs whose patents are set to expire in each year.
- To help interpret the effect of the review time changes, we rescale the number of backlog applications using an affine transformation to approximate the review time in months.

# 5. Model and Estimation
## 5.1. Model Motivation
- We perform two sets of regression analyses to gain preliminary insights into firms’ entry decisions: market-level Poisson analysis and firm-market-level reduced-form analysis.
- We test how the number of generics entrants changes with the regulatory environment and explore how market-specific characteristics are associated with the market structure.
- Because the number of entrants is count data, we characterize the model using Poission regression.
- A structural entry game that explicitly models the endogenous entry decisions from competitors would allow us to obtain unbiased estimates of the review time while incorporating both market- and firm-level characteristics.
    - Importantly, a structural analysis also allows us to conduct counterfactual analysis.

## 5.2. Framework
- We assume that manufacturers play an entry game with complete information, simultaneously deciding whether to enter a market.
- Postentry payoffs are determined by characteristics of the market and of firms that enter.
- We specify manufacturer i’s payoff in market m as a linear function of market- and firm-specific characteristics and the regulatory environment.
- The payoff of entering a market depends not only on a firm’s profitability while operating in the market, but also the cost the firm incurs during the approval process.
- We implement an instrumental variable approach to alleviate the reverse causality concern resulting from the endogenous regulatory environment measure Pt(m), the imputed ANDA review time.

## 5.3. Dimension Reduction
- We introduce our modified framework to consolidate the decision of multiple firms, enabling a prediction of the number of two types of entrants and allowing the entry of the two types of firms to have different effects on the payoff of the competitors.
    - This is done by defining leader and non-leader firms.
- Compared with the original framework, a key simplifying assumption is that entry from the same type of firm generates identical competitive effects; that is, we assume δj  δL for all j ∈ SL and δj  δNL for all j ∈ S NL.
- We model the decision of the small firms as if they formed a single nonleader firm and collaboratively made an entry decision.

## 5.4. Identification and Estimation
- The dimension reduction transforms the problem to predict the equilibrium number of leader and non-leader entrants.
- We then complete the model by specifying a selection rule (i.e., Nash product maximizer) to ensure uniqueness of the equilibrium [Bajari et al. 2010].
- Because there is no closed-form solution to the equation system, we estimate the model using the simulated method of moments.
- To identify the parameters of the entry model, we need to recognize a sufficient number of exogenous variables to specify at least k moment conditions.
- We simulate the entry games to obtain the model predicted equilibrium and the prediction errors [McFadden 1989].
- We estimate the model parameter by minimizing the violations of the moment conditions.

# 6. Estimation Results
- We present estimation results for the parameters included in our structural model that explicitly captures the competitive effects and allows for market-specific shocks.
- We find that the ANDA review time has a significantly negative impact on the likelihood of a manufacturer entering a market.
    - If the anticipated review time increases from two to three years, the average marginal effects of review time reduce the entry probability from 10.23% to 8.35%.
- We calculate the effect size relative to that of the branded market size.
- Apart from the review time, our estimates for market and firm-specific characteristics also suggest that production complexity significantly deters manufacturers from entering a market.
    - One interesting feature is the large impact of the number of APIs.
    - We see that the cost of acquiring APIs can be partially offset if the manufacturer has prior experience in the same active ingredient.
- Our structural model with heterogeneous competitive effects overall performs well.
- However, the entry model tends to slightly overpredict the proportion of markets with exactly one entrant and underpredict the proportion of markets with two or more entrants [Aguirregabiria and Ho 2012].

# 7. Policy Simulations
- We empirically test the influence of the ANDA review time on the level of competition in the U.S. drug markets.
- We perform a policy experiment on all 820 markets in the study period.
- By allowing firms to enter the market sooner, a shorter ANDA queue directly increases manufacturers’ probability of entry.
- For the drug markets in our study period, the count of average number of entrants decreases by a total of 25.4% as the average ANDA review time increases from 20 months to 42 months.
- When the average review time increases, high potential markets on average lose 0.78 generics entrant.
    - The impact of ANDA review time on low potential markets, however, is considerably smaller; the average number of entrants reduces from 1.065 to 0.789.
- When the review time is shortened, in the low potential markets we see proportionally more entries from leader firms (e.g., Teva and Mylan).
- We observe a lower fraction of markets with no generics entry when the review time decreases from 42 to 20 months.
- Even with a 20-month review time, 35.3% of the low potential markets and 10% of the high potential markets are still unattractive to generics manufacturers.
- For low potential markets, we also find that review time reduction has negligible effects on the tail distribution.
- For high potential markets, we see a much stronger impact of the review time on the tail distribution.
- These distributional changes have implications on generic drug prices, because the marginal price impact of each additional entrant is decreasing [FDA 2019; Dave et al. 2017].

# 8. Conclusions
- We collect data from six different sources and analyze a host of factors that may have impacted the level of competition in 820 drug markets that were opened to generics manufacturers between 2003 and 2017.
- Besides market size and a firm’s prior experience in similar drugs, we find that product complexity significantly affects the level of competition in a drug market.
- Moreover, we also find that delays in ANDA approvals can significantly dampen competition in all drug markets.
- Our policy simulation results show that excessive delays in the review process was one of the main causes of restricted competition in the drug markets.
- To achieve the goal of increasing generics competition and lowering pharmaceutical costs, the FDA needs to avoid long backlogs in the review system.
- The simulation results also quantify the benefits of lowering ANDA approval delays on entry probability that creates social value, allowing the FDA to assess appropriate investments needed to reduce delays and design priority policies to balance entry between markets with different sales potential.

---

# Executive summary of 1. Introduction
- Generic drugs provide significant savings to the healthcare system, but these savings are realized only when the drug market is competitive.
- Market competition is related to entry of generics drug manufacturers, which in turn is influenced by **manufacturing complexity**, and the **FDA’s regulatory actions** (specifically, ANDA approval process).
- The questions are: (1) What are the key determinants of a generics manufacturer’s entry decision? (2) Would a shorter review time necessarily lead to more competitive drug markets?
- To answer these questions, the authors develop a structural model that captures a firm’s decision in a simultaneous entry game.

# Executive summary of 2. Literature Review
- Previous research regarding the generic pharmaceutical industry has looked at the efficiency of generics entry on drug price reductions.
- A stream of literature in the generic pharmaceutical industry has studied the impact of several supply and demand characteristics on a manufacturer’s entry decision.
- To the best of our knowledge, Ching [2010] is the only other study that looks at the role of the application review process in generics entry decisions.
- The effect of delays on the decision to join a queue has been studied in the operations literature.
- Our work also contributes to the growing literature on the use of structural estimation in the operations management literature to identify underlying drivers of operational efficiency and conduct policy simulations.

# Executive summary of 3. Data
- **Prescription medications** make up a significant portion of the United States' healthcare spending.
- The **generics industry** started its boom period after the Drug Price Competition and Patent Term Restoration Act of 1984, often referred to as the Waxman-Hatch Act.
- With an increasing number of generics applications submitted to the FDA since 2005, the agency was unable to keep up with the pace of demand.
- In response, to reduce the backlog and the long review time, the FDA initiated the five-year GDUFA program in 2012 to speed up the review of ANDAs.
- Data is obtained from: (1) Annual Editions of the Orange Book (2000-2019), (2) Clinformatics Data Mart, (3) NDC Directory, (4) DrugBank database (Version 5.0), (5) FDA Inspections database, and (6) Implementation of the Generic Drug User Fee Amendments of 2012.

# Executive summary of 4. Sample and Variable Construction
- A **drug market** is defined as a potential drug market to enter when the patent of the branded version expires, focusing on patents with expiration dates during the period from 2003 to 2017.
- We focus on the **initial entries**, that is, entries with ANDA approval dates within the first two years after the patent expiry date.
- We include several attributes that affect **market profitability and production cost** for all manufacturers.
- We focus on the **number of APIs** and the **route of administration** to characterize the **manufacturing complexity** of drug products.
- We also consider **market-firm specific attributes** that affect entry or production costs of a firm in a drug market.
- To ensure robustness of our results, we adopt an **instrumental variable strategy** when estimating the effect of queue length.

# Executive summary of 5. Model and Estimation
- Two sets of regression analyses are performed to gain preliminary insights into firms’ entry decisions: **market-level Poisson analysis** and **firm-market-level reduced-form analysis**.
- A **structural entry game** that explicitly models the endogenous entry decisions from competitors would allow us to obtain unbiased estimates of the review time while incorporating both market- and firm-level characteristics.
- We assume that manufacturers play an **entry game** with complete information, simultaneously deciding whether to enter a market.
- The authors introduce a modified framework to consolidate the decision of multiple firms, enabling a prediction of the number of two types of entrants and allowing the entry of the two types of firms to have different effects on the payoff of the competitors.
- The model is estimated using the **simulated method of moments.**

# Executive summary of 6. Estimation Results
- The estimation results for the parameters included in the structural model explicitly capture the competitive effects and allows for market-specific shocks.
- We find that the **ANDA review time** has a significantly negative impact on the likelihood of a manufacturer entering a market.
- Apart from the review time, our estimates for market and firm-specific characteristics also suggest that **production complexity** significantly deters manufacturers from entering a market.
- The model predicts **number of entrants** accurately.

# Executive summary of 7. Policy Simulations
- In the policy simulations, the authors empirically test the influence of the **ANDA review time** on the level of competition in the U.S. drug markets.
- A shorter ANDA queue directly increases manufacturers’ probability of entry.
- The average number of entrants decreases when average ANDA review time increases.
- Reduction of review time from 42 to 20 months in low potential markets did not increase the number of entrants in these markets significantly
- The FDA needs to seek complementary methods, other than the shortened review time, to encourage sufficient generics competition.

# Executive summary of 8. Conclusions
- This study collects data from six different sources and analyze factors that may have impacted the level of competition in 820 drug markets that were opened to generics manufacturers between 2003 and 2017.
- Besides market size and a firm’s prior experience in similar drugs, the authors find that **product complexity** and **ANDA delays** affect generics market.
- Policy simulation results show that excessive delays in the review process were one of the main causes of restricted competition in the drug markets.
- This study suggests alternative methods to FDA for encouraging generics competition.

</details>

```
title: Can Employees’ Past Helping Behavior be Used to Improve Shift Scheduling? Evidence from ICU Nurses
authors: Zhaohui (Zoey) Jiang, John Silberholz, Yixin (Iris) Wang, Deena Costa, Michael Sjoding
journal: Management Science
published: 2023
```
# Executive Summary
- We examine the impact of **past helping behavior (organizational citizenship behavior, OCB)** among employees on **shift performance**, focusing on **ICU nurses** and their impact on **patient length of stay (LOS)**.
- We hypothesize that **past helping behavior**, measured by **TotalHelp** (total past helping of each employee) and **DirectedHelp** (past helping between each pair of employees), is associated with improved shift performance, specifically **reduced patient LOS**.
- *H1*: The amount of total helping actions previously performed by members of a focal shift (TotalHelp) is positively associated with focal shift performance.
- *H2A*: The volume of helping activities previously performed between pairs of members allocated to a shift (DirectedHelp) is positively associated with shift performance.
- *H2B*: The recipient of the past help matters — the DirectedHelp measure is predictive of shift performance even when controlling for the TotalHelp measure.
- We empirically validate these hypotheses using **retrospective data** from six ICUs, leveraging **electronic medical records (EMR)** to identify nurse helping instances.
    - **Identification Strategy**: We utilize **ICU-diagnosis code fixed effects models** to control for unobserved heterogeneity across ICUs and patient diagnoses, along with clustering standard errors at the ICU-diagnosis code level. We use **shift-level measures of nurse interactions** to address potential endogeneity concerns related to non-random nurse-to-patient assignments.
- *H3A*: The number of past shifts worked together by pairs of members allocated to a shift (TeamFamiliarity) is positively associated with shift performance.
- *H3B*: The TeamFamiliarity measure is a marginal predictor of shift performance given the TotalHelp and DirectedHelp measures.
- We compare the predictive power of past helping behaviors against **team familiarity**, a widely studied concept, finding that past helping is more predictive of patient LOS.
- **Counterfactual analysis** suggests that small changes in shift composition based on past helping can significantly reduce total LOS, demonstrating the managerial significance of the findings.
- Key findings include:
    - A 1-unit increase in DirectedHelp reduces patient LOS by 1.5% (approximately 3.3 hours).
    - The impact of DirectedHelp is more pronounced for patients with higher acuity scores.
    - Past helping interactions and nurse workload are major drivers of helping actions.
- We propose incorporating **pairwise affinity between coworkers** into scheduling algorithms to improve operational measures.

#nurse_scheduling #organizational_citizenship_behavior #hospital_operations #empirical_operations #helping_behavior #team_familiarity #ICU #patient_length_of_stay #nurse_interactions

<details>
    
  <summary>Click to expand sections</summary>


# 1. Introduction
- Employees' discretionary actions, termed **organizational citizenship behavior (OCB)**, improve organizational performance [Podsakoff et al., 2009].
- OCB is linked to improvements in productivity, efficiency, costs, profitability, customer satisfaction, and unit turnover [Podsakoff et al., 2009].
- Much research focuses on how to benefit from OCB, such as identifying job applicants likely to exhibit OCB [Latham and Skarlicki, 1995; Allen et al., 2004], mentoring [Donaldson et al., 2000; Tepper and Taylor, 2003; Kwan et al., 2011], and management styles that encourage OCB [Podsakoff et al., 1990, 1996; Ilies et al., 2007; van Dierendonck, 2011].
- We extend the OCB literature by focusing on the **impact of OCB on shift performance**.
    - Specifically, we hypothesize that **pairs of employees who have helped each other in the past will continue to do so**, improving the performance of shifts where they are scheduled together.
- We propose scheduling shifts to consider past helping behaviors by pairing employees who have helped one another.
- In workplaces where coworker interactions are captured by existing IT processes, scheduling that takes into account past helping behaviors is a potentially low-cost and non-disruptive approach that managers can use to bolster workplace OCB and performance.
- We empirically study whether changes to shift composition to encourage OCB can yield statistically and managerially significant improvements in overall performance.
- We focus on nurses in hospital intensive care units (ICUs), where employee interactions are captured by IT processes.
    - The **electronic medical record (EMR)** documents instances of nurses helping each other, enabling us to study the impacts of scheduling pairs of nurses with a history of helping one another to the same shift.
- The nursing literature contains evidence that an ICU’s overall level of helping and collaboration among nurses is associated with performance [Kalisch and Lee, 2010].
    - However, whether the benefits of helping can be magnified via scheduling decisions remains an unanswered question.
- Our empirical setting of hospital ICUs is an important application area where performance improvements are critical.
    - Care in ICUs is expensive, comprising roughly 13% of hospital expenses, 4% of overall healthcare expenses, and 0.7% of the GDP within the United States [Halpern and Pastores, 2010].
    - Maintaining available ICU capacity is critical to the proper functioning of the hospital.
    - Inadequate ICU resources could lead to canceled surgeries, patients suffering health setbacks, or the inability to provide adequate care to all critically ill patients, a concern that has been brought to the fore during the COVID-19 pandemic.
    - Overcrowding has been linked to increased patient mortality in hospital settings [Kuntz et al., 2015].
- We hypothesize a link between past helping behavior and shift performance.
- We empirically confirm this link using exogenous variation in shift composition within historical nurse scheduling data from six ICUs in a large academic medical center.
- We show that past helping data is more predictive of shift performance than team familiarity.
- We perform a counterfactual analysis, revealing that relatively small changes to shift scheduling decisions taking into account past helping behaviors might achieve managerially relevant improvements in ICU patients’ total length of stay within the hospital.
- We summarize key implications of this study and suggest future directions.

# 2. Literature Review and Theory Development
- We begin by considering whether shifts with historically more helpful employees exhibit better performance.
    - We define **TotalHelp** as the total past helping behavior of shift members.
    - We expect past helping behavior to predict helping behavior on the focal shift, and helping behavior on the focal shift to yield improved productivity on that shift.
    - *H1*: The amount of total helping actions previously performed by members of a focal shift (TotalHelp) is positively associated with focal shift performance.

## 2.1. Antecedents of OCB and Operational Improvements
- There is theoretical and empirical support suggesting that the volume of past helping behaviors might predict helping behaviors on a focal shift.
    - The key antecedents of employee OCB have long been studied in the organizational behavior literature [Organ et al., 2006].
- Some antecedents, such as employee personality [Organ and Ryan, 1995; Borman et al., 2001], job satisfaction [Organ and Ryan, 1995], supervisor leadership style [Podsakoff et al., 1990, 1996; Ilies et al., 2007; van Dierendonck, 2011], or perceived organizational support for employees [Rhoades and Eisenberger, 2002], are expected to be reasonably stable over short-to-medium time periods.
- Empirical evidence from longitudinal studies of OCB supports the notion that helping levels might be reasonably stable through time [Ilies et al., 2006; Dalal et al., 2009].
- There is also reason to believe that helping on the focal shift might yield short-term operational improvements.
    - The organizational behavior literature has posited a number of mechanisms by which helping behaviors can boost coworker productivity [Podsakoff and MacKenzie, 1994; Podsakoff et al., 1997].
- The totality of this literature leads us to expect a positive link between past helping and focal shift performance.

## 2.2. Impact of Past Helping Between Shift Members
- Though employees express a range of different motivations for engaging in OCB, one commonly reported motivation in empirical work is **reciprocity** [Taber and Deosthali, 2014].
    - Reciprocity often emerges via interdependent exchanges such as one coworker helping another with an unstated but mutually understood expectation of later receiving help back from them [Gouldner, 1960].
    - Social exchange theory [Gouldner, 1960; Blau, 1964; Cropanzano and Mitchell, 2005]
- Since a pair of coworkers with a social exchange relationship would be expected to have exchanged help with one another in the past and to continue exchanging help going forward, we would expect the number of past helping interactions between a coworker pair to predict their volume of helping behaviors and their performance on a focal shift to which they’re both scheduled.
- We define **DirectedHelp** as the volume of helping activities previously performed between pairs of members allocated to a shift.
    - *H2A*: The volume of helping activities previously performed between pairs of members allocated to a shift (DirectedHelp) is positively associated with shift performance.
    - *H2B*: The recipient of the past help matters — the DirectedHelp measure is predictive of shift performance even when controlling for the TotalHelp measure.
- These hypotheses represent a point of departure from the extant OCB literature, which largely focuses on the link between overall OCB in a unit/organization and overall unit/organization performance, as opposed to the link between the composition of a specific shift and that shift’s performance.

## 2.3. Impact of Team Familiarity
- Group composition has received significant focus in the literature.
- One area of great interest is group composition in **fluid teams**— short-term teams whose membership may change through time.
- Beyond team members’ experience with the task at hand, **team familiarity** has been theorized as a key contributor to team performance [Edmondson et al., 2001; Reagans et al., 2005].
- Team familiarity can improve coordination between individuals [Edmondson et al., 2001; Reagans et al., 2005] and build willingness to work together [Edmondson, 1999; Edmondson et al., 2001].
- Team familiarity has generally been measured as the average number of past teams on which pairs of team members have served together [Reagans et al., 2005].
- Much of this empirical literature has focused on formally defined teams assigned to the care of a patient or set of patients (in healthcare settings) or to a work project or set of projects (in other settings).
- We define **TeamFamiliarity** as the number of past shifts worked together by pairs of members allocated to a shift.
    - *H3A*: The number of past shifts worked together by pairs of members allocated to a shift (TeamFamiliarity) is positively associated with shift performance.
    - *H3B*: The TeamFamiliarity measure is a marginal predictor of shift performance given the TotalHelp and DirectedHelp measures.

# 3. Empirical Setting and Data
- We describe our research setting and the data source.
- We define the cohort and introduce the variables for the empirical analysis.

## 3.1. Research Setting
- Our dataset is collected from six ICUs at a large, urban academic medical center in the United States.
- Nurses in the six ICUs work 12-hour shifts (day-shifts and night-shifts).
- During a shift, nurses are assigned to patients staying there.
    - A nurse assigned to a patient on a shift is referred to as this patient’s **primary nurse**.
    - Primary nurse assignment is non-random.
    - The EMR does not consistently record the primary nurse assigned to a patient for each shift, we compute a nurse-to-patient allocation based on the number of observed care actions observed in the EMR.
    - On average, the primary nurse recovered and assigned to each patient-shift accounted for 86.8% of the observed nursing care actions performed on the patient during the shift.
- There is significant variability in the sets of primary nurses assigned to a unit from one shift to another.
- Despite this formal division of labor, nurses sometimes help each other by taking care actions on patients who are assigned to other nurses, and these helping actions are sometimes documented in the EMR via nursing flow-sheets.
    - We captured data from six key flow-sheets (vital signs, lines/drains/airways, transport, care planning, monitoring device data, ICU/OR transfer checklist).
    - All flow-sheet data attributed to a nurse in the EMR must be entered/approved by the nurse while they are logged into the system.
- An example illustrates nurse helping behaviors (i.e., OCB) in our setting.
    - Such helping behavior is encouraged and happens from time to time in ICUs.
- Descriptive analyses show that there are, on average, 1.5 helping sessions per patient during a shift, with an average helping session involving 15.8 helping actions and lasting for 13.8 minutes.
    - A helping session is defined as a period during which one non-primary nurse continuously performed actions on a patient while no actions were recorded from the primary nurse.
- The occurrence of seemingly urgent actions and transfer actions was notably higher during helping sessions compared to non-helping sessions.
- Our dataset contains only a subset of nursing flow-sheets, so some nursing actions that are logged in the EMR are not included in our study.
- The goal of our research is to understand how past interactions among nurses in ICUs may benefit patients’ health outcomes.

## 3.2. Data and Cohort Definition
- We utilize various sources of data: (1) encounter data, (2) transfer data, (3) treatment team data, (4) all data from our six selected types of ICU nursing flow-sheets.
- We requested all the sources of data mentioned above for all patient encounters in which the patient spent time in an ICU from June 2017 to December 2019.
- In our analyses, we exclude encounters that occurred during the first 6 months to avoid censored measures.
- We also remove the patient encounters involving multiple ICU units to purposefully minimize the heterogeneity among patients in our sample.
- We exclude a small subset of patients (1.62% in our sample) who need continuous renal replacement therapy (CRRT) or extracorporeal membrane oxygenation (ECMO) treatments to again minimize the heterogeneity among patients in our sample.
- Our working sample consists of 11,579 encounters across 10,399 unique patients, and 2,702 unique caregivers.
- Our working sample also consists of 33,563,506 patients-caregivers interactions.
- By grouping all treatments with the same recording timestamp for each patient-caregiver pair, we find that there are on average 13.69 unique treatment recording times by caregivers on a patient during each shift.

## 3.3. Outcome Variables
- We consider the **length of stay (LOS)** as our main patient-level outcome measure [Danielis et al., 2020].
- We focus on the remaining hospital LOS, defined as the total time a patient spent in the hospital from the first ICU admission until the hospital discharge [Chan et al., 2019; Song et al., 2020].
- We separately study the impact of the ICU nurse past interaction on **in-hospital mortality** to complement our findings.
- Among the 11,579 patient encounters in our sample, 9.15% of them end with death during the stay.
- We thus focus on the 10,519 encounters for which the patient remained alive during the hospital stay in our main analysis.

## 3.4. Control Variables
- Our rich data allow us to construct a comprehensive set of explanatory variables to use as controls in our empirical analyses.
- We account for several patient demographic characteristics (age, gender, race, ethnicity, and marital status).
- We also account for details of the patient’s condition and complexity (admission type, diagnosis code, ECMO or renal replacement therapy (CRRT) treatment, SOFA score, Elixhauser and Charlson comorbidity indexes, the number of patient transfers per day, the ICU admission shift, and the patient’s insurance type).
- We control for a key operational characteristic: the unit-shift-level ICU utilization (linear and quadratic terms).
    - We define utilization as the number of beds occupied among all the beds in a unit during a given shift.
- We also control for the experience of a patient’s primary nurse in three ways: nurse skills (advanced training), nurse tenure, and nurse primary work location.
- Besides the nurse characteristics, we also incorporate the physician effect in the regression analysis.

# 4. Nurse Interaction Metrics
- We explain how we construct the key metrics to describe past interactions among nurses in ICUs, including past helping behaviors and team familiarity.
- We focus on three key metrics to describe these interactions:
    - (1) **TotalHelp**: the amount of total helping actions previously performed by members of a focal shift.
    - (2) **DirectedHelp**: the volume of helping activities previously performed between pairs of members allocated to a shift.
    - (3) **TeamFamiliarity**: the number of past shifts worked together by pairs of members allocated to a shift.
- We discuss some potential endogeneity issues and the solution we propose to resolve these issues.
- We use data from the ICU nursing flow-sheets for the past six months before t to calculate the individual-level metrics for this focal nurse n.
    - TotalHelpn,t,u: the number of cases when nurse n helped another nurse with their patient on a past shift.
    - DirectedHelpn,t,u: the number of cases in a past shift when nurse n and some other nurse k cared for the same patient, limiting to interactions with other nurses on the focal shift (i.e., k ∈ Nt,u \ {n}).
    - TeamFamiliarityn,t,u: the number of past shifts worked together by nurse n and some other nurse k on the focal shift.
- We take these individual-level interaction metrics for a focal nurse (n) and aggregate them among all primary nurses on the same shift (Nt,u) to come up with a set of shift-level interaction metrics.
    - TotalHelpShift t,u = 1 |Nt,u| ∑ n∈Nt,u TotalHelpn,t,u,
    - DirectedHelpShift t,u = 1 |Nt,u| ∑ n∈Nt,u DirectedHelpn,t,u
    - TeamFamiliarityShift t,u = 1 |Nt,u| ∑ n∈Nt,u TeamFamiliarityn,t,u,
- Not only does this set of shift-level metrics provide economic meaning — they help describe the interactions among a group of nurses working together — but they also help alleviate one endogeneity concern that the individual-level metrics may suffer.
- The shift-level metrics alleviate these concerns — they are unlikely to be correlated with patients’ health conditions.
- Shift-level interaction measures still capture the effect of interest — patients on shifts with high shift-level interactions receive care, on average, from a nurse with a high individual-level interaction measure.

# 5. Empirical Analysis
- We introduce the model specification we use to test the effect of primary nurse interactions on patient outcomes.
- We then present the empirical results and investigate whether nurse interactions exhibit heterogeneous impacts on outcomes across patients with various conditions.

## 5.1. Model Specification
- Our main regression specification for the patient-level analysis is as follows:
    - log(Yp) = α0 + β1 · Past Interaction Metricp + XT p · γ + μp,
- Yp denotes the LOS since ICU admission for patient p, Past Interaction Metricp represents the (set of) nurse interaction metrics for patient p.
- We estimate the equation using ICU-diagnosis code fixed-effects models to account for potentially unobserved characteristics related to each ICU and each patient diagnosis code.
- We cluster the standard errors at the ICU diagnosis code level to further control for potential correlations in error terms.
- We also include physician indicators and month and day-of-week dummies to adjust for potential temporal trends.

## 5.2. Empirical Results
- Table 5 presents the main findings of the paper.
- In the first column, we show the estimation results from the base regression model, which does not include any nurse interaction metrics.
- In columns (2)–(4), we report the estimates using each of the three interaction metrics: DirectedHelp, TotalHelp, and TeamFamiliarity.
    - The coefficient estimates of all three metrics are statistically significant at the five percent level.
    - We also separately conduct a likelihood-ratio test to confirm that each of the three metrics explains additional variation in the outcome beyond the base regression model.
    - Both sets of statistical results provide evidence to support our hypothesis 1, hypothesis 2A, and hypothesis 3A.
- To test for hypothesis 2B (that DirectedHelp predicts shift performance above and beyond TotalHelp), we include both helping metrics in the regression model and compare the goodness of fit between the model with only the TotalHelp metric and the model with both help metrics using the likelihood-ratio test.
    - From column (5) of Table 5 that when controlling for the TotalHelp metric that the coefficient estimate of DirectedHelp is still significant at the one percent level.
    - The likelihood-ratio test result shown in Table 6 also demonstrates the additional predictive power of DirectedHelp, and these results jointly support hypothesis 2B.
- The last column of Table 5 presents the estimation results from the model that includes all three nurse interaction metrics.
    - Among the three interaction metrics, team familiarity is the only insignificant metric.
    - To formally compare the predictability between the two helping metrics and the team familiarity metric, we again rely on the likelihood-ratio test and obtain consistent results.
    - When controlling for the TeamFamiliarity metric, the inclusion of the two helping metrics significantly improves the model fit (p-value < 0.001).
    - When controlling for the two helping metrics, the inclusion of TeamFamiliarity does not significantly improve the model fit (p-value = 0.393).
    - These findings combine to support hypothesis 3B.
- Based on these observations, we now consider the model specification with both helping metrics (column (5) of Table 5) as our main model, which we will use to derive the remaining findings in this paper.
- When controlling for patient and operational characteristics, a 1-unit increase in the DirectedHelp metric reduces the patient LOS by 1.5%.
    - In our final sample, the LOS since ICU admission for an average patient is 9.26 days, so a 1.5% change translates into 3.3 hours (=9.26*1.5%*24) of LOS reduction per patient.

## 5.3. Heterogeneity across Patient Conditions
- We investigate the heterogeneous impact of the two helping metrics on patient LOS.
- We categorize patients into less severe and more severe groups to examine potential heterogeneous impacts across different patient groups.
- We investigate whether the impact of our helping metrics varies between patients with lower or higher acuity scores and patients with less complex or more complex comorbid conditions.
- We find that the effect of total help is significant for both low-acuity and high-acuity patients.
- The effect of directed help is significant and economically large only on patients with higher acuity scores.
    - Compared to low-acuity patients, high-acuity patients might demand more helping actions due to their health conditions.
- With either the Elixhauser or the Charlson comorbidity indexes, the coefficients of DirectedHelp are significant for both sub-groups.

## 5.4. Mechanisms of Helping
- We run formal analyses to gain a deeper understanding of the major factors that drive helping actions among nurses.
- We propose that past interaction among nurses is a potential factor, while nurse experience could certainly also be associated with helping.
- To determine which of these factors plays a more significant role in driving helping actions, we conducted a regression analysis at the nurse-shift level.
- We also account for nurse workload in the regression, together with patient severity and other demographic controls.
- We separately regress the logged amount of help given and the logged amount of help received using Equation (5) to identify the predictors of directional helping actions.
- The number of past helping interactions is positively associated with both the help given and the help received, suggesting that past interactions among nurses foster more helping on the current shift.
- The nurse’s workload and main location are also significant predictors.
- Nurses offer less help when assigned to two patients instead of one, and receive more help in such a scenario.
- Nurses whose main location is the focal ICU unit tend to offer more help to their peers but receive less help.
- The reciprocal nature of helping actions is supported by the observation that past helping actions from nurse B to nurse A are associated with an increased number of helping actions from nurse A to nurse B in the focal shift.
- Pairs of nurses assigned to adjacent beds help each other more often, while a higher number of past helping actions between nearby nurses further increases the intensity of help given.
- Nurse pairs with more past helping interactions self-select to stay closer, implying a synergy effect between bed choices and helping interactions.
- We discuss why such helping might improve patient outcomes, specifically by reducing LOS.
    - Timely assistance with urgent actions can prevent deterioration and complications, while timely transfers can prevent delays in critical pathways and ensure smoother patient transitions, both likely contributing to a shorter LOS.

## 5.5. Robustness Analysis
- We conduct several sets of robustness tests to ensure the reliability of our findings.
- We conduct additional analyses to alleviate the concern of nurse composition as an uncontrolled confounding factor.
    - We identify nurses who were consistently present throughout the study period and include nurse contribution indicators representing the proportion of shifts nurses worked while a patient was in the ICU.
- We assess whether our results could be explained by ICU congestion levels.
    - We use total LOS and ICU LOS as alternative dependent variables and obtain similar results.
    - We perform sub-sample analyses to focus on patients who have experienced different ICU utilization rates and obtain consistent results.
- We test the robustness of our results against alternative definitions of nurse interaction metrics.
    - We adjust the time windows for identifying past helping behavior, explore an alternative method to recover primary nurse assignments, and apply different weights to aggregate shift-level nurse interactions for the patient-level analysis.
- We examine the potential impact of sample bias issues.
    - We perform sub-sample analyses by appending patients with special healthcare needs, excluding patients with missing acuity scores or multiple ICU encounters, and focusing on patients with mostly primary nurse care actions identified.

# 6. Managerial Implications
- We perform a counterfactual analysis, studying whether relatively small changes to weekly shift assignments in our ICUs could yield meaningful improvements in performance.
- For an ICU with k nurse-to-shift assignments in a given week, we use mathematical optimization to change at most α · k of those assignments to maximize past helping behaviors for the shifts, keeping the same number of shifts for each nurse and the same number of nurses for each shift.
- Maximizing the DirectedHelp metric across the week’s shifts is equivalent to solving the optimization problem.
- Even when making a relatively small adjustment of α = 10% of all nurse assignments can yield significant gains: on average a 41–45% increase in the weekly DirectedHelp depending on how stringently S and γ are selected.
- Given an average historical DirectedHelp value of 8.89 (see Table 4) and the coefficient estimates from column (5) in Table 5, we estimate that the optimal adjustments with α = 10% could yield between a 5.3% and 5.8% improvement in the length of stay since ICU admission.
- Our revised schedule based on prior interactions may improve job performance and increase job satisfaction by catering to nurses’ preference to work with familiar colleagues they have previously helped.
- We assume that the marginal benefit brought by a unit change in DirectedHelp remains on the same level as in the empirical sample.
- Even a fraction of a 5.3% LOS improvement would be managerially relevant, so we believe this counterfactual analysis does show the promise of considering past helping behaviors when performing shift scheduling.
- Optimizing TeamFamiliarity instead of DirectedHelp does little to actually improve the true DirectedHelp of the nurses scheduled for shifts in the ICU.

# 7. Discussion and Conclusions
- We defined two metrics of past helping among employees assigned to a focal shift t: TotalHelp and DirectedHelp.
- Motivated by the organizational behavior literature, we hypothesized that both metrics would be predictive of performance on the shift, and that they would better predict performance than the well-studied team familiarity measure from the literature.
- We confirmed these hypotheses using nurse scheduling data from six ICUs at a large academic medical center.
- We performed a counterfactual analysis to show that relatively minor schedule adjustments based on the DirectedHelp metric might yield managerially relevant improvements in shift performance.
- First, shift scheduling should be added to the managerial tool set related to OCB.
    - Scheduling adjustments based on past helping behavior represent a low-cost adjustment (assuming the helping data is already being captured by IT systems) that leads to minimal disruption in current workflows.
- Another key implication is that there may be value in employers finding low-cost ways to quantify helping behaviors between employees.
- This work indicates that it would be of great value for the scheduling community to extend standard scheduling algorithms to be able to take into account pairwise affinity between coworkers (due to past helping between the pair).

---

# Executive summary of 1. Introduction
- Employees' discretionary actions, termed **organizational citizenship behavior (OCB)**, improve organizational performance [Podsakoff et al., 2009].
- We extend the OCB literature by focusing on the **impact of OCB on shift performance**.
    - Specifically, we hypothesize that **pairs of employees who have helped each other in the past will continue to do so**, improving the performance of shifts where they are scheduled together.
- We focus on nurses in hospital intensive care units (ICUs), where employee interactions are captured by IT processes.
    - The **electronic medical record (EMR)** documents instances of nurses helping each other, enabling us to study the impacts of scheduling pairs of nurses with a history of helping one another to the same shift.

# Executive summary of 2. Literature Review and Theory Development
- We consider whether shifts with historically more helpful employees exhibit better performance.
    - We define **TotalHelp** as the total past helping behavior of shift members.
    - *H1*: The amount of total helping actions previously performed by members of a focal shift (TotalHelp) is positively associated with focal shift performance.
- We define **DirectedHelp** as the volume of helping activities previously performed between pairs of members allocated to a shift.
    - *H2A*: The volume of helping activities previously performed between pairs of members allocated to a shift (DirectedHelp) is positively associated with shift performance.
    - *H2B*: The recipient of the past help matters — the DirectedHelp measure is predictive of shift performance even when controlling for the TotalHelp measure.
- We define **TeamFamiliarity** as the number of past shifts worked together by pairs of members allocated to a shift.
    - *H3A*: The number of past shifts worked together by pairs of members allocated to a shift (TeamFamiliarity) is positively associated with shift performance.
    - *H3B*: The TeamFamiliarity measure is a marginal predictor of shift performance given the TotalHelp and DirectedHelp measures.

# Executive summary of 3. Empirical Setting and Data
- Our dataset is collected from six ICUs at a large, urban academic medical center in the United States.
- We utilize various sources of data: (1) encounter data, (2) transfer data, (3) treatment team data, (4) all data from our six selected types of ICU nursing flow-sheets.
- We define the **length of stay (LOS)** as our main patient-level outcome measure.
- We account for several patient demographic characteristics (age, gender, race, ethnicity, and marital status).
- We also account for details of the patient’s condition and complexity (admission type, diagnosis code, ECMO or renal replacement therapy (CRRT) treatment, SOFA score, Elixhauser and Charlson comorbidity indexes, the number of patient transfers per day, the ICU admission shift, and the patient’s insurance type).
- We control for a key operational characteristic: the unit-shift-level ICU utilization (linear and quadratic terms).
    - We define utilization as the number of beds occupied among all the beds in a unit during a given shift.
- We also control for the experience of a patient’s primary nurse in three ways: nurse skills (advanced training), nurse tenure, and nurse primary work location.

# Executive summary of 4. Nurse Interaction Metrics
- We explain how we construct the key metrics to describe past interactions among nurses in ICUs, including past helping behaviors and team familiarity.
- We focus on three key metrics to describe these interactions:
    - (1) **TotalHelp**: the amount of total helping actions previously performed by members of a focal shift.
    - (2) **DirectedHelp**: the volume of helping activities previously performed between pairs of members allocated to a shift.
    - (3) **TeamFamiliarity**: the number of past shifts worked together by pairs of members allocated to a shift.

# Executive summary of 5. Empirical Analysis
- We introduce the model specification we use to test the effect of primary nurse interactions on patient outcomes.
- We then present the empirical results and investigate whether nurse interactions exhibit heterogeneous impacts on outcomes across patients with various conditions.
- The coefficient estimates of all three metrics are statistically significant at the five percent level.
- The likelihood-ratio test result shown in Table 6 also demonstrates the additional predictive power of DirectedHelp, and these results jointly support hypothesis 2B.
- Among the three interaction metrics, team familiarity is the only insignificant metric.
- When controlling for patient and operational characteristics, a 1-unit increase in the DirectedHelp metric reduces the patient LOS by 1.5%.
    - In our final sample, the LOS since ICU admission for an average patient is 9.26 days, so a 1.5% change translates into 3.3 hours (=9.26*1.5%*24) of LOS reduction per patient.
- The effect of directed help is significant and economically large only on patients with higher acuity scores.
    - Compared to low-acuity patients, high-acuity patients might demand more helping actions due to their health conditions.
- The number of past helping interactions is positively associated with both the help given and the help received, suggesting that past interactions among nurses foster more helping on the current shift.
- The reciprocal nature of helping actions is supported by the observation that past helping actions from nurse B to nurse A are associated with an increased number of helping actions from nurse A to nurse B in the focal shift.

# Executive summary of 6. Managerial Implications
- We perform a counterfactual analysis, studying whether relatively small changes to weekly shift assignments in our ICUs could yield meaningful improvements in performance.
- Even when making a relatively small adjustment of α = 10% of all nurse assignments can yield significant gains: on average a 41–45% increase in the weekly DirectedHelp depending on how stringently S and γ are selected.
- Given an average historical DirectedHelp value of 8.89 (see Table 4) and the coefficient estimates from column (5) in Table 5, we estimate that the optimal adjustments with α = 10% could yield between a 5.3% and 5.8% improvement in the length of stay since ICU admission.

# Executive summary of 7. Discussion and Conclusions
- We defined two metrics of past helping among employees assigned to a focal shift t: TotalHelp and DirectedHelp.
- We confirmed these hypotheses using nurse scheduling data from six ICUs at a large academic medical center.
- First, shift scheduling should be added to the managerial tool set related to OCB.
    - Scheduling adjustments based on past helping behavior represent a low-cost adjustment (assuming the helping data is already being captured by IT systems) that leads to minimal disruption in current workflows.
- Another key implication is that there may be value in employers finding low-cost ways to quantify helping behaviors between employees.
- This work indicates that it would be of great value for the scheduling community to extend standard scheduling algorithms to be able to take into account pairwise affinity between coworkers (due to past helping between the pair).

</details>

```
title: The More Monitoring, the Better Quality? Empirical Evidence of Heterogeneous Compliance from the Generic Drug Industry
authors: Anqi (Angie) Wu, Yixin (Iris) Wang
journal: SSRN
published: Working Paper
```
# Executive Summary
- **Problem**: The FDA uses a risk-based model to prioritize inspections of high-risk drug manufacturing facilities.
    - The study investigates whether more frequent inspections lead to better drug quality across facilities with different risk levels.
- **Method**: We use instrumental variable methods to analyze the effect of inspection frequency on drug quality failures (adverse events, recalls) from 2005–2019.
- **Key Theoretical/Conceptual Framework**: The study uses the principal-agent framework, where the FDA (principal) aims to ensure compliance for public interest, while drug manufacturers (agents) are interested in profit.
    - Inspections are a mechanism to prevent opportunistic behavior by agents.
- **Key Findings**:
    - Frequent inspections generally reduce quality failures, but with **diminishing returns**.
    - The effect of inspection frequency is significantly **more effective at low-risk facilities** than high-risk facilities.
    - Frequent inspections reduce quality failures for **non-injectable and older drugs** produced at high-risk facilities.
- **Managerial Implications**:
    - Surveillance inspections should not be scheduled too frequently to avoid **diminishing returns**.
    - Incorporate facility incentive considerations into the risk-based site selection model.
    - Reallocate resources from overly inspected facilities to high-risk facilities producing **non-injectable and older products**.
    - Consider complementary approaches to incentivize and reinforce quality compliance at high-risk facilities (e.g., reward-based policies).

#quality_management #public_policy #empirical_analysis #generic_drugs #fda #inspections #risk_based_model #heterogeneous_compliance #instrumental_variable

<details>
    
  <summary>Click to expand sections</summary>

# 1. Introduction
- Regulatory inspections are crucial for ensuring compliance and maintaining safety standards across industries [Shimshack, 2014; Gray and Shimshack, 2011; Mejia et al., 2019].
    - The principal-agent framework models the interaction between regulators (principal) and inspected establishments (agent).
    -  Regulators aim to ensure compliance, while establishments pursue profit.
    - Regulatory inspections prevent agents from harming the principal [Tosi et al., 1997].
- Limited resources pose a challenge for regulators to increase inspection frequency [Robinson et al., 2007].
    - Many agencies adopt target models prioritizing high-risk establishments, such as OSHA and the FDA.
    - The FDA uses a site selection model with risk-based algorithms to prioritize high-risk drug manufacturing facilities [FDA, 2018].
- The goal is to reduce safety and health issues, not just increase inspection frequency.
    - Identifying establishment conditions where increased frequency is more effective is important.
    - Johnson et al. [2023] showed alternative models targeting high expected injuries could avert many incident cases.
    - *The paper seeks to identify establishment conditions under which increased inspection frequency is more effective in alleviating compliance issues.*
- Recent literature has explored heterogeneity in inspection effectiveness [Mejia et al., 2019; Ball et al., 2017; Ibanez and Toffel, 2020].
    - Facility behavior (how establishments respond) and regulator behavior (execution of procedures) can impact compliance.
    - Understanding incentives behind heterogeneous compliance is essential for targeted strategies.
- The study examines the pharmaceutical manufacturing industry, where inspection history and product information are accessible.
    - The FDA ensures facilities adhere to quality standards for safe, effective, low-cost generic drugs.
    - The complex supply chain, numerous foreign suppliers, and public concerns pose challenges.
    - The FDA computes a facility risk score based on product risk and historical performance [FDA, 2018].

## 1.1. Research Questions
- The paper investigates how inspection frequency affects product quality failures differently across facilities and drugs.
- *RQ1*: Do generics facilities adjust their quality compliance in response to the monitoring intensity?
    - In other words, is there an association between inspection frequency and drug quality failures?
- *RQ2*: Does this association vary with the facility risk level?
- *RQ3*: What facility incentive mechanisms moderate the effectiveness of inspections?
    - A conventional risk classifier based on previous inspection outcomes is used.
    - Product-level characteristics serve as indicators to investigate how facility incentives moderate the impact of inspection frequency.

- A product-year-level dataset for 2005–2019 is collated, focusing on generic products manufactured by domestic facilities.
    - The FDA did not systematically document drug quality issues for generics from foreign facilities [GAO, 2008].
    - Domestic inspections are unannounced, while foreign inspections are sometimes pre-announced [GAO, 2021].
    - Focusing on domestic facilities helps mitigate potential bias issues.
- An empirical model directly regressing drug quality failures on inspection frequency is subject to **endogeneity concerns**.
    - The instrumental variable approach is employed to alleviate endogeneity.
    - Two instruments for inspection frequency: inspector availability at each FDA district office and the distance between a facility and its designated district office.
- Findings: Frequent inspections reduce quality failures (adverse events, recalls), but this impact is subject to diminishing returns.
    - Low-risk facilities significantly reduce quality failures, while high-risk facilities less so.
    - The heterogeneous effect of inspections is not attributed to diminishing returns or regulatory actions regarding supply continuity concerns.
    - Frequent inspection can reduce quality failures for non-injectable drugs and older drugs produced at high-risk facilities.
- This paper reveals a nuanced relationship between inspection frequency and drug quality performance.
    - Inspection effectiveness depends on facility risk levels and product characteristics.
    - High-risk facilities do not always improve in response to inspections; low-risk facilities are typically sensitive to inspection frequency changes.

- Findings offer practical implications for regulatory agencies:
    - Schedule routine surveillance inspections regularly, but not too frequently, to avoid quality decays.
    - Enhance the risk-based site selection model to further incorporate facility incentives.
    - The FDA could benefit from reallocating inspection resources from facilities inspected within a year to high-risk facilities producing non-injectable and older products.
    - Communicate the facility incentive issues to inspectors and empower them to exercise discretion in identifying facilities that demonstrate significant quality improvement in response to frequent inspections.
- Explore complementary approaches to incentivize compliance behavior for high-risk facilities.
    - Consider implementing reward-based policies.
    - Actions align with concerns in the FDA drug shortage report [FDA, 2019].

# 2. Literature and Hypotheses
- The FDA uses a risk-based approach based on two streams of literature:
    - Models that predict quality risk based on previous inspection outcomes, adverse events, and feedback [Ball et al., 2020].
    - Designing risk-based allocation algorithms that optimize the use of limited resources [Deidda et al., 2018].
- Challenges involved in risk-based regulations: goal ambiguities, methodological difficulties in assessing risks, conflicts with political and social pressures [Black and Baldwin, 2012; Lloyd-Bostock and Hutter, 2008; Beaussier et al., 2016].
    - The study examines the varying effectiveness of inspections along the predicted facility risk dimension.
- This paper is relevant to the literature on optimal monitoring scheduling [Kim, 2015; Chen et al., 2020].
    - These research streams assume a more intensive schedule can reduce quality failures.
    - *We empirically investigate the validity of the assumption and, in particular, focus on the heterogeneous impact of inspections on facility compliance.*
- Research has explored inspection effectiveness in environmental performance, food hygiene, and healthcare compliance [Dhanorkar et al., 2017; Mani and Muthulingam, 2018; Mejia et al., 2019; Staats et al., 2017; Ball et al., 2018].
    - The literature identifies factors that can lead to ineffective implementation, including lack of clarity in safety standards [Ho, 2017], conflicts of financial interest [Toffel et al., 2015], language barriers [Gray and Massimino, 2014], inspectors’ site-specific experience [Ball et al., 2017], and inspector schedules [Ibanez and Toffel, 2020].
    - *We offer empirical evidence that supports the varying inspection effectiveness with the facility risk level. More importantly, we identify potential incentive mechanisms that lead to distinct compliance outcomes between high-risk and low-risk facilities, which provides regulators with actionable insights to consider and incorporate into the design of inspection procedures.*

## 2.1. Hypothesis Development
- Maintaining high quality can be costly, especially for low-cost generics [Videau, 2000; Edney, 2019].
    - Inspections monitor quality and ensure compliance.
    - Inspections can act as renewals to halt decays in adherence [Anand et al., 2012].
    - The effectiveness of inspections is contingent on their execution procedures [GAO, 2010; Ball et al., 2017; Ibanez and Toffel, 2020] and the compliance behavior of the regulated establishments [Helland, 1998; Mejia et al., 2019].
    - We focus on the moderating role of facility risk levels.
- Facility risk level is defined based on quality concerns and inspection history.
    - This risk level is determined by the facility’s process design and quality practices, reflecting its production capability and quality management strategy.
    - Product characteristics can also influence incentives for quality maintenance and investment.
- High-risk facilities may achieve higher returns from investments in quality improvements due to potential negative regulatory outcomes and reputation damage [Sriram et al., 2015; Board and Meyer-ter Vehn, 2013; Johnson, 2020].
    - In generic markets, compliance performance is essential for remaining competitive [Alfonso-Cristancho et al., 2015; Anderson et al., 1999].
    - Therefore, high-risk facilities could take inspections more seriously to minimize reputation damage and better align their production processes with quality regulations.
- In contrast, low-risk facilities do not suffer from the same pressure to modify practices.
    - Low-risk facilities consider other business objectives, making quality investments less of a priority.
    - Low-risk facilities have established habits for high-quality standards [Charness and Gneezy, 2009].
    - *H1A*: The negative effect of inspection frequency on product failure occurrences will be greater among facilities with higher risk levels than facilities with lower risk levels.

- High-quality firms tend to outperform their peers [Hendricks and Singhal, 1997].
    - The performance gap between facilities of different risk levels could further discourage high-risk facilities from investing in quality improvements.
    - Post-inspection quality remediation is not always appealing for high-risk facilities; they sometimes choose to cease production [FDA, 2019].
    - Manufacturing complexity could make quality improvement more costly.
- Stability decays over time [Anand et al., 2012], and low-risk facilities can leverage inspections [Ball et al., 2017].
    - Low-risk facilities could more efficiently allocate investments toward quality enhancement.
    - *H1B*: The negative effect of inspection frequency on product failure occurrences will be greater among facilities with lower risk levels than facilities with higher risk levels.

# 3. Data
- A product-year-level dataset is collated from multiple data sources to study how FDA surveillance inspections affect drug quality performance.
- The medications sold in the United States include brand-name drugs under patent protection and low-cost generic drugs that are bio-equivalent to off-patent brand-name ones.
- This study focuses on generics and examines how FDA surveillance inspections affect drug quality performance.
- *Data Sources*: the FDA Inspection Database, FDA Adverse Event Reporting System (FAERS), FDA Recall Database, DailyMed structured product labeling (SPL) database, FDA National Drug Code (NDC) Directory, Annual Editions of the Orange Book, University of Utah Information Service (UUDIS), and Medicaid State Drug Utilization database.
- Facility-level inspection records are obtained through a Freedom of Information Act (FOIA) request.
    - The outcome classification reflects a facility’s compliance status (NAI, VAI, or OAI).
    - Citation records summarize violated Federal regulations.
    - The inspection date is used to construct the past inspection frequency.
    - Inspector names are collected to create variables for inspector rotations [Ball et al., 2017].
- Product-level recall history, also obtained via the FOIA request, is used to construct the product recall rate.
- The number of adverse events is retrieved through the openFDA API, which collects data from the FAERS.
- Inspection records are connected with drug-level quality metrics using the DailyMed SPL database.
- Product-level controls are created using the remaining databases.
    - The NDC Directory contains product-level characteristics.
    - The Medicaid State Drug Utilization database is used to obtain annual NDC sales volume and total reimbursement amounts.
    - The UUDIS is used to retrieve the list of historical drug shortage events.
    - The Annual Editions of the Orange Book document provides yearly updates on FDA-approved drug products and discontinued products.

## 3.1. Sample Construction
- The inspection database comprises records of all inspections conducted from 2000 to 2019.
    - The study period is 2005 to 2019 to allow accurate calculation of past inspection frequency.
    - Generics manufactured at domestic facilities are the focus.
    - The FDA did not systematically document data for drugs produced at foreign facilities [GAO, 2008].
    - Domestic inspections are always unannounced, while foreign inspections are sometimes announced [GAO, 2021].
- The complete list of generic drug products from the NDC directory is used to identify generics manufactured at domestic facilities.
    - Generics with sales records documented in the Medicaid’s drug sales database are focused on.
    - The link between facilities and drug products is established using the structured product labeling information extracted from the DailyMed SPL database.
    - 69% of generics are mapped to their manufacturing facilities.
    - The final data sample characterizes 1,681 generics that have been consistently sourced domestically.

## 3.2. Variable Definition
- Inspection Frequency is constructed using data from the FDA inspection database.
    - The main analysis focuses on surveillance inspections.
    - The number of inspections in the preceding five years is counted.
    - A five-year window is chosen because most facilities would have had at least one inspection within the timeframe.
- Quality Failures: Adverse Events and Product Recalls are used as dependent variables.
    - Adverse events measure the quality deficiencies associated with a generic drug product [Ahuja et al., 2021].
    - Serious adverse events are focused on to mitigate potential biases in data reporting.
    - Recalls are used as a proxy for quality failures [Chen et al., 2009; Ball et al., 2017].
    - Class I and II recalls are focused on.
- Other Independent Variables: Inspection-related controls are constructed based on the past five-year history.
    - The percentage of OAI received and the average number of citations per inspection are considered.
    - Inspector rotations account for the impact of inspector experience [Ball et al., 2017].
- Product characteristics potentially associated with drug quality performance are considered.
    - The age of a generic drug is determined using the market start date listed in the NDC directory.
    - The number of active products indicates the production scale of each product’s manufacturing facility [Shah et al., 2016].
    - Three binary variables denote manufacturing complexity (injectable), disease classification (chronic), and supply continuity risk (shortage).
    - Annual product sales volume from the Medicaid State Drug Utilization database is obtained.
    - The average drug sales price is computed as the ratio of the yearly reimbursement amount to the sales volume.
    - The number of applicants approved to produce generics with the same active ingredient is analyzed to assess the competition intensity.
- The summary statistics of aforementioned variables are reported in Table 1 and include the bivariate correlation matrix in Appendix Table C1.

# 4. Analysis and Results
- This section introduces the model specification for exploring the relationship between past inspection frequency and product quality failures.
- The empirical findings are presented and a post-hoc analysis is conducted to evaluate how facility risk levels and product characteristics moderate the results.
- The section concludes by assessing the robustness of the results against alternative model specifications.

## 4.1. Empirical Strategy
### 4.1.1. Model Specification
- The product-year-level panel dataset is utilized to investigate the impact of inspection frequency on subsequent drug quality failures.
- The Poisson regression model is adopted given the discrete nature of the outcome variables.
- *log(Yit) = β0 + β1 · log(F requencyit) + X′
it · γ + λt + θi + μit.*
    -  Yit: the quality failure rate of product i in year t
    -  F requencyit: the five-year inspection frequency at the product’s manufacturing facility prior to the focal year t
    -  Xit: the various facility and product controls
    -  λt: yearly time dummies to adjust for unobserved temporal trends
    -  θi: product fixed effects to account for unobserved product characteristics
    -  μit: the error terms
    -  β1: the coefficient of interest that captures the impact of inspections on adverse events and recalls
- Standard errors are clustered at the facility level.

### 4.1.2. Instrumental Variables
- The link between inspection frequency and drug quality failures is subject to a **reverse causality problem**.
- The instrumental variable approach is adopted to reduce estimation biases.
- The availability of inspectors at each FDA district office is used as an instrument.
    - Annually, for each district office, its inspector availability is calculated by determining the ratio of the number of unique inspectors to the number of inspected facilities over the previous five years.
    - Before 2017, the FDA’s Office of Regulatory Affairs (ORA) had been assigning inspectors to facilities based on geography.
    - Since 2017, inspectors are assigned to facilities based on product characteristics.
    - Inspector availability instrument is likely exogenous to the facility risk level.
- The distance between a drug manufacturing facility and its designated district office is used as an instrument.
    - It is unlikely for a pharmaceutical manufacturer to strategically locate a facility site based on its distance to the FDA’s district office.
    - The distance is not related to facility risk levels.
- Both instruments partially explain the variation in inspection frequency.
    - Facilities situated in districts with more inspector resources tended to experience more frequent inspections.
    - Facilities located closer to their designated district offices also underwent more frequent inspections.

## 4.2. Estimation Results
- The estimation results are reported in Table 2.
- Column (1) presents the first-stage estimation results, where inspection frequency is regressed on the two instruments together with other controls.
    - Both instruments are significant predictors of inspection frequency.
    - The Cragg-Donald F-statistic (F = 25.444 with p < 0.01) suggests that the instruments meet the relevance condition.
    - The insignificant statistics (χ2 = 2.648 with p > 0.1) from the Sargan over-identification tests suggest that, conditional on the exogeneity of one instrument, the other instrument is uncorrelated with the error term and satisfies the exclusion condition.
- Random effects estimators are utilized for models with two instruments.
- Columns (2)–(3) and (4)–(5) of Table 2 present the results with adverse events and recalls as the dependent variables, respectively.
    - The Poisson estimates without instruments, followed by the instrumental variable (IV) estimates are presented.
    - The IV models are estimated using the control function approach.
    - The residual obtained from the first stage regression is included as an additional control variable [Lin and Wooldridge, 2019].
    - The significant coefficients of the residuals support the endogeneity of inspection frequency.
    - The coefficient estimates for inspection frequency are negative and significant after accounting for endogeneity, suggesting that inspection frequency has a negative impact on product quality failures.
    - For example, an increase in inspection frequency from once every two years to once every year would reduce the expected number of adverse events by 14.5% = −0.505 × [log(1 + 1) − log(0.5 + 1)].

- Whether inspection effectiveness is contingent on the facility risk level is examined.
    - Sub-sample analyses are performed to compare the effect of inspection frequency across facilities with different risk levels.
    - A conventional hazard signal measurement, the facility inspection outcome, is considered to classify product samples into low-risk and high-risk groups.
    - The classification is based on the median cutoff of OAI counts.
- The regression model specified in Equation (1) is estimated for each sub-sample and the estimation results are presented in Table 3.
    - The negative effect of inspection frequency on quality failures remains evident when examining drug products at low-risk facilities.
    - The influence of inspections becomes insignificant for high-risk facilities.
    - The coefficient equivalence is statistically tested by performing a marginal effect analysis and confirm that the coefficients of inspection frequency for low-risk and high-risk subgroups are significantly different (χ2 = 4.13 with p < 0.05 for adverse events and χ2 = 3.27 with p < 0.1 for recalls).
- A revised model specification, depicted in Equation (2), is introduced to provide further evidence of heterogeneous compliance between low-risk and high-risk groups.
    - This model includes the interaction term between inspection frequency and the facility risk classifier based on OAIs.
    - The control function approach is employed to address endogeneity and additional include the residual from estimating the interaction term.
    - *log(Yit) = β0 + β1 · log(F requencyit) + X′
it · γ + λt + θi + μit +β2 · log(F requencyit) × OAI.*
- The positive and significant estimates of the interaction term in columns (3) and (6) of Table 3 indicate that the effect of inspection frequency on reducing quality failures is less pronounced for high-risk facilities, thus supporting Hypothesis H1B.
    - For example, the collective impact of inspection frequency on adverse events for drug products manufactured at high-risk facilities is −0.646 = −0.779 + 0.133 × 1.
    - This represents a 17% reduction compared to the impact for products at low-risk facilities.
- The FDA implemented the site selection model to leverage inspection frequency to decrease quality failures at targeted high-risk facilities.
    - *Contrary to what the agency might think, we do not find strong evidence that supports inspection frequency as a typical effective lever for incentivizing better quality practices at high-risk facilities. More importantly, we observe that low-risk facilities are sensitive to inspection frequency changes and tend to experience more adverse events and recalls when they undergo less frequent inspections.*
    - The findings indicate the value of incorporating the facility reaction as an additional factor into the resource allocation decision to enhance inspection effectiveness.

## 4.3. Post-Hoc Analysis: Mechanism Examination
### 4.3.1. Diminishing Returns of Inspection Frequency
- A segmented analysis is conducted to investigate the non-linear effect of inspections on quality failures.
- Drug product samples are divided into two groups based on whether the manufacturing facility was inspected more than once every year during the study period.
    - The findings remain consistent when using the median inspection frequency as an alternative cutoff.
- The product-level Poisson regression analysis with instrumental variables is repeated for each segment.
- Columns (1) and (2) of Table 4 show that the impact of inspections on quality failures is subject to diminishing returns.
    - The impact is more pronounced when inspection frequency falls below once every year.
    - When inspections occur more frequently than once every year, facilities show less sensitivity to further increases in inspection frequency.
- Columns (3)–(4) of Table 4 show that the diminishing effect of inspection frequency observed in the full sample persists in low-risk facilities.
- The last two columns indicate insignificant effects of inspection frequency in both segments for high-risk facilities.
- *This post-hoc analysis validates that the diminishing effect of inspections on quality failures is unlikely the cause of the heterogeneous compliance between low-risk and high-risk facilities.*

### 4.3.2. Facility Incentives
- Incentive mechanisms that may contribute to heterogeneous compliance outcomes are explored.
- How product characteristics influence facilities’ motivation for quality maintenance and improvement is considered, particularly among high-risk ones.
- The moderating effects of various drug characteristics, such as manufacturing complexity, revenue potential, competition intensity, product maturity, and supply chain continuity are examined.
- All interaction terms between inspection frequency and the list of moderators are incorporated into the base instrumented model as specified in Equation (1).
- The two-stage IV estimation is performed using the control function approach and residuals from estimating the inspection frequency and each of the interaction terms are included.
- High-risk facilities are further stratified based on the moderators and investigate conditions under which inspection frequency can significantly affect quality failures at high-risk facilities.
- The manufacturing quality of drug products comprises two elements: drug manufacturing complexity and facility quality practices.
- While frequent inspections can ensure better process compliance and bring down quality risks related to quality management exercise, the manufacturing complexity embedded in the production process is determined by drug characteristics and largely unaffected by inspections [Mayer et al., 2004].
- The drug delivery route is examined to characterize the manufacturing complexity of drug products [Wang et al., 2023].
- The instrumented model as specified in Equation (1) is rerun with the additional interaction term between inspection frequency and the non-injectable indicator.
- *As indicated by the estimated coefficients for the interaction term in Table 5, we observe that the drug delivery route moderates the effect of inspection frequency for high-risk facilities. Specifically, more frequent inspections significantly reduce quality failures of non-injectable drugs manufactured at high-risk facilities.*
-  The main analysis in Table 3 is replicated, this time using only the inspector availability instrument to incorporate product-level fixed effects into the model.

- Revenue and margin levels are also important product attributes affecting facilities incentives for quality investments.
- Drug sales volume is utilized as a proxy for market size.
- Whether the drug treats chronic diseases is considered as an indicator of its future revenue potential.
    - In Table 5, it is observed that manufacturers of best-selling and chronic drugs are less responsive to changes in inspection frequency, though such significant moderating effects are observed only in adverse events.
- The competition intensity might promote higher and more consistent quality levels as the cost of quality deviation increases with competition.
    - *We follow Morton [1999] to define a market based on the active ingredient and infer the competition intensity by the number of applicants producing drugs with the same ingredient documented in the FDA’s Orange Book.*
    - *Conversely, we do not find the competition level significantly moderates the effect of inspections on high-risk facilities.*

- The larger opportunity window of newly introduced drug products provides these facilities with greater incentives to invest in quality performance [Cohen et al., 2000; Levin, 2000].
    - Facilities that produce older products can leverage this learning to comply with regulatory requirements more cost-effectively.
    - *As such, they can be more responsive to monitoring that helps identify and address quality issues that may arise [Ball et al., 2017].*
    - *To examine the moderating role of product age, we rerun Equation (1) by including the interaction term between inspection frequency and product age. The significant negative coefficients of the interaction term, as shown in columns (3) and (6) of Table 5, indicate that high-risk facilities with older products are more sensitive to inspections.*

- The FDA may utilize different quality monitoring strategies for facilities producing drugs susceptible to supply shortages.
    - The interaction term between inspection frequency and the non-shortage indicator is constructed.
    - *Despite the conjecture, we consistently observe insignificant coefficients for the interaction term in Table 5 for both high-risk and low-risk sub-groups.*
- While it is important to ensure drug quality to minimize patient safety concerns, it is equally important to ensure drug accessibility so that patients can get the drug products in need.
- This set of analyses suggests that product characteristics can affect facility incentives in quality investment and should be considered when designing the inspection scheme.
- It is observed that although not all high-risk facilities respond to inspections, the inspection frequency can significantly impact the quality level of non-injectable drugs and older drugs manufactured at high-risk facilities.
-  Partial evidence is observed for the significant inspection effect on non-chronic medications.
-  A causal tree analysis is conducted leveraging a shift in inspection resources to provide further evidence of heterogeneous compliance.

## 4.4. Robustness Checks
- A series of robustness analyses are conducted to ensure that the findings are not driven by the choice of sample and model specifications, as well as the inspection frequency definition.

### 4.4.1. Alternative Sample and Model Specifications
- The lagged effect of inspections is accounted for by examining the annual count of adverse events and recalls in relation to the number of inspections conducted over the preceding five years.
- A robustness analysis with one-year-lagged independent variables to allow for a longer response time is conducted.
- Facilities with no quality failure records are excluded to repeat the analysis.
- Inspector availability or distance to the district office are included as the instrument, rather than both.
- *In our main analysis, we utilize two instruments to address the potential endogeneity in inspection frequency. To validate the reliability of each instrument, we conduct additional analysis by including only one instrument at a time. The estimation results are presented in Columns (3)–(4). We find that the results remain consistent and robust.*
- A Hausman specification test is conducted to ensure that the findings are not influenced by the use of random effects models.
- Specifically, inspector availability is utilized as the sole instrument and compare the estimates between random effects and fixed effects models.
- *The Hausman test result (χ2 = 22.56, p-value> 0.05) suggests that the estimation from the random effects model is consistent with that from the fixed effects model.*

### 4.4.2. Alternative Measurement of Inspection Frequency
- The main analysis focuses on surveillance inspections.
- For-cause inspections are alternatively included in the construction of the inspection frequency variable.
- A three-year period is adopted to ensure that the results are robust to variations in the time frame used to construct the inspection frequency variable.

# 5. Discussion and Conclusion
- The FDA’s resource allocation decision is critical for ensuring drug quality and enhancing public safety.
- The government agency believes that more frequent inspections at high-risk facilities can reduce safety concerns; however, whether facilities react to frequent inspections remains unclear due to thin profit margins and high quality maintenance costs [Edney, 2019].
- This study compiles a product-year-level database to analyze the impact of inspection frequency on subsequent adverse events and recalls.
- The Poisson regression models with instrumental variables alleviate the endogeneity problem.
- This study finds that frequent inspections significantly reduce the quality failure rate, though this effect is subject to diminishing returns.
- More importantly, the impact of inspections is heterogeneous across facilities with different risk levels.
- The results consistently show that inspection frequency is significantly more effective in mitigating quality failures at low-risk facilities than at high-risk facilities.
- The heterogeneous compliance across facility risk levels is robust to alternative explanations, such as diminishing returns and supply continuity issues.
- Manufacturing complexity and product age moderate facility compliance outcomes.
- It is observed that although not all high-risk facilities respond to inspections, non-injectable and older drugs manufactured at high-risk facilities show reduced quality failures with increased inspection frequency.
- The findings provide evidence that more frequent inspections do not elevate drug quality levels if such inspections are scheduled too often.
- Significant quality improvements are observed only when the facilities experience fewer than annual inspections.
- The FDA could consider putting a cap on inspection frequency to achieve the most improvement from its inspection resource allocation when implementing the site selection model (FDA 2018).
-  For low-risk facilities, inspection frequency seems an effective lever to reduce quality failures.
-   For high-risk facilities, especially those manufacturing injectable and newly launched drugs, frequent inspections are likely insufficient.
-   Reward-based policies might prove more effective than solely relying on punishment-based approaches such as increased inspections.

## 5.1. Limitations
- This study acknowledges the limitations in the analyses.
- Due to the lack of valid drug quality measures and the existence of pre-announced inspections, foreign facilities cannot be incorporated.
- Although various empirical techniques alleviate endogeneity concerns, it is not completely resolved.
- A field experiment conducted in cooperation with the FDA could strengthen these findings.

---

# Executive summary of 1. Introduction
- Regulatory inspections are crucial for ensuring compliance and safety standards, and the principal-agent framework helps model the interaction between regulators and inspected establishments.
- However, regulators face challenges due to limited resources, leading them to prioritize high-risk establishments.
- The paper aims to identify establishment conditions where increased inspection frequency is more effective in alleviating compliance issues, contributing to the understanding of risk-based approaches.
- Recent literature has highlighted heterogeneity in inspection effectiveness, underscoring the importance of understanding incentives behind compliance for targeted strategies.
- Focusing on the pharmaceutical manufacturing industry, the study investigates how inspection frequency affects product quality failures differently across facilities and drugs.

# Executive summary of 2. Literature and Hypotheses
- The FDA's risk-based approach is based on models predicting quality risk and designing risk-based allocation algorithms, though challenges exist in implementing such regulations.
- Research on optimal monitoring scheduling assumes more intensive schedules reduce quality failures, which the paper empirically investigates by focusing on the heterogeneous impact of inspections.
- Studies on inspection effectiveness in various settings identify factors leading to ineffective implementation, prompting the need to identify incentive mechanisms leading to distinct compliance outcomes.
- Hypotheses are developed based on the moderating role of facility risk levels, with H1A suggesting a greater negative effect of inspection frequency on product failure occurrences among higher-risk facilities and H1B suggesting the opposite.

# Executive summary of 3. Data
- A product-year-level dataset is collated from multiple data sources to study how FDA surveillance inspections affect drug quality performance.
- Focus is on generics and examines how FDA surveillance inspections affect drug quality performance.
- Key data sources include the FDA Inspection Database, FDA Adverse Event Reporting System (FAERS), FDA Recall Database, DailyMed structured product labeling (SPL) database, FDA National Drug Code (NDC) Directory, Annual Editions of the Orange Book, University of Utah Information Service (UUDIS), and Medicaid State Drug Utilization database.
- Variable definition covers inspection frequency, quality failures (adverse events and product recalls), and other independent variables like inspection outcomes, inspector rotations, and product characteristics.

# Executive summary of 4. Analysis and Results
- Model specification for exploring the relationship between past inspection frequency and product quality failures.
- The instrumental variable approach is adopted to reduce estimation biases with the availability of inspectors at each FDA district office and the distance between a facility and its designated district office.
- Findings show that frequent inspections generally reduce quality failures, but with diminishing returns, and the effect of inspection frequency is significantly more effective at low-risk facilities than high-risk facilities.
- A post-hoc analysis on incentive mechanisms, finding that frequent inspections reduce quality failures for non-injectable and older drugs produced at high-risk facilities.
- Robustness checks are performed to ensure the findings are not driven by the choice of sample and model specifications.

# Executive summary of 5. Discussion and Conclusion
- Given its limited inspection resources, the FDA’s resource allocation decision is critical for ensuring drug quality and enhancing public safety.
- Frequent inspections significantly reduce the quality failure rate, though this effect is subject to diminishing returns, and the impact of inspections is heterogeneous across facilities with different risk levels.
- Policy implications suggest capping inspection frequency and utilizing distinctive quality monitoring strategies for low-risk and high-risk facilities, with reward-based policies for high-risk manufacturers.
- Limitations include the lack of valid drug quality measures for foreign facilities and potential endogeneity in inspection assignment.


</details>
